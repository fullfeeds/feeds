<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" version="2.0">
<channel><title>NADDOD</title>
<lastBuildDate>Sat, 01 Nov 2025 17:53:17 -0000</lastBuildDate>
<item>
<title> In-depth Analysis of the Differences Between NVIDIA ConnectX-8 SuperNIC and ConnectX-7    </title>
<link>https://naddod.medium.com/in-depth-analysis-of-the-differences-between-nvidia-connectx-8-supernic-and-connectx-7-6d46778a794c</link>
<pubDate>Thu, 30 Oct 2025 07:17:55 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the rapid development of AI and HPC, the continuous improvement of GPU computing power necessitates a corresponding enhancement of data center network performance to support the efficient operation of multi-node systems. NVIDIA’s ConnectX series of smart network interface cards (NICs) has long held a core position in the field of high-performance interconnects.
           &lt;a href=&quot;https://www.naddod.com/collections/nvidia-networking/connectx-7-adapters&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            ConnectX-7
           &lt;/a&gt;
           met the communication needs of the previous generation of AI clusters with a speed of 400Gb/s. However, as the scale and complexity of AI models grow exponentially, simply increasing bandwidth is no longer sufficient to meet the challenges. The emergence of NVIDIA
           &lt;a href=&quot;https://www.naddod.com/collections/connectx-8-adapters&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            ConnectX-8 SuperNIC
           &lt;/a&gt;
           not only increases network throughput to 800Gb/s but also redefines the role of NICs in the data center through a series of innovative hardware features — transforming from a passive data pipeline into an active, intelligent data processing core.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;285&quot; src=&quot;https://miro.medium.com/v2/1*VOSI7UZa0hkOakOWF5vW4w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Dual Revolution: 800G Networking and PCIe I/O Hub
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Network Speed ​​and Transmission Capacity
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The ConnectX-7 boasts a maximum bandwidth of 400Gb/s and supports the NDR InfiniBand standard. Its core technology utilizes 100G
           &lt;a href=&quot;https://www.naddod.com/blog/understanding-pam4-modulation-a-beginner-guide&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            PAM4
           &lt;/a&gt;
           signaling, meaning each data lane can transmit at a rate of 100Gb/s. The CX8, however, directly boosts this to a maximum bandwidth of 800Gb/s, supporting the more advanced XDR InfiniBand standard.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This improvement stems from advancements in underlying signaling technology. The CX8 employs the more advanced 200G PAM4 signaling, doubling the single-lane speed. This advancement is significant in practical applications, especially in large-scale AI training and HPC clusters. For example, when training massive language models with trillions of parameters, GPUs require extensive gradient synchronization and data exchange. The CX8’s 800Gb/s bandwidth halves communication time, accelerating model training cycles and supporting larger-scale, more complex parallel model training.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;262&quot; src=&quot;https://miro.medium.com/v2/1*AtRGrwaFPKvYFiP67taEZA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Host System Connectivity
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Regarding connectivity with the host system, the ConnectX-8 represents a significant leap forward in PCIe interface technology, completely resolving I/O bottlenecks that previous generations may have faced.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The CX7 used a PCIe Gen5 x32 interface with a bidirectional bandwidth of approximately 128GB/s, while the CX8 upgrades to a PCIe Gen6 x48 interface, theoretically offering approximately 384GB/s bidirectional bandwidth — three times that of the CX7. Simultaneously, the CX8 integrates PCIe Switch and Bifurcation functionality for the first time. This means the CX8 is not just a network card, but also an I/O hub. Furthermore, within the NVIDIA GB200 network, the CX8 directly connects to the GPU or NVMe SSD via its built-in PCIe switch and provides unified network access, simplifying system design and reducing latency for data exchange within nodes. This architecture is crucial for building high-density, high-efficiency “AI factories,” enabling computing, storage, and networking to work together more efficiently.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;389&quot; src=&quot;https://miro.medium.com/v2/1*HvfxHFSZyUVn5N591NUY9Q.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Deep Acceleration Through Hardware and Software Collaboration
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Data Placement Technology
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The CX8 represents a significant leap forward from Out-of-Order Execution (OOO) to Direct Data Placement (DDP). The CX7 did not support any placement technology, while the Bluefield3 supported OOO for RDMA read/write operations. Technically, OOO allows the network card to process packets without strictly adhering to the order they arrive; when a packet arrives late, subsequent arriving packets can be processed first, which can improve network throughput to some extent. However, before data is written to its final target memory (such as GPU memory), it may still need to be buffered and reordered in host memory, a process that consumes CPU resources and introduces additional memory copy latency.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*yUVaPcaqBnr_iV947j5fhg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The DDP technology supported by the CX8 optimizes this process. DDP allows the network card to directly and accurately place received network packets at the final memory address specified by the target application (such as an AI training framework) without CPU intervention. This means that data can bypass the buffering steps of the host CPU and system memory, directly landing in the correct location in GPU memory.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In large-scale AI model training applications, such as training a language model with hundreds of billions of parameters, nodes need to frequently exchange massive amounts of tensor data. Using CX8’s DDP technology, model gradient or weight data sent from other nodes can be directly written to the local node’s GPU’s computation cache, shortening the data path, reducing end-to-end latency, and thus improving the effective computation time ratio of the GPU and overall training efficiency.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NCCL SHARP Optimization
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In terms of NCCL SHARP optimization, CX8 upgrades SHARP from v3 to v4, significantly enhancing in-network computing capabilities. NCCL is the core library for NVIDIA’s multi-GPU/multi-node communication, and SHARP (Scalable Hierarchical Aggregation and Reduction Protocol) is its key acceleration technology. Its core theory is to offload the aggregation communication operations that originally needed to be performed on the CPU or GPU to the network switch. When multiple nodes send gradient data, SHARP-enabled switches aggregate this data along the network path and then distribute the final results back to each node, thereby reducing the total amount of data that needs to traverse the entire network.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Upgrading to SHARPv4 significantly improves scalability, algorithm efficiency, and data type support, better meeting the training needs of tens of thousands of GPUs, reducing network congestion, and potentially supporting higher-precision floating-point numbers (such as FP32) for direct reduction operations within the network, ensuring the accuracy and stability of model training.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*HMvyMNPwGHBeVOLGD69LUw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Furthermore, the CX8 features enhanced design for general NCCL optimization. Its hardware architecture works deeply with the NCCL software stack, providing dedicated pathways for specific communication operations (such as Send/Recv, Broadcast, and All-Reduce), and combining GPUDirect technology to further reduce latency. These optimizations effectively improve communication stability and consistency, making the computational progress of multi-node training more synchronized.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Intelligent Management of Programmable Ethernet
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Programmable Congestion Control
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The CX7 primarily relies on fixed algorithms such as DCQCN, making it difficult to perfectly adapt to all application scenarios. The CX8 introduces programmable congestion control through DOCA PCC, enabling users to customize algorithms based on synchronous and bursty traffic characteristics during AI training. This allows for the early prediction and mitigation of incast congestion, significantly reducing long-tail latency and accelerating the training of large models. This means network administrators and developers can write and deploy customized congestion control logic based on the synchronous and bursty traffic patterns unique to AI workloads (such as All-Reduce and All-to-All).
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In specific applications, such as training large language models, gradient exchanges between nodes generate typical “incast” (many-to-one) traffic. Using the CX8’s DOCA PCC, a congestion algorithm can be designed to predict incast traffic and adjust the sending rate in advance, proactively avoiding network congestion and packet loss. This significantly reduces the “long-tail effect” of communication latency, ultimately shortening the overall training time of AI models.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*v3vM2aaxrDJ0qqJshKQLww.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Network Telemetry
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           While the CX8 supports high-frequency telemetry as the CX7, the CX8 explicitly adds “additional HW counters.” The CX7’s high-frequency telemetry provides key performance indicators such as network traffic and latency, but finer-grained event tracking may consume host CPU resources. The CX8’s new hardware counters directly embed monitoring of specific network events (such as RoCE NACKs, packet loss in specific queues, and buffer usage) into the network interface card (ASIC). This hardware-level monitoring has virtually zero overhead, consumes no CPU cycles, and provides ultra-high precision statistics at line-rate.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In practical applications, when a large-scale distributed AI inference cluster experiences performance fluctuations, the CX8’s additional hardware counters allow the operations team to precisely pinpoint whether a micro-burst of traffic from a particular GPU caused momentary congestion and packet loss in the ASIC’s egress queue, enabling fine-grained traffic scheduling or QoS policy adjustments. This hardware-level fine-grained insight is crucial for ensuring the SLA (Service Level Agreement) of AI services in production environments.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Spectrum-X Network Optimization
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The CX8 natively supports and deeply integrates Spectrum-X network optimization, a capability absent in the CX7.
           &lt;a href=&quot;https://www.naddod.com/blog/spectrum-x-nvidia-s-answer-to-gen-ai-ethernet-challenges&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Spectrum-X
           &lt;/a&gt;
           is an end-to-end network platform integrating the CX8 SuperNIC, BlueField DPU, and
           &lt;a href=&quot;https://www.naddod.com/collections/nvidia-networking/spectrum-4-sn5000-ethernet-switches&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Spectrum-4 switches
           &lt;/a&gt;
           . Through a co-design of hardware and software, it achieves network-wide AI load awareness and optimization. Key technologies include Adaptive Routing and Performance Isolation. The platform can dynamically adjust data paths at the switch level using fine-grained telemetry data collected from the CX8 network interface card, bypassing congested nodes.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For example, in a multi-tenant AI public cloud environment, one tenant is running a large-scale data ETL task, generating significant network traffic and potentially creating a network hotspot. Using the CX7, this congestion could impact another tenant on a neighboring node performing a latency-sensitive AI model training task. Platforms deploying CX8 and Spectrum-X can detect emerging network hotspots through end-to-end telemetry. The Spectrum-4 switches then proactively reroute AI training traffic to less busy paths, achieving performance isolation between workloads. This ensures the predictability and stability of critical AI tasks, improving resource utilization and throughput across the entire cluster.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Application Scenarios of 800G Modules
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Achieving 800Gb/s bandwidth with ConnectX-8 depends not only on the performance of the network interface card (NIC) itself, but also on matching high-speed optical modules as a key component of the physical layer transmission. As a crucial foundation for achieving 800Gb/s high-speed interconnection, 800G Ethernet optical modules play a vital role in AI clusters, hyperscale data centers, and high-performance computing platforms. They are not only necessary for ConnectX-8 to unleash its full performance potential, but also constitute the physical layer of the next-generation AI network infrastructure.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In actual deployment, the appropriate optical module model should be selected based on the transmission distance and deployment density:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;a href=&quot;https://www.naddod.com/products/102106.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             &lt;strong&gt;
              OSFP-800G-2xSR4
             &lt;/strong&gt;
            &lt;/a&gt;
            : Suitable for short-distance interconnection within data centers or racks, with low power consumption and high density.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;a href=&quot;https://www.naddod.com/products/32799.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             &lt;strong&gt;
              OSFP-800G-2xDR4
             &lt;/strong&gt;
            &lt;/a&gt;
            : Supports campus-level deployment, covering medium-distance interconnections between adjacent data centers.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;a href=&quot;https://www.naddod.com/products/102869.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            OSFP-800G-2SR4F
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/products/102870.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            OSFP-800G-2DR4F
           &lt;/a&gt;
           ​​are flat-top versions provided for ConnectX-8 system links, specifically designed to connect CX8. Their counterparts can be OSFP-800G-2SR/DR4. For example, one end uses an OSFP-800G-2SR4 connected to a switch, and the other end uses an OSFP-800G-2SR4F connected to CX8.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;266&quot; src=&quot;https://miro.medium.com/v2/1*5-iXWSrUVzAQaqjDoJ1Bfw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Deployment Solutions for Key Scenarios
          &lt;/h2&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             AI/GPU Cluster Interconnection
            &lt;/strong&gt;
            : AI training (such as Deepseek, GPT-like models) requires thousands of GPUs to work collaboratively. Traditional 100G/400G interconnection has become a bottleneck, leading to queuing of training tasks and idle resources. Using 800G optical modules (such as OSFP/QSFP-DD packages) to connect GPU servers (such as NVIDIA DGX systems) and switches increases the cluster interconnection bandwidth to 800G, significantly shortening the training cycle.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Data Center Spine-Leaf Architecture Upgrade
            &lt;/strong&gt;
            : Traditional data center spine (core) layers mostly use 400G interconnection, but leaf (access) layer servers have generally been upgraded to 25G/100G, making the spine layer prone to congestion. Deploying 800G optical modules on spine switches and core routers enables non-blocking forwarding and avoids network congestion.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Cross-Data Center Interconnection (DCI):
            &lt;/strong&gt;
            Cross-regional disaster recovery and data synchronization require high-speed interconnection, but existing 100G/400G links can no longer meet the needs of data growth (such as backing up petabyte-scale data). Connecting two data centers via 800G ZR/ZR+ optical modules (supporting 80–120km transmission) reduces fiber optic resource consumption and lowers unit bit cost.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             High-Performance Computing (HPC) and Financial Transactions
            &lt;/strong&gt;
            : Every microsecond reduction in microsecond-level transaction latency in finance can bring a competitive advantage; HPC simulations (such as climate prediction and gene sequencing) require high-speed data exchange. Deploying 800G optical modules between switches and servers, combined with low-latency protocols (such as
            &lt;a href=&quot;https://www.naddod.com/blog/roce-v2-deployment-and-application&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             RoCEv2
            &lt;/a&gt;
            ), enables end-to-end high-speed transmission.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion: The Beginning of a New Era
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Compared to ConnectX-7, ConnectX-8 SuperNIC represents a comprehensive upgrade in transmission rate, I/O architecture, data placement, and network collaboration capabilities. It forms a complete technical system encompassing 800G optical interconnect, PCIe Gen6 architecture, DDP data path, SHARPv4 aggregate computing, and DOCA programmable network control.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ConnectX-8 not only boasts enhanced data transmission capabilities but also intelligent data processing and scheduling features, effectively supporting the network demands of large-scale AI training and high-performance computing platforms. It provides a highly efficient and scalable interconnect foundation for next-generation data centers. To learn how ConnectX-8 revolutionizes AI server design for NVIDIA RTX Pro through PCIe Gen6, click to read:
           &lt;a href=&quot;https://www.naddod.com/blog/how-nvidia-connectx8-supernic-transforms-server-architecture-fuels-800g-1.6t-optics&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            How NVIDIA ConnectX-8 SuperNIC Became a Revolutionary NIC?
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To meet the ever-growing demands for computing power and increasingly complex network architectures, NADDOD continues to provide high-performance interconnect products and solutions.
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Contact us
           &lt;/a&gt;
           to explore more possibilities for AI network interconnection.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> NVIDIA NVQLink Introduction：Connecting Quantum and GPU Computing    </title>
<link>https://naddod.medium.com/nvidia-nvqlink-introduction-connecting-quantum-and-gpu-computing-29bbe295e857</link>
<pubDate>Wed, 29 Oct 2025 09:37:07 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the latest NVIDIA GTC conference in Washington, D.C., NVIDIA officially launched NVQLink. This isn’t just a traditional interconnect technology upgrade, but rather a complete hybrid computing system designed to address the bottlenecks in quantum computing implementation. NVQLink aims to extend the GPU’s computing power scheduling capabilities to quantum processors through high-bandwidth, low-latency links, enabling quantum computing to receive real-time control and algorithm support from AI supercomputers. This article will detail NVQLink’s core technical capabilities, performance metrics, and how it integrates with the CUDA-Q platform.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;467&quot; src=&quot;https://miro.medium.com/v2/1*sFCou5_VZE6odg2bjkgDSg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           What is NVQLink?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVQLink is a connectivity technology designed to connect quantum processing units (QPUs) with GPUs. This new solution aims to foster closer collaboration between quantum and classical computing, particularly in the fields of artificial intelligence and scientific computing. Simply put, NVQLink is a high-speed bridge that allows fragile and sensitive quantum hardware to leverage the powerful error correction, control, and acceleration capabilities of GPUs to jointly solve complex scientific problems.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;362&quot; src=&quot;https://miro.medium.com/v2/1*3Eoqm2n1HoKe3McNED5Zxw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA’s announcement indicates that NVQLink can deliver up to 400 Gb/s of data throughput and reduce round-trip instruction latency between the QPU and GPU to less than 4 microseconds. Furthermore, based on the Grace Blackwell architecture, it can deliver up to 40 PFLOPS FP4 of inference-level computing power.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;300&quot; src=&quot;https://miro.medium.com/v2/1*lZz_ZaBwIB_Gu9Zo64IwkA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Thus, NVQLink not only solves the connectivity problem for quantum hardware but also serves as the “computing engine for quantum computing’s real-time operating system.” At the launch event, Jensen Huang stated that NVQLink is a crucial foundation for the practical application of quantum computing technology. It will serve as a “Rosetta Stone” connecting traditional GPU-accelerated supercomputing with future quantum processors, ushering in the era of quantum-GPU hybrid computing.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVQLink: Solving the Error Correction Challenge of Quantum Computing
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The enormous potential of quantum computing is limited by its core problem: the physical fragility of quantum bits (qubits), which are susceptible to environmental interference such as noise, temperature fluctuations, and magnetic fields, resulting in high error rates. As the fundamental unit for information processing in quantum computing, qubits (qubits) require overcoming the core technological hurdle of quantum error correction (QEC) to achieve stable operation. The diagram below illustrates the principle of quantum error correction.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;895&quot; src=&quot;https://miro.medium.com/v2/1*lyj0JSFQn2mDwtWNtYoyEw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           These control processes are not completed independently within the quantum processor but rely on frequent and demanding data interactions with classical computing systems. NVQLink is designed to address this need, providing a low-latency, high-throughput connection channel to link the QPU to AI supercomputers (GPU clusters).
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Provides real-time control capabilities
            &lt;/strong&gt;
            : Precise calibration of quantum error correction and qubits requires running extremely complex classical control algorithms, which must be completed within millisecond-level time constraints. The powerful parallel computing capabilities of GPUs are perfectly suited for these intensive classical computational tasks. NVQLink ensures that these control commands are processed at high speed on the GPU and then transmitted to the quantum processor at extremely high speed, receiving real-time feedback.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Hybrid quantum-classical computing
            &lt;/strong&gt;
            : Most cutting-edge applications (such as drug discovery and materials science) require quantum and classical computing systems to work together. The deep integration of NVQLink with the NVIDIA CUDA-Q software platform enables researchers to seamlessly utilize CPU, GPU, and QPU resources within a unified framework to develop hybrid applications.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Openness and Unity of the Ecosystem
            &lt;/strong&gt;
            : NVQLink adopts an open architecture design and currently supports collaborations from 17 QPU manufacturers, 5 controller manufacturers, and U.S. Department of Energy national laboratories. Through this open strategy, NVIDIA positions itself as a general-purpose infrastructure provider for the quantum computing ecosystem.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVQLink and CUDA-Q
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The tight integration of NVQLink and CUDA-Q makes hybrid quantum-classical computing more feasible.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             The Role of NVQLink
            &lt;/strong&gt;
            : NVQLink is an important extension of the CUDA-Q platform strategy. It connects quantum hardware to the platform, enabling researchers and developers to develop and test hybrid applications that utilize CPUs, GPUs, and quantum processors simultaneously, preparing for future quantum-classical supercomputers.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Platform Architecture Upgrade
            &lt;/strong&gt;
            : The CUDA-Q platform initially primarily provided quantum circuit simulation capabilities, helping researchers validate quantum algorithms on classical hardware. The addition of NVQLink enables the platform to control quantum hardware in real time, expanding its functionality from software simulation to hardware manipulation.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Unified Programming Interface
            &lt;/strong&gt;
            : Through a unified programming model, developers can use standard programming languages ​​to build hybrid quantum-classical applications without needing in-depth knowledge of underlying hardware details. This lowers the barrier to entry for quantum computing applications, allowing researchers to focus on algorithm development.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;355&quot; src=&quot;https://miro.medium.com/v2/1*4GhkGFGJp0FFbM2yHXw_-w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVQLink is a connectivity technology and open standard that leverages the mature power of GPU supercomputing to provide the necessary real-time control and error correction capabilities for currently fragile quantum processors. It represents a crucial step in moving quantum computing from the laboratory to practical application.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD continues to focus on the development of artificial intelligence networks, providing high-performance, high-speed optical interconnect products. Please follow our
           &lt;a href=&quot;https://www.naddod.com/blog&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            blog
           &lt;/a&gt;
           or visit
           &lt;a href=&quot;https://www.naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD.COM
           &lt;/a&gt;
           for more industry insights and the latest product updates.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> NVIDIA Vera Rubin NVL144 : Next-Generation High-Performance Computing Platform    </title>
<link>https://naddod.medium.com/nvidia-vera-rubin-nvl144-next-generation-high-performance-computing-platform-2ed0435686b1</link>
<pubDate>Wed, 29 Oct 2025 07:41:24 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;402&quot; src=&quot;https://miro.medium.com/v2/1*z3gPmDrtLB0DPC5CTT8pbQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Over the past three years, NVIDIA has laid the core foundation for AI training and inference systems with the GB200 NVL72 architecture. From the shipment of the GB200 Grace Blackwell in 2024 to the mass production ramp-up in 2025, AI infrastructure has leaped from “thousand-card clusters” to “ten-thousand-card systems.”
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the GTC conference in the fall of 2025, NVIDIA officially unveiled its next-generation supercomputing platform — the NVIDIA Vera Rubin NVL144. It is not only the successor to the GB300 NVL72, but also represents a crucial step for NVIDIA towards the era of “AI and scientific computing convergence.” This platform is NVIDIA’s most complex computing platform to date, integrating the new Vera CPU and Rubin GPU, truly becoming a computing platform that combines high versatility with extreme computing density.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Core Components of the NVIDIA Vera Rubin NVL144
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The Vera Rubin NVL144 platform utilizes a two-core chip design:
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Rubin GPU
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Core Design:
           &lt;/strong&gt;
           Employs two Reticle-sized compute chips (3nm process), each optimized for large-scale matrix computation, specifically designed for AI inference and training workloads.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Computational Performance:
           &lt;/strong&gt;
           Achieves up to 50 PFLOPS of total computing power at FP4 precision, supporting parallel computation of ultra-large models and significantly improving deep learning training and inference efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Memory Configuration:
           &lt;/strong&gt;
           Equipped with 288GB of HBM4 memory, enabling high-bandwidth data access through eight HBM4 interfaces, allowing for rapid loading and processing of large-scale model weights.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            High-Speed ​​Interconnect:
           &lt;/strong&gt;
           Extremely low latency between GPU chips and with the system bus optimizes multi-GPU collaborative computing performance.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;449&quot; src=&quot;https://miro.medium.com/v2/1*yHYDlQDPeOkCsnSqJdCmrw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Vera CPU
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Core Architecture:
           &lt;/strong&gt;
           Equipped with 88 custom Arm architecture cores, each with an instruction set optimized for AI workloads, it supports 176 threads of parallel execution, balancing high throughput and energy efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            GPU Interconnect:
           &lt;/strong&gt;
           A 1.8 TB/s NVLINK-C2C channel provides low-latency interconnection with Rubin GPUs, ensuring fast data transfer between the CPU and GPU, reducing latency during training and inference.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*zVxss1OSnvWxy4t41jD2vw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           System Integration of NVIDIA Vera Rubin NVL144
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           System Scale:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Utilizing the same Oberon rack architecture as the NVIDIA GB200/GB300, it achieves high-density integration of GPUs and CPUs through modular stacking. Power supply and cooling systems are optimized to support higher computing power and a more compact package layout.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● One NVL144 unit (rack) contains 72 Rubin GPU packages. Since each GPU package contains 2 compute chips, it contains a total of 144 compute chips (hence the name “NVL144”).
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           System Performance
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● FP4 inference performance up to 3.6 Exaflops
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● FP8 training performance up to 1.2 Exaflops
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Compared to the previous generation GB300 NVL72, overall AI performance is improved by approximately 3.3 times.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           System Memory
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The total system memory capacity reaches 75 TB, using the latest HBM4 memory, providing a total bandwidth of 13 TB/s. Compared to the GB300 platform, both memory bandwidth and capacity are improved by approximately 60%, significantly alleviating memory bottlenecks during large model training.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           System Network and Interconnect
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The Rubin GPU adopts an I/O separation design, freeing up more space on the main computing chip for core unit placement, and improves yield and reliability under large-scale packaging through built-in redundancy repair logic.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            GPU Interconnect:
           &lt;/strong&gt;
           Utilizing NVLink Gen6, with a total bandwidth of up to 260 TB/s, it supports efficient collaboration between multiple GPUs, doubling the bandwidth compared to the previous generation.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            External network connectivity:
           &lt;/strong&gt;
           Integrated ConnectX-9 network interface card, the system’s total network speed can reach 28.8 TB/s, also achieving a 2x performance improvement.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;525&quot; src=&quot;https://miro.medium.com/v2/1*r3PjVmKuhdWu3aMjsa5uHg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The Rubin platform will first be deployed on the Vision system at Los Alamos National Laboratory (LANL) for open research missions. Its sister system, Mission, will serve national-level nuclear safety and AI model training missions and is scheduled to go live in 2027.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA’s Platform Ecosystem Layout
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA’s AI platform evolution has consistently followed a clear technological path — continuous integration and expansion from architecture to system. In 2024, the
           &lt;a href=&quot;https://www.naddod.com/blog/nvidia-gb200-interconnect-architecture-analysis-nvlink-infiniband-and-future-trends&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            GB200 NVL72
           &lt;/a&gt;
           marked the first large-scale commercial deployment of the Grace Blackwell architecture, laying the foundation for CPU-GPU collaborative computing. Subsequently, in 2025, the
           &lt;a href=&quot;https://www.naddod.com/blog/nvidia-gb300-deep-dive-performance-breakthroughs-vs-gb200-liquid-cooling-innovations-and-copper-interconnect-advancements&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            GB300 NVL72
           &lt;/a&gt;
           entered the mass production ramp-up phase, further maturing in terms of interconnect bandwidth and power consumption control, accumulating stable hardware and software stack experience for the next-generation platform.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The true turning point in this evolution occurred in 2026. At that time, the Vera Rubin NVL144 became the core platform for NVIDIA’s formal entry into the “converged computing” phase. It not only achieved system scaling from 72 to 144, but more importantly, introduced a new Rubin GPU architecture and Vera CPU co-design, completely reconstructing the underlying connectivity of heterogeneous computing. Through a higher-density NVLink Gen6 and CX9 network interconnect, the Rubin platform unifies AI inference and scientific computing capabilities within a single computing architecture for the first time.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA plans to launch the higher-end Rubin Ultra NVL576 in 2027, further expanding to a configuration level of 576 GPUs. The Ultra version of the Rubin architecture will employ a four-relict-level core design, boosting performance to approximately 15 ExaFLOPS for FP4 and approximately 5 ExaFLOPS for FP8, targeting ultra-large-scale multimodal model training and scientific research-grade supercomputing tasks.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*5EHBfOGDhHBiaN0Artuceg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           From the GB series to the Rubin architecture, NVIDIA’s roadmap demonstrates an evolution from simply stacking computing power to comprehensive system integration. The Vera Rubin NVL144 sits at a critical juncture in this development phase — building on the Grace–Blackwell co-design, it provides the infrastructure for the convergence of AI and HPC.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The Vera Rubin NVL144 demonstrates NVIDIA’s latest breakthroughs in system design and computing architecture. Through the co-design of the Rubin GPU and Vera CPU, along with high-density NVLink Gen6 interconnect and CX9 network interfaces, the NVL144 achieves significant improvements in computing power, memory bandwidth, and system integration, providing a reliable, high-performance platform for large-scale model training and scientific research computing. With the introduction of the Rubin Ultra NVL576, NVIDIA’s Rubin series will further expand computing scale and performance, laying the foundation for future hyperscale computing.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> NVIDIA RTX PRO Server Powered by ConnectX-8 SuperNIC Builds Next-Generation AI Architecture    </title>
<link>https://naddod.medium.com/nvidia-rtx-pro-server-powered-by-connectx-8-supernic-builds-next-generation-ai-architecture-8a7fd09ba015</link>
<pubDate>Tue, 28 Oct 2025 09:40:41 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As the scale of large-scale model training, AIGC, rendering, and simulation computing continues to expand, enterprise data centers are placing higher demands on computing power, network bandwidth, and system scalability. The RTX PRO Server, based on the Blackwell architecture, provides a high-performance computing foundation for enterprise-level AI workloads by improving GPU computing resources and data connectivity. This article systematically analyzes its architectural design and technical features.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*AwM55XOQyuP9_KynzB6PEA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA RTX PRO Server Overview
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA RTX PRO Server is a high-performance server platform designed for enterprise and industrial AI applications, designed to accelerate critical tasks such as AI inference, high-precision graphics rendering, and scientific computing. The RTX PRO Server utilizes eight RTX 6000 Blackwell professional GPUs as its core components. Each GPU features 24,064 CUDA cores and 96GB of GDDR7 memory. Combined with a 512-bit memory bus, it delivers up to 13TB/s of memory bandwidth, increasing the total memory capacity to approximately 800GB. This combination easily supports large-scale AI model deployments and highly complex graphics tasks, meeting the ever-increasing computing power demands of today’s data centers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In terms of networking, the RTX PRO server utilizes a
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102802&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            ConnectX-8 SuperNIC
           &lt;/a&gt;
           for bidirectional high-speed interconnection, totaling 800GB/s. A single GPU consumes approximately 600W of power, and the entire system consumes over 5kW. At FP32 precision, it achieves 30 PFlops of AI computing power and 3 PFlops of graphics processing speed, meeting the demanding performance requirements of smart manufacturing, virtual simulation, and other fields.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;253&quot; src=&quot;https://miro.medium.com/v2/1*og2T5QRyo6JhKHkk5VehPw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The system also integrates a BlueField-3 DPU and a ConnectX-8 SuperNIC with PCIe Gen6 switching, supporting up to eight RTX 6000 Blackwell GPUs working together to form a high-density computing platform for AI factories.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Whether performing distributed acceleration in the data center or executing high-performance AI tasks via remote access, the RTX PRO server delivers an excellent operational experience. It is accelerating the evolution of traditional IT infrastructure toward AI-oriented computing architectures, becoming an indispensable productivity engine for designers, research teams, and engineering fields.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Notably,
           &lt;a href=&quot;https://www.naddod.com/blog/understanding-nvidia-dgx-spark-ai-supercomputer&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NVIDIA DGX Spark
           &lt;/a&gt;
           , a personal AI supercomputer environment, is often used for early development verification and minimal inference environment testing. After individual developers complete model prototyping, debugging, and early inference verification, when enterprises or teams are ready to deploy these development results in production environments for large-scale, continuous, and reliable AI inference and training, the RTX PRO server becomes the essential platform for production deployment. DGX SPARK provides flexible development and testing capabilities, while the RTX PRO server offers enterprise-grade computing power, networking, and scalability, enabling a smooth migration from personal supercomputers to enterprise-grade production environments.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD also offers
           &lt;a href=&quot;https://www.naddod.com/products/13888.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            200G QSFP56 DAC
           &lt;/a&gt;
           that support DGX Spark testing, with an ultra-low BER of 15E-255, providing developers with high-bandwidth, low-latency interconnect. Learn more:
           &lt;a href=&quot;https://www.naddod.com/blog/naddod-dac-cables-for-nvidia-dgx-spark-connectivity&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD DAC Cables for NVIDIA DGX Spark Connectivity
           &lt;/a&gt;
           .
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;418&quot; src=&quot;https://miro.medium.com/v2/1*_4nhOcPDJqBbksjBYSYMmQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA RTX PRO Server Advantages
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Accelerate Workloads
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RTX PRO Server accelerates tasks such as proxy AI, LLM inference, and synthetic data generation, providing enterprises with highly efficient inference capabilities. For example, the NVIDIA Llama Nemotron Super inference model, running on a single RTX PRO 6000 GPU using NVFP4, can achieve up to 3x better price-performance compared to running on an NVIDIA H100 GPU using FP8, enabling higher-precision inference at a lower cost. This means that, for the same cost, enterprises can achieve faster response times and higher-accuracy inference services.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the industrial AI field, equipment requires virtual-reality simulation testing and optimization before deployment. The RTX PRO Server offers up to 4x performance improvement for digital twins, industrial simulation, and synthetic data generation compared to systems using NVIDIA L40S GPUs, helping to improve training efficiency and accelerate time-to-market for robotics and manufacturing automation systems.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Designed for flexibility, the RTX PRO server supports Windows, Linux, and major hypervisors, enabling IT administrators to deploy AI at scale in familiar environments without compromising performance. The server utilizes an air-cooled, PCIe-based x86 architecture, making it compatible with virtually all enterprise workloads while providing enterprise-grade security, manageability, and ease of maintenance.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Comprehensive Software Support
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RTX PRO servers, along with other NVIDIA Blackwell architecture products, are supported by the NVIDIA AI Enterprise software platform. This platform integrates NVIDIA NIM™ microservices, AI frameworks, libraries, and tools for deployment in accelerated cloud, data center, and workstation environments.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As part of the NVIDIA Enterprise AI Factory validated design, RTX PRO servers are ideal for companies looking to build AI factories on-premises. They are also a core component of the NVIDIA AI Data Platform, which provides a customizable, modern storage reference design for enterprise AI agents.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For industrial and physical AI, RTX PRO servers can run the NVIDIA Omniverse™ library and the NVIDIA Cosmos™ world base model, helping developers build applications such as factory and robotics simulations, digital twins, and large-scale synthetic data generation.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Breakthrough AI Performance
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA RTX PRO servers provide powerful support for a variety of high-performance computing scenarios, including AI inference and training, machine learning, data analytics, 3D graphics rendering, and scientific simulations. This platform integrates the latest Blackwell architecture innovations to achieve comprehensive improvements in computing, visualization, and energy efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Its fifth-generation Tensor Cores and second-generation Transformer Engine support FP4 precision, delivering up to 6x faster inference performance than the previous-generation NVIDIA L40S GPU, significantly accelerating large model workloads. Furthermore, fourth-generation NVIDIA RTX™ technology provides photorealistic rendering and visualization capabilities, with a 4x improvement in graphics performance compared to the L40S, bringing higher image fidelity and real-time interactive experiences to creative design and engineering simulations.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For multi-user AI deployments, the NVIDIA RTX PRO Server leverages virtualization and NVIDIA Multi-Instance GPU (MIG) technology to partition a single GPU into up to four independent instances, ensuring isolation and resource utilization. Furthermore, this platform significantly improves performance per watt, helping data centers achieve greater energy efficiency and sustainable operations, laying the foundation for the next generation of high-density AI infrastructure.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;569&quot; src=&quot;https://miro.medium.com/v2/1*zuLmc68ESfkicQFPyClHVg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA RTX PRO Servers Achieve an Architecture Upgrade with ConnectX-8 SuperNICs
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The NVIDIA ConnectX-8 SuperNIC, officially unveiled at COMPUTEX 2025 is widely used in NVIDIA RTX PRO servers from system partners worldwide. It’s more than just a high-performance network card; it’s a key component reshaping server architecture.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As shown in the figure below, traditional server designs rely on multiple independent PCIe switches to connect the CPU, GPU, and NIC. This creates complexity and introduces additional latency and power consumption. In an optimized architecture, RTX PRO servers utilize the ConnectX-8 SuperNIC, combining PCIe 6.0 switching capabilities with 800 Gb/s networking capacity into a single device, significantly simplifying system design.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;262&quot; src=&quot;https://miro.medium.com/v2/1*xFsTQBadibMU2WAcv8xmxQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This integrated solution increases network bandwidth to 400 Gb/s per GPU (a 2:1 GPU:NIC ratio), making data transfer between the GPU, NIC, and storage more efficient. This boosts the RTX PRO platform’s NCCL all-to-all communication performance by approximately 2x, providing improved scalability and throughput for multi-GPU, multi-node AI training.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           From a system perspective, ConnectX-8 is more than just a network interface card; it serves as a central node for converged computing and interconnection. It handles high-speed communication between GPUs, GPUs, and NICs, as well as across CPU sockets, allowing all inter-GPU traffic to be routed directly over the network without requiring additional host intervention. A further comparison chart shows that the optimized architecture improves three major communication paths:
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;284&quot; src=&quot;https://miro.medium.com/v2/1*ovvEoEhmurpPO-MIFRm7mw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           · Inter-GPU communication is no longer constrained by CPU socket bandwidth, increasing bandwidth to 50 GB/s;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           · GPU-to-NIC channels maintain a stable 50 GB/s, regardless of whether the system uses PCIe 5.0 or PCIe 6.0;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           · GPU interconnect speeds within the same switch are doubled, significantly reducing latency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           By deeply integrating the ConnectX-8 SuperNIC into NVIDIA RTX PRO servers, the system not only achieves higher bandwidth and efficiency, but also provides a more compact, energy-efficient, and scalable network foundation for next-generation AI factories.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The combination of NVIDIA RTX PRO servers and NVIDIA ConnectX-8 SuperNICs revolutionizes both computing power and connectivity for enterprise-grade AI computing platforms. By integrating a high-performance GPU cluster with 800Gb/s network interconnection, the system achieves comprehensive improvements in bandwidth utilization, communication latency, and energy efficiency, providing strong support for multi-GPU collaboration and large-model training. This architecture not only accelerates the implementation of AI factories but also lays a solid foundation for future high-density, scalable intelligent computing infrastructure.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As a provider of high-speed optical interconnect solutions, NADDOD offers a comprehensive product portfolio, including
           &lt;a href=&quot;https://www.naddod.com/collection/optical-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            optical modules
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/collection/dac-and-aoc-cables&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            cables
           &lt;/a&gt;
           , and
           &lt;a href=&quot;https://www.naddod.com/collection/networking&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            switches
           &lt;/a&gt;
           . Please
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            contact us
           &lt;/a&gt;
           for more solution support.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Understanding AI Frontend Network    </title>
<link>https://naddod.medium.com/understanding-ai-frontend-network-46b3a2c4b875</link>
<pubDate>Mon, 27 Oct 2025 08:49:13 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*rjuvVSaxQP_aS9zJLf3VHA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In modern AI data centers, the network architecture is typically divided into two major components: the frontend network and the backend network. Both support the entire model training and inference process, but they perform distinct responsibilities. The frontend network is the key layer connecting users with AI computing resources, providing communication channels for model invocation, task scheduling, data access, and monitoring and maintenance.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           It serves as the “external loop” of the AI ​​system, facilitating interaction between the external world and the internal cluster, ensuring smooth and stable operation of inference services and management functions. In any large-scale AI cluster, a well-designed and clearly architected frontend network is the foundation for an efficient AI system.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;441&quot; src=&quot;https://miro.medium.com/v2/1*_xpNggSn1uiNpOLBW2bqfA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           What is the Frontend Network?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The frontend network can be considered the “entry layer” of an AI cluster, serving as the first checkpoint for service access, resource scheduling, and data input. It is typically built on a standard Ethernet switching fabric, boasting a mature ecosystem, flexible configuration, and excellent scalability.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           During cluster operation, the frontend network’s primary responsibilities include:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Connecting users and datasets to GPUs and AI acceleration nodes;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Supporting model inference, data preprocessing, and traffic management;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Providing inter-service communication and monitoring access.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Typical AI data centers use a spine-leaf or Clos topology to build their frontend networks, enabling low blocking ratios and linear scalability even at scales of thousands of nodes. This architecture efficiently carries north-south traffic, ensuring that external requests can quickly reach internal compute nodes. The
           &lt;a href=&quot;https://www.naddod.com/blog/ai-backend-network-and-development-trends&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            backend network
           &lt;/a&gt;
           , on the other hand, handles core east-west traffic, carrying the core communication tasks of AI model training and large-scale computing.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Unlike the high-speed interconnect network for backend GPUs, the frontend network’s primary goals are not extreme bandwidth or microsecond latency, but rather service stability, access flexibility, and traffic isolation. It serves as the “facade” of the AI ​​system, ensuring orderly and secure external access.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;382&quot; src=&quot;https://miro.medium.com/v2/1*tDieYuHOo2y4VIFzcbXNIQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Main Function of the Frontend Network
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Inference and Access Interface
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           During the inference phase, the frontend network serves as the communication bridge between the model and the user. When a user initiates an inference request, the traffic first enters the frontend network, where it is distributed to specific compute nodes by the load balancing or gateway module for processing.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The network’s responsibilities during this phase include request routing, model loading instruction delivery, and result transmission. Its performance directly impacts the response speed of the inference service and the overall user experience. In a multi-model or multi-tenant environment, the frontend network also provides independent access channels for different services, achieving resource isolation and load balancing through virtual network technology.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Task Scheduling and Control Communication
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The frontend network also serves as the backbone for task scheduling and control communication. The scheduler interacts with the cluster’s control nodes, storage systems, and GPU nodes through this network to perform operations such as resource allocation, job launches, and state synchronization. This type of communication typically involves small amounts of data but requires high reliability and consistency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At this level, the frontend network serves as a “control bus,” ensuring coordinated operation across the entire cluster.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Data Access and Migration
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The frontend network is also responsible for data inflow and outflow. During the inference phase, it handles the transmission of model weights, input data, logs, and result files. While this traffic is not as intensive as inter-GPU communication, it can still become a cluster performance bottleneck if not properly designed. Therefore, the frontend network must have stable data channels and sufficient redundant bandwidth to prevent storage access latency from impacting overall task progress.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Security Isolation and Access Management
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Security is at the core of frontend network design. As the first line of defense against external access, it must prevent unauthorized access and potential attacks. Through multi-layered protection and appropriate isolation, the frontend network ensures system stability and security while guaranteeing performance.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Frontend Network Architecture Design
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Architecturally, the frontend network is typically built on standard Ethernet, using a leaf-spine architecture. Top-of-Rack (ToR) switches interconnect with backbone switches to enable internal and external communications within the data center. It typically uses RoCE or TCP/IP protocols, balancing out-of-band management and data transmission requirements.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD offers
           &lt;a href=&quot;https://www.naddod.com/collections/ethernet-transceiver-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            100G to 800G Ethernet optical modules
           &lt;/a&gt;
           , as well as
           &lt;a href=&quot;https://www.naddod.com/collections/ethernet-direct-attach-copper-dac-cables&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            DAC cables
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/collections/ethernet-active-optical-cables-aoc&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            AOC cables
           &lt;/a&gt;
           . These solutions provide comprehensive coverage for AI frontend networks, from short-haul rack interconnects to long-haul data center interoperability, ensuring high bandwidth and reliability for deployments of varying scales.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;408&quot; src=&quot;https://miro.medium.com/v2/1*b8CH5r7qUjR_FVjQzit5xg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Control Plane Design
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Currently, virtual network isolation is often achieved through
           &lt;a href=&quot;https://www.naddod.com/blog/evpn-vxlan-in-high-performance-network-deployment-and-o-m&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            BGP EVPN
           &lt;/a&gt;
           and VXLAN encapsulation. This design enables the creation of flexible virtual topologies on top of the physical network to adapt to diverse service environments. Operations personnel can dynamically adjust network boundaries without modifying the underlying infrastructure, enabling rapid deployment and service expansion.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;337&quot; src=&quot;https://miro.medium.com/v2/1*j_h51cHULLa6r8oAjIXjxA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Connection Layer Design
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The frontend network primarily connects CPU nodes and storage systems, and interoperates with systems such as load balancers and scheduling platforms. Because it primarily carries control and access plane traffic, overall bandwidth requirements are relatively manageable, but high reliability and low latency fluctuation are required. A dual-plane architecture is typically adopted to ensure that any link or device failure does not interrupt service.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Security and Isolation Mechanisms
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To ensure system security and service quality, strict logical isolation is typically maintained between the frontend network and the back-end training network. This isolation can be achieved through physical partitioning, dedicated
           &lt;a href=&quot;https://www.naddod.com/blog/how-much-do-you-know-about-the-vlan-technology-of-switches&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            VLANs
           &lt;/a&gt;
           , VXLAN overlays, or SDN control policies.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In addition, API gateways and WAFs (Web Application Firewalls) are often deployed in the frontend network to provide a unified external entry point and perform access control and threat detection.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Operational Layer
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the operational level, the frontend network requires high observability. Through traffic sampling, telemetry, and real-time log analysis, administrators can monitor key metrics such as service latency, packet loss, and path health to promptly identify and address potential issues.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This “visualized operations” capability has become a crucial component of frontend network design in modern AI clusters.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Traffic Characteristics of Frontend Networks
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Frontend network traffic patterns exhibit distinct characteristics, contrasting with the high-bandwidth, low-latency back-end GPU interconnects.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. Small Flow
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Frontend network communication patterns are characterized by short duration, small data volumes, and high concurrency (so-called small flows). These flows typically last only milliseconds to seconds, transmit relatively light content, but are large in volume and randomly distributed.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In inference scenarios, thousands of user requests simultaneously trigger model access and response returns, resulting in high-frequency, low-volume, short connections. This traffic pattern requires the network to possess fast forwarding, minimal queue congestion, and high connection concurrency.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. Burstability and Irregularity
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Frontend network traffic is driven by external access behavior and exhibits significant burst characteristics. During peak periods (such as when models are released or inference services go online), traffic can surge rapidly, followed by a brief decline.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To address this instability, network equipment must have sufficient caching and dynamic scheduling mechanisms to prevent transient congestion from causing response delays.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3. Moderate Latency Sensitivity
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Frontend networks are sensitive to latency between control systems and training networks.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For batch tasks or asynchronous inference, slight latency fluctuations are acceptable; however, in real-time interactive scenarios (such as voice assistants and smart search), any increase in latency will impact the responsiveness experience. Therefore, frontend network design must strike a balance between bandwidth utilization and latency control.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           4. Diverse Traffic Structure
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Overall, frontend networks typically have low average link utilization, but traffic types are complex, including request data, control signals, logs, and monitoring information. This diverse traffic structure necessitates flexible Quality of Service (QoS) management and queue scheduling capabilities to prioritize communication requests.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;480&quot; src=&quot;https://miro.medium.com/v2/1*G-1SUrc-PQt4Rdy2oT9r_g.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The AI ​​frontend network is a critical bridge between AI systems and the outside world. Based on an open Ethernet architecture, it carries core communication tasks such as model inference, task scheduling, and data transmission, ensuring efficient and stable service responses. As Ethernet technology continues to evolve to the 800G and even 1.6T era, AI frontend networks will see comprehensive improvements in bandwidth, energy efficiency, and flexibility. frontend networks are evolving towards higher reliability, better latency control, and greater resilience. A well-designed frontend network not only improves the operational efficiency of AI systems but also lays a solid foundation for more complex intelligent computing architectures in the future.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD continuously focuses on Ethernet innovation and practical applications in AI networks, providing
           &lt;a href=&quot;https://www.naddod.com/collections/800g-osfp-ethernet-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            high-speed 800G Ethernet optical modules
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-transceivers-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T optical modules
           &lt;/a&gt;
           , as well as
           &lt;a href=&quot;https://www.naddod.com/collections/400g-ai-data-center-switches&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            400G AIDC switches
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/collections/800g-ai-data-center-switches&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            800G AIDC switches
           &lt;/a&gt;
           . These help build efficient and scalable AI infrastructure and deliver reliable and high-performance network solutions for modern data centers. To learn more about AI networks, please visit
           &lt;a href=&quot;https://NADDOD.com&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD.com
           &lt;/a&gt;
           or
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            contact us directly
           &lt;/a&gt;
           .
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Key Interconnects in the AI Era: PCIe and CXL    </title>
<link>https://naddod.medium.com/key-interconnects-in-the-ai-era-pcie-and-cxl-3e1dcbbb080a</link>
<pubDate>Mon, 27 Oct 2025 08:21:52 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the continued expansion of artificial intelligence (AI) and machine learning (ML) applications, particularly the deployment of generative AI technologies like large language models (LLMs), data centers are placing higher demands on computing, memory, and data transfer capabilities. To address the challenges of high bandwidth, low latency, and efficient resource utilization, the PCI (Peripheral Component Interconnect Express) and CXL (Compute Express Link) specifications are undergoing key technical integration to provide the essential interconnect foundation for next-generation AI infrastructure.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This article briefly describes the technological evolution and synergy of PCIe and CXL in the AI ​​era, explaining how they jointly support the key interconnect architecture for high-performance computing and next-generation data centers.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           PCIe: A High-Speed ​​Interconnect Standard for AI Computing Systems
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           PCIe is a high-speed serial computer expansion bus standard that has long been widely used to connect CPUs and GPUs. The overall bandwidth of PCIe depends on two key parameters: the PCIe generation (Gen) and the number of lanes (xN). The PCIe generation determines the data rate per lane, while the number of lanes determines the number of parallel transfers. Devices can choose different lane configurations (such as x1, x4, x8, and x16) based on performance requirements. Therefore, total bandwidth can be expressed as “single lane rate x number of lanes.” For example, PCIe 4.0 x16 offers 16 times the bandwidth of a single lane.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;429&quot; src=&quot;https://miro.medium.com/v2/1*prYWsBZsiLBRJ41Qz_23fQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           PCIe Technological Evolution
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Over the past decade, PCIe has doubled its bandwidth almost every three years, with bandwidth continuing to increase.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;419&quot; src=&quot;https://miro.medium.com/v2/1*_vcltSgvaMgz8WMzEID1vg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Within AI servers, PCIe primarily handles data communication between the CPU and GPU, and between GPUs. The PAM4 modulation technology introduced in PCIe 6.0 doubles the signal rate, while ensuring signal quality through
           &lt;a href=&quot;https://www.naddod.com/blog/what-is-forward-error-correction-fec&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            forward error correction
           &lt;/a&gt;
           (FEC) and supporting low-latency transmission. PCIe 6.0 is also compatible with the CXL 3.x protocol, laying the foundation for next-generation AI computing platforms.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Notably, the evolution of PCIe has not only improved internal server performance but also accelerated its deep integration with the Ethernet ecosystem. Ethernet-based
           &lt;a href=&quot;https://www.naddod.com/blog/high-performance-computing-analysis-and-application-of-roce-technology&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            RoCE
           &lt;/a&gt;
           (RDMA over Converged Ethernet) technology enables low-latency, high-bandwidth data transmission between server nodes. Unlike traditional TCP/IP communication, RoCE implements
           &lt;a href=&quot;https://www.naddod.com/blog/easily-understand-rdma-technology&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            RDMA
           &lt;/a&gt;
           (Remote Direct Memory Access) over Ethernet, allowing data to be transferred directly between the memories of different hosts without CPU involvement, significantly reducing latency and improving efficiency. Importantly, it can still run on standard Ethernet switches, significantly lowering cluster deployment costs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In this architecture, RoCE-enabled network cards serve as a crucial bridge between PCIe and Ethernet. NADDOD’s
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102418&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            ConnectX®-7 network adapter
           &lt;/a&gt;
           supports PCIe 4.0/5.0 x16 host interfaces and Ethernet protocols, providing up to 200Gb/s dual-port transmission capacity. It not only enables high-speed data access within the host via PCIe, but also enables low-latency RDMA communication between nodes via Ethernet, providing an ideal interconnect solution for AI cluster training and high-performance computing environments. It forms a continuous data path from “within the server” to “between clusters.”
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           PCIe Limitations
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           GPUs and CPUs form a tree-like interconnect structure through PCIe switches to enable multi-GPU collaboration. However, with the increasing complexity of AI training tasks and the increase in the number of GPUs, PCIe’s performance bottlenecks have become increasingly apparent. Compared to direct connections between CPUs and DRAM, PCIe’s data transfer rate is relatively low, limiting the expansion of GPU computing. As the number of GPUs in a system increases, bandwidth contention and latency accumulation on the link significantly increase.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;504&quot; src=&quot;https://miro.medium.com/v2/1*wPO5MjKjmrqhyaKNkMB8Og.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For example, PCIe 3.0 offers only approximately 32GB/s bandwidth in a 16-lane configuration, making it difficult to support the high-frequency data exchange required for large model training. Even in the PCIe 6.0 era, with transfer rates increased to 64GT/s and x16 bandwidth reaching 256GB/s, its efficiency in multi-GPU parallel communication scenarios remains limited. This is primarily due to the multiple latency factors of the PCIe architecture: signal forwarding and processing through PCIe switches, additional latency caused by serial-to-parallel conversion within the link, and the combined access latency caused by the separation of the PCIe bus and memory addressing.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In some early scientific computing platforms, these structural bottlenecks significantly reduced the performance of multi-GPU parallel training. For example, in an experiment using PCIe interconnects to train a medium-sized image recognition model, the overall training cycle took approximately 30% longer than expected due to bandwidth limitations and high latency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To address these issues, the industry introduced CXL technology, which significantly improves interconnect efficiency by adding cache coherence and memory sharing mechanisms to the PCIe physical layer.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           CXL: A Low-Latency Shared Memory Interconnect for AI
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           CXL is an open interconnect standard based on the PCIe physical layer, designed to enable high-speed and efficient interconnection between CPUs and GPUs, FPGAs, and other accelerators. CXL maintains data coherence between the CPU memory space and the memory of connected devices, enabling processors and external devices to share memory resources, significantly improving system performance, simplifying the software stack, and reducing overall system cost.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           CXL Protocol
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The CXL standard defines three protocols, which are dynamically multiplexed before being transmitted over the standard PCIe 5.0 PHY at 32 GT/s.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            CXL.io:
           &lt;/strong&gt;
           This protocol is an optimized PCIe 5.0 protocol primarily used for system initialization, link management, device discovery, and register access. It provides a non-coherent load/store interface for I/O devices.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            CXL.cache:
           &lt;/strong&gt;
           This protocol defines cache interaction between the host and device, enabling low-latency access to host memory for high-speed processing.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            CXL.mem:
           &lt;/strong&gt;
           This protocol allows the host processor to access device-attached memory using load and store instructions. In this case, the host CPU acts as the master, and the CXL device acts as a slave, supporting both volatile and persistent memory architectures.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;253&quot; src=&quot;https://miro.medium.com/v2/1*zbTa7u8SrqKGnMf-i-ELXA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           CXL Device Types
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The CXL.io protocol is a mandatory protocol for all CXL devices because it is responsible for initialization and link establishment; failure of this protocol can render the link inoperable. Combining different combinations of the other two protocols, CXL defines three device types, each corresponding to different application scenarios and memory access methods.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Type 1:
           &lt;/strong&gt;
           Typically, an accelerator lacks local memory and relies on host memory for operation.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Type 2:
           &lt;/strong&gt;
           Devices with local memory, such as GPUs or FPGAs, can access both the host and its own storage.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Type 3:
           &lt;/strong&gt;
           Devices extend host memory, enabling resource pooling and multi-host sharing.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;309&quot; src=&quot;https://miro.medium.com/v2/1*VC3EAOEa8w1xF67cP2RRrg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Type 2 devices introduce two coherency “biased” modes: host-biased and device-biased. The host-biased mode is enabled during task submission, with the host maintaining coherency. The device-biased mode is used during computation execution, allowing the device to directly access local memory, reducing coherency engine intervention and significantly reducing latency. The biased mode can be controlled by software or switched autonomously by hardware. The system defaults to host-biased mode.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           PCIe and CXL Synergy
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Although CXL has changed memory management, PCIe still plays a key role in high-speed data transmission. PCIe 5.0 already offers bidirectional bandwidth of up to 128GB/s, providing robust data throughput for AI inference and training tasks. With the further evolution of PCIe 6.2, transmission speeds will leap even higher. Its greatest breakthrough lies in its interoperability with CXL, introducing a truly hybrid interconnect model for AI computing.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           When PCIe 6.2 and CXL 3.1 work together to form a unified architecture, their strengths complement each other: the former ensures ultra-fast data exchange channels, while the latter optimizes memory sharing and access efficiency. This hybrid design is particularly suitable for large-scale AI training clusters and multi-accelerator collaborative systems, balancing bandwidth, latency, and scalability.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           By integrating a hybrid switching architecture supporting both PCIe 6.2 and CXL 3.1 on a single SoC, AI systems can achieve higher bandwidth utilization and flexible resource scheduling, providing a smooth upgrade path and exceptional scalability for next-generation intelligent computing platforms.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;273&quot; src=&quot;https://miro.medium.com/v2/1*ULUnjX7vLKNPxDvMBJc90g.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Continuous improvements in AI computing performance no longer rely solely on increases in single-chip computing power, but rather on the efficiency of the interconnect architecture. PCIe provides stable, high-speed data channels, while CXL implements cache coherence and shared memory mechanisms on top. The combination of these two enables data centers to shift from a CPU-centric design to a resource-centric, distributed architecture. This shift not only improves system bandwidth and energy efficiency, but also significantly enhances computing scalability and resource utilization.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD leverages its years of experience in the AI ​​industry and high-quality products (including
           &lt;a href=&quot;https://www.naddod.com/collection/optical-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            optical modules
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/collection/dac-and-aoc-cables&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            AOC/DAC cables
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/collection/networking&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            switches
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/collections/nvidia-networking/ethernet-adapters-or-nics&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            network cards
           &lt;/a&gt;
           ) to help customers build efficient and intelligent computing networks. Contact us today to explore the endless possibilities of the future of intelligent computing!
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> CPO (Co-Packaged Optics): A Key Technology Path for Optical Interconnects in AI Data Centers    </title>
<link>https://naddod.medium.com/cpo-co-packaged-optics-a-key-technology-path-for-optical-interconnects-in-ai-data-centers-aa1230d090ba</link>
<pubDate>Mon, 27 Oct 2025 03:43:12 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At this year’s OCP Global Summit, “Co-Packaged Optics (CPO)” became a frequently mentioned buzzword. With the continuous expansion of ultra-large model training scale and AI compute clusters, data centers face increasingly acute conflicts regarding power consumption, bandwidth, and architecture. Traditional electrical interconnects and pluggable optical module technologies are approaching their performance limits when dealing with network speed demands of 800G, 1.6T, and even 3.2T.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           CPO, a technology that deeply co-packages the optical engine with the switch chip, offers a solution for next-generation AI cluster interconnects by shortening the signal transmission path, reducing power consumption, and increasing bandwidth density. It is not just an optical module technology innovation, but a critical node in the evolution of data center architecture. This article will introduce the principle and evolution path of CPO technology, analyze its advantages in performance, power consumption, and reliability, and discuss its practical application and future development trends in AI data centers.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Fundamental Principles and Evolution Path of CPO Technology
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The evolution of CPO was not a sudden success; it evolved gradually from NPO (Near-Package Optics). NPO places the optical engine and packaged xPU (e.g., GPU, switch chip) adjacent to each other on the same high-performance PCB substrate, connected by extremely short (less than a few centimeters) high-performance electrical links. Compared to pluggable modules, NPO increases interconnect density by 2–3 times and significantly reduces channel loss and power consumption. It represents a transitional technology toward higher integration, laying the foundation for further evolution toward CPO.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;231&quot; src=&quot;https://miro.medium.com/v2/1*Q4qKL-OKzHaxf9wv89dL6Q.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Building on this foundation, CPO achieves a qualitative improvement in performance and energy efficiency through a tighter packaging design. The core concept of CPO is to tightly integrate the optical engine (OE) and switch ASIC chip within the same package through advanced packaging processes, shortening the transmission distance of electrical signals from centimeters to millimeters. This change not only significantly reduces signal attenuation but also leads to systematic reductions in power consumption and latency.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;156&quot; src=&quot;https://miro.medium.com/v2/1*RkrAOPhjDcgPL-CVHKoOag.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           CPO packaging technology itself is constantly evolving and is primarily categorized into three levels:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Type A CPO (2.5D packaging): The distance between the ASIC and the optical engine is further shortened to less than 10 cm.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Type B CPO (2.5D chiplet packaging): The introduction of wafer-level packaging technology improves interconnection efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Type C CPO (3D packaging): The final form, achieving vertical integration of optoelectronic chips and the shortest interconnect paths.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;525&quot; src=&quot;https://miro.medium.com/v2/1*oqZWgkbW-PJOVGmO3-TzFA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Notably, the development of
           &lt;a href=&quot;https://www.naddod.com/blog/what-is-silicon-photonics&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            silicon photonics
           &lt;/a&gt;
           is closely coupled with the development of CPO. Silicon photonics technology provides CPO with a highly integrated, low-power, and low-cost mainstream optical engine solution, which is a key foundation for CPO’s rapid maturity.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Performance Advantages of Co-Packaged Optics (CPO)
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           CPO (Co-Packaged Optics) achieves significant breakthroughs in bandwidth, power consumption, and space utilization. Traditional pluggable optical modules connect to the switch motherboard (PCB) via an interface, requiring electrical signals to travel several centimeters of traces, which can easily lead to signal attenuation and crosstalk. CPO’s core innovation lies in integrating optical components directly into the switch ASIC package using silicon interposers or micro-bump interconnect technology, fundamentally shortening the signal transmission path to millimeters. This architecture not only effectively alleviates the bottleneck caused by the high density of optical modules in ultra-high-performance computing systems, but also demonstrates a significant leap in performance, energy efficiency, and space utilization.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;235&quot; src=&quot;https://miro.medium.com/v2/1*4wYHGiPoF3cq7KankWLzNQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In terms of cost, the CPO solution also has long-term optimization potential. Although its initial investment (CAPEX) is currently higher than traditional pluggable solutions due to manufacturing complexity and maintenance difficulty, the high integration and miniaturization of components will significantly reduce the cost per module with technology maturity and mass production. Simultaneously, CPO has a clear advantage in operating costs (OPEX), mainly reflected in improved power consumption, latency, and conversion efficiency. According to Broadcom data, the power consumption of pluggable optical modules is generally 15–20 pJ/bit, while a CPO system can reduce this by over 50%, to only 5–10 pJ/bit.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This energy efficiency advantage stems from its highly integrated packaging design. In traditional optical communication systems, long-distance electrical connections between the electronic chip and optical components cause significant energy loss; under the CPO architecture, optical components and the electronic IC are in the same package, significantly reducing signal transmission distance and loss. The result is lower power consumption, shorter latency, and higher data transmission efficiency. Furthermore, this compact integration also reduces the system’s sensitivity to external noise and electromagnetic interference, further improving signal integrity, reliability, and overall system performance.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;363&quot; src=&quot;https://miro.medium.com/v2/1*BIBnJCubvb4PYBqgdvSCxw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Analysis of Broadcom Bailly 51.2T CPO Switch
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           CPO not only optimizes the performance of optoelectronic interconnects but also provides assurance for reliable deployment in large-scale systems. To more intuitively understand the effect of CPO technology, the following analysis is based on Meta’s test data for the Broadcom Bailly 51.2Tbps switch, examining its performance in design, power optimization, optical performance, and link reliability.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The Broadcom Bailly 51.2Tbps switch integrates eight 6.4Tbps Silicon Photonics Optical Engines (OE), with each OE supporting 64 Tx/Rx channels, totaling 128 400Gbps FR4 optical interfaces, connected via standard single-mode fiber. The CPO optoelectronic package is directly mounted on the Switch Main Board (SMB), providing power and control functions. The system uses a 4RU chassis design, with the front panel equipped with 128 LC duplex fiber interfaces and Pluggable Laser Sources (PLS). Each PLS supports 8 400G ports. The rear of the chassis is configured with 8 fans for efficient air cooling.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div&gt;
            &lt;img height=&quot;250&quot; src=&quot;https://miro.medium.com/v2/1*EpN75KdJ0pBAp2Cypk6F0g.png&quot; width=&quot;250&quot;/&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Power Consumption Analysis
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the CPO architecture, the high integration of the ASIC and optical interfaces effectively reduces energy losses in the electrical path, significantly outperforming traditional pluggable optical modules in terms of power consumption. Testing of 16 OEs and 32 PLSs in two CPO systems revealed that temperature variations have minimal impact on optical power consumption. Even by adjusting fan speed to alter operating temperature, power consumption fluctuations are largely negligible.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div&gt;
            &lt;img height=&quot;792&quot; src=&quot;https://miro.medium.com/v2/1*Sa42O76-NTXli3WQoeSt1g.png&quot; width=&quot;608&quot;/&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Operational data from 120 OEs and 240 PLSs in 15 CPO systems was collected at an ambient temperature of 40°C. The data was then compared to the performance of 48 2x400Gbps FR4 pluggable modules from four vendors, operating at a high temperature of 70°C. Results show that CPO optical modules reduce power consumption by up to 65% compared to pluggable modules, saving over 500W of total power when the Minipack3 system is fully loaded.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Optical Performance
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Tight integration requires careful validation of the optical transmitter and receiver performance to ensure reliable data transmission and guarantee interoperability with existing pluggable optical modules.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Key optical transmit and receive parameters were tested on all ports of the CPO switching system. At 100 Gbps per channel, the extinction ratio, TDECQ, and optical modulation amplitude of all 512 transmit channels met the 400GBASE-FR4 standard specifications.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;309&quot; src=&quot;https://miro.medium.com/v2/1*aGWoNPTEGPZM0_7yIg9rVw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Bit error rate (BER) and receive OMA tests were performed on all channels of two randomly selected ports of the CPO unit. Using a 400G FR4 pluggable module as a reference transmitter (TDECQ ≈ 2 dB, ER ≈ 5 dB), the results show that the CPO achieves a 512-channel bit error rate of less than 5×10⁻⁸, and the receiver’s OMA sensitivity distribution is better than -7 dBm, fully validating its link performance and interoperability.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;638&quot; src=&quot;https://miro.medium.com/v2/1*FFBijPw6UDH7aO0WW9SRvw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Link Reliability and Performance
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The primary challenges of CPO technology deployment in large-scale data centers are: Since co-packaged optical components cannot be replaced in the field, a field-replaceable PLS design not only addresses reliability issues common failure points in optical modules but also reduces quality and supply chain risks through multi-sourcing. Furthermore, under the most stringent operating conditions, the laser temperature in CPO typically remains below 50°C, while traditional pluggable transceivers often require the use of TECs or uncooled lasers with temperatures of 80°C. For CPO to operate reliably in hyperscale scenarios, the OE failure rate must match or exceed that of other components in the system.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;885&quot; src=&quot;https://miro.medium.com/v2/1*aAfw1gnhwHwNB0ftzAu_mQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To evaluate link reliability, researchers established a large-scale CPO test infrastructure. The CPO system operated continuously at 128 x 400 Gbps (with all ports optically looped back) in a 40°C environment, recording key reliability and performance parameters every 5 minutes.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Throughout the testing process, the system maintained stable operation, with no link interruptions or uncorrectable codewords (UCWs) observed. After a cumulative operation of 1.05 million device hours, the maximum non-zero FEC interval remained less than 7 on 75% of the ports, indicating that the optical link’s mean time between failures meets the requirements for an AI cluster supporting 24,000 GPUs.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;461&quot; src=&quot;https://miro.medium.com/v2/1*T2VKWLBgDmgP0KwHKLUQKQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The results show that CPO reduces optical power consumption by approximately 65% ​​compared to traditional pluggable optical modules, while maintaining standard compatibility and high reliability. This achievement demonstrates that CPO technology can achieve significant energy efficiency improvements and stable operation in large-scale data center environments, providing a viable technical path for next-generation AI network architectures.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Future Outlook: Will CPO Replace Pluggable Optical Modules?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the OCP Global Summit, the consensus was that “pluggable optics modules are essential to optical innovation.” Pluggable optical modules, with their mature ecosystem and flexible upgrade capabilities, remain the preferred solution for AI clusters. Their architecture is independent of the GPU and ASIC update cycle, making system upgrades and maintenance more efficient. “Ethernet + pluggable” remains the optimal solution for the current stage.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Both CPO and pluggable optical modules aim to reduce power consumption in high-speed interconnects, but their technical approaches and application directions differ. CPO achieves higher bandwidth and lower power consumption by directly packaging optical components with electrical chips. It is primarily targeted at high-speed, high-density scenarios in front-end networks, such as the spine layer. Pluggable optical modules, on the other hand, retain flexible pluggability, reducing power consumption while maintaining cost-effectiveness. They are more suitable for short-distance interconnects in
           &lt;a href=&quot;https://www.naddod.com/blog/ai-backend-network-and-development-trends&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            back-end networks
           &lt;/a&gt;
           . Linear-driven pluggable modules (LPO) are a typical implementation. For example, NADDOD’s LPO module, the
           &lt;a href=&quot;https://www.naddod.com/products/102508.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            800G OSFP 2xDR4/DR8
           &lt;/a&gt;
           , is a typical product of this technology path, providing a highly energy-efficient and low-latency interconnect solution for data centers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the long term, however, CPO remains a key direction in the evolution of optical interconnects. By co-packaging optical and electrical components, CPO offers significant advantages in power consumption, bandwidth density, and signal integrity, effectively addressing energy efficiency and interconnection bottlenecks in AI networks. Market research firm LightCounting predicts that by 2027, CPO ports will account for nearly 30% of the total 800G and 1.6T ports.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;297&quot; src=&quot;https://miro.medium.com/v2/1*nYgmjMv3W0x0bMJcq9jjHw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the future, CPO and pluggable optical modules will not replace each other but will coexist at different levels of network architecture. The combination of the two will form the mainstream form of optical interconnect in the AI ​​era.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the continuous increase in AI computing power and model complexity, data center network architecture is undergoing a profound transformation from “electrical interconnect” to “optical interconnect.” CPO significantly optimizes bandwidth, power consumption, and signal integrity by deeply co-packaging optical engines with ASICs, providing a future-proof, efficient interconnect solution for ultra-large-scale AI clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As a leading provider of high-speed data transmission solutions, NADDOD offers a product portfolio covering
           &lt;a href=&quot;https://www.naddod.com/collections/200g-qsfp56-or-qsfp-dd&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            200G
           &lt;/a&gt;
           、
           &lt;a href=&quot;https://www.naddod.com/collections/200g-400g-or-800g-modules?data_rate=400G&amp;page=1&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            400G
           &lt;/a&gt;
           、
           &lt;a href=&quot;https://www.naddod.com/collections/800g-qsfp-dd-or-osfp&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            800G
           &lt;/a&gt;
           , and
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-transceivers-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T
           &lt;/a&gt;
           optical modules. These modules have been proven in large-scale cluster deployments, demonstrating excellent performance, stability, and compatibility, and seamlessly support
           &lt;a href=&quot;https://www.naddod.com/solution/ai-networking/infiniband&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand
           &lt;/a&gt;
           /
           &lt;a href=&quot;https://www.naddod.com/solution/ai-networking/roce-rdma-over-converged-ethernet&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            RoCE
           &lt;/a&gt;
           AI infrastructure.
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Contact
           &lt;/a&gt;
           NADDOD’s team of experts today for customized optical interconnect solutions that meet your infrastructure needs.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> ESUN Workstream Advances Ethernet for Scale Up AI Infrastructure    </title>
<link>https://naddod.medium.com/esun-workstream-advances-ethernet-for-scale-up-ai-infrastructure-570984551d76</link>
<pubDate>Fri, 24 Oct 2025 07:31:53 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*p9aqSKCBnV8DJ3Vh8WvVng.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the 2025 OCP (Open Compute Project Foundation) meeting, OCP officially announced the launch of a new networking project working group, ESUN (Ethernet for Scale-Up Networking). This initiative marks a new phase in the industry’s use of Ethernet in AI hyperscale computing interconnection, aiming to build an open, scalable, and future-oriented high-performance network ecosystem. This article will introduce the definition of ESUN, its background, collaborative framework, and technical architecture, and explore its profound impact on the development of the AI ​​industry.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Introduction of ESUN
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ESUN is an open collaborative workstream initiated by the OCP with the goal of “leveraging the ecosystem and cost advantages of standard Ethernet to build a scalable, low-latency, and high-bandwidth AI accelerator interconnect architecture.” It is open to all.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ESUN was co-sponsored by 12 industry leaders, encompassing the entire ecosystem from chipsets to networks to cloud infrastructure: AMD, Arista, ARM, Broadcom, Cisco, HPE Networking, Marvell, Meta, Microsoft, NVIDIA, OpenAI, and Oracle. This collaboration reflects the industry’s consensus on “Ethernet Unified AI Scale-Up Interconnect,” establishing an open, standardized network foundation to enable more efficient and compatible communication between AI compute nodes.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;389&quot; src=&quot;https://miro.medium.com/v2/1*wJpZ4o4kPOO6EuJppj3-aw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Modern AI workloads demand not only high-throughput data exchange but also extremely low latency, deterministic transmission, and linear scalability. However, existing network architectures have significant limitations in supporting this trend. Traditional data center architectures, primarily based on scale-out, struggle to adapt to the high-density, fully interconnected communication requirements of AI clusters. While scale-up fabrics offer more efficient inter-GPU connectivity, they have long relied on proprietary protocols and closed architectures. For more information on scale-up and scale-out networks, please see:
           &lt;a href=&quot;https://www.naddod.com/blog/scale-up-vs-scale-out-in-ai-infrastructure&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Understanding Scale-Up vs. Scale-Out in AI Infrastructure
           &lt;/a&gt;
           . This has led to the phenomenon of “technology silos”:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;a href=&quot;https://www.naddod.com/blog/brief-discussion-on-nvidia-nvlink-network&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/blog/brief-discussion-on-nvidia-nvlink-network&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NVLink
           &lt;/a&gt;
           , and CXL, currently mainstream proprietary interconnect solutions offer performance advantages, they are highly closed, costly, and have limited scalability, making them unable to meet the requirements of open interoperability and large-scale deployment.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● While open standard Ethernet boasts a strong ecosystem and broad compatibility, it still leaves room for improvement in terms of microsecond latency and lossless transmission.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;410&quot; src=&quot;https://miro.medium.com/v2/1*tSz9glv5nM6y7lCxxO2Oog.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Therefore, the development of AI infrastructure urgently requires a new interconnection standard that combines openness, scalability, and high performance. ESUN was created to address the performance bottlenecks of open Ethernet in large-scale AI training and inference scenarios, building a future-oriented unified communications architecture.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           ESUN’s Core and Technical Directions
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. Collaboration Framework
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ESUN’s work is divided into two complementary directions: ESUN and SUE-T.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Network Function Layer (responsible for ESUN)
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Focuses on how data is transmitted between switches, including protocol design, error handling, and lossless communication. This is the problem ESUN aims to address. Initial candidate research directions include:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            L2/L3 Framing:
           &lt;/strong&gt;
           Efficiently encapsulating AI communication headers within Ethernet frames enables low-latency and high-bandwidth transmission, ensuring stable network performance under intensive computing loads.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Intelligent Error Recovery:
           &lt;/strong&gt;
           Rapid bit error detection and correction without sacrificing throughput, ensuring data integrity and link reliability.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Efficient Link Utilization:
           &lt;/strong&gt;
           Optimizing the link layer structure and scheduling mechanism to improve line utilization and overall transmission efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Lossless Transport:
           &lt;/strong&gt;
           Leveraging standard Ethernet congestion control technology, this mechanism prevents data loss or performance degradation caused by network congestion, which is particularly critical for AI workloads with high concurrency and low latency requirements.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;310&quot; src=&quot;https://miro.medium.com/v2/1*3EjRdiFjVHkCK7CaH5vNbg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            XPU Endpoint Layer (driven by SUE-T)
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Responsible for communication logic and resource scheduling at the endpoint level (e.g., GPU/XPU). SUE-T (Scale-Up Ethernet Transport), initiated by Broadcom, continues the SUE 1.0 specification, focusing on mechanisms such as workload distribution, memory access sequencing, and transaction scheduling to support efficient and reliable communication within AI clusters.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*TBHjibwgvZD8DNWuC8Y1Jw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. Technical Architecture
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ESUN is built on a standard Ethernet architecture and consists of three key layers:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Ethernet Header Layer:
           &lt;/strong&gt;
           ESUN is built on a standard Ethernet architecture, designed to support the widest range of upper-layer protocols and diverse application scenarios. By maintaining compatibility with existing Ethernet frame structures, ESUN enables seamless interoperability between different devices and systems, providing an open interoperability foundation for large-scale AI clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Data Link Layer:
           &lt;/strong&gt;
           ESUN supports XPU-level AI clusters through an open Ethernet data link layer. It utilizes standardized mechanisms (such as LLR, PFC, and CBFC) to achieve low-latency and low-congestion transmission performance, thereby meeting the stringent real-time and high-bandwidth requirements of large-scale training tasks while maintaining manageable costs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Physical Layer:
           &lt;/strong&gt;
           ESUN relies on mature and widely deployed Ethernet physical layer standards to ensure cross-vendor and cross-platform interoperability. Whether using optical fiber or copper cable interconnection, ESUN can maintain consistent performance and connection reliability across different physical media and vendor solutions, laying the foundation for building an open AI network ecosystem.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div&gt;
            &lt;img height=&quot;568&quot; src=&quot;https://miro.medium.com/v2/1*FvqtrGvYVrrrlG6Fp3xRhA.png&quot; width=&quot;626&quot;/&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3. Open Ecosystem and Standards Collaboration
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ESUN will collaborate closely with organizations such as IEEE 802.3 and the Ultra Ethernet Consortium (UEC) to maintain technical consistency and incorporate industry best practices. ESUN’s open framework allows operators, equipment manufacturers, and chip designers to collaborate and innovate under common standards, accelerating the implementation and widespread adoption of Ethernet in AI-enabled scale-out networks.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ESUN also clearly defines technical no-go areas, excluding host-side protocol stacks, non-Ethernet protocols, application-layer solutions, and proprietary technologies from its scope of work, ensuring resources are focused on upgrading Ethernet core capabilities.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Impact of ESUN
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The introduction of ESUN is not only a technological innovation but also represents a profound shift in the industry landscape. By building a scale-out network system based on open Ethernet standards, ESUN brings new possibilities to the entire AI infrastructure. It not only redefines interconnection within AI clusters but also promotes the rapid penetration of optical communications within racks. Ethernet connections, once limited to inter-rack connections, are now extending to the short-distance interconnection layer between GPUs and accelerators, bringing new changes to network architecture and hardware form factors.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. Open Standards Reshape the Innovation Model
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The greatest value of open standards lies in lowering the barrier to innovation. In the past, the development of scale-out networks often relied on closed, proprietary protocols, resulting in high entry costs and a limited ecosystem. Under the ESUN framework, small and medium-sized manufacturers no longer need to design architectures from scratch. Instead, they can quickly develop compatible products by following the specifications, accelerating the process from prototype to commercialization. This open model not only fosters more dynamic industry innovation but also promotes a more diverse and stable supply chain structure.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. Ethernet Ecosystem Expansion and Growing Demand for Optical Modules
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           It’s crucial to note that Ethernet itself boasts a solid industrial foundation. From PHY chips to switch ASICs, from optical modules to copper cables, hundreds of established suppliers worldwide are already involved. As ESUN drives optical communications deeper into cabinets, demand for short-haul, high-speed, and compact optical modules will grow significantly. NADDOD offers a comprehensive portfolio of Ethernet optical modules covering multiple speeds, including
           &lt;a href=&quot;https://www.naddod.com/collections/800g-osfp-ethernet-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            800G Ethernet modules
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-transceivers-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T high-speed Ethernet modules
           &lt;/a&gt;
           . These modules cover transmission distances from 50 meters to 50 kilometers, meeting the needs of diverse deployment scenarios. Furthermore, for short- to medium-haul interconnection or cost-sensitive scenarios, NADDOD also offers
           &lt;a href=&quot;https://www.naddod.com/collections/100g-qsfp28-ethernet-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            100G
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/collections/200g-qsfp56-ethernet-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            200G
           &lt;/a&gt;
           , and
           &lt;a href=&quot;https://www.naddod.com/collections/400g-osfp-qsfp-dd-or-qsfp112-ethernet-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            400G
           &lt;/a&gt;
           low- and medium-speed Ethernet modules, providing flexible options for AIDCs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Ethernet’s open ecosystem and mature supply chain ensure greater cost control and mass production feasibility. If ESUN can fully unleash the potential of Ethernet in scale-up networks, the overall cost structure and deployment efficiency of AI data centers will be systematically optimized, bringing a new round of incremental development space to the optical communications industry.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3. Technical Challenges and Standardization Process
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Despite its promising prospects, ESUN’s technical path still requires large-scale validation. Whether Ethernet can consistently meet the demanding AI interconnect requirements of microsecond latency and zero packet loss in multi-hop communication scenarios remains to be proven in practice. Furthermore, whether the mechanism proposed by ESUN will be adopted by international standards organizations such as the IEEE will require time and the participation and consensus of more industry partners.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Furthermore, the rise of high-density optical interconnects within cabinets has also brought engineering challenges such as power consumption, heat dissipation, and packaging. Balancing high bandwidth and low power consumption in short-reach links will become a new challenge for both optical module and network equipment manufacturers. NADDOD’s
           &lt;a href=&quot;https://www.naddod.com/solution/ai-networking/immersion-liquid-cooling&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            liquid-cooled optical module solution
           &lt;/a&gt;
           has emerged in this context. Its liquid cooling system can support up to 100kW of heat dissipation per rack, providing reliable thermal management for high-density AI data centers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As ESUN drives Ethernet deeper into cabinets, these high-efficiency, low-power optical modules will become a key enabler for large-scale AI network deployments, laying a solid foundation for the future expansion and upgrade of AI data centers.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The release of ESUN represents another key advancement in Ethernet’s role in high-performance AI interconnectivity. Based on open standards, it aims to scale vertically beyond the traditional Ethernet ecosystem, meeting the latency, bandwidth, and interoperability requirements of large-scale AI clusters. By collaborating with standards organizations such as IEEE and UEC, ESUN is promoting industry compatibility and collaboration under unified specifications, providing a framework for co-evolution for chip, system, and network equipment vendors.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           From a broader perspective, the emergence of ESUN provides a pragmatic and sustainable path for the development of AI infrastructure. With the gradual improvement of standards and broad industry participation, Ethernet is expected to play an even more core role in AI interconnectivity, becoming a key foundation for future intelligent computing. For more information on AI infrastructure, please visit our official website at
           &lt;a href=&quot;https://naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD.com
           &lt;/a&gt;
           or
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            speak with our experts
           &lt;/a&gt;
           .
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Optical Transceiver: Packaging Methods &amp; Optical Chip Types    </title>
<link>https://naddod.medium.com/optical-transceiver-packaging-methods-optical-chip-types-155406d9cf0e</link>
<pubDate>Thu, 23 Oct 2025 07:31:38 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the field of modern communications, optical transceivers play a crucial role as essential components in optical communication systems, carrying and transmitting optical signals. For the design and manufacturing of fiber optic transceivers, the choice of packaging methods and optical chip types is one of the key factors.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This article analyzes the requirements of optical transceivers and discusses packaging methods and optical chip types to help readers better understand their design and manufacturing process.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           From Requirement Input to Completion of
           &lt;a href=&quot;https://www.naddod.com/collection/optical-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Optical Transceiver
           &lt;/a&gt;
           Design
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This article describes the entire process of optical transceiver design and production, starting from customer requirements, material selection, and design. It includes a detailed analysis of various terms, technologies, and categories in the optical module industry, as well as a brief overview of industry trends.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Requirement Analysis
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Example customer requirement: 500-meter transmission distance, 100G transmission rate, QSFP28 interface, considering overall system cost. The customer’s concise requirement is associated with every detail of the selection in the production and design of fiber optical transceivers:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Selection 1: Packaging method and process: Hermetic packaging (TO-CAN, BOX, butterfly), non-hermetic packaging (COB, COC, etc.)
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Selection 2: Optical chip types: VCSEL, DFB, EML, narrow linewidth tunable.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;350&quot; src=&quot;https://miro.medium.com/v2/1*fYesR3fAXSyKU8AnqQ3vsw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Each option is directly related to certain performance requirements of the product and is strongly correlated with the final product’s reliability, cost, and other factors. The competition in the optical industry involves optimizing multiple parameters. While pursuing performance (rate, miniaturization, transmission distance), it also brings significant power consumption and heat dissipation pressure. Addressing heat dissipation and other issues adds cost pressure, and controlling costs introduces risks to stability and reliability.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Although packaging, product appearance, and electrical interfaces are standardized, optical modules involve a significant amount of design and process experience. Understanding customer requirements and balancing performance, power consumption, cost, reliability, and other indicators is the core competitiveness of an optical transceiver module company.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Packaging Selection
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Hermetic Packaging
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Based on customer requirements, in outdoor environments with significant temperature and humidity variations, and considering the substantial impact of water vapor corrosion and temperature on laser chips’ wavelength, we consider adopting a hermetic packaging approach. This involves sealing the laser chip inside a metal casing filled with inert gas and equipped with a sealed window.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Based on specific transmission distance, chip heat dissipation, cost requirements, channel count, etc., different hermetic packaging methods can be chosen:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            1. TO-CAN Packaging:
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The laser is mounted on a small heat sink (heat dissipator) and connected to electrical pins through wires. It is then further packaged with a metal cap and a sealed window for laser emission. TO-CAN packaging is compact and relatively cost-effective. However, its drawback is that it has a small volume, making it difficult to install larger heat dissipation devices. It is not suitable for high power, high current, and long-distance scenarios. It is more suitable for applications in the telecommunications market, such as 10km 10G/25G, such as base station fronthaul and home broadband.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;345&quot; src=&quot;https://miro.medium.com/v2/1*AGH2x8XnEUclV7JwxSzzdg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           After being converted to TO-CAN, it has the basic laser packaging. However, the laser’s beam diameter and the optical fiber are still different. Further alignment and coupling with lenses and optical fibers are required to focus most of the energy into the optical fiber. After complete packaging, it becomes a TOSA (Transmitting Optical Sub-Assembly). Similarly, on the other end, if it is replaced with a receiver chip, it is called ROSA (Receiver Optical Sub-Assembly). Together, they are referred to as OSA (Optical Sub-Assembly).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            2. Butterfly Packaging:
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To meet high-power requirements, butterfly packaging can be used. The laser is mounted on a larger heat sink (which can also include temperature control using TEC). Optical components such as lenses and isolators are also installed within the metal casing. Butterfly packaging integrates the laser and the entire optical path. Categorically, a butterfly packaged device is a higher-level component compared to TO-CAN.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;448&quot; src=&quot;https://miro.medium.com/v2/1*ayR5SqWcCxNw4tfXmpIiNg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            3. BOX Packaging:
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           BOX packaging is a special form of butterfly packaging mainly designed for multi-channel requirements. We will discuss multi-channels in detail later. In this packaging, multiple lasers are integrated within a single package, and transmission is achieved through a single optical fiber. This packaging is suitable for applications that require high temperature control, air tightness, and reliability.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;280&quot; src=&quot;https://miro.medium.com/v2/1*MxMWPNPmYWZYNXulblIE5A.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Non-Hermetic Packaging
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Since the widespread use of optical transceivers in the data center market, the overall working environment has been significantly optimized compared to the telecommunications market, which is exposed to outdoor conditions such as wind and sun, due to the installation of air conditioning, environmental monitoring, and other equipment in data centers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the same time, there is a high demand for cost control due to the large quantity of optical transceivers used. As a result, non-hermetic packaging has gradually emerged. The technology of non-hermetic packaging has been continuously iterated, leading to rapid improvements in reliability, and its applicability in various scenarios is increasing.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Non-hermetic packaging, in simple terms, refers to directly attaching/welding the optical chip to the circuit board and providing simple sealing protection using epoxy resin or other adhesives. This approach reduces the need for a large number of auxiliary components, resulting in cost savings and increased integration.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           One example of non-hermetic packaging is Chip-on-Board (COB) packaging. COB packaging involves attaching the laser chip to a PCB substrate and integrating components such as laser arrays and receiver arrays in a small space to achieve miniaturization. By reducing certain protective measures and the number of auxiliary components, the cost is relatively lower compared to other packaging methods.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;604&quot; src=&quot;https://miro.medium.com/v2/1*s9HM9_0e9s64g6rmlqX8sg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           25G and below rate optical transceivers often use single-channel TO or butterfly packaged optical transceiver components that are soldered onto PCB boards to form the optical module. However, for high-speed optical transceivers with rates of 40G and above, they are primarily implemented through multi-channel parallelization due to the limitations of laser rates (often 25G). For example, 40G is achieved by 4x10G, while 100G is achieved by 4x25G.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The packaging of high-speed optical transceivers imposes higher requirements on parallel optical design, high-speed electromagnetic interference, size reduction, and increased power dissipation and heat dissipation issues. COB packaging can integrate TIA/LA chips, laser arrays, and receiver arrays into a small space to achieve miniaturization.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In summary, hermetic packaging uses metal and glass to provide tight protection for fragile optical chips, enabling them to withstand various usage environments. There are several specific packaging methods based on different device design requirements. However, the overall device complexity and the need for a costly flexible printed circuit (FPC), also known as a “flex board,” to route high-frequency signals out of the hermetic enclosure, result in higher costs. In cases where the working environment is relatively controllable and the reliability requirements can be met, non-hermetic packaging can optimize costs.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Selection of Optical Chips
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           There are multiple chip options available based on considerations such as transmission distance, modulation scheme, and cost. When selecting optical chips, supply chain considerations are also important. Popular products often experience shortages during the initial production phase, and instances of delays and delivery postponements are common.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;391&quot; src=&quot;https://miro.medium.com/v2/1*FIBTyqv39mfqG1hIAEKreQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           VCSEL
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           VCSEL (Vertical-Cavity Surface-Emitting Laser) chips are the lowest-cost option. However, they have a larger emitting angle and are typically used with relatively thick multimode fibers. Considering the overall system cost, VCSEL chips are commonly applied in short-distance scenarios such as a few meters for AOC (Active Optical Cable) and around 100 meters for SR (Short Reach) optical transceivers.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           DFB
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           DFB (Distributed Feedback) chips are processed with gratings on original FP (Fabry-Perot) lasers, enabling more precise wavelength selection and higher output wavelength accuracy. DFB chips have a smaller emitting angle, allowing for more efficient optical coupling. Therefore, they are widely used in medium to long-distance applications, such as 500 meters or 2 kilometers, and offer a relatively moderate cost.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           EML
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           EML (Electro-absorption Modulated Laser) chips are one of the highest-cost options. They consist of an emitting chip (which can be DFB/DBR, etc.) combined with an external absorption modulator. During operation, the laser chip remains in an emitting state, and the EML laser’s signal output is controlled by switching the modulator’s absorption (simply put, it can be transparent or opaque).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Now, let’s explain what happens during the transmission of optical signals.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           1. Light of different wavelengths travels at different speeds in optical fibers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           2. When the laser chip is powered on and emits light, there is a slight wavelength variation (chirping) that occurs within a femtosecond-level micro-timescale.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Based on these two phenomena, when a DFB chip receives an electrical signal, it emits a laser signal (which contains a certain range of wavelength components). During long-distance transmission through optical fibers, dispersion occurs, meaning that signals of different wavelengths arrive at the receiving end with significant time differences, potentially causing interference.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the example shown in the figure below, two pulse signals are emitted from the DFB laser transmitter. Due to the different transmission speeds of different wavelengths in the optical fiber, the arrival times at the receiving end differ. In severe cases, the two pulse signals can mix together (inter-symbol interference), resulting in communication failure.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;264&quot; src=&quot;https://miro.medium.com/v2/1*U1K_7SdqroWc27Qh_VaFlQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The advantage of using EML (Electro-absorption Modulated Laser) lasers is that the laser chip operates in a stable state, resulting in a “purer” emitted wavelength. Even after modulation through an external modulator and long-distance transmission, the signal quality remains high. Therefore, EML is suitable for long-distance transmission applications (10km, 20km, 40km, or even longer).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           However, due to the addition of an external absorption modulator and the higher overall quality requirements for chips in long-distance scenarios, EML chips of the same speed have a cost that is 50% or even several times higher than DFB chips.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           On the other hand, the response speed of the external absorption modulator is higher compared to direct modulation of DFB lasers, making it more suitable for certain modulation techniques such as PAM4 (which will be further discussed later).
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Tunable Narrow Linewidth Lasers (referred to as “Tunable Lasers”)
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As mentioned earlier, long-distance transmission is affected by dispersion, and EML lasers can address the chirping issue. However, the inherent emission wavelength range (referred to as “linewidth”) of lasers still exists, and dispersion remains a challenge in applications like ultra-long-haul ODN (Optical Distribution Network) spanning 80km, 100km, or even longer distances.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           On the other hand, for long-haul backbone transmission, overall system cost needs to be considered. Therefore, Dense Wavelength Division Multiplexing (DWDM) technology is introduced to transmit signals of different wavelengths over a single fiber, greatly increasing the transmission capacity of a single fiber and reducing the overall system cost of long-haul backbone networks. To meet these two requirements, tunable narrow linewidth lasers are used.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;349&quot; src=&quot;https://miro.medium.com/v2/1*Ynroz0GHcAJ8ieNML4wsCA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Tunable narrow linewidth laser structures are more complex, and there are various approaches, including current control, temperature control, mechanical control, and more. Taking the example of an external cavity mechanical control scheme, a grating structure is added to the ordinary laser, and the output wavelength is adjusted through mechanical control to achieve more precise wavelength control.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the past, tunable lasers had relatively limited applications. However, with the potential introduction of wavelength division multiplexing (WDM) technology in 5G fronthaul, some manufacturers are also exploring the possibilities of using tunable lasers. Future demand may see significant changes.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For this reason, NADDOD provides the
           &lt;a href=&quot;https://www.naddod.com/products/29819.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Tunable SFP+/XFP 10G DWDM 80km Optical Transceiver
           &lt;/a&gt;
           , which can meet the current needs of 5G fronthaul WDM applications. The schematic diagram of the module’s principle is as follows:
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;346&quot; src=&quot;https://miro.medium.com/v2/1*d-fZ_olhufZyHbir1tRmqg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In summary, VCSEL chips are chosen for low-cost short-distance applications, DFB chips for medium-distance applications, EML chips for medium to long-distance and special modulation requirements, and tunable narrow linewidth lasers for ultra-long distances and certain specific applications.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In conclusion, the design and manufacturing of optical transceivers are complex and critical processes. When analyzing the requirements for transceivers, consideration should be given to the choice between hermetic and non-hermetic packaging, as well as the characteristics and application scenarios of different types of optical chips.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Through appropriate packaging selection and matching of optical chip types, efficient, stable, and reliable optical transceiver designs can be achieved to meet communication needs in various fields. In the future, with the continuous development of optical communication technology, we can expect more innovations and breakthroughs that will bring greater possibilities for optical module design.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> What is InfiniBand?. Since the beginning of the 21st…    </title>
<link>https://naddod.medium.com/what-is-infiniband-8b4222332804</link>
<pubDate>Thu, 23 Oct 2025 03:40:54 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*Fk6YtGlWub1oOO4OjE8_Ow.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Since the beginning of the 21st century, with the increasing popularity of cloud computing and big data, data centers have developed rapidly. InfiniBand is a key technology in the data center and occupies a very important position. In particular, starting from 2023, the rise of large AI models represented by ChatGPT has further increased the focus on InfiniBand. This is because the network used by gpt is built on NVIDIA’s InfiniBand.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           So what exactly is InfiniBand technology? Why is it so popular? What is the oft-discussed “InfiniBand vs. Ethernet” debate? This article will answer each of these questions.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;393&quot; src=&quot;https://miro.medium.com/v2/1*whIJfdLGegG3RAuXV_lZIg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Development History of InfiniBand
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand (IB for short) is a powerful communication protocol. To tell the story of its birth, we need to start with the architecture of computers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As we all know, modern digital computers have been using von Neumann architecture since their inception. In this architecture, there are CPUs (arithmetic logic unit and control unit), memory (RAM, hard disk), and I/O (input/output) devices.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the early 1990s, in order to support more and more external devices, Intel was the first to introduce the Peripheral Component Interconnect (PCI) bus design into the standard PC architecture.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;277&quot; src=&quot;https://miro.medium.com/v2/1*TrlsiECEUvrpyQ9apgTC2A.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Shortly thereafter, the Internet entered a phase of rapid development. The continuous growth of online businesses and user base posed significant challenges to the capacity of IT systems.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At that time, with the support of Moore’s Law, components such as CPUs, memory, and hard drives were rapidly advancing. However, the PCI bus was upgrading at a slower pace, greatly limiting I/O performance and becoming a bottleneck for the entire system.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To address this issue, Intel, Microsoft, and SUN led the development of the “Next Generation I/O (NGIO)” technology standard. IBM, Compaq, and Hewlett-Packard, on the other hand, led the development of “Future I/O (FIO).” These three companies, together, also created the PCI-X standard in 1998.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 1999, the FIO Developers Forum and NGIO Forum merged to establish the InfiniBand Trade Association (IBTA).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Soon, in the year 2000, the 1.0 version of the InfiniBand Architecture Specification was officially released.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Simply put, the birth purpose of InfiniBand was to replace the PCI bus. It introduced the RDMA protocol, offering lower latency, higher bandwidth, and greater reliability, thereby enabling more powerful I/O performance.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           When it comes to InfiniBand, one company that must be mentioned is the renowned Mellanox.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;439&quot; src=&quot;https://miro.medium.com/v2/1*qN12x-nplNYhR3SfhIWSjw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In May 1999, several employees who had resigned from Intel and Galileo Technology founded a chip company in Israel, naming it Mellanox.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           After its establishment, Mellanox joined NGIO. Later, NGIO and FIO merged, and Mellanox subsequently became part of the InfiniBand camp. In 2001, they introduced their first InfiniBand product.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2002, a significant change occurred within the InfiniBand camp.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           That year, Intel made a sudden decision to shift its focus to developing PCI Express (PCIe), which was launched in 2004. Another major player, Microsoft, also withdrew from InfiniBand development.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Although companies like SUN and Hitachi chose to persist, the development of InfiniBand was cast with shadows.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Starting in 2003, InfiniBand shifted towards a new application domain, which was computer cluster interconnectivity.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           During that year, Virginia Tech created a cluster based on InfiniBand technology, ranking third in the TOP500 list (a global ranking of supercomputers).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2004, another significant InfiniBand non-profit organization was established — the Open Fabrics Alliance (OFA).
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div&gt;
            &lt;img height=&quot;258&quot; src=&quot;https://miro.medium.com/v2/1*bTazFVBDYfJPwsVpubboTg.png&quot; width=&quot;559&quot;/&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           OFA and IBTA have a collaborative relationship. IBTA is primarily responsible for the development, maintenance, and enhancement of the InfiniBand protocol standards, while OFA is responsible for developing and maintaining the InfiniBand protocol and higher-level application APIs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2005, InfiniBand found another new application scenario — the connection of storage devices.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           During that time, InfiniBand and Fibre Channel (FC) were popular SAN (Storage Area Network) technologies. It was at this time that many people became aware of InfiniBand technology.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Subsequently, InfiniBand technology gradually gained popularity, attracting an increasing number of users, and its market share continued to rise.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           By 2009, there were already 181 systems utilizing InfiniBand technology in the TOP500 list. (Of course, Gigabit Ethernet was still the mainstream with 259 systems.)
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As InfiniBand started to rise, Mellanox also grew continuously, gradually becoming a leader in the InfiniBand market.
           &lt;br/&gt;
           In 2010, Mellanox merged with Voltaire, leaving Mellanox and QLogic as the primary InfiniBand suppliers. Soon after, in 2012, Intel invested in acquiring QLogic’s InfiniBand technology, reentering the competition in the InfiniBand market.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           After 2012, with the continuous growth of high-performance computing (HPC) demands, InfiniBand technology continued to make significant progress, increasing its market share.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2015, InfiniBand technology’s share in the TOP500 list exceeded 50% for the first time, reaching 51.4% (257 systems).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This marked the first time InfiniBand technology had successfully challenged Ethernet technology. InfiniBand became the preferred internal interconnect technology for supercomputers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2013, Mellanox made further advancements by acquiring silicon photonics technology company Kotura and parallel optical interconnect chip manufacturer IPtronics, further strengthening its industry presence. By 2015, Mellanox had captured an 80% share of the global InfiniBand market. Their business scope expanded from chips to encompass network cards, switches/gateways, remote communication systems, cables, and modules, establishing themselves as a world-class networking provider.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the face of InfiniBand’s progress, Ethernet did not remain idle.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In April 2010, the IBTA introduced RoCE (RDMA over Converged Ethernet), which “ported” the RDMA technology from InfiniBand to Ethernet. In 2014, they proposed a more mature version, RoCE v2.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With RoCE v2, Ethernet significantly narrowed the technological performance gap with InfiniBand. Combined with its inherent cost and compatibility advantages, Ethernet began to make a comeback.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The chart below shows the technology shares in the TOP500 list from 2007 to 2021.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;256&quot; src=&quot;https://miro.medium.com/v2/1*OLTW-VYkUi9TqNp8geWvhQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As shown in the graph, starting from 2015, 25G and higher-speed Ethernet (represented by the dark green line) began to rise and quickly became the industry favorite, temporarily suppressing InfiniBand.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2019, Nvidia, the company, made a bold move by acquiring Mellanox for a staggering $6.9 billion, surpassing rival offers from Intel and Microsoft, who bid $6 billion and $5.5 billion, respectively. This successful acquisition further strengthened Nvidia’s position in the high-performance computing and data center domains, establishing them as another significant player in the networking technology market, following InfiniBand.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;351&quot; src=&quot;https://miro.medium.com/v2/1*5QPpc2VAaWrxNvH_r2QvYg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           According to Nvidia CEO Jensen Huang, the reason for the acquisition was explained as follows: “This is the combination of two leading global high-performance computing companies. We focus on accelerated computing, while Mellanox specializes in interconnect and storage.”
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In hindsight, his decision appears to have been very visionary.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As we can see, with the rise of large AI language models like GPT-3, there has been an exponential surge in the demand for high-performance computing and intelligent computing across society.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;470&quot; src=&quot;https://miro.medium.com/v2/1*e2GWEQ4dC8xcklRc45AqGg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To support such a massive computational demand, high-performance computing clusters are essential. In terms of performance, InfiniBand is considered the top choice for high-performance computing clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           By combining their own GPU computing power with Mellanox’s networking expertise, Nvidia has effectively created a powerful “computing engine.” In terms of computational infrastructure, Nvidia undoubtedly holds a leading advantage.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Today, the competition in high-performance networking is between InfiniBand and high-speed Ethernet. Both sides are evenly matched. Manufacturers with abundant resources are more likely to choose InfiniBand, while those seeking cost-effectiveness may lean towards high-speed Ethernet.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           There are also other technologies remaining, such as IBM’s BlueGene, Cray, and Intel’s OmniPath, which generally belong to the second tier of options.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Technical Principles of InfiniBand
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           After introducing the development history of InfiniBand, let’s now take a look at its working principle and why it is superior to traditional Ethernet. How does it achieve low latency and high performance?
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Initial Expertis-RDMA
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As mentioned earlier, one of the most prominent advantages of InfiniBand is its early adoption of the Remote Direct Memory Access (RDMA) protocol.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In traditional TCP/IP, data from the network card is first copied to the main memory and then further copied to the application’s storage space. Similarly, data from the application space is copied to the main memory before being sent out through the network card to the Internet.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This I/O operation requires intermediate copying in the main memory, which increases the length of the data transfer path, adds burden to the CPU, and introduces transmission latency.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;300&quot; src=&quot;https://miro.medium.com/v2/1*me9zbCKVD8D2IMylh05XWQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RDMA can be seen as a technology that “eliminates intermediaries.”
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With RDMA’s kernel bypass mechanism, it enables direct data reads and writes between applications and the network card, reducing data transmission latency within servers to nearly 1 microsecond.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Furthermore, RDMA’s zero-copy mechanism allows the receiving end to directly read data from the sender’s memory, bypassing the involvement of the main memory. This greatly reduces CPU burden and improves CPU efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As mentioned earlier, the rapid rise of InfiniBand can be attributed to the significant contributions of RDMA.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            InfiniBand Network Architecture
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The network topology structure of InfiniBand is illustrated in the diagram below:
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;468&quot; src=&quot;https://miro.medium.com/v2/1*kpMn1qwNjtGnn1GDl04TFw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand is a channel-based architecture, and its components can be mainly categorized into four types:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           · HCA (Host Channel Adapter)
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           · TCA (Target Channel Adapter)
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           · InfiniBand links (connecting channels, which can be cables or fibers, or even on-board links)
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           · InfiniBand switches and routers (used for networking)
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Channel adapters are used to establish InfiniBand channels. All transmissions start or end with a channel adapter to ensure security or operate at a given Quality of Service (QoS) level.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Systems using InfiniBand can be composed of multiple subnets, with each subnet capable of accommodating over 60,000 nodes. Within a subnet, InfiniBand switches perform layer 2 processing. Between subnets, routers or bridges are used for connectivity.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;357&quot; src=&quot;https://miro.medium.com/v2/1*Ix_UcFTr17YdBWJjlx-kvg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The second-layer processing in InfiniBand is straightforward. Each InfiniBand subnet has a subnet manager that generates a 16-bit Local Identifier (LID). InfiniBand switches consist of multiple InfiniBand ports and forward data packets from one port to another based on the LID included in the Layer 2 Local Routing Header. Apart from managing packets, switches do not consume or generate data packets.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With its simple processing and proprietary Cut-Through technology, InfiniBand significantly reduces forwarding latency to below 100 ns, which is notably faster than traditional Ethernet switches.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the InfiniBand network, data is also transmitted in the form of packets, with a maximum size of 4 KB, using a serial approach.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            InfiniBand Protocol Stack
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The InfiniBand protocol also adopts a layered structure, where each layer is independent and provides services to the layer above it. Please refer to the diagram below for illustration.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;368&quot; src=&quot;https://miro.medium.com/v2/1*SFgCdAZWHM8FMOcjw8cHzQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The physical layer defines how bit signals are composed into symbols on the wire, and further into frames, data symbols, and data padding between packets. It provides detailed specifications for signaling protocols to construct efficient packets.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The link layer defines the format of data packets and protocols for packet operations such as flow control, routing selection, encoding, and decoding.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The network layer performs routing selection by adding a 40-byte Global Route Header (GRH) to the data packet, enabling data forwarding.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           During the forwarding process, routers perform only variable CRC checks, ensuring end-to-end data transmission integrity.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;306&quot; src=&quot;https://miro.medium.com/v2/1*npXzLiE0mi2b4C57_AMkjw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The transport layer further delivers the data packet to a designated Queue Pair (QP) and instructs the QP on how to process the packet.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           It can be observed that InfiniBand has its own defined layers 1–4, making it a complete network protocol. End-to-end flow control forms the foundation of InfiniBand network packet transmission and reception, enabling lossless networks.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Speaking of Queue Pairs (QPs), a few more points are worth mentioning. They are the fundamental communication units in RDMA technology.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           A Queue Pair consists of two queues: the Send Queue (SQ) and the Receive Queue (RQ). When a user invokes API calls to send or receive data, they are actually placing the data into the QP. The requests in the QP are then processed one by one using a polling mechanism.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;427&quot; src=&quot;https://miro.medium.com/v2/1*95hnDutUg9OTyxGoROChKQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            InfiniBand Link Rate
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand links can be established using either copper cables or fiber optic cables. Depending on the specific connection requirements, dedicated InfiniBand cables are used.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand defines multiple link speeds at the physical layer, such as 1X, 4X, and 12X. Each individual link is a four-wire serial differential connection, with two wires in each direction.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Taking the example of the early SDR (Single Data Rate) specification, the original signal bandwidth for a 1X link was 2.5 Gbps, while a 4X link had a bandwidth of 10 Gbps, and a 12X link had a bandwidth of 30 Gbps.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The actual data bandwidth for a 1X link was 2.0 Gbps due to the use of 8b/10b encoding. Since the link is bidirectional, the total bandwidth relative to the bus is 4 Gbps.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Over time, InfiniBand’s network bandwidth has continuously upgraded, progressing from the early SDR, DDR, QDR, FDR, EDR, and HDR to NDR, XDR, and GDR, as shown in the diagram below:
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;586&quot; src=&quot;https://miro.medium.com/v2/1*iMhg_h7noCSGP9actNPcqw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Commercial Products of InfiniBand
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Finally, let’s take a look at the commercial InfiniBand products available on the market.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The NVIDIA Quantum-X800
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102796&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Q3200-RA switch
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102612&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Q3400-RA switch
           &lt;/a&gt;
           with its high performance for AI workloads. Q3400-RA Leverags 200Gb/s-per-lane serializer/deserializer (SerDes) technology significantly enhances network performance and bandwidth. NADDOD SiPh-based
           &lt;a href=&quot;https://www.naddod.com/products/102514.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            OSFP-1.6T-2xDR4H
           &lt;/a&gt;
           modules typically connect with Q3400-RA for delivering high bandwidth and low power consumption in hyperscale data centers. The NVIDIA Quantum-X800 Q3400-RA features 144 ports at 800Gb/s distributed across 72 Octal Small Form-factor Pluggable (
           &lt;a href=&quot;https://www.naddod.com/form-factor/osfp-transceivers-cables&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            OSFP
           &lt;/a&gt;
           ) cages with ultra-low latency and high bandwidth for next-gen AI.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;a href=&quot;https://www.naddod.com/collections/nvidia-networking/connectx-8&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NVIDIA ConnectX-8 SuperNICs
           &lt;/a&gt;
           , which enable advanced routing and telemetry-based congestion control capabilities, achieving the highest network performance and peak AI workload efficiency. It supports for both InfiniBand and Ethernet networking.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           When connecting switches, network cards, and adapters in an IB network, choosing high-quality InfiniBand cables is essential for ensuring smooth network connectivity. Currently, the original InfiniBand HDR, NDR, and EDR AOC/DAC cables from NVIDIA are expensive and in short supply, which poses an obstacle for companies looking to quickly deploy high-performance computing networks.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As a leading provider of overall optical network solutions, NADDOD offers lossless
           &lt;a href=&quot;https://www.naddod.com/solution/ai-networking/infiniband&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            AI network solutions
           &lt;/a&gt;
           based on InfiniBand and RoCE (RDMA over Converged Ethernet) to build lossless network environments and high-performance computing capabilities for users. NADDOD can choose the optimal solution tailored to specific situations and user requirements, providing high bandwidth, low latency, and high-performance data transmission to effectively address network bottleneck issues and enhance network performance and user experience.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD manufactures InfiniBand AOC/DAC cables that meet the connectivity requirements from 0.5m to 100m distances, supporting various rates including NDR, HDR, EDR, FRD, and more. Additionally, they offer fast delivery, free sample trials, lifetime warranty, and technical support. With their excellent customer service and products, NADDOD provides superior performance while reducing costs and complexity, catering to server clusters’ high-performance needs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With a professional technical team and extensive experience in implementing and servicing various application scenarios,
           &lt;a href=&quot;http://www.naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD
           &lt;/a&gt;
           ’s products and solutions have gained trust and popularity among customers, widely applied in industries and critical fields such as high-performance computing, data centers, education and research, biomedicine, finance, energy, autonomous driving, internet, manufacturing, and telecommunications.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In conclusion, the future of InfiniBand looks promising, driven by high-performance computing and artificial intelligence.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand, as a high-performance and low-latency interconnect technology, has been widely adopted in large-scale computing clusters and supercomputers. It provides higher bandwidth and lower latency to meet the demands of large-scale data transfers and high-concurrency computing. InfiniBand also supports more flexible topologies and complex communication patterns, giving it a unique advantage in high-performance computing and AI domains.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           However, Ethernet, as a widely adopted networking technology, is also evolving. With increasing Ethernet speeds and technological advancements, it has solidified its position in data centers and has caught up with InfiniBand in certain aspects. Ethernet has a broad ecosystem and mature standardization support, making it easier to deploy and manage in general data center environments.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As technology continues to evolve and demands change, both InfiniBand and Ethernet may play to their respective strengths in different application scenarios. Whether InfiniBand or Ethernet will have the last laugh, only time will tell. They will continue to drive the development and innovation of information technology, meeting the growing bandwidth demands and providing efficient data transmission and processing capabilities.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Data Center Liquid Cooling Technology and Trends Analysis    </title>
<link>https://naddod.medium.com/data-center-liquid-cooling-technology-and-trends-analysis-871c17059f0a</link>
<pubDate>Mon, 20 Oct 2025 01:50:33 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the wave of digital economy, data centers have become the core infrastructure for computing power, storage, and computing. With the rapid development of cutting-edge fields like AI and 5G, the demand for computing power is expanding exponentially. However, the increase in chip performance has also led to a continuous increase in power consumption and heat dissipation, and traditional cooling technologies are no longer able to meet this challenge.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Industrial Background of Liquid Cooling
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Liquid cooling has become an inevitable choice for data center cooling, primarily driven by the following challenges.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Explosive Growth in Chip Power Consumption:
            &lt;/strong&gt;
            From the NVIDIA V100 (300W) to the H100 (700W), and finally to Blackwell (1200W), the power consumption of high-performance chips has continued to rise. The NVIDIA GB200 boasts a power consumption of up to 2700W. Industry forecasts suggest that single-chip power consumption may exceed 5000W in the future.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Rack Density Challenge:
            &lt;/strong&gt;
            The average power density of traditional racks is approximately 8 kW, while AI racks are soaring to 100 kW or even higher. Vertiv predicts that the peak density of AI GPU racks will exceed 1 MW by 2029. This order of magnitude increase makes traditional cooling methods no longer sufficient.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;367&quot; src=&quot;https://miro.medium.com/v2/1*jKSUY_LqawWf-gzvW0qIOw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Power Usage Effectiveness (PUE) and Energy Efficiency Bottlenecks:
            &lt;/strong&gt;
            Data from the Uptime Institute shows that the average Power Use Effectiveness (PUE) of large data centers worldwide has hovered between 1.57 and 1.59 for many years, with little prospect of further reduction. Liquid cooling technology is considered the primary path to achieving high-density heat dissipation and reducing power usage effectiveness (PUE).
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             External environmental and market pressures:
            &lt;/strong&gt;
            Frequent extreme weather events also place higher demands on cooling systems. During the 2023 UK heatwave, both Google and Oracle’s data centers experienced failures due to cooling system failures. With the increasing frequency of high-temperature weather, data centers must re-evaluate the environmental adaptability of their cooling systems.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Core Advantages of Liquid Cooling Technology
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The advantages of liquid cooling technology lie not only in its heat dissipation capacity, but also in energy efficiency, space utilization, and overall system design flexibility.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             More efficient thermal conductivity:
            &lt;/strong&gt;
            Water has significantly higher specific heat and density than air. Its heat absorption capacity is approximately 3,200 times that of air for the same volume, and its thermal conductivity is 23 times higher. This allows for more efficient heat removal and supports more compact, high-density designs.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Reduced Power Usage Effectiveness (PUE) and improved energy efficiency:
            &lt;/strong&gt;
            Liquid cooling can significantly reduce cooling energy consumption, lowering PUE to 1.2 or even below 1.1. Schneider data shows that in data centers of the same capacity, deploying liquid cooling at 20kW and 40kW per rack reduces investment costs by 10% and 14%, respectively, compared to traditional air cooling.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Support for high-density deployments:
            &lt;/strong&gt;
            As AI chip power consumption continues to rise, rack density and temperatures continue to increase. Liquid cooling technology has become an inevitable trend in addressing the cooling challenges of high-density data centers. Compared to traditional air cooling solutions that rely on large-scale air conditioning units and complex airflow designs, liquid cooling systems occupy less space and enable higher computing density integration.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Liquid Cooling Technology Roadmap
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Liquid cooling is not a single technology but a suite of systemic engineering solutions focused on “how to efficiently transfer heat away from the chip.” The main types are Cold Plate, Immersion, and Spray Cooling.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Cold Plate Liquid Cooling
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Cold plate liquid cooling is currently the most mature and widely used liquid cooling solution. This technology uses a cold plate to indirectly transfer heat generated by heat-generating components to a coolant in a closed-loop circuit. The coolant then removes the heat from the system. Cold plate liquid cooling can be further categorized as single-phase and two-phase.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Single-phase liquid cooling maintains the coolant in a liquid state throughout the entire cycle. Common examples include Liquid cooling with Liquid to Air Side Car (L2A) and Liquid cooling with Liquid-to-Liquid CDU (L2L). L2A requires no additional liquid cooling infrastructure and is suitable for traditional data centers.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;283&quot; src=&quot;https://miro.medium.com/v2/1*ODYfcr-D6Ng4rjTx-oY8gQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           L2L uses a cooling distribution unit (CDU) to cool high-power heat loads (MW-level), but requires higher infrastructure requirements.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;243&quot; src=&quot;https://miro.medium.com/v2/1*MACVCy-SFKpOGnHGlT4KYg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Two-phase liquid cooling involves the coolant undergoing a phase change during the heat dissipation process (absorbing heat and evaporating to condensing and recirculating), significantly improving heat dissipation capacity and supporting applications where a single chip consumes more than 2500W.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The advantages of cold plate liquid cooling include strong compatibility, easy maintenance, and high reliability. However, due to the lack of full liquid cooling coverage, energy savings are relatively limited when the cabinet power density is low. Furthermore, its structural design requires close compatibility with existing IT equipment, making standardization challenging.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Microchannel Lid Cold Plate（MLCP）
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           MLCP is a key technology for high-power chip cooling. It integrates the traditional chip lid and liquid cold plate into a single component, allowing the coolant to flow directly over the chip through micro-scale channels (30–150 microns). This drastically shortens the heat transfer path and reduces thermal resistance.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;234&quot; src=&quot;https://miro.medium.com/v2/1*7DHdNkWXasntQO2NglddFA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           MLCP has two major technical features:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Micro-Scale Internal Structure:
            &lt;/strong&gt;
            Reduces flow channel size to increase the contact area between the coolant and the heat source, improving convective heat transfer efficiency.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Highly Integrated Design:
            &lt;/strong&gt;
            Combines the heat spreader, cold plate, and chip lid, eliminating the Thermal Interface Material (TIM) layers found in traditional cold plate designs, creating a “zero-interruption” thermal path from chip to coolant.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This “Embedded Liquid Cooling” architecture signifies a deep coupling between chip packaging and cooling system design, providing the technical foundation to support future kilowatt-scale chips and significantly lower data center PUE for green, efficient computing infrastructure.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Immersion Cooling
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Immersion cooling is a direct-contact liquid cooling technique where all heat-generating components are fully submerged in a non-conductive dielectric fluid. Heat is transferred directly through convection and/or phase change.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Single-Phase Immersion:
            &lt;/strong&gt;
            The coolant remains in a liquid state and is circulated by a pump for heat exchange.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Two-Phase Immersion:
            &lt;/strong&gt;
            The coolant boils into vapor upon absorbing heat, which rises to a condenser to change back into liquid and recirculate, offering highly efficient heat dissipation.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Servers are typically deployed in horizontal racks, and cooled fluid is pumped into the tank, absorbs the heat from the vertically inserted servers, and then flows out to a heat exchanger for cooling and recirculation.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD offers
           &lt;a href=&quot;https://www.naddod.com/solution/ai-networking/immersion-liquid-cooling&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Immersion Liquid Cooling solution
           &lt;/a&gt;
           , validated in large-scale data centers for its feasibility, efficiency, and reliability, demonstrating excellent performance in high-density deployments with NVIDIA InfiniBand and RoCE.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Spray Cooling
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Spray cooling is also a direct-contact technology. It uses system pressure or gravity to precisely spray a non-conductive coolant onto critical heat sources, such as chips. The coolant then removes the heat and is collected and recycled.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This technology achieves on-demand cooling, concentrating cooling resources on critical hotspots where heat dissipation is most needed, resulting in high efficiency. However, its system structure is relatively complex, placing extremely stringent requirements on nozzle design, piping layout, and leak prevention.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA GB200/GB300 Liquid Cooling System Analysis
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As a leader in AI computing, NVIDIA GB200/GB300 series liquid cooling solutions have become industry benchmarks, heralding the widespread adoption of liquid cooling technology in high-end computing.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;535&quot; src=&quot;https://miro.medium.com/v2/1*CP1OpEkCwhay8ECzJUq_sg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In high-density AI servers, the liquid cooling system acts like an efficient “blood circulation network,” working together to stably and rapidly dissipate the enormous amounts of heat generated during operation. It comprises the IT rack, manifold, cooling distribution unit (CDU), and cooling tower.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;450&quot; src=&quot;https://miro.medium.com/v2/1*-6H6Y9VUJeuzugSmFLiVIg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The numerous servers deployed within the rack are the source of heat. The liquid cooling system circulates coolant throughout the system via the manifold: the left manifold transports low-temperature coolant from the CDU, acting as the “arteries,” while the right manifold recovers high-temperature coolant discharged from the servers, acting as the “veins.” The CDU, with its built-in pump, water tank, and plate heat exchanger, is the power center of the entire system, providing circulation power and enabling the energy exchange between hot and cold liquids. The cooling tower is responsible for the final heat dissipation, exchanging heat with the outside air and dissipating it to the surrounding environment, completing the cooling process.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The NVIDIA GB200/GB300 architecture overview reflects the evolution from “integrated cooling” to “precision cooling.” In the
           &lt;a href=&quot;https://www.naddod.com/blog/nvidia-gb200-interconnect-architecture-analysis-nvlink-infiniband-and-future-trends&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            GB200 architecture
           &lt;/a&gt;
           , the core unit of each rack consists of 18 compute trays and 9 swap trays. Each compute tray contains four GB200 chips, corresponding to two Bianca boards. Each board houses one Grace CPU and two Blackwell GPUs, forming a “1CPU + 2GPU” unit. The GB200 utilizes a large cold plate covering the entire Bianca board, meaning that the CPU and two GPUs within a single integrated unit share the same cold plate for cooling.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*r4yy-BjNQmg1J4KpYgPX-A.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In contrast, the
           &lt;a href=&quot;https://www.naddod.com/blog/technical-analysis-of-gb300-liquid-cooling&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            GB300 cooling strategy
           &lt;/a&gt;
           is more refined. Unlike the GB200 integrated cold plate design, the GB300 shifts to an independent cold plate design, employing a “separate cold plate for each chip.” Only the switch portion retains the “two chips, one cold plate” design, enabling more precise heat dissipation control. This also results in a doubling of the number of cold plates: the GB200 requires 36 compute-side cold plates and 9 switch-side cold plates per rack, for a total of 45 large cold plates. With each chip equipped with an independent cold plate, the GB300’s compute-side cold plate count rises to 108, and with the addition of the 9 switch-side cold plates, the total number reaches 117, a 2.7-fold increase.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This change in cold plate structure not only represents a leap forward in the thermal management precision of liquid cooling systems, but also directly drives a significant increase in the value of the entire liquid cooling system. The GB200 focuses on integration and deployment efficiency, while the GB300 emphasizes refined heat dissipation and performance in high-power scenarios. This evolutionary trend is closely tied to the GB300 overall architectural upgrades. For further information, please read:
           &lt;a href=&quot;https://www.naddod.com/blog/nvidia-gb300-deep-dive-performance-breakthroughs-vs-gb200-liquid-cooling-innovations-and-copper-interconnect-advancements&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NVIDIA GB300 Deep Dive: Performance Breakthroughs vs. GB200, Liquid Cooling Innovations, and Copper Interconnect Advancements
           &lt;/a&gt;
           .
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the continuous rise in power consumption in high-performance computing, liquid cooling systems are no longer just supporting facilities but a key component in determining the cost and competitiveness of computing platforms.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Future Development Directions and Trends
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Increasing Facility Temperature and Waste Heat Recovery
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The key to resolving cooling challenges lies in reducing the thermal resistance of the secondary circuit. The fundamental reason for the current cooling system’s limited efficiency is the small temperature difference between the facility and the surrounding environment, resulting in inadequate heat dissipation. By increasing the heat transfer efficiency of the chip-to-facility supply circuit by 10 times through technological innovation, the secondary circuit’s thermal resistance, R, can be effectively reduced, and the facility’s supply water temperature can be raised from the traditional 10–32°C to 60–80°C.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This shift will bring significant benefits:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Improved heat dissipation efficiency:
            &lt;/strong&gt;
            With a wider temperature difference, dry coolers can independently dissipate heat without relying on chillers or evaporative cooling, achieving 100% free cooling.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Reduced energy and water consumption:
            &lt;/strong&gt;
            Higher supply water temperatures reduce cooling energy consumption while avoiding the water consumption associated with evaporative cooling.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Improved energy efficiency:
            &lt;/strong&gt;
            Waste heat above 60°C can be reused through waste heat recovery (WHR) systems, achieving efficient, cascaded energy utilization.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Environmental Protection and Sustainability Requirements
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the context of carbon neutrality and sustainable development, liquid cooling systems are gradually adopting low-GWP (&amp;lt;10) refrigerants, requiring them to demonstrate enhanced environmental and occupational health safety. This will become a key direction for future liquid cooling technology standardization and policy regulations.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Traditional refrigerants such as R-22 and R-410A struggle to strike a balance between greenhouse gas potential and safety. Newer refrigerants such as R-161 and R-1270, while maintaining their low GWP, have effectively reduced safety risks associated with flammability through improvements in formulation and system technology.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Reliability and Standardization
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Data center downtime can cost millions of dollars per minute, and cooling system failures are a major cause of downtime. Future reliability design for liquid cooling systems will resemble that of the aviation industry. By incorporating FMEA (Failure Mode and Effects Analysis), Markov chain modeling, and full lifecycle simulation tools, liquid cooling systems will achieve greater fault tolerance and predictability. Furthermore, standardized testing infrastructure and interoperability standards will gradually emerge, driving liquid cooling from a project-based approach to a large-scale industry.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Liquid cooling technology is no longer simply an upgrade for data center cooling solutions; it is also a key path to addressing the explosion of computing power in the AI ​​era and achieving green and sustainable data center development. Compared to air cooling, liquid cooling not only significantly reduces power usage effectiveness (PUE) and improves energy efficiency, but also enhances data center stability and scalability in high-temperature and extreme environments. This makes it a core supporting technology for future hyperscale AI clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD
           &lt;a href=&quot;https://www.naddod.com/collection/liquid-cooling-networking&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            liquid-cooled optical modules
           &lt;/a&gt;
           are designed specifically for high-density computing scenarios. These products have been fully validated in real-world, large-scale immersion liquid-cooled data centers, boasting mass delivery capabilities and exceptional reliability.
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Contact us
           &lt;/a&gt;
           for customized network solutions.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> AI Backend Network and Development Trends    </title>
<link>https://naddod.medium.com/ai-backend-network-and-development-trends-b0cb81ec8053</link>
<pubDate>Mon, 20 Oct 2025 01:43:05 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;402&quot; src=&quot;https://miro.medium.com/v2/1*OVCdwCDj83caElY3nacLrw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In AI data centers, the network architecture is generally divided into two parts: the frontend network and the AI backend network. The frontend network is responsible for data access and task distribution, connecting servers with external systems, while the backend network supports high-speed communication within the cluster, enabling parameter synchronization for model training and inference. This division stems from differences in function, bandwidth requirements, and communication patterns. The frontend network focuses on connection breadth and scalability, whereas the backend network emphasizes bandwidth density and low latency to ensure efficient distributed computing. As model scales continue to grow, the backend network is evolving from traditional Ethernet toward higher-bandwidth, lower-latency interconnects such as InfiniBand and high-performance Ethernet, becoming a key direction in
           &lt;strong&gt;
            AI infrastructure
           &lt;/strong&gt;
           upgrades.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*ImTRFZtaKisqtc1-L-LXjA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            AIDC multi-layer interconnection structure
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Introduction for AI Frontend Network and AI Backend Network
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To fully understand AI network architecture, it is essential to examine the roles of the frontend network and backend network within the system. The two differ significantly in structure, communication models, and performance goals, yet together they support the entire process from data input to model training.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           AI Frontend Network
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The frontend network serves as the main channel connecting users, application services, and AI acceleration resources. It handles “north-south” traffic — external data, inference requests, and service responses. Acting as the control and data input layer of the AI data center, it connects compute, storage, and management nodes, supporting functions such as data ingestion, task scheduling, model distribution, checkpointing, and logging.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Architecturally, the frontend network is typically built on standard Ethernet using a leaf-spine topology, where Top-of-Rack (ToR) switches interconnect with spine switches to enable both internal and external communication within the data center. It generally adopts RoCE or TCP/IP protocols, supporting both out-of-band management and data transmission needs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The primary responsibilities of the frontend network include:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Connecting users and datasets to GPU and AI acceleration nodes;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Supporting model inference, data preprocessing, and management traffic;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Providing inter-service communication and monitoring access.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Due to the fluctuating traffic patterns of AI applications, the frontend network needs to have flexible bandwidth allocation and stable quality of service (QoS) mechanisms to cope with sudden traffic bursts and multi-type task requests.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;383&quot; src=&quot;https://miro.medium.com/v2/1*Ppot2ini5obrhFNwqQD-_w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            front network topology
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           AI Backend Network
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The backend network undertakes core communication tasks in AI model training and large-scale computing, handling “east-west” traffic, specifically data exchange and gradient synchronization between nodes. It forms a high-speed GPU interconnect layer that enables parameter updates and model coordination during distributed training. Compared with the frontend network, the backend network places higher emphasis on ultra-low latency, extreme bandwidth, and zero-packet-loss transmission — all critical for large-scale model performance. Common communication protocols include RDMA, NCCL, InfiniBand, and RoCE.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;375&quot; src=&quot;https://miro.medium.com/v2/1*PBlyJ4Ye7WJYnTYgaYQonQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            backend network topology
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Scale-Up
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Directly connecting multiple GPUs within the same server via high-speed interconnects (such as NVIDIA NVLink) enables them to share memory and process tasks in parallel. This architecture offers extremely high bandwidth and latency as low as microseconds, making it suitable for high-performance training scenarios on single nodes or small clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Scale-Out
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Connecting GPU clusters across multiple servers or racks typically uses high-performance interconnects such as InfiniBand or Enhanced Ethernet (RoCE). This requires maintaining bandwidth continuity and lossless transmission during massively parallel computing, as well as sophisticated congestion control capabilities to ensure stable and efficient communication during training.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;312&quot; src=&quot;https://miro.medium.com/v2/1*tzMBZInqJzg1yU7eA3WWzw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For more information on the role of horizontal and vertical networks in AI infrastructure, please read
           &lt;a href=&quot;https://www.naddod.com/blog/scale-up-vs-scale-out-in-ai-infrastructure&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Understanding Scale-Up vs. Scale-Out in AI Infrastructure.
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           AI Backend Network Architecture
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To fully understand the operating mechanisms of the backend network, we need to start with its architecture. The design of the AI ​​backend network not only determines the efficiency of data transmission between nodes in the cluster but also impacts the scalability and stability of model training.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*v4CVUaq_Im_Nx6LunQQOQg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            Architecture and Anatomy of Al Clusters
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The AI ​​backend network architecture can generally be divided into two layers:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           1.
           &lt;strong&gt;
            Intra-node / intra-rack interconnect
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the same server or rack, multiple GPUs may be interconnected via
           &lt;a href=&quot;https://www.naddod.com/blog/brief-discussion-on-nvidia-nvlink-network&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NVIDIA Link
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/collections/nvidia-networking/infiniband-switching&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NVIDIA Switch
           &lt;/a&gt;
           , or PCIe, providing extremely high-bandwidth, low-latency direct communication. NVIDIA’s NVLink and NVSwitch architecture is often used to connect multiple GPUs into a high-bandwidth intra-network. This interconnect is considered “scale-up” and is part of the backend network, but it focuses more on high-performance communication within the machine or rack.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;360&quot; src=&quot;https://miro.medium.com/v2/1*C2XmUypUgQNwYeg6PriC8g.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            NVLink
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           2.
           &lt;strong&gt;
            Inter-node / scale-out interconnect
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In distributed training scenarios, overall computing power does not scale linearly with the number of compute nodes. This is because cross-node communication introduces additional latency, resulting in a speedup ratio typically less than 1. To reduce this communication overhead, the backend network uses
           &lt;a href=&quot;https://www.naddod.com/blog/easily-understand-rdma-technology&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            RDMA (Remote Direct Memory Access) technology
           &lt;/a&gt;
           , enabling GPUs or compute nodes to bypass the operating system kernel and directly access remote node memory, significantly reducing cross-node data transmission latency. Solutions for implementing RDMA primarily include InfiniBand and RoCEv2.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;466&quot; src=&quot;https://miro.medium.com/v2/1*7yQ9fIjEmpOTb9oFXM4Lkg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            RDMA
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           When the training scale exceeds the capacity of a single machine or rack, inter-node interconnection becomes necessary. The backend network needs to connect multiple GPU nodes, each of which may contain multiple GPUs. In this scenario, the network must cope with greater physical distances, more complex traffic patterns, and stricter latency requirements to ensure the efficiency and stability of large-scale distributed training.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Component Composition
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The backend network consists of a variety of hardware and software components that work together to ensure high-bandwidth, low-latency, and stable data transmission within the AI ​​cluster. This hardware primarily includes the following categories:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           1.
           &lt;strong&gt;
            Switches
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Backend network switches typically require high bandwidth, low latency, and support for RDMA/RoCE features, QoS, and flow control (PFC/ECN). NADDOD’s AIDC series, including the
           &lt;a href=&quot;https://www.naddod.com/collections/400g-ai-data-center-switches&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            400G AIDC switch
           &lt;/a&gt;
           and the
           &lt;a href=&quot;https://www.naddod.com/collections/800g-ai-data-center-switches&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            800G AIDC switch
           &lt;/a&gt;
           , are designed specifically for these workloads and are ideally suited for hyperscale AI clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           2.
           &lt;strong&gt;
            SuperNIC/SmartNIC/Programmable NIC
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In larger or more complex network environments, the introduction of SmartNICs or SuperNICs separates the control path from the data path, enabling advanced features such as traffic scheduling, congestion control, and link multiplexing. NADDOD offers the
           &lt;a href=&quot;https://www.naddod.com/collections/nvidia-networking/connectx-8&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NVIDIA ConnectX-8 superNIC
           &lt;/a&gt;
           , which supports advanced routing and telemetry-based congestion control, achieving maximum network performance and peak AI workload efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           3.
           &lt;strong&gt;
            Optical Modules/Cables
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Back-end networks cover short to long distances. Hardware includes common Ethernet optical modules (such as
           &lt;a href=&quot;https://www.naddod.com/form-factor/osfp-transceivers-cables&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            OSFP form factors
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/form-factor/qsfpdd-transceivers-cables&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            QSFP-DD form factors
           &lt;/a&gt;
           ),
           &lt;a href=&quot;https://www.naddod.com/collections/ethernet-active-optical-cables-aoc&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            RoCE AOC cables
           &lt;/a&gt;
           , and
           &lt;a href=&quot;https://www.naddod.com/collections/ethernet-direct-attach-copper-dac-cables&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            RoCE DAC cables
           &lt;/a&gt;
           . Longer distances may require
           &lt;a href=&quot;https://www.naddod.com/collections/fiber-patch-cables&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            fiber optic patch cables
           &lt;/a&gt;
           . NADDOD offers a full range of optical modules and interconnect cables to meet diverse bandwidth, distance, and power requirements, while balancing latency and cost optimization.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           4.
           &lt;strong&gt;
            Routers/Controllers/Network Management Devices
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Large-scale clusters require support for the network control plane, routing policies, traffic engineering, and fault recovery mechanisms. In most designs, these functions are integrated into the switch or network operating system, such as
           &lt;a href=&quot;https://www.naddod.com/blog/open-source-sonic-cost-efficient-flexible-choice-for-data-center-switching&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            SONiC
           &lt;/a&gt;
           or
           &lt;a href=&quot;https://www.naddod.com/blog/nvidia-spectrum-x-an-ethernet-network-platform-specifically-designed-for-ai&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Spectrum-X
           &lt;/a&gt;
           . For example, NADDOD’s
           &lt;a href=&quot;https://www.naddod.com/products/102322.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            N9500–64OC 51.2T switch
           &lt;/a&gt;
           , based on NADDOD’s enterprise-grade SONiC technology, decouples software from hardware, breaking the limitations of proprietary routing solutions.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Software includes the following categories:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           1.
           &lt;strong&gt;
            RDMA over Converged Ethernet (RoCEv2)
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCEv2 has become the mainstream choice in many AI backend designs, enabling Ethernet networks to support RDMA communication.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;339&quot; src=&quot;https://miro.medium.com/v2/1*M9x_G-ruUJrBi6uVdrIlhA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            RoCEv2
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           2.
           &lt;strong&gt;
            Flow Control/Congestion Control Mechanisms
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To achieve a lossless or minimally error-prone network,
           &lt;a href=&quot;https://www.naddod.com/blog/pfc-flow-control-technology-and-challenges-in-rocev2-network-deployment&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Priority Flow Control (PFC)
           &lt;/a&gt;
           , Explicit Congestion Notification (ECN), Data Center Quantized Congestion Notification (DCQCN), or other RDMA congestion control algorithms are often used. Many AI backend network designs require strict tuning of these mechanisms to avoid congestion and packet loss.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;207&quot; src=&quot;https://miro.medium.com/v2/1*8xwbfU2N9-_p19Nd9PybaQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            PFC
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           3.
           &lt;strong&gt;
            NCCL/Distributed Communication Library
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           On the GPU side, NVIDIA’s NCCL (or similar communication libraries) is responsible for mapping high-level algorithms (such as all-reduce) to low-level network communication operations. The network architecture must provide good support for NCCL communication patterns (such as ring, tree, and bandwidth balancing).
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div&gt;
            &lt;img height=&quot;180&quot; src=&quot;https://miro.medium.com/v2/1*KVSDxjdmMcQVUmi3dPfcpQ.png&quot; width=&quot;254&quot;/&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            NCCL
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           4.
           &lt;strong&gt;
            Network Scheduling/Intelligent Flow Control/SDN
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In future or large-scale designs, software control, SDN, or custom control planes will be introduced to separate the control path from the data path, enabling more refined traffic distribution, congestion scheduling, and path selection.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Ethernet’s Growing Role in AI Backend Networks
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As AI models continue to scale, backend network design is evolving towards higher bandwidth, lower latency, and greater scalability. To address the communication bottlenecks caused by the rapid growth in the number of GPUs, the industry is accelerating the development of new standards, such as the Ultra Ethernet Consortium (UEC), to further optimize Ethernet transmission efficiency and deterministic performance in backend networks, enabling it to meet the needs of large-scale distributed training.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Historically, AI training tasks primarily relied on the InfiniBand (HDR/NDR/XDR) architecture for the lowest-latency inter-GPU communication. In 2020, InfiniBand dominated the AI ​​backend network switch market, while Ethernet’s share was almost negligible. However, driven by technological advancements and an open ecosystem, Ethernet’s market share will overlap with InfiniBand by 2025. It is expected that between 2025 and 2029, Ethernet will surpass InfiniBand in performance and deployment scale, becoming the mainstream backend interconnect solution.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The new generation of HPC significantly improves communication efficiency within AI clusters by combining RDMA-optimized protocols such as RoCEv2, ECN, and PFC, along with advanced congestion control and traffic management mechanisms. Ethernet speeds have also continued to rise, from 100G to 400G to 800G and finally to 1.6T Ethernet. Leveraging high-performance ASIC chips like the Broadcom Tomahawk 5/6 and the maturity of
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-800g-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            800G/1.6T OSFP optical interconnect technology
           &lt;/a&gt;
           , Ethernet is gradually approaching and even surpassing InfiniBand in terms of latency and bandwidth.
           &lt;a href=&quot;https://www.naddod.com/products/102323.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD’s N9500–128QC switch
           &lt;/a&gt;
           featuring the high-throughput and low-latency Broadcom Tomahawk 5 51.2Tbps chip, provides unparalleled network support for AI workloads.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;359&quot; src=&quot;https://miro.medium.com/v2/1*wy_syWgT5j7v0bvaBpF1oQ.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the same time, the emergence of AI-specific network cards, such as the NVIDIA ConnectX-8 SuperNIC, has further strengthened Ethernet’s competitiveness in backend networks. This type of network card supports collective communication acceleration, intelligent traffic scheduling, and programmable control, significantly improving bandwidth utilization and system throughput in multi-node GPU communications, making it a crucial component of next-generation AI data centers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Overall, Ethernet with its open standards, scalability, cost advantages, and broad ecosystem compatibility, is evolving from a traditional general-purpose network technology to a critical infrastructure supporting AI backend interconnection, driving the evolution of AI network architecture towards higher performance, larger scale, and greater openness.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The rapid evolution of AI networks is driving changes across data center architectures. From front-end inference to back-end training, network performance has become a crucial determinant of overall system efficiency. Ethernet is evolving from a traditional general-purpose network technology to a critical infrastructure supporting AI back-end interconnection. With its open standards, robust scalability, and cost advantages, it is gradually replacing proprietary protocols and becoming the mainstream choice for large-scale AI training.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD will continue to monitor AI network development trends and continuously enhance its high-speed optical interconnect product portfolio to help build high-performance, scalable AI infrastructure. Please continue to follow our blog or visit
           &lt;a href=&quot;https://naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD.COM
           &lt;/a&gt;
           for more AI industry insights and the latest product updates.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> RoCE v2 Deployment and Application    </title>
<link>https://naddod.medium.com/roce-v2-deployment-and-application-840496dec85f</link>
<pubDate>Thu, 16 Oct 2025 01:25:30 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*9Kxdn6Wkp4NOYsOT5Zh35w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           1. RoCE v2 Configuration Steps
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The configuration of RoCE v2 involves the setup of adapters (network interface cards) and switches.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Note：Before proceeding with the configuration, please ensure that all hardware and drivers support RoCE v2, and that the network infrastructure is ready.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Adapter Configuration:
          &lt;/h2&gt;
          &lt;ol&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Install the Adapter: Insert the RoCE v2 compatible adapter into the PCIe slot of the server.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Install Adapter Drivers:Install the RoCE v2 drivers provided by the adapter manufacturer. Ensure that you use the latest version of the drivers for optimal performance and stability.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Enable RDMA Functionality:Enable RDMA functionality on the server, typically through network settings in the operating system or adapter configuration. Make sure that RDMA functionality is enabled.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Configure IP Address and Subnet Mask:Configure the IP address and subnet mask for the RoCE v2 adapter. This can be done through the network settings in the operating system or adapter management tool.
           &lt;/li&gt;
          &lt;/ol&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Switch Configuration:
          &lt;/h2&gt;
          &lt;ol&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Choose a RoCE v2 Compatible Switch:Ensure that the network switch supports RoCE v2. When selecting a switch, it is preferable to choose a model that has RoCE v2 support and refer to the manufacturer’s documentation for detailed information.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Enable PFC (Priority Flow Control):RoCE v2 relies on PFC to ensure ordered transmission of flows. Enable PFC on the switch and configure the appropriate priorities for RoCE flows.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Configure DCB (Data Center Bridging): Configure DCB to ensure that RoCE v2 traffic receives the appropriate bandwidth and priority. Make sure to allocate sufficient bandwidth for RoCE v2 flows to meet performance requirements.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Enable ECN (Explicit Congestion Notification): RoCE v2 supports ECN, which helps with traffic control during network congestion. Enable ECN on the switch and configure it accordingly on the adapters.
           &lt;/li&gt;
          &lt;/ol&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Network Parameter Settings:
          &lt;/h2&gt;
          &lt;ol&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Configure Subnet:RoCE v2 typically operates within a subnet, so ensure that the subnet configured on the adapters and switches is consistent.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Configure MTU (Maximum Transmission Unit):RoCE v2 often requires a larger MTU for improved performance. Configure the same MTU value on the adapters and switches to maintain consistency across the network links.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Enable IPv6:RoCE v2 can utilize IPv6 for communication. If your network supports IPv6, ensure that IPv6 is enabled on the adapters and switches, and configure the addresses accordingly.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Validate the Connection:After completing the configuration, validate the RoCE v2 connection by using RDMA tools or other testing utilities. Ensure that data can be correctly transmitted over the RoCE v2 network.
           &lt;/li&gt;
          &lt;/ol&gt;
          &lt;figure&gt;
           &lt;div&gt;
            &lt;img height=&quot;419&quot; src=&quot;https://miro.medium.com/v2/1*O2Qc_8PWzz9X_eTWKhcywg.jpeg&quot; width=&quot;550&quot;/&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           2. Typical Deployment Scenarios
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 offers various deployment scenarios to meet the requirements of different applications in different network environments.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. Data Center Networks:
          &lt;/h3&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Large-scale distributed storage
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Virtualized environments
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            High-performance computing clusters
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Deploy a leaf-spine topology to ensure low latency and high throughput. Use RoCE v2 adapters and switches to build a high-performance, RDMA-enabled network.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Enable PFC and DCB on the leaf and spine switches to support ordered transmission of RoCE flows and bandwidth allocation.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Configure Jumbo Frames to support larger MTUs for improved RoCE performance.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Run RoCE v2 within different subnets to ensure network isolation and performance optimization.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Configure RoCE v2 on virtual machine hosts to enable high-performance communication between virtual machines, enhancing the performance of virtualized workloads.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. Enterprise Networks:
          &lt;/h3&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Database applications
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Large-scale file sharing
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Video streaming
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Deploy RoCE v2 in critical nodes of a traditional three-tier network structure suitable for enterprise environments, where high-performance communication is required.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Enable PFC and DCB on critical switches to ensure high-performance support for mission-critical applications.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Increase the MTU as needed to improve performance, particularly in scenarios like large-scale file sharing or video streaming.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Run RoCE v2 within different subnets to ensure network isolation for critical applications.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Configure necessary network security measures to ensure secure data transmission over RoCE v2.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3. High-Performance Computing:
          &lt;/h3&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Scientific computing
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Simulation
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Rendering
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Deploy a high-performance network topology such as Fat-Tree or Dragonfly that meets the requirements of high-performance computing clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Enable PFC and DCB in the high-performance computing network to support RDMA communication in large-scale computing clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Configure larger MTUs to enhance data transfer efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Run RoCE v2 within different subnets to enable high-performance communication between compute nodes.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Configure a distributed file system that supports RDMA to enhance file access performance.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           3. Hardware and Software Requirements
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Deployment of a RoCE v2 network involves hardware and software requirements across multiple aspects. Here is a detailed checklist, including adapter models, switch specifications, and operating system support:
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Adapter (Network Interface Card) Requirements:
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. Manufacturer and Model:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Choose RoCE v2-compatible InfiniBand and Ethernet hybrid adapters, such as Mellanox ConnectX-6, ConnectX-6 Dx, etc.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. RoCE v2 Support:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The adapter must explicitly support RoCE v2. Ensure that the chosen adapter’s driver and firmware versions are compatible with the latest RoCE v2 specifications.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3. Performance Features:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Select an adapter that suits your performance requirements, considering bandwidth, end-to-end latency, and other performance features.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           4. PCIe Compatibility:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The adapter must be compatible with the server’s PCIe slot to ensure proper installation and performance.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Switch Requirements:
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. Manufacturer and Model:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Select RoCE v2-compatible switches, such as Mellanox Spectrum switch series, Cisco Nexus 9000 series, etc.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. RoCE v2 Support:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The switch must support RoCE v2, and the documentation regarding configuration options must be clear and comprehensive.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3. PFC and DCB Support:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The switch must support Priority Flow Control (PFC) and Data Center Bridging (DCB) to ensure ordered transmission of RoCE traffic and bandwidth allocation.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           4. High-Bandwidth Ports:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Choose switches with sufficient port bandwidth to meet the demands of a high-performance network.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Operating System and Driver Requirements:
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. Operating System Support:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Ensure that the selected adapters and switches support the operating system you are using, such as Linux (especially RDMA-supported Linux distributions like RHEL, Ubuntu), Windows Server, etc.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. Driver and Firmware:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Install the latest driver and firmware versions provided by the adapter manufacturer to ensure compatibility and optimal performance.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*SrU5d4DXilqzw6EcPZ7dbA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Other Requirements:
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. MTU Settings:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Configure Jumbo Frames on the network links to support larger MTUs and improve RoCE performance.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. Subnet Partitioning:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Run RoCE v2 within different subnets to ensure network isolation and performance optimization.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3. Security Configuration:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Configure appropriate network security measures based on specific environment security requirements.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           4. Network Management Tools:
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Configuring and managing a RoCE v2 network may require the use of suitable network management tools to ensure comprehensive visibility into network status.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Note: Before deploying a RoCE v2 network, carefully review the documentation for all relevant hardware and software to ensure that the chosen devices and configurations meet the specific requirements of your environment. Additionally, perform appropriate performance testing and validation to ensure the stability and performance of the RoCE v2 network.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           4. RoCE v2 Application Cases
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1. Optimization of Data Center Networks
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The application of RoCE v2 in data center networks can significantly improve performance and efficiency, especially in virtualized environments.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           By implementing RDMA, RoCE v2 eliminates the dependency on the host CPU for data transfers between virtual machines, reducing processing overhead and improving performance. One of the design goals of RoCE v2 is to provide low latency and high throughput, which is crucial for high-performance communication between multiple virtual machines in virtualized environments. RoCE v2 supports network isolation in virtualized environments, ensuring secure communication between virtual machines, which is particularly important in multi-tenant data centers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 typically supports Jumbo Frames and is suitable for large-scale concurrent workloads, especially in data center environments where there are numerous simultaneous data transfers, improving overall performance.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In distributed computing clusters, the low latency and high throughput of RoCE v2 enable more efficient communication between nodes, which is essential for collaborative processing of distributed computing tasks. RoCE v2 is widely used to optimize distributed storage systems, accelerating storage access and improving storage performance, which is critical for large-scale data storage and processing.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2. Improving Storage System Performance
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The application of RoCE v2 in storage systems can significantly enhance storage performance and reduce access latency, especially in large-scale data storage and high-frequency read/write operations.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For example, a research institution with a large scientific dataset needs efficient data transfers and access in a distributed storage system. To improve storage performance, they adopt RoCE v2 technology. RoCE v2 compatible network adapters are selected, and it is ensured that each storage node supports RoCE v2. Common choices include adapters like Mellanox ConnectX-6. RoCE v2 compatible switches are deployed, ensuring the enablement of PFC and DCB in the storage network to support ordered transmission and bandwidth allocation for RoCE flows. Distributed storage systems supporting RDMA are deployed to enable direct memory access between storage nodes using RoCE v2, reducing CPU overhead. Configuration of MTU supporting Jumbo Frames improves packet size to reduce header overhead and enhance data transfer efficiency. Storage system optimization is performed for high-frequency read/write operations, ensuring the system fully leverages the low latency and high throughput of RoCE v2.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The low latency of RoCE v2 allows for faster communication between storage nodes, reducing data access latency, particularly evident in large-scale data storage. RoCE v2 supports high-throughput data transfers, enabling storage systems to handle more requests and improve overall storage performance. In scenarios involving large-scale data transfers, RoCE v2’s superior performance allows data to be transmitted between storage nodes more efficiently, accelerating data backup, recovery, and migration processes. For high-frequency read/write operations, RoCE v2’s performance advantages enable storage systems to respond to requests more quickly, providing better response times.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3. Applications in Ultra-Scale Clusters
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 has a significant impact on network performance and effectively supports large-scale parallel computing in ultra-scale clusters such as cloud computing environments and massively parallel computing clusters. In cloud computing environments, RoCE v2 can be used to enhance communication performance between virtual machines, especially in scenarios that require low latency and high throughput. RoCE v2 can accelerate distributed storage systems, improving storage performance in cloud environments. RoCE v2’s characteristics enable low latency and high throughput communication in cloud environments, providing higher-performance services for cloud service providers. In virtualized environments, RoCE v2 optimizes data transfers between virtual machines, reducing CPU overhead and improving overall virtualization performance. RoCE v2’s network isolation feature allows for effective support of multi-tenancy in cloud environments, ensuring secure and isolated communication between each tenant.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 can be used in High-Performance Computing (HPC) clusters to support communication requirements for large-scale distributed computing tasks. In scenarios such as scientific research and weather simulations, RoCE v2 accelerates large-scale data transfers, improving data processing efficiency. RoCE v2 supports highly concurrent communication, making it suitable for communication needs among thousands of nodes in massively parallel computing clusters. For large-scale computing tasks that require collaboration, RoCE v2’s low latency characteristics help reduce communication delays and improve overall computing efficiency. RoCE v2 can be used to optimize distributed file systems, accelerating read/write operations for large-scale data.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2’s low latency characteristics are crucial for task collaboration and data transfers in ultra-scale clusters, helping to reduce communication latency and improve response speed. RoCE v2 provides high throughput in the network, supporting fast data transfers for large-scale data, which is essential for high-performance computing and large-scale storage systems. RoCE v2 supports network isolation, ensuring secure and isolated communication among multiple tenants or tasks in ultra-scale clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 utilizes optical connectivity technology to achieve high-performance network communication over Ethernet. As a supplier of optical connectivity solutions,
           &lt;a href=&quot;https://www.naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD
           &lt;/a&gt;
           can provide high-quality and highly reliable optical modules and fiber optic products to meet the requirements of RoCE v2 deployments.Multiple successful deliveries and real-world application cases serve as the best endorsement of our quality assurance. Inquire now for more details!
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> What Is InfiniBand Network and Its Architecture?    </title>
<link>https://naddod.medium.com/what-is-infiniband-network-and-its-architecture-2faccba3117e</link>
<pubDate>Wed, 15 Oct 2025 02:51:44 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;398&quot; src=&quot;https://miro.medium.com/v2/1*ty4PBZ9lamhzg-68V2Xxcw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the rapid growth of central processing unit (CPU) computing power, high-speed interconnect networks (HSI) have become a key factor in the development of high-performance computers. HSI is a new technology proposed to improve the performance of the Peripheral Component Interface (PCI) for computer peripheral components. After years of development, the main HSIs that support high-performance computing (HPC) are currently Gigabit Ethernet and InfiniBand, with InfiniBand being the fastest-growing HSI. In this article, we will delve deeper into the InfiniBand architecture and compare it to traditional IP networks.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           What Is InfiniBand Architecture?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand is a communication link for data flow between processors and I/O devices, supporting up to 64,000 addressable devices. The InfiniBand Architecture (IBA) is an industry-standard specification that defines a point-to-point switching input/output framework, typically used for interconnecting servers, communication infrastructure, storage devices, and embedded systems.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;381&quot; src=&quot;https://miro.medium.com/v2/1*uqg40LIr_znKJ8j995b8NA.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand features universality, low latency, high bandwidth, and low management costs, making it an ideal connection network for single connection multiple data streams (clustering, communication, storage, management), with interconnected nodes reaching thousands. The smallest complete IBA unit is a subnet, and multiple subnets are connected by routers to form a large IBA network. An IBA subnet consists of end-nodes, switches, links, and subnet managers.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;343&quot; src=&quot;https://miro.medium.com/v2/1*--qh_l0gMplPnUB2xLKMQA.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand Network was applied to scenarios such as data centers, cloud computing, HPC, machine learning, and AI. The core visions are Maximum network utilization, Maximum CPU utilization, Maximum application performance.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;218&quot; src=&quot;https://miro.medium.com/v2/1*VXpPdIDT6cnJk4lv4d6VNA.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand Communication Channels
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Traditionally, applications relied on the operating system to provide them with the communication services they needed. In contrast, InfiniBand enables applications to exchange data across a network, without directly involving the operating system. This application-centric approach, is the key differentiator between InfiniBand networks and traditional networks.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;311&quot; src=&quot;https://miro.medium.com/v2/1*OXEh68XVioCmERTUiSNrOQ.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Main Components in Building InfiniBand Network
          &lt;/h2&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            InfiniBand Switches — Moves the traffic
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Subnet Manager — Manages all network activities
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Network Hosts — The clients for which the fabric is built
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Host Channel Adapters — Enable an InfiniBand connection between the Hosts and switches
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            InfiniBand to Ethernet Gateway — Allows for IP traffic exchanges between InfiniBand and Ethernet based networks
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            InfiniBand Router — Allows for inter connectivity between multiple InfiniBand subnets
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand Architecture vs TCP/IP
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The InfiniBand architecture is divided into five layers, in a similar way to the traditional TCP/IP model, though there are many differences between InfiniBand and IP networks. In distributed storage networks, the protocols we use include RoCE, Infiniband (IB), and TCP/IP. RoCE and IB belong to the RDMA (Remote Direct Memory Access) technology. Faced with IO high concurrency and low latency applications such as high-performance computing and big data analysis, the existing TCP/IP software and hardware architecture cannot meet the application requirements. This is mainly reflected in the fact that traditional TCP/IP network communication sends messages through the kernel, and this communication method has high data movement and data replication costs. RDMA technology was developed to solve the delay in server-side data processing in network transmission. RDMA technology can directly access memory data through the network interface without the intervention of the operating system kernel. This allows for high throughput and low latency network communication, making it especially suitable for use in large-scale parallel computer clusters. As commonly used network protocols in distributed storage, IB is often used in the storage front-end network of DPC scenarios, while TCP/IP is often used in business networks.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;355&quot; src=&quot;https://miro.medium.com/v2/1*W0b_YH-edPBAt-h1iU5Sfg.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand Architecture Layers
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Upper Layer
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand Message Service
           &lt;br/&gt;
           Applications are the “consumers” of the InfiniBand message service. The top layer of the InfiniBand architecture defines the methods that an application uses, to access the set of services provided by InfiniBand.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;338&quot; src=&quot;https://miro.medium.com/v2/1*0K6tTIm-xSU9AaD5DeXkCA.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Upper Layer Protocols
           &lt;/strong&gt;
           &lt;br/&gt;
           The upper layer protocols present a standard interface, easily recognizable by the application. Some of the supported upper layer protocols are:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            MPI (Message Passing Interface) — a library interface for distributed/parallel computing
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            NCCL — NVIDIA Collective Communication Library
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            RDMA Storage Protocols
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            IP over InfiniBand
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;355&quot; src=&quot;https://miro.medium.com/v2/1*bKk01ZLJNX21_5HHn5PTSA.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Transport Layer
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            TCP/IP vs. InfiniBand Transport Service
           &lt;/strong&gt;
           &lt;br/&gt;
           The InfiniBand messaging service is different than the one provided by the traditional TCP/IP which moves data from the operating system in one node, to the operating system in another node.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;423&quot; src=&quot;https://miro.medium.com/v2/1*b-TLqufFrjw7VfDivt4rGw.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand provides hardware-based transport services implemented by the network adapters, also known as HCAs or Host Channel Adapters.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;410&quot; src=&quot;https://miro.medium.com/v2/1*CyNAK1--SLAvkiXnAdKF6A.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Hardware-based Transport Layer
           &lt;/strong&gt;
           &lt;br/&gt;
           An end-to-end ‘virtual channel’ is created, connecting two applications that exist in entirely separate address spaces. Once an application has requested transport of a message, it is transmitted by the sending hardware.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;384&quot; src=&quot;https://miro.medium.com/v2/1*VL8JdoMGKVBeMUXULkNcdg.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           When the message arrives to the receiving hardware, it is delivered directly into the receiving application’s buffer.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;392&quot; src=&quot;https://miro.medium.com/v2/1*OPTSkEIWK5V0w0Q5wuXGdw.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Network Layer
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Until now we talked about the end nodes. But probably there is a bunch of networking devices connecting those nodes, such as routers and switches.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            InfiniBand routers are used to connect between different InfiniBand subnets.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            This allows network scaling, traffic isolation, and usage of common resources by multiple subnets.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            The network layer describes the protocol that allows routing of packets between different subnets.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Routers use network layer addresses called global IDs or GIDs to route packets to the destination node.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;181&quot; src=&quot;https://miro.medium.com/v2/1*9YoEIb-K5r1R3y-1Prk1tQ.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Link Layer
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            LIDs — Local IDs
           &lt;/strong&gt;
           &lt;br/&gt;
           Each node in a subnet is assigned with an address called the Local ID or LID. LIDs are assigned and maintained by the Subnet Manager that manages the subnet.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;386&quot; src=&quot;https://miro.medium.com/v2/1*t80RqiGZTHncwODnvK2NyA.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The switches’ forwarding tables are populated with entries that map destination LIDs to exit ports.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;323&quot; src=&quot;https://miro.medium.com/v2/1*cOO81aEwg7FndeYitFRZOA.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Those forwarding tables are calculated by the subnet manager and programmed in the switches’ hardware.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;315&quot; src=&quot;https://miro.medium.com/v2/1*olbR-aXuKDQEay31c4F8eQ.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Switching InfiniBand Packets
           &lt;/strong&gt;
           &lt;br/&gt;
           When a packet is generated by an end node, it includes a source LID and a destination LID. When the packet arrives to a switch its destination LID is matched against the switch entries and sent over the respective exit port.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;359&quot; src=&quot;https://miro.medium.com/v2/1*TElEf-EplSdF74XQno0I7g.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Flow Control
           &lt;/strong&gt;
           &lt;br/&gt;
           Another key element of InfiniBand is the Link Layer flow control protocol.Flow control mechanisms are used to adjust the transmission rate between a sender and a receiver so that a fast sender does not overload a slow receiver.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;151&quot; src=&quot;https://miro.medium.com/v2/1*PyaCwHPm_u828-0-HcrLrw.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Lossless Fabric
           &lt;/strong&gt;
           &lt;br/&gt;
           The sending node dynamically tracks receive buffer usage and transmits data only if there is space for it in the receiving node buffer.This is what makes InfiniBand a lossless network. Packets are not dropped in the network during normal operation.Lossless flow control leads to very efficient use of bandwidth within the data center.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;406&quot; src=&quot;https://miro.medium.com/v2/1*un4RcnOMTGAJduWXIVsxvQ.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Physical Layer
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           All those mechanisms sound great, but eventually data transfer occurs when bits are transmitted over a physical medium. The physical layer specifies how bits are placed on the wire and the signaling protocol to determine what constitutes a valid packet. In addition, the physical layer defines the characteristics and specifications for copper and optical cables. The following pics are examples of Mellanox LinkX
           &lt;a href=&quot;https://www.naddod.com/special/naddod-infiniband-products-family&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand DAC and AOC cables
           &lt;/a&gt;
           .
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;284&quot; src=&quot;https://miro.medium.com/v2/1*ZPGxq-AquFw-bo-f8otWxA.webp&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As an leading manufacturer in high-performance network, NADDOD delivers full-stack
           &lt;a href=&quot;https://www.naddod.com/solution/ai-networking/infiniband&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand solutions
           &lt;/a&gt;
           tailored for AI data centers, empowering mission-critical workloads like billion-parameter model training and real-time inference with unparalleled performance and reliability.
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us#here&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Reach out to our experts
           &lt;/a&gt;
           to start optimizing your AI network.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Introducing 800G Ethernet: A Brief Overview    </title>
<link>https://naddod.medium.com/introducing-800g-ethernet-a-brief-overview-8874f529dae8</link>
<pubDate>Sat, 11 Oct 2025 07:09:28 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*CM0vLvShXZbwW4JdY8qN-w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           1. What is 800G Ethernet?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           800G Ethernet is a high-bandwidth Ethernet standard that can transmit data at a rate of 800Gbps (gigabits per second). It represents the latest advancement in Ethernet technology, aiming to meet the increasing demands for data transmission and processing.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The 25G and 50G Ethernet Alliance standards provide technical specifications based on eight 100Gb/s lanes, enabling the adoption of advanced high-bandwidth Ethernet technology.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           800G Ethernet is primarily used in large-scale data centers, cloud service environments, and applications that require high bandwidth. It offers higher speeds, greater throughput, and improved network performance, supporting faster and more efficient data communication.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           2. 800G Ethernet Architecture
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The 800Gb/s Ethernet technology is designed as an interface that utilizes eight 106Gb/s channels, connected with a 2xClause 119 PCS (400G) to operate at 800Gb/s with a single MAC (although the 400G PCS has been modified, this is just a high-level conceptual view). The following diagram illustrates the high-level architecture.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;591&quot; src=&quot;https://miro.medium.com/v2/1*pOuINAV_RKofuadvQTjvDg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the specific implementation process, the 800GBASE-R specification is not simply the concatenation of two 400G connections. Instead, it introduces new Medium Access Control (MAC) and Physical Coding Sublayer (PCS) to achieve 800G with minimal cost. The new PCS, which includes reuse of the previous PCS, retains the standard RS (544, 514) forward error correction and provides excellent backward compatibility features.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           PCS/FEC
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The capability to support 800Gb/s is achieved by utilizing two 400Gb/s PCS (including FEC) and supporting 32 PCS lanes (each lane operating at 25Gb/s). The following diagram illustrates the TX PCS data flow and functionality. The two PCS stacks generate 2x16 PCS lanes, which are then multiplexed by the PMA to the PMD in a 4:1 bit multiplexing scheme, creating 8x106G PMD lanes.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;449&quot; src=&quot;https://miro.medium.com/v2/1*UkOT9wSUcab3YkhA597oBw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The following diagram, provided by the 800G Pluggable MSA Working Group in the “800G MSA White Paper,” presents a conceptual illustration of a fast-to-market 800G implementation. It involves reconfiguring two 400G PMAs to obtain an 800G PMA, defining a cost-effective 800G PMD, and ultimately achieving 800G Ethernet based on 8x100Gb/s channel technology.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;417&quot; src=&quot;https://miro.medium.com/v2/1*sAAwP98ybuimT5UUMuu-IA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           3. 800G Ethernet challenges and solutions
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the process of promoting and adopting 800G Ethernet technology, there are several challenges to consider, including:
          &lt;/p&gt;
          &lt;ol&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            High complexity: Implementing 800G Ethernet involves higher data rates and more complex technical requirements. Designing and developing hardware and software solutions that meet these requirements require in-depth expertise and technical knowledge.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Signal integrity: As data rates increase, signal integrity becomes critical. High-speed signals are susceptible to noise, attenuation, and crosstalk during transmission, requiring advanced signal conditioning and equalization techniques to maintain signal stability and accuracy.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Power consumption and heat dissipation: The high data rates of 800G Ethernet require more power to drive and process the signals. This can lead to power consumption and heat dissipation issues, especially in high-density data centers and network devices. Therefore, energy efficiency and heat management become crucial considerations.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Compatibility and interoperability: Ensuring compatibility and interoperability with existing Ethernet standards and devices is essential during the transition to 800G Ethernet. This involves ensuring interconnectivity between protocols, interfaces, and devices to ensure smooth network migration and interoperability.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Cost-effectiveness: The cost of deploying 800G Ethernet technology is a significant consideration. High-speed hardware and equipment may come at a higher cost, and additional investments may be required during implementation and maintenance. Therefore, finding cost-effective solutions for sustainable deployment of 800G Ethernet becomes crucial.
           &lt;/li&gt;
          &lt;/ol&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;255&quot; src=&quot;https://miro.medium.com/v2/1*LSk12VrPQDmZ8EMmauBg8g.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To address these challenges, the following solutions can be implemented:
          &lt;/p&gt;
          &lt;ol&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Technological research and innovation: Continuously engage in research and innovation to address the challenges of high complexity and signal integrity. This includes developing advanced signal conditioning and equalization techniques, improving circuit design and layout to enhance signal stability and accuracy.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Power optimization design: Adopt power-optimized design methods to reduce the power consumption of key components such as SerDes in 800G Ethernet. This includes using advanced CMOS processes and low-power circuit design techniques, such as low-power transistors, voltage regulation, and power management technologies.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Heat management: Implement effective heat management strategies, particularly in high-density data centers and network devices. This can involve using advanced cooling techniques, optimizing hardware layout and ventilation design to ensure stable operation of equipment at high data rates.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Standardization and interoperability: Ensuring compatibility and interoperability with existing Ethernet standards and devices is critical in driving the adoption and deployment of 800G Ethernet. Participate in standardization organizations and contribute to the development of unified protocol and interface standards to facilitate interoperability among devices.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Cost-effective solutions: When considering cost-effectiveness, finding economically efficient solutions is crucial. This may involve devising reasonable procurement strategies, seeking cost-effective hardware suppliers, and considering the lifecycle costs of equipment.
           &lt;/li&gt;
          &lt;/ol&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           By implementing these solutions collectively, the promotion and deployment of 800G Ethernet can be facilitated, overcoming the challenges involved. Additionally, industry collaboration and knowledge sharing are vital for driving the development of 800G Ethernet.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           4. 800G Ethernet Products
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The existing Ethernet technology is currently deploying 400G on a large scale, and there is still a long way to go before reaching the data rate of 800G. However, Ethernet transmission technology is continuously evolving and innovating. 800G multimode optical modules/AOCs/DACs are expected to continue leading the development in the networking field, providing strong support for the network requirements of the digital era. As a professional module manufacturer,
           &lt;a href=&quot;http://www.naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD
           &lt;/a&gt;
           produces optical modules ranging from 1G to 800G. We welcome everyone to learn about and purchase our products.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;174&quot; src=&quot;https://miro.medium.com/v2/1*3EAzo7t9uArR3K3VKmGGOg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> All You Want to Know about Optical Transceivers    </title>
<link>https://naddod.medium.com/all-you-want-to-know-about-optical-transceivers-fecae6092bb1</link>
<pubDate>Sat, 11 Oct 2025 06:51:54 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*RZM1bi6IhUDBdF0NxzBmcw.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           1.Definition
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;a href=&quot;https://www.naddod.com/collection/optical-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Optical transceiver
           &lt;/a&gt;
           : also known as optical integrated transceiver.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*u9RbibcRhb1uT1-NBVezrA.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           2.Structure
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Optical transceiver consists of optoelectronic devices, functional circuits and optical interfaces, etc. The optoelectronic devices include two parts: transmitting and receiving.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The transmitting part is: input a certain code rate of electrical signals through the internal driver chip processing to drive the semiconductor laser (LD) or light-emitting diode (LED) to emit the corresponding rate of modulated light signal, its internal with optical power automatic control circuit, so that the output optical signal power to maintain stability.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The receiving part is: the optical signal of a certain code rate is input to the transceiver and converted into an electrical signal by a light detecting diode. After the preamplifier output the corresponding code rate of the electrical signal, the output signal is generally PECL level. At the same time, an alarm signal is output when the input optical power is below a certain value.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           3.Optical Transceiver Parameters and Significance
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Optical transceivers have many important photoelectric technical parameters, but for GBIC and
           &lt;a href=&quot;https://www.naddod.com/collections/1g-sfp&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            SFP
           &lt;/a&gt;
           , the two hot-swappable optical transceivers, the following three parameters are the most important when choosing.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1) Central Wavelength
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The unit nanometre (nm), there are currently three main types.
           &lt;br/&gt;
           850nm (MM, multi-mode, low cost but short transmission distance, generally only 500M).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           1310nm (SM, single mode, high loss during transmission but low dispersion, generally used for transmission within 40KM).
           &lt;br/&gt;
           1550nm (SM, single mode, low loss but high dispersion during transmission, generally used for long distance transmission over 40KM, up to 120KM without relay).
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2) Transmission Rate
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The number of bits of data transmitted per second (bit), in bps.
           &lt;br/&gt;
           There are currently four commonly used rates: 155Mbps, 1.25Gbps, 2.5Gbps, 10Gbps, etc. The transmission rates are generally backward compatible, so 155M optical transceivers are also known as FE (100 Gigabit) optical transceivers and 1.25G optical transceivers are also known as GE (Gigabit) optical transceivers, which are currently the most used transceivers in optical transmission equipment. In addition, in the optical fiber storage system (SAN) it has 2Gbps, 4Gbps and 8Gbps transmission rate.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3) Transmission Distance
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The distance that an optical signal can be transmitted directly without relay amplification, in kilometres (also known as kilometres, km).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Optical transceivers are generally available in the following specifications: multimode 550m, single-mode 15km, 40km, 80km and 120km, etc.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In addition to the above 3 main technical parameters (wavelength, rate, distance), optical transceivers have the following basic concepts, which only need to be understood briefly.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           a. Laser Category
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The laser is the most central device in the optical transceiver, injecting current into the semiconductor material and emitting laser light through the photon oscillation and gain of the resonant cavity. Currently the most commonly used lasers are FP and DFB lasers, the difference between them is that the semiconductor material and resonant cavity structure is different, the price of DFB lasers is much more expensive than FP lasers. The price of DFB lasers is much more expensive than that of FP lasers. FP lasers are generally used for optical transceivers with transmission distance within 40KM; DFB lasers are generally used for optical transceivers with transmission distance ≥ 40KM.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           b.Loss and Dispersion
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Loss is the loss of light energy due to absorption, scattering and leakage of light in the optical fiber. Dispersion is caused by the fact that different wavelengths of electromagnetic waves propagate at different speeds in the same medium, resulting in different wavelength components of the optical signal reaching the receiving end at different times due to the accumulation of transmission distances, resulting in pulse spreading and the inability to distinguish signal values. These two parameters mainly affect the transmission distance of the optical transceiver, in the actual application process, 1310nm optical transceiver is generally calculated by 0.35dBm/km link loss, 1550nm optical transceiver is generally calculated by .20dBm/km link loss, dispersion value calculation is very complex, generally only for reference.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           c.Transmit Optical Power and Receive Sensitivity
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Transmit optical power refers to the optical transceiver transmitter light source output optical power, receive sensitivity refers to the minimum received optical power of the optical transceiver at a certain rate, BER case. The unit of these two parameters are dBm (meaning decibel milliwatt, the logarithmic form of the power unit mw, the calculation formula for 10lg, 1mw converted to 0dBm), mainly used to define the transmission distance of the product, different wavelengths, transmission rates and transmission distance of the optical transceiver optical transmit power and receive sensitivity will be different, as long as the transmission distance can be ensure.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           d. The Service Life of The Optical Transceiver
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           International unified standard, 7Х24 hours of uninterrupted work 50,000 hours (equivalent to 5 years).
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           e.Fibre Optic Interface
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           SFP optical transceivers are LC interface, GBIC optical transceivers are SC interface, other interfaces and FC and ST, etc.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Optical Transceiver Classification
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1.Classification by Application
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Ethernet application rate: 100Base (100 megabit), 1000Base (gigabit), 10GE.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           SDH application rate: 155M, 622M, 2.5G,
           &lt;a href=&quot;https://www.naddod.com/collections/10g-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            10G
           &lt;/a&gt;
           .
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2.Classification by Package
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           According to package classification: 1×9, SFF, SFP, GBIC, XENPAK, XFP.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           1×9 package — solder type optical transceiver, general speed is not higher than gigabit, more SC interface.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           SFF package — soldered small package optical transceiver, generally no higher than gigabit speed, mostly using LC interface.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           GBIC package — hot-swappable gigabit interface optical transceiver, using SC interface.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           SFP package — hot-swappable small package transceiver, currently up to 4G, mostly using LC interface.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           XENPAK package — for 10 Gigabit Ethernet applications, using SC interface.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           XFP package — 10G optical transceiver, available in 10 Gigabit Ethernet, SONET and other systems, mostly using LC interface.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           3.Classification by Laser
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           LED, VCSEL, FPLD, DFB L.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           4.Classification by Wavelength
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           850nm, 1310nm, 1550nm, etc.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           5.Classified by Usage
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Non-hot-swappable (1×9, SFF), hot-swappable (GBIC, SFP, XENPAK, XFP).
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Choice of Optical Transceivers
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1.The Classification of Fiber Optic Connectors and The Main Specification Parameters
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Fiber optic connectors are installed on both ends of an optical fiber connector, mainly for optical wiring use.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           According to the type of fiber: single-mode fiber optic connectors (generally G.652 fiber: fiber inner diameter 9um, outer diameter 125um), multimode fiber optic connectors.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           According to the fiber optic connector connector form: FC, SC, ST, LC, MU, MTRJ, etc., currently commonly used FC, SC, ST, LC.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           According to the fiber optic connector connector end face: PC, SPC, UPC, APC According to the fiber optic connector diameter: Φ3, Φ2, Φ0.9.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The performance of the optical fibre connector is mainly optical performance, interchangeability, mechanical performance, environmental performance and life. One of the most important is the insertion loss and return loss of these two indicators.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2.Optical Transceiver Transmitting Optical Power and Receiving Sensitivity
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Transmitting optical power refers to the light intensity at the transmitting end, and receiving sensitivity refers to the light intensity that can be detected. Both are measured in dBm, which is an important parameter affecting the transmission distance. The distance that can be transmitted by an optical transceiver is mainly limited by both loss and dispersion. The loss limit can be estimated from the formula: Loss-limited distance = (transmitted optical power — received sensitivity) / fibre attenuation. The amount of fibre attenuation is related to the actual fibre chosen. The current G.652 fibres can achieve 0.5dB/km in the 1310nm band and 0.3dB/km or better in the 1550nm band. 50um multimode fibres are 4dB/km in the 850nm band and 2dB/km in the 1310nm band. For 100GbE and Gigabit optical transceivers the dispersion limit is much greater than the loss limit and can be ignored. 10GE optical transceivers follow the 802.3ae standard, the transmission distance and the choice of fiber type, optical transceiver optical performance related.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Tech Week Singapore 2025 Recap. On October 9th, the two-day Tech Week…    </title>
<link>https://naddod.medium.com/https-www-naddod-com-blog-tech-week-singapore-2025-recap-0ad72e1a584f</link>
<pubDate>Sat, 11 Oct 2025 02:48:56 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           On October 9th, the two-day Tech Week Singapore 2025 concluded successfully. Embracing the theme “Connected Future, Infinite Impact,” the event attracted over 28,000 senior IT leaders, over 550 global exhibitors, and over 550 regional and international speakers. Throughout this intellectually stimulating event, a central theme reigned: AI is reshaping every corner of global industry with unprecedented depth and breadth.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;467&quot; src=&quot;https://miro.medium.com/v2/1*XRn3Wmk3sf2GZwtl2qZiOQ.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the acceleration of large-scale AI deployment, AI network infrastructure is undergoing unprecedented transformation. As a leading provider of core AI products and integrated solutions, NADDOD showcased its full-stack AIDC network application products, covering 1.6T/800G/400G, to global customers and partners. Let’s take a look back at NADDOD’s highlights from Tech Week Singapore 2025!
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Live from the Exhibition Floor
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The exhibition was packed with visitors, and the product demonstrations and immersive interactive experiences at the NADDOD booth attracted a large number of visitors. Data center builders, cloud computing service providers, AI enterprise representatives, and technical experts from around the world engaged in in-depth discussions with the NADDOD team, jointly exploring the future development path of AIDC networks.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;467&quot; src=&quot;https://miro.medium.com/v2/1*VgoSwXXXkWumP1tsGH4SPg.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Product and Highlights: Flexible AIDC Network Construction
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Meeting the industry’s urgent need for computing infrastructure, NADDOD showcased a series of precisely matched, cutting-edge products at the exhibition. This included a portfolio of high-speed optical modules and cables covering 1.6T, 800G, and 400G, as well as switches, ConnectX-8 SuperNIC, and other complete AIDC solutions, providing a solid physical connection layer for building next-generation AI data centers.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;390&quot; src=&quot;https://miro.medium.com/v2/1*Oz4D4U3NpiZUTgQbHj0CIw.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           1.6T/800G/400G Transceivers &amp; Cables
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD offers a high-performance, lossless optical interconnect portfolio, including a full range of 1.6T, 800G, and 400G transceivers and cables, specifically designed for AI data centers and hyperscale computing scenarios.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The comprehensive 1.6T product line, encompassing modules, DACs, and ACCs, has been pioneered in delivery and deployment, providing cost-effective, high-speed interconnect solutions for dense data centers and delivering seamless 1.6T performance. The latest
           &lt;a href=&quot;https://www.naddod.com/products/102514.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T silicon photonic transceiver
           &lt;/a&gt;
           utilizes Broadcom’s 3nm DSP and independently developed silicon photonic chips, significantly improving energy efficiency and transmission performance. The combination of cutting-edge processes and advanced SiPh technology delivers powerful FEC error correction capabilities, ensuring low bit error rates and highly reliable data transmission.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Notably, NADDOD also showcased its latest
           &lt;a href=&quot;https://www.naddod.com/products/102557.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            400G OSFP-RHS SR8
           &lt;/a&gt;
           optical module, bringing even more competitive cost advantages to efficient network architectures. For 256 nodes (8xGPUs), the use of the 400G OSFP-RHS SR8 module can reduce overall costs by approximately 19.38%. Compared to traditional 400G OSFP SR4 modules, the module cost is reduced by approximately 54%. For a more in-depth look at the 400G OSFP-RHS SR8, please refer to:
           &lt;a href=&quot;https://www.naddod.com/blog/naddod-delivers-the-first-400g-osfp-rhs-sr8-module-to-the-market&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD Delivers the First 400G OSFP-RHS SR8 Module to the Market
           &lt;/a&gt;
           .
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;330&quot; src=&quot;https://miro.medium.com/v2/1*Ei_cO6Q8yoifyOwhUSuW2w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This product series leverages ultra-high speed and ultra-low latency to achieve efficient data transmission with zero packet loss, stably carrying massive amounts of parallel traffic in large-scale AI training and inference. Leveraging advanced silicon photonics technology and a high-performance DSP architecture, these products achieve ultra-low bit error rates and stable links at full load. Specifically tailored for top-tier AI chips, they fully unleash the bandwidth potential of the underlying network and meet the stringent reliability and stability requirements of next-generation data centers.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In addition, all NADDOD optical modules and cables undergo rigorous testing to ensure a perfect balance of performance and reliability. These tests cover hardware compatibility (plug-in, reboot), software compatibility (connectivity, parameters), and performance testing (DDM, BER, and stability) to ensure stable performance in various application scenarios. 100% factory-tested on real devices, it seamlessly adapts to mainstream equipment from NVIDIA/Mellanox, Arista, Cisco, and others, ensuring truly lossless interconnection and large-scale, stable deployment in RoCE and InfiniBand networks.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;1050&quot; src=&quot;https://miro.medium.com/v2/1*AQf2ky53OMbhijcfeqHklQ.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As a provider of high-performance AIDC network solutions, NADDOD has accumulated over 20 years of OEM production experience and offers professional, comprehensive network solutions tailored to the personalized needs of customers across various industries. Contact our AI networking experts today to explore customized InfiniBand and RoCE network solutions.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Immersion Cooling Transceivers
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD’s immersion cooling transceivers (
           &lt;a href=&quot;https://www.naddod.com/products/102336.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            OSFP400I-SR4
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/products/102368.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            QSFP200I-SR4
           &lt;/a&gt;
           ) on display at the exhibition are designed specifically for liquid-cooled data centers. These products and solutions have proven stable operation in leading immersion cooling data centers worldwide and have demonstrated outstanding performance in both NVIDIA InfiniBand and RoCE deployments.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Sealed Protection:
            &lt;/strong&gt;
            Advanced sealing design prevents coolant intrusion, ensuring stable performance in immersion environments.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Reliable Verification:
            &lt;/strong&gt;
            Passed a 400-hour continuous immersion test in 60°C coolant, demonstrating excellent leak-proof and pressure-resistant performance.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Intelligent Monitoring:
            &lt;/strong&gt;
            Supports real-time monitoring and abnormality alerts for temperature, power, bit error rate, and other indicators, preventing AI cluster interruptions.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Efficient Maintenance:
            &lt;/strong&gt;
            Equipped with coolant-resistant pigtail tags, it facilitates identification and positioning, and links with the monitoring system for rapid maintenance.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;467&quot; src=&quot;https://miro.medium.com/v2/1*6Te-35VWbOoxuajxA5czuw.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For more information about the NADDOD liquid-cooled transceiver, please read:
           &lt;a href=&quot;https://www.naddod.com/blog/overview-of-naddod-immersion-liquid-cooling-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD Immersion Cooling Transceivers: Powering the Future of AI Data Centers
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           51.2T 64x800G OSFP Switch
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The
           &lt;a href=&quot;https://www.naddod.com/products/102322.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD N9500–64OC switch
           &lt;/a&gt;
           , unveiled at the exhibition, features 64 800G OSFP ports with a total switching capacity of 51.2Tbps and utilizes the Broadcom Tomahawk 5 chip architecture. This switch supports enterprise-grade SONiC systems, offers flexible network configuration, and open management capabilities, making it the core of stable AI computing.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This switch meets the interconnection needs of large-scale GPU clusters and supports the large-scale deployment of 8K GPUs. Integrated intelligent RoCE optimization features (DLB and DECN) enable more efficient traffic distribution and bandwidth utilization for high-load AI training or inference tasks. Designed for AI, cloud computing, and high-performance computing scenarios, it provides high-bandwidth, low-latency underlying interconnect support for AIDC networks.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;467&quot; src=&quot;https://miro.medium.com/v2/1*DvhC1vZZK1oCTMsriCZxnw.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA ConnectX-8 SuperNIC
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NADDOD showcased two ConnectX-8 SuperNICs: the 800G NVIDIA C8180 and the 400G NVIDIA C8240, designed for AI high-performance computing clusters, supercomputing networks, and next-generation data center architectures. These SuperNICs support advanced routing and telemetry-based congestion control for maximum network performance and peak AI workload efficiency, supporting both InfiniBand and Ethernet networks. For more information on the technology behind NVIDIA ConnectX-8, please visit:
           &lt;a href=&quot;https://www.naddod.com/blog/how-nvidia-connectx8-supernic-transforms-server-architecture-fuels-800g-1.6t-optics&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            How NVIDIA ConnectX-8 SuperNIC Became a Revolutionary NIC?
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The NVIDIA C8180 is NVIDIA’s first 800Gb/s dual-protocol SuperNIC for InfiniBand and Ethernet, supporting encryption and secure boot to ensure secure data transmission and system operation. NADDOD offers two options: the
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102722&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            900–9X81E-00EX-ST0
           &lt;/a&gt;
           , which supports SocketDirect/Multi-Host, and the
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102723&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            900–9X81E-00EX-DT0
           &lt;/a&gt;
           , which supports downstream ports (DSP). Both use an OSFP 800G interface.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;467&quot; src=&quot;https://miro.medium.com/v2/1*C2aLGP7x_duy1iPqeunIbA.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102802&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NVIDIA C8240
           &lt;/a&gt;
           utilizes a HHHL form factor, allowing it to be installed directly into compatible server slots. This network card supports the NDR IB network protocol, making it suitable for a variety of network deployment scenarios. Its dual-port QSFP112 interface design enables efficient and flexible data transmission.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;439&quot; src=&quot;https://miro.medium.com/v2/1*YK7YaIy0w8A7xnE5x48sWQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Continuous Innovation, Creating a Shared Future
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the Singapore Tech Week 2025 exhibition, NADDOD showcased its latest achievements in AIDC network interconnection. The exhibits, including 1.6T optical modules and cables, 800G high-speed connectivity solutions, and liquid cooling technologies, demonstrated the company’s continued investment in high-bandwidth, low-power interconnection.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The exhibition provided NADDOD with an opportunity to engage face-to-face with global data center, cloud service, and AI infrastructure companies, helping the team gain a clearer understanding of market trends and actual customer deployment needs. Going forward, NADDOD will continue to focus on optimizing the performance and efficiency of AIDC interconnection networks, supporting the stable expansion and sustainable development of AI computing infrastructure through ongoing technical validation and collaboration.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Understanding Broadcom Scale-up Optical Interconnect Solution    </title>
<link>https://naddod.medium.com/understanding-broadcom-scale-up-optical-interconnect-solution-905d2bbab427</link>
<pubDate>Fri, 10 Oct 2025 02:34:25 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*Dw6FpX917AEzL-Sn93pkzA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As AI clusters continue to expand, interconnect bandwidth and energy efficiency requirements are rapidly increasing. Traditional copper cables are gradually reaching bottlenecks in terms of power consumption and transmission distance, and optical interconnects are being considered a key solution. Scale-up and scale-out correspond to different cluster expansion methods and play a key role in interconnect architecture. For definitions of these two methods, please refer to
           &lt;a href=&quot;https://www.naddod.com/blog/scale-up-vs-scale-out-in-ai-infrastructure&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Understanding Scale-Up vs. Scale-Out in AI Infrastructure
           &lt;/a&gt;
           . At this year’s Infotainment Optical Communications Conference, Broadcom showcased its two key solutions for scale-up optical interconnects: VCSEL NPO and Silicon Photonics CPO.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;312&quot; src=&quot;https://miro.medium.com/v2/1*TI3Ef-6eWRUpr6jcchy47Q.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            scale up and scale out
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Optical Interconnect Development Trends
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           According to Broadcom’s forecast, the power consumption of
           &lt;a href=&quot;https://www.naddod.com/blog/development-trend-of-optical-transceivers-cpo&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Co-Packaged Optics (CPO)
           &lt;/a&gt;
           solutions is still higher than copper cables, but this trend is expected to gradually improve over the next few years. By 2028, mature CPO and VCSEL Near Package Optics (NPO) solutions are expected to outperform retimed copper interconnects, and by 2029, advanced CPO power consumption is expected to reach approximately 5 pJ/bit. Optical interconnects’ advantages in long-distance transmission will be key to supporting clusters of hundreds of GPUs (such as 512 GPU scale-up clusters).
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;525&quot; src=&quot;https://miro.medium.com/v2/1*xEGv7WLLL40DBkP3O7hcfA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            NPO
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the same time, industry trends indicate that as bandwidth demands for AI training tasks and HPC workloads rapidly increase, the value of optical interconnects in reducing system power consumption and improving signal integrity will become even more prominent.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Introduction to Broadcom’s Scale-up Solution
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           VCSEL NPO
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           VCSELs are vertical-cavity surface-emitting lasers, characterized by their simple structure, low cost, and high energy efficiency. NPO (Non-Purpose Optical Arrays) places optical components close to the computing or switching chip package, shortening signal transmission distances and reducing power consumption.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Taken together, VCSEL NPO utilizes low-cost, high-efficiency VCSEL lasers to achieve near-package optical interconnects, primarily addressing short- to medium-distance high-speed interconnect needs. Key features include:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Low power consumption: Single-channel power consumption can be as low as 1 pJ/bit, helping to reduce overall system energy consumption.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Cost advantage: Nearly comparable to copper cable solutions, making deployment within racks or for inter-rack interconnections highly cost-effective.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Bandwidth: A single NPO optical engine supports 12.8 Tbps, providing sufficient bandwidth for rack-level scale-up clusters.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Transmission distance: Supports links of approximately 30 meters, suitable for intra-rack and inter-rack interconnections.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            System heat dissipation: Low power consumption reduces system heat dissipation pressure, reducing fan and cooling costs.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;700&quot; src=&quot;https://miro.medium.com/v2/1*k_ek3AHVgLJVwcgm-NDokQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Furthermore, the VCSEL NPO optical module utilizes proven 100G VCSEL technology, offering high mass production and reliability, making it suitable for rapid deployment and medium-sized AI clusters.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           SiPh CPO
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Silicon photonics CPO is a solution that combines silicon photonics technology with chip packaging. It integrates optical components next to the chip or switch chip, transmitting signals via silicon optical waveguides. Compared to VCSEL NPO, SiPh CPO is more suitable for large-scale, long-distance interconnection and is suitable for scenarios requiring high bandwidth density. It can provide greater bandwidth within limited space while reducing system power consumption.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Transmission Capacity: A single channel can achieve 200G/lane (mass production planned for 2025), scalable to 400G/lane in the future, supporting high-speed interconnection between large-scale GPU clusters.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            High Bandwidth Density: Edge bandwidth density exceeds 2 Tbps/mm, enabling high-density interconnection within limited cabinet space.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Reliability: Failure rate is less than 0.1 FIT. Reliability testing covers temperature cycling, mechanical shock, vibration, and high-temperature life. Some high-temperature life tests have accumulated over 120,000 hours.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Transmission Distance: Supports links of approximately 2 km, suitable for interconnection within computer rooms or across cabinets in data centers.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Mass Production Plan: Broadcom plans to gradually achieve mass production of CPO and continuously optimize power consumption and performance, with the goal of reducing it to approximately 5 pJ/bit.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           SiPh CPO complements VCSEL NPO, covering diverse scenarios from short intra-rack links to long data center links. NADDOD also offers
           &lt;a href=&quot;https://www.naddod.com/products/102514.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T SiPh modules
           &lt;/a&gt;
           that enable high-speed interconnection between devices and operate stably over long distances.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*AErywZ5g95BPKK2EAZEPDg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            CPO
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Reliability and Mass Production Progress
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In terms of reliability, Broadcom continues its years of technological development in optical devices. Its VCSEL products have been proven in large-scale commercial applications and have a strong track record of long-term operation and stability. In the CPO sector, Broadcom has largely completed verification work, demonstrating its maturity for mass production.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In terms of mass production progress, VCSEL NPO, leveraging mature processes, boasts rapid scalability and can quickly meet the needs of short- and medium-distance interconnects in data centers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           CPO is at a critical stage of moving from verification to application. Its implementation presents key challenges: packaging and thermal management must balance bandwidth density while minimizing the impact of heat from electronic chips on optical components. Some solutions utilize silicon interposers to balance interconnect density and heat dissipation requirements, while also incorporating microstructure cooling and spacing optimization to mitigate thermal coupling. Striking a balance between performance, power consumption, and reliability will determine whether CPO can successfully transition to mass production. Currently, related products have entered the engineering verification and application testing phases, gradually approaching large-scale deployment.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           From a user’s perspective, the choice between VCSEL NPO and silicon photonics CPO requires a balance between compatibility, cost, reliability, and upgrade paths. Operators should consider compatibility with existing platform interfaces, initial deployment costs, and subsequent power consumption and cooling costs. Furthermore, appropriate redundancy and fault recovery mechanisms should be designed. Pilot verification is recommended at selected nodes, with operational experience accumulated before gradual expansion.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Overall, VCSEL NPO’s strengths lie in reliability and stable supply, while silicon photonics CPO is considered a key path for long-haul, high-bandwidth interconnects. The complementary nature of the two ensures the continuous evolution of Broadcom’s solutions for diverse scales and scenarios.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Broadcom’s technology roadmap demonstrates that optical innovation is becoming a key approach to breaking through the copper cable bottleneck. VCSEL NPO, with its low power consumption and low cost, meets short- to medium-distance interconnection needs. Silicon photonics CPO, on the other hand, demonstrates unique value in long-distance and high-bandwidth density, making it suitable for larger-scale cluster expansion. The two complement each other in their applications, providing AI data centers with a complete interconnect solution from within a rack to across cabinets.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In line with this trend, NADDOD has also developed a comprehensive portfolio of optical interconnect products, including
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-800g-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand XDR modules and cables
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/collections/infiniband-ndr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand NDR modules and cables
           &lt;/a&gt;
           , and
           &lt;a href=&quot;https://www.naddod.com/collections/infiniband-hdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand HDR modules and cables
           &lt;/a&gt;
           . These products are compatible with mainstream GPUs and switches, supporting scenarios ranging from short-haul direct connections to long-haul optical interconnects. With these products, users can now experience the energy efficiency and scalability benefits of optical interconnects. For more information on optical interconnect products, please visit NADDOD.com or
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            contact our experts
           &lt;/a&gt;
           .
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> NVIDIA Spectrum SN5610: Unleashing the Potential of AIDC Networking    </title>
<link>https://naddod.medium.com/nvidia-spectrum-sn5610-unleashing-the-potential-of-aidc-networking-116be21f0727</link>
<pubDate>Thu, 09 Oct 2025 01:26:31 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The rapid development of artificial intelligence (AI), particularly generative AI and the training and inference of large models (LLMs), is driving the evolution of data center architectures at an unprecedented pace. For example, in large-scale training clusters with tens of thousands of GPU nodes, the rapid iteration of GPU computing power has made the network a key bottleneck in unleashing computing power. In this context, artificial intelligence data centers (AIDC) place new demands on networks: ultra-high bandwidth, ultra-low latency, strong scalability, and higher energy efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           If the GPU is the “brain” of AI, then the network is the nervous system, whose efficiency directly impacts the overall performance of AI systems. Driven by this trend, the 800GbE Spectrum SN5610 switch was born.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Spectrum-X: NVIDIA’s Second “Money Printer”
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The data center market for Ethernet switches is undergoing dramatic changes. IDC data shows that in Q2 2025, the total revenue of the global Ethernet switch market reached $14.5 billion, a year-over-year increase of 42.1%. The data center sub-segment alone saw its quarterly revenue surge by 71.6%, becoming the core driving force.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The power behind this is the AI Scaling Law. As model parameters and training data grow exponentially, compute density, memory bandwidth, and network communication performance are under unprecedented pressure. Hyperscale enterprises and cloud service providers are rapidly deploying switches that support 800GbE and higher speeds, which is quickly pulling market demand. In Q2 2025, global 800GbE switch revenue surged 222.1% quarter-over-quarter, while 200/400GbE products grew 175.5% year-over-year, accounting for nearly half of the market share. Together, these products now account for nearly half of the market share. Furthermore, the ODM Direct model grew by 76.9%, reflecting the trend toward large-scale and customized AI infrastructure.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;511&quot; src=&quot;https://miro.medium.com/v2/1*SuT4l1jKsHR_hq5fc4ruAQ.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In terms of vendor performance, NVIDIA stands out. Its switch business revenue skyrocketed by 647% YoY to $2.3 billion, increasing its market share to 25.9% and securing the number one spot globally.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Spectrum-X was officially launched in 2023. Its initial design was to solve the latency, jitter, and scalability issues of traditional Ethernet in distributed AI training, enabling Ethernet to achieve near-InfiniBand performance while retaining its open and cost-effective advantages. The further release of Spectrum-XGS in 2025 achieved cross-regional scaling for the first time, laying the foundation for future million-GPU scale clusters.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           It can be said that AI scaling is reshaping the Ethernet switch market landscape, and the emergence of Spectrum-X is a microcosm of this transformation. The SN5610, as a representative model, is a crucial part of unleashing the potential of AIDC networks.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA Spectrum-X Series: Ethernet Evolution for AI
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To meet the complex demands of AIDC, the NVIDIA Spectrum-X series of Ethernet switches has established a development roadmap characterized by “layered collaboration and scenario adaptation,” providing comprehensive network support for AI infrastructure from within the rack to across clusters.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Core Technology: Empowering Ethernet with AI-Native Capabilities
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Traditional Ethernet is primarily designed for the service traffic of a single server. However, in AI scenarios, where thousands of GPUs communicate simultaneously, traditional networks suffer from congestion, latency, and low bandwidth utilization. The Spectrum series addresses these issues through three key technologies.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Lossless Networking and Adaptive Routing:
            &lt;/strong&gt;
            NVIDIA RoCE Adaptive Routing is an intelligent traffic scheduling mechanism operating on the Spectrum-X switch. It automatically distributes traffic to the least congested port based on the real-time load of the egress queue, achieving adaptive equalization across ECMP paths. Even if different paths cause packets to arrive out of order, the SuperNIC automatically reorders them at the RoCE transport layer, eliminating the need for application-side processing. This ultimately provides a stable and efficient network foundation for large-scale AI training.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Congestion Control:
            &lt;/strong&gt;
            Spectrum-X RoCE Congestion Control is an end-to-end technology where the Spectrum-X switch provides real-time network telemetry information for instantaneous congestion data. This telemetry-based congestion control can monitor network traffic in real time and intelligently select data transmission paths. This approach avoids data conflicts and can increase bandwidth utilization to over 95% (compared to typically around 60% for traditional Ethernet). Without this congestion control, many-to-one scenarios can lead to network backpressure, congestion spread, and even packet loss, severely degrading network and application performance.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Full-Stack Optimization:
            &lt;/strong&gt;
            End-to-end testing, tuning, and integration — from hardware to software and up to the AI framework — ensure not only the performance of individual components (switches, NICs, GPUs, etc.) but also their compatibility and collaborative efficiency. Through this process, the system maximizes bandwidth utilization, reduces latency, and boosts the training and inference efficiency of large-scale models while improving overall reliability, preventing local failures from causing global performance degradation, and ensuring seamless collaboration with mainstream AI frameworks. Verified in real-world scenarios, full-stack optimization also guarantees the system’s stability and usability during actual deployment, helping AI data centers achieve a combination of high performance, strong reliability, and easy operation.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;275&quot; src=&quot;https://miro.medium.com/v2/1*zQqBctAiRhbCv03Pkb1QIA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Product Portfolio: Covering AI Data Centers of Different Scales
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA Spectrum series has formed a complete product portfolio capable of precisely matching AI clusters of various sizes:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102423&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             &lt;strong&gt;
              Spectrum SN5400
             &lt;/strong&gt;
            &lt;/a&gt;
            &lt;strong&gt;
             (920–9N42C-00RB-7C0)：
            &lt;/strong&gt;
            400GbE high-density switch. The 2U rack supports 64 QSFP-DD ports with a total throughput of 25.6Tb/s, suitable for small-to-medium AI labs and edge AI nodes.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;310&quot; src=&quot;https://miro.medium.com/v2/1*hfm_MgCkF-n2GFLgaokC_g.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102424&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             &lt;strong&gt;
              Spectrum SN5600
             &lt;/strong&gt;
            &lt;/a&gt;
            &lt;strong&gt;
             (920–9N42F-00RI-7C0)：
            &lt;/strong&gt;
            800GbE spine switch with a total throughput of 51.2Tb/s. It supports AC power and DC bus power (SN5600D), making it suitable for core interconnection in standardized AI clusters like DGX SuperPODs.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;405&quot; src=&quot;https://miro.medium.com/v2/1*CnBrtgPAPvAJDBH5QuCcSQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;a href=&quot;https://www.naddod.com/products/102969.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             &lt;strong&gt;
              Spectrum SN5610
             &lt;/strong&gt;
            &lt;/a&gt;
            &lt;strong&gt;
             (920–9N42F-00RI-3C1)：
            &lt;/strong&gt;
            Designed for hyperscale clusters, it inherits the core performance of the SN5600 and effectively reduces power consumption through hardware optimization, making it an ideal choice for energy-efficient AIDC.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           SN5610: Unifying High Performance and Energy Efficiency
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As the efficiency benchmark for the Spectrum series, the SN5610 fully meets the long-term operational needs of hyperscale AIDC with its three core advantages: high performance, low power consumption, and strong compatibility.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           High-Performance Networking: Supporting Million-GPU Scale AI Clusters
          &lt;/h2&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             High-Density Port Design:
            &lt;/strong&gt;
            The 2U rack integrates 64 OSFP 800GbE ports and 2 SFP28 25GbE management ports, supporting multi-rate compatibility (10GbE/25GbE/100GbE/400GbE) to flexibly adapt to AI networks of different scales.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;303&quot; src=&quot;https://miro.medium.com/v2/1*QX13i8xhQVrADllhJCOysw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             2Tb/s Non-Blocking Switching:
            &lt;/strong&gt;
            Based on the Spectrum-4 ASIC, it provides full-port line-rate forwarding and a 160MB fully shared buffer, ensuring no bandwidth bottlenecks in large-scale GPU parallel communication.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             AI Optimization Features:
            &lt;/strong&gt;
            Supports the NCCL optimized communication library to enhance multi-node training efficiency. As a core component of the Spectrum-X AI Ethernet platform, it can boost Generative AI performance by up to 1.6 times compared to traditional Ethernet.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Energy Efficiency: Reducing AIDC Operational Costs
          &lt;/h2&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Low Power Design:
            &lt;/strong&gt;
            Equipped with an AMD 8-core CPU and optimized power management, its typical power consumption is only 900W, a reduction of 5%-8% compared to similar products.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Intelligent Cooling System:
            &lt;/strong&gt;
            Utilizes five “4+1” redundant fans that automatically adjust their speed, reducing unnecessary energy consumption.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Power Redundancy Assurance:
            &lt;/strong&gt;
            2+2 redundant power supplies support hot-swapping, ensuring stable operation while preventing energy waste from overload.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Open Compatibility: Seamless Integration into Existing Ethernet
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           SN5610 maintains Ethernet’s open ecosystem in terms of compatibility, helping enterprises quickly upgrade to AI networks:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Multi-OS Support:
            &lt;/strong&gt;
            Compatible with open-source
            &lt;a href=&quot;https://www.naddod.com/blog/open-source-sonic-cost-efficient-flexible-choice-for-data-center-switching&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             SONiC
            &lt;/a&gt;
            and NVIDIA Cumulus Linux, balancing flexibility with AI optimization. For how to configure NADDOD 800G AI switches with SONiC-OS, please read:
            &lt;a href=&quot;https://www.naddod.com/blog/sonic-os-configuration-naddod-ai-switches&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             Configuring NADDOD 400G/800G AI Switches with SONiC-OS: A Step-by-Step Guide
            &lt;/a&gt;
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Standard Protocol Compatibility:
            &lt;/strong&gt;
            Fully compliant with IEEE 802.3 Ethernet standards, allowing direct interoperability with existing switches and server NICs without hardware replacement.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Full-Stack Synergy:
            &lt;/strong&gt;
            Works perfectly with the NVIDIA ConnectX-8 SuperNIC and BlueField-3 DPU, supporting GPU-to-storage direct connection via RoCE to achieve integrated acceleration of compute, network, and storage.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           SN5610 is an ideal choice for NVIDIA Spectrum-X deployment, offering a diverse combination of 1 to 800GbE connectivity and industry-leading 51.2Tb/s total throughput. Beyond switches, NADDOD also provides full-stack networking solutions, including compute networking, storage networking, in-band and out-of-band networking (front-end, back-end), and one-stop service for supporting
           &lt;a href=&quot;https://www.naddod.com/collection/optical-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            optical modules
           &lt;/a&gt;
           and connectors.
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Contact us
           &lt;/a&gt;
           now to explore how our solutions can accelerate your intelligent infrastructure.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As AI models continue to expand, the network is moving from a “supporting role” to a “leading role” in the AI data center. The NVIDIA Spectrum SN5610 offers a highly effective choice for AIDC in terms of performance, energy efficiency, and compatibility. In the future, with the popularization of 1.6T interconnects and
           &lt;a href=&quot;https://www.naddod.com/blog/next-gen-data-centers-embracing-liquid-cooling&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            liquid cooling
           &lt;/a&gt;
           deployment, the SN5610 and its subsequent products will continue to play a key role in AIDC networks. For enterprises to stand out in the AI competition, building an efficient, stable, and scalable network infrastructure will be the key to success.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Silicon Photonics for AI — Balancing Cost, Power, and Reliability    </title>
<link>https://naddod.medium.com/silicon-photonics-for-ai-balancing-cost-power-and-reliability-9cc6e98ff8fd</link>
<pubDate>Mon, 29 Sep 2025 06:27:42 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*LhVD4tGGMVgysjgiBU58LA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Silicon photonics technology in AI scenarios prioritizes three core demands: low cost, low power consumption, and high reliability, aligning with NVIDIA’s requirements.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;349&quot; src=&quot;https://miro.medium.com/v2/1*1GfNPwRufPmXnkds_3WkFw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            Three core requirements of silicon photonics technology in AI
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Low Cost
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Aiming for a cost of $0.25 per Gbps, which is about a quarter of current optical module costs. The goal is to afford a high-capacity, highly integrated 3.2T silicon photonics module at the price of today’s 800G module. Broadcom’s 2023 release of a 6.4T CPO silicon photonics module shows promising advancements in reducing costs as the industry matures.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Low Power Consumption
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Achieving low power consumption remains a significant technical challenge.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*fSaECA0JcYZp4azfEsPiEg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            Stringent requirement for power consumption
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For instance, to meet the demanding power efficiency goals for AI silicon photonics modules, current 800G DR8 modules consume 13W, equating to 16 picojoules per bit (pj/bit). Reducing this to 1.5 pj/bit requires a 90% reduction in power consumption. Increasing bandwidth is one approach, but more significantly, CPO (Co-Packaged Optics) packaging can eliminate the need for DSPs, cutting power use in half without sacrificing RF performance, unlike LPO (Laser Package Optics). Replacing MZ modulators with micro-ring modulators can further reduce energy consumption.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2022, Broadcom’s analysis of power distribution in switches showed that lasers and CPO components (modulators, detectors, DRV, TIA) share power consumption evenly, with their CPO based on cascaded MZ modulators achieving 5.5 pj/bit.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;401&quot; src=&quot;https://miro.medium.com/v2/1*IK_No7A7Piost8j3o7b6Rw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           MZ modulators have VπL of about 1–2 V.cm, while micro-ring modulators reduce this to 0.5 V.cm, enhancing efficiency with a smaller footprint.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*iw0flIZGBooSkkgN_KXipQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*AL99RtmFvFcVckcCtrEamg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2023, IMEC conducted a detailed analysis of the power consumption for systems based on micro-ring modulators.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*CveiPGtzbomc6e6f781Zuw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            IMEC’s power consumption analysis of micro-ring modulator: 4pj/bit
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Ranovus uses micro-ring modulators.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*ZNmcsF_h9yaCZk1JmQHl3w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            Ranovus’ CPO power consumption trends
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Intel also employs micro-ring modulators, and in 2023, Intel’s CPO with integrated lasers demonstrated power consumption of 3 pj/bit.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*poUbTHnHjSnSryKxaVlIgg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*OjXbJPTaD2TBc6hbU5ISHQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            Intel silicon photonics integrated CPO
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           An analysis of CPO structures using micro-rings and MZ modulators shows that laser choice significantly impacts energy efficiency.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Below is the analysis of power consumption for CPO based on silicon photonics technology.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div&gt;
            &lt;img height=&quot;538&quot; src=&quot;https://miro.medium.com/v2/1*Z4ejYNYHADqbRgKwB1MI_w.png&quot; width=&quot;562&quot;/&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Controlling laser selection and thermal compensation in micro-ring modulators can further reduce power consumption.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Below is the table of power consumption for micro-ring modulators from Ranovus, IMEC, Intel, and HP.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div&gt;
            &lt;img height=&quot;627&quot; src=&quot;https://miro.medium.com/v2/1*qAVtCm-CH0SXF6dCouADwg.png&quot; width=&quot;619&quot;/&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Why does Micro-ring need thermal compensation?
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Micro-rings require thermal compensation due to resonance wavelength drift under varying bias voltages and temperatures, with drift coefficients around 0.03 nm/V for voltage and 0.1 nm/°C for temperature, necessitating precise wavelength stabilization when interfacing lasers with micro-ring modulators.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*ZRPqBsoFJERhzgEUKTen8w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The wavelength stability of micro-ring modulators can be managed by either thermal tuning or electrical tuning. NVIDIA has specified a thermal tuning precision of 0.4°C. However, thermal tuning consumes significant power, while electrical tuning offers weaker performance. HP’s approach eliminates thermal tuning but uses PN heterojunctions of silicon and GaAs instead of pure silicon, which impacts reliability.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This results in trade-offs between performance, power consumption, and reliability in micro-ring designs.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*D2OSkGY3OM6gA4IKaovGVQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            Difference between electrical tuning and thermal tuning
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Regarding thermal tuning, challenges include crosstalk and mutual thermal effects among cascaded micro-rings. To maximize wavelength alignment between micro-ring resonators and lasers, integrating lasers made of InP with silicon modulators is beneficial as their temperature drift coefficients are similar, reducing compensation difficulties.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*w5fQxOEEAyA6HMOP9UF39g.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           However, integrating lasers also introduces reliability risks, as lasers are among the most failure-prone components in optical modules.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           High Reliability
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Regarding reliability, conventional optical modules generally require a laser FIT (Failure In Time rate) of 125, meaning a 5-year lifespan with a failure rate of 0.5%. Silicon photonics modules now target 100 FIT, with a 5-year failure rate of 0.4%, which appears acceptable. However, silicon photonics involve multiple integrated channels — like Broadcom’s 6.4T CPO with 64 modulators and Intel’s CPO with 96 micro-ring modulators per chip. Each modulator requires a laser, and with a laser FIT of 100, a 10-laser array could escalate the failure rate to 1000 FIT, or 4%.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           A key challenge is managing the FIT rate as it scales with laser array size. HP addresses this by using optical frequency combs with multi-wavelength lasers, where a single laser supports multiple micro-ring modulators, balancing low power consumption and high reliability. Intel initially paired one laser with each micro-ring modulator but began developing multi-wavelength lasers in 2023, reducing the number of lasers and thereby lowering the failure rate.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Another approach, similar to Finisar’s, involves adding backup lasers. Finisar developed a VCSEL-based multimode CPO for IBM, doubling the laser count and reducing the failure rate by three orders of magnitude. Intel also evaluated the reliability of backup lasers between 2020 and 2021, showing that backup lasers can significantly reduce failure rates.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*703uCfw1j6evKxRv0hZC0w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Summary
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Reducing energy consumption in optical modules involves multiple strategies. For standard 800G DR8 modules, typical energy usage is around 16 pj/bit. One approach is to eliminate the DSP, which can cut consumption to approximately 8 pj/bit, as demonstrated by Arista. Utilizing CPO packaging with external lasers in a one-to-many configuration can further reduce consumption to around 5 pj/bit, as seen in Broadcom’s solution. Integrating micro-ring modulators can push this reduction to 3 pj/bit, a strategy employed by Intel. HP goes even further with micro-ring modulators and optical frequency comb lasers, achieving a remarkably low 1.5 pj/bit without thermal control.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To enhance reliability, strategies include using external lasers with ELSFP packaging, where one laser can drive multiple modulators, although additional MUX design is required. Integrated lasers combined with optical frequency combs and quantum dots can serve multiple micro-ring modulators without needing extra MUX units, thereby balancing efficiency. Another effective approach is incorporating backup lasers, which dramatically lower failure rates, as evidenced by various industry evaluations, including Intel’s assessments between 2020–2021.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As
           &lt;a href=&quot;https://www.naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD
           &lt;/a&gt;
           expands its portfolio to include
           &lt;a href=&quot;https://www.naddod.com/collections/silicon-photonics-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            silicon photonics module
           &lt;/a&gt;
           (1.6T/800G/400G), it continues to advance in the field of optical connectivity, offering innovative solutions that meet the growing demands of AI and high-speed data center applications.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Three Core Broadcom Optical Interconnect Technologies    </title>
<link>https://naddod.medium.com/three-core-broadcom-optical-interconnect-technologies-c02658946f39</link>
<pubDate>Mon, 29 Sep 2025 02:49:52 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*KtuC3G71oIqx0zI4a4gJwQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As AI server clusters expand, optical links become crucial for long-distance connectivity due to their scalability and cost benefits. Broadcom is at the forefront of three core optical interconnect technologies essential for large-scale AI networks. This article will explore these three technologies that Broadcom employs.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Vertical Cavity Surface Emitting Laser (VCSEL)
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           VCSELs are widely used in AI interconnects for their low power consumption and cost efficiency. Broadcom’s VCSEL technology leads the industry in performance, reliability, and market availability.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;547&quot; src=&quot;https://miro.medium.com/v2/1*BehgYZLkgtttSD3ydlfSsw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At OFC 2024, Broadcom demonstrated significant advancements with 200G/lane VCSEL, achieving transmission over 100 meters on OM3 fiber at 100 GBd PAM4 and 53.125 GBd PAM4.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;202&quot; src=&quot;https://miro.medium.com/v2/1*HTrUdeKK7lWt4MXHv9c0JA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Optical interconnect solutions like NADDOD’s InfiniBand NDR multimode modules, including the
           &lt;a href=&quot;https://www.naddod.com/products/101489.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            800G OSFP SR8
           &lt;/a&gt;
           (MMA4Z00-NS) and
           &lt;a href=&quot;https://www.naddod.com/products/101490.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            400G OSFP SR4
           &lt;/a&gt;
           (MMA4Z00-NS400), leverage Broadcom’s VCSEL and Broadcom DSP chips to ensure low BER, stable, high-speed performance without link interruptions, making them ideal for high-performance data center environments.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;326&quot; src=&quot;https://miro.medium.com/v2/1*9i-1neesUyRUc2VlT-dNcw.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Electro-Absorption Modulated Laser (EML)
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           EMLs are ideal for extending AI systems to tens or hundreds of thousands of units due to their ability to handle very high bandwidths over longer distances. Broadcom has scaled EML technology from 100G to 200G per lane and achieved mass production.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;541&quot; src=&quot;https://miro.medium.com/v2/1*PXS_UEobJUp4MDOBjGOYuA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At OFC 2023, Broadcom presented a CMBH-based EML for 800G DR4/FR4 and 1.6T applications. This EML supports 20–70°C operation with a 3dB EO-BW over 60 GHz, suitable for 200G/lane use, and demonstrates excellent performance metrics, including 7 dBm output power and TDECQ below 3.25 dB at 100Gbd over 2 km.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;302&quot; src=&quot;https://miro.medium.com/v2/1*yV3A0HJGIMBMtDCIgtFVEA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;468&quot; src=&quot;https://miro.medium.com/v2/1*WF8AdBfhnMPp5flnv6qKYg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Co-Packaged Optics (CPO)
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           CPO integrates advanced silicon photonics with ICs to address next-gen bandwidth and power challenges, providing a performance and cost advantage for future AI systems. Broadcom’s 51.2T Bailly CPO, revealed at OFC 2024, integrates optical links directly into the package, featuring 512 channels connected optically to the switch. It uses eight FR4 silicon photonics engines, each with 64 channels and a total capacity of 51.2T. The lasers are external and pluggable, simplifying replacements.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;462&quot; src=&quot;https://miro.medium.com/v2/1*mi_7JWAx8bb9fBiHAvnBLg.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Broadcom identifies several key advantages of CPO technology:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Cost Reduction: As bandwidth and component numbers grow, CPO uses silicon photonics to integrate more components onto a single chip, lowering overall costs.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Power Efficiency: By eliminating complex electronic links between ASICs and optics, CPO significantly reduces power consumption. For example, a typical 800G pluggable transceiver uses about 16W per link, while a CPO system reduces this to 5W. At 1.6T, pluggable transceivers consume around 25W, but CPO only needs 8W.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Enhanced Reliability: Pluggable transceivers typically have a 2% failure rate. CPO increases reliability by integrating more components on the chip. Broadcom’s approach involves using pluggable and easily replaceable laser components, with all other parts built on core silicon technology.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Additionally, Broadcom offers a range of high-power DFB laser diode chips with wavelengths from the O to C band, featuring a CMBH structure with output power ranging from 20 to 100mW, suitable for non-cooled applications.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> 1.6T Transceiver Market Insights：Future of AI and HPC Networking    </title>
<link>https://naddod.medium.com/1-6t-transceiver-market-insights-future-of-ai-and-hpc-networking-586ffb40971b</link>
<pubDate>Sun, 28 Sep 2025 02:48:18 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*L1VCDMWXPNZ-DWd7NoVZcw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The rapidly increasing scale and complexity of AI/ML models are driving a relentless demand for ultra-high-speed interconnects in data centers. Taking AI/ML training clusters like the NVIDIA GB200 NVL72 as an example, a single GPU’s throughput has already reached the 800Gbps level. Traditional network architectures struggle to meet such high-speed data transmission requirements. To avoid computational bottlenecks, network bandwidth must be upgraded in sync with computing power, which is accelerating the development of 1.6T high-speed interconnect products.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Drivers of 1.6T Development
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the rapid rise of large AI models and hyperscale data centers, 1.6T products are facing unprecedented growth opportunities. 1.6T optical modules, in particular, have become a key component supporting the next-generation computing infrastructure, driven by multiple factors.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Explosion of AI Computing Demand:
           &lt;/strong&gt;
           Large-scale model training and inference are rapidly increasing the computing power threshold, directly driving the demand for high-speed optical modules. For example, a single NVIDIA GB200 server requires up to 72 1.6T optical modules, creating an unprecedented market pull for the industry.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Data Center Architecture Upgrade:
           &lt;/strong&gt;
           Global cloud vendors (such as Google, Microsoft, Amazon, and Meta) are continuously increasing their investment in AI data centers. 1.6T optical modules have become critical interconnect infrastructure during 400G/800G backbone network upgrades and large-scale AI cluster deployments.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Rapid Market Size Growth:
           &lt;/strong&gt;
           The global datacom optical module market exceeded US$6.25 billion in 2023 and is projected to reach US$25.8 billion in 2029, a compound annual growth rate of 27%. The Chinese market is projected to exceed 46.5 billion yuan in 2029. Specifically, global demand for 1.6T optical modules is projected to reach 3–5 million units in 2025, with a market value exceeding US$1 billion. Some analysts even predict that this number is expected to exceed 10 million units by 2026.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           1.6T Technical Solution Introduction
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the face of stringent requirements for bandwidth and latency from AI/ML and High-Performance Computing (HPC), the 1.6T interconnect is not only an inevitable trend but also presents new technical challenges and opportunities.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Main Rate: 1.6T (1600Gbps)
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Modulation Format: 200G/lane has become the necessary choice to achieve 1.6T (8x200G).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Form Factors:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ○ OSFP-XD: Currently the absolute mainstream due to its larger size and superior heat dissipation capability. It can accommodate more complex Digital Signal Processors (DSP) and optical engines, making it better suited for the high power consumption demands of 1.6T.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ○ QSFP-DD: Faces significant challenges. At the 1.6T rate, the QSFP-DD form factor severely limits heat dissipation and power consumption. Only a few vendors are exploring it for specific short-reach scenarios, with its future prospects remaining uncertain.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Optical Technologies:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ○ DR/FR (Mid to Long Reach): Single-mode solutions where EML (Electro-absorption Modulated Laser) and SiPh (Silicon Photonics) technologies compete. EML has traditional advantages, but SiPh offers immense potential in terms of integration, cost, and power consumption, making it a key focus for competition among major manufacturers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ○ LPO/CPO: LPO (Linear-drive Pluggable Optics) and the more aggressive CPO (Co-Packaged Optics) are crucial pathways for reducing power consumption. In the 1.6T era, LPO solutions are moving from concept validation to early application, while CPO is expected to begin pilot deployment in the next 2–3 years. To delve deeper into key technologies like LPO, LRO, and CPO, you can refer to:
           &lt;a href=&quot;https://www.naddod.com/blog/development-trends-in-optical-module-technology-siph-coherent-lpo-lro-and-cpo&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Development Trends in Optical Module Technology: SiPh, Coherent, LPO, LRO, and CPO
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Introduction to the Main Types of 1.6T Optical Modules
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           After clarifying the 1.6T rate, form factors, and optical technologies, the next core question is how these technologies translate into actual product forms. Different combinations of technologies and design focuses have led to the emergence of various 1.6T optical module types, each balancing trade-offs in power consumption, thermal management, distance, and cost. Understanding these module types is essential for grasping the technical trends of 1.6T products.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;444&quot; src=&quot;https://miro.medium.com/v2/1*-v6GWcxA_Ao-WI0TJS8GyA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Product Distribution of Mainstream Optical Module Manufacturers
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Different types of 1.6T optical modules correspond to diverse application scenarios and design strategies, resulting in a diverse competitive landscape in the global optical module market. Major international manufacturers, based on their technological advantages and market positioning, have deployed distinctive modules within their 1.6T product lines, ranging from high-density solutions for short-haul, high-speed interconnects to low-power, single-mode solutions for medium- and long-haul transmission. This distribution not only reflects the manufacturers’ choice of technology path but also the maturity and development trends of the entire 1.6T optical module market.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;373&quot; src=&quot;https://miro.medium.com/v2/1*SbwOXy7UdaKpNIjSToqiKQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In addition to optical modules, cables also play a key role in the 1.6T market. High-speed fiber and copper cable solutions have a direct impact on overall system bandwidth, latency, and power consumption. Consequently, some global manufacturers are actively developing 1.6T cable product lines. These various cable products complement optical modules, providing complete interconnect solutions for diverse scenarios.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;340&quot; src=&quot;https://miro.medium.com/v2/1*wQgrr8cysXh3eB_hCmn6Jw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Product Distribution of Mainstream Optical Chip Manufacturers
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Improving the performance of optical modules and cables requires the support of upstream optical chip technology. In the 1.6T era, high-speed modulators, silicon photonics, and EML lasers are key factors in determining bandwidth, power consumption, and transmission distance. Leading global optical chip manufacturers, leveraging their respective technological advancements and production capacity, have launched chips tailored to different speeds and application scenarios, driving the rapid development of the 1.6T optical interconnect ecosystem. This comprehensive industry chain, from chips to modules to systems, reflects the continued expansion of the 1.6T market in high-performance computing and AI/ML scenarios.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;405&quot; src=&quot;https://miro.medium.com/v2/1*mLkP2F2IqXHhyzlTqg4PkA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Key 1.6T Customer Groups
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The core drivers of the 1.6T market are two key customer groups: AI data centers and supercomputer builders pursuing extreme computing power, and telecom operators focused on long-haul, high-capacity transmission. The former’s demand for high bandwidth and low latency in rack-level clusters directly drives technological upgrades in optical modules, cables, and chipsets; the latter’s focus on cross-metropolitan and even cross-submarine network interconnection capabilities creates a potential market for 1.6T coherent pluggable optical modules.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           AI computing power giants (core drivers)
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           These include OpenAI, Microsoft, Google, Meta, Amazon, Alibaba, Tencent, and ByteDance.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Meta:
           &lt;/strong&gt;
           In its public AI infrastructure plan, it explicitly stated that its next-generation AI data center will be based on a rack-scale cluster design, similar to NVIDIA’s GB200 NVL72. This will significantly increase the demand for 1.6T high-speed interconnects. Meta plans to have 350,000 NVIDIA H100s by the end of 2024, and its next-generation clusters will inevitably utilize 1.6T.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Microsoft &amp; OpenAI:
           &lt;/strong&gt;
           To support the training of ChatGPT and next-generation AI models (such as GPT-5), they are building ultra-large-scale AI supercomputers such as Stargate, with an investment of over $100 billion. Their network infrastructure will inevitably utilize cutting-edge 1.6T technology.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Google:
           &lt;/strong&gt;
           Its TPU v5p cluster already utilizes 3.6T bisectional bandwidth, and the interconnect bandwidth demand for next-generation TPU clusters will only increase, making them natural users of 1.6T.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Amazon (AWS):
           &lt;/strong&gt;
           Its self-developed Trainium2 chip claims to support 4.8Tbps of interconnection bandwidth. Its EC2 instance cluster that carries these chips requires a next-generation network, and 1.6T is the inevitable choice.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Telecom Operators (Future Potential Market)
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The needs of telecom operators differ fundamentally from those of major AI computing power manufacturers. They prioritize long distances, high capacity, and network intelligence.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Telecom Operators: NTT, China Mobile, AT&amp;T, Deutsche Telekom.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Core Requirement:
           &lt;/strong&gt;
           1.6T coherent pluggable optical modules (such as CFP2-DCO/OSFP-DCO formats). These are used for transmission equipment upgrades in metropolitan area networks (MANs), backbone networks, and submarine cables.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Deployment Cycle:
           &lt;/strong&gt;
           Telecom network deployment cycles are much longer than those of data centers. Currently, the mainstream in operator networks is single-wavelength 400G, with 800G just beginning to be deployed.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Application Scenarios:
           &lt;/strong&gt;
           East-West Data Center Computing; 5.5G/6G Backhaul; and submarine cable systems.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Future Trends and Challenges
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Development Trends
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Continuous Speed ​​Increase: The evolution of optical module technology towards higher speeds is a major trend. Following 1.6T, products such as 3.2T are already under development to meet the growing demand for data transmission.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Cost Control and Optimization: Copper cable solutions demonstrate significant cost advantages, and cost reduction has become a key factor in the development of optical modules. Manufacturers are continuously reducing costs and enhancing competitiveness while ensuring performance through technological innovation, process optimization, and supply chain management.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Product Diversification and Application Expansion: The application of optical modules continues to expand, extending beyond data centers and AI to emerging fields such as in-vehicle communications, 5G, and biophotonics. Product form factors are also becoming increasingly diverse, with innovative solutions such as integrated synaesthesia emerging.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As technical solutions become increasingly clear and the industry chain accelerates its development, 1.6T optical modules not only represent a trend towards bandwidth upgrades but will also have a profound impact on the industry landscape, upstream and downstream collaboration, and future applications.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Reshaping the Industry Landscape:
           &lt;/strong&gt;
           The development of 1.6T will reshape the industry landscape. Manufacturers with technological and mass production advantages will stand out from the competition, accelerating industry concentration.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Driving Industry Chain Development:
           &lt;/strong&gt;
           The R&amp;D and mass production of 1.6T will drive breakthroughs in upstream optical chips and devices, while also facilitating data center construction and AI applications, driving collaboration across the entire industry chain.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Future Development Outlook:
           &lt;/strong&gt;
           As the technology matures and the market expands, 1.6T is expected to achieve large-scale application in the coming years, bringing new momentum to optical communications and promoting the overall upgrade of optical technology.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Risks and Challenges
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           While recognizing the potential, we must also acknowledge the risks inherent in the development of 1.6T:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Technology iteration risk: If new technologies like CPO accelerate commercialization, they could disrupt the existing pluggable ecosystem, impacting the industry chain and investment returns. This presents the greatest risk.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Intensified market competition: The influx of vendors will create price pressure. 800G prices are declining by 5%–10% annually. While 1.6T commands a premium, this trend is inevitable as production capacity is released.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Supply chain risk: High-end optical chips (such as 200G PAM4 EML) remain dependent on imports, and geopolitical uncertainty could impact supply stability.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Customer concentration risk: Leading vendors are highly dependent on major North American customers, and changes in the international trade environment could lead to performance fluctuations.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the rapid growth in the scale of AI training and inference models, and the continued demand for bandwidth and energy efficiency in hyperscale data centers, 1.6T optical modules are becoming a key node in next-generation optical interconnects. From a technological perspective, 1.6T optical modules will primarily develop along the lines of OSFP-XD, CPO, and silicon photonics integration, achieving higher data rates, lower power consumption, and a more optimized cost structure. It is expected that over the next two to three years, as technology matures and large-scale production expands, 1.6T optical modules will rapidly be adopted in AI clusters, hyperscale cloud data centers, and high-performance computing (HPC) scenarios.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           It is foreseeable that 1.6T optical modules will not only be the natural evolution after 800G, but also the inevitable choice for high-bandwidth interconnects in the AI ​​era. Those who can strike a balance between technology, cost, and scale will be well-positioned to take the lead in future market competition. NADDOD provides high-performance
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-800g-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T optical modules and DACs
           &lt;/a&gt;
           , enabling high-speed interconnects and accelerating AI infrastructure deployment.
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Contact our AI network experts
           &lt;/a&gt;
           today to experience the exceptional performance of cutting-edge optical interconnect products!
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Rubin CPX: NVIDIA Dedicated Inference GPU, Redefining AI Acceleration    </title>
<link>https://naddod.medium.com/https-www-naddod-com-blog-rubin-cpx-nvidia-dedicated-inference-gpu-19f013a69479</link>
<pubDate>Sun, 28 Sep 2025 01:18:28 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The rapid development of artificial intelligence is driving a continuous increase in demand for computing power. In particular, as large language models (LLMs) enter the long-context phase with millions of tokens, existing general-purpose GPU architectures are increasingly revealing inefficiencies in performance and resource utilization for inference tasks. The key challenge for the evolution of AI infrastructure is how to maintain performance while reducing costs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Against this backdrop, NVIDIA introduced the Rubin CPX, a dedicated inference GPU. Unlike traditional GPUs, Rubin CPX is specifically optimized for the Prefill stage of long-context inference, aiming to resolve the mismatch between compute and bandwidth resources. Through this architectural innovation, Rubin CPX not only improves efficiency in large-scale inference tasks but also offers a new perspective on the future development of AI acceleration.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The launch of the Rubin CPX signifies that AI hardware architecture is gradually moving from “general-purpose” to “specialized,” demonstrating greater value in scenarios such as long-context inference, code generation, and multi-modal content generation.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Why Is Rubin CPX Needed?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To understand the necessity of the Rubin CPX, we must first understand the two key stages of AI inference: Prefill and Decode. For more details, you can refer to:
           &lt;a href=&quot;https://www.naddod.com/blog/understanding-the-prefill-decode-separation-technique-in-large-model-inference&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Understanding the Prefill-decode Disaggregation in LLM Inference Optimization
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Prefill Stage:
            &lt;/strong&gt;
            In this stage, the model needs to read and analyze user input (e.g., a long text, a piece of code, or a video segment) and generate the first output token. This process demands extremely high computing power, requiring the GPU to perform complex operations with high throughput. However, its reliance on memory bandwidth is relatively low. This often leaves expensive HBM memory resources idle, leading to significant waste.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Decode Stage:
            &lt;/strong&gt;
            After the first token is generated, the model enters the decode stage, generating subsequent tokens one by one. This process is memory-intensive, as it requires constantly loading previously generated tokens from the KV cache. Therefore, high-speed memory bandwidth is crucial, but the demand for computing power is relatively low.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;347&quot; src=&quot;https://miro.medium.com/v2/1*xpwh_NzR-6oggee90Lbysw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This leads to a fundamental contradiction: when the GPU is fully processing the context, its expensive high-bandwidth memory (HBM) is idle due to a different data access pattern. Conversely, when generating tokens, the powerful computing units cannot be fully utilized. This “compute” vs. “bandwidth” resource conflict prevents the system from providing optimal performance for both stages simultaneously, leading to a decrease in overall efficiency and cost-effectiveness.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           By separating the demands for computing power and memory bandwidth, the Rubin CPX can more efficiently handle high-throughput, long-context tasks and work collaboratively with general-purpose GPUs responsible for generation tasks. This clearly divided architecture not only significantly increases overall throughput and reduces latency but also greatly improves resource utilization.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In short, the Rubin CPX was created to optimize the most critical context processing part of a distributed inference architecture, thereby providing unprecedented performance for long-context, high-value AI workloads.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Rubin CPX Chip Design
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The Rubin CPX adopts a monolithic die design, avoiding the complexity of dual-die packaging. This improves yield and reduces manufacturing costs. Its core computing power reaches 30 PFLOPS (in NVFP4 precision). In Softmax-related calculations within the attention mechanism, it shows an approximately 3x improvement compared to the GB300, making it highly suitable for high-concurrency inference tasks in long-context scenarios.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Its memory configuration is 128GB of GDDR7, striking a balance between capacity and cost. Compared to expensive HBM, GDDR7 offers higher cost-effectiveness: with a 512-bit interface and 30Gbps rate, the bandwidth can reach 1.8 to 2.0 TB/s, which is fully capable of meeting the needs of millions of tokens for inference.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*mF5YeU-QCIRchIDXNzMq2g.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           From a chip specification perspective, it can be seen as an extension of the RTX 5090/6000 Pro (GB202). It is speculated to have 192 SMs and use an 8 x 64-bit GDDR7 bus width design. It is noteworthy that the cumulative computing power reaches 30 PFLOPS, and the proportion of TensorCores and SFUs per SM has significantly increased. This may mean that while low-precision computing power has been enhanced, some high-precision computing capability has been sacrificed.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;745&quot; src=&quot;https://miro.medium.com/v2/1*HTch0ukKnXfuRVJ0q-6aRw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Rubin CPX NVL144
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The new Vera Rubin architecture continues the GB300’s Oberon rack structure, with a standard rack containing 18 ComputeTrays and 9 SwitchTrays. Each ComputeTray integrates 4 Rubin GPUs (each with 2 dies), 2 Vera CPUs, and 8 CX9 network cards. NVIDIA provides two main deployment options for the Rubin CPX.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;451&quot; src=&quot;https://miro.medium.com/v2/1*t1e9u12kE0CAOwISEnUFVA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Vera Rubin CPX NVL144
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This solution adds 8 Rubin CPX cards to a standard ComputeTray. Its advantage is that the Rubin CPX connects directly to the Vera CPU via the PCIe bus, resulting in lower power consumption for KV Cache transmission. However, its main drawback is the fixed ratio of Prefill to Decode nodes, which offers poor flexibility.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;465&quot; src=&quot;https://miro.medium.com/v2/1*2CAhhXGnEfG26ZoIu6Je_w.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           VR CPX + VR NVL144 Dual Rack
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The Rubin CPX does not have to be used in isolation. Another option is a dual-rack deployment: one rack specifically houses Vera CPUs + Rubin CPX cards (without Rubin chips), while the other is a conventional NVL144 rack. This forms the Vera Rubin NVL144 CPX platform.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The advantage of this separate deployment is the ability to flexibly adjust the P-D node ratio based on actual needs. However, the drawbacks are also obvious: the number of Vera CPUs and CX9 network cards is doubled, and KV Cache transmission must go through the RDMA network, which leads to higher power consumption and cost.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*xoXviUtCsr-rxiuPrDcoaA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Rubin CPX In-Rack Topology Analysis
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           A comparison of the PCIe topology between the standard Vera Rubin NVL144 and the CPX version is as follows:
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;295&quot; src=&quot;https://miro.medium.com/v2/1*aEmMT1T-nduo-ycFLgq8Og.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The CX9 network card has not been upgraded to 1.6 Tbps as teased at GTC25, remaining at 800 Gbps, primarily fixing bugs and adding custom features on top of the CX8. Its built-in PCIe Switch still has 48 lanes.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Standard VR NVL144: The CX9 connects the CPU and Rubin via 16x PCIe. The remaining 16x can connect to NVMe drives (limited to 2 per CX9 due to front panel space) and supports GPU-Direct-RDMA (GDR) communication.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           VR CPX NVL144: Since a PCIe Gen6x16 lane needs to be reserved for the Rubin CPX, the PCIe connection on the CX9+Rubin CPX sub-card is disconnected. This means the CX9 can use GDR to the Rubin CPX, leveraging the ScaleOut network to perform Prefill calculations. However, the CX9 cannot directly connect to the Rubin via ScaleOut GDR.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The compute board topology for a “Vera Rubin CPX only” setup is as follows:
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;401&quot; src=&quot;https://miro.medium.com/v2/1*RoOnOM30oJz5DfkC8tdCaQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Prefill-Decode Strategy with Rubin CPX
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           NVIDIA defines Prefill phase as compute-bound and Decode phase as memory-bound. Rubin CPX foregoes expensive HBM and NVLink requirements, focusing solely on compute for Prefill, thereby lowering cost. Below are a few typical deployment strategies.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           ScaleOut Prefill Cluster
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The Dual-Rack solution is based on a two-rack deployment. The dedicated Rubin CPX racks form a large Prefill cluster, performing distributed Prefill calculations via the RDMA network. After the calculation is complete, the generated KV Cache is then transmitted to the VR NVL144 racks responsible for decoding via the RDMA network.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Leveraging NVLink
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Another approach is to utilize the high bandwidth of NVLink. After the Prefill calculation is finished, the tokens are transmitted to the Vera CPU and then distributed to other ComputeTrays via NVLink. This solution can reduce network traffic and leverage NVLink’s bandwidth. However, a potential issue is whether the Vera CPU’s PCIe interface can handle the massive traffic of 8x800 Gbps.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Hybrid Scheduling
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           A viable scheduling strategy is to offload only extremely long-sequence tasks to the Rubin CPX for processing. This approach needs to consider the memory footprint of the KV Cache, as the Rubin CPX only has 128GB of memory, which will limit its concurrency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In these solutions, the performance of the interconnect network becomes critical. Especially for large-scale Prefill clusters in a Dual-Rack setup, higher demands are placed on the bandwidth and stability of the RDMA network. NADDOD has collaborated with industry-leading manufacturers to conduct cross-vendor Prefill-Decode disaggregation tests.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In this process, NADDOD provides high-performance network solutions to support high-speed interconnection between the compute and storage networks. The solution uses low bit error rate
           &lt;a href=&quot;https://www.naddod.com/collections/200g-400g-or-800g-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            optical modules
           &lt;/a&gt;
           to ensure high-bandwidth transmission and stable interoperability between servers and switches, and between switches, thus ensuring efficient and smooth testing. For more information, please read:
           &lt;a href=&quot;https://www.naddod.com/blog/naddod-joins-nvidia-llm-inference-optimization-prefill-decode-disaggregation-and-kv-cache-offload&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            LLM Inference Optimization: NADDOD Joins NVIDIA and Industry Leaders to Validate Prefill-Decode Disaggregation &amp; KV Cache Offload
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Application Value of Rubin CPX
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The value of the Rubin CPX is first reflected in the field of software development. For large-scale code generation tasks, traditional GPUs may take several minutes or longer to process hundreds of thousands of lines of code. The Rubin CPX, through its powerful computing capabilities, significantly shortens latency, helping developers get results faster and thus increasing productivity.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In video generation and multi-modal AI scenarios, long video tasks require processing a large number of frame embeddings and context information, and latency is often difficult to control. The Rubin CPX’s integrated video codec allows it to efficiently handle millions of tokens of input, making long video generation and complex content creation possible.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In research and engineering, the Rubin CPX can also support complex long-sequence tasks, extending from code generation to software engineering optimization and continuous inference in scientific research, all with significant efficiency improvements. More importantly, the launch of the Rubin CPX brings a huge change in inference economics. According to NVIDIA’s estimates, a $100 million investment in Rubin CPX hardware can generate up to $5 billion in token revenue for customers, an investment efficiency increase of 30 to 50 times compared to traditional solutions.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The emergence of the Rubin CPX marks the gradual “specialization” of AI inference architecture. By distinguishing the Prefill and Decode stages at the hardware level, it achieves a more reasonable balance between computing power and bandwidth utilization. For scenarios like millions of tokens in long-context inference, code generation, and multi-modal generation, the Rubin CPX provides a new approach to improving performance and reducing costs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This trend not only drives the evolution of GPU architecture but also brings new demands and opportunities to upstream and downstream industries such as PCB, optical modules, liquid cooling, and power supplies. In the future, with the advent of more specialized chips, AI inference hardware will continue to diversify.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the same time, the interconnect network remains a key factor affecting overall performance. For different data centers and high-performance computing environments, NADDOD provides a wide range of
           &lt;a href=&quot;https://www.naddod.com/special/naddod-roce-products-family&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            RoCE
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/special/naddod-infiniband-products-family&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand
           &lt;/a&gt;
           optical interconnect solutions covering 100G/200G/400G/800G. These solutions can meet diverse network deployment needs, helping users build more efficient and scalable AI infrastructures.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Scaling AI Compute Performance with InfiniBand Networking Parallel Storage    </title>
<link>https://naddod.medium.com/scaling-ai-compute-performance-with-infiniband-networking-parallel-storage-0e3856e2d0de</link>
<pubDate>Fri, 26 Sep 2025 05:38:32 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In AI and high-performance computing, storage system bandwidth and parallel processing capabilities have become key bottlenecks that affect overall compute performance. As NVIDIA introduces its new generation of Blackwell and Rubin GPUs, regarding the NVIDIA Rubin GPU architecture and its impact on the network, learn more detail in
           &lt;a href=&quot;https://www.naddod.com/blog/vera-rubin-superchip-transformative-force-in-accelerated-ai-compute&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Vera Rubin Superchip — Transformative Force in Accelerated AI Compute
           &lt;/a&gt;
           . Their powerful computing requires a matching high-speed, highly parallel storage architecture. This article will analyze why intelligent computing centers should adopt a parallel storage solution based on an InfiniBand network. This approach fully leverages the performance of the latest GPUs and meets the needs of multi-client, high-concurrency data processing.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*NPWDiKPKnfBgRZiUdNaJmA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Deep Learning Places High Demands on Storage System
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Deep learning training is a highly iterative process. The same data often needs to be read repeatedly, which means:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Read performance is essential. Some models, such as recommendation systems, can be trained in a fraction of an epoch, requiring the system to have extremely high data access speeds. During the inference phase, I/O pressure is even greater than during training.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Write performance is also crucial. As models grow larger and training times lengthen, checkpoint files become essential. These files are often terabytes in size. While writes are infrequent, when they are, they are performed synchronously, blocking the training process. If storage performance can’t keep up, GPUs will wait in vain, and their computing power will be unused.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In summary, the GPU handles computation, while the storage and cache systems provide the power. The GPU’s immense computing power can only be fully unleashed when data is plentiful and efficiently supplied. Ideally, data is cached the first time it’s read, eliminating the need to repeatedly access the network. Generally, shared file systems use RAM as a first-level cache — reading data from the cache is orders of magnitude faster than directly pulling data from remote storage. Furthermore, GPU servers come with local NVMe storage, which can be used for data caching or as a staging point for staged data loading, making overall data processing more efficient.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Advantages of InfiniBand Parallel Storage
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Extreme bandwidth and high concurrent processing capabilities
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Doubled Bandwidth:
           &lt;/strong&gt;
           The bandwidth of a single compute node has been increased from 200Gb/s to 400Gb/s. In scenarios with concurrent access from multiple clients, the 200/400G InfiniBand network ensures that GPU servers have sufficient storage bandwidth resources, avoiding performance bottlenecks.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            High Concurrency Support:
           &lt;/strong&gt;
           The 200G InfiniBand network supports more concurrent connections, capable of handling thousands of client I/O requests simultaneously, significantly improving the throughput of parallel storage systems. For example, in large-scale AI training scenarios, multiple GPU nodes can simultaneously read data from the storage system without bandwidth contention or increased latency.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;448&quot; src=&quot;https://miro.medium.com/v2/1*j6z2uk_zxJ-miJVlQUL9-A.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Ultra-low latency and efficient data access
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            Extreme end-to-end latency:
           &lt;/strong&gt;
           The ultra-low latency of the 200G InfiniBand network enables parallel storage systems to quickly respond to client requests, reducing data access wait times. This is crucial for real-time applications such as recommendation systems and autonomous driving.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ●
           &lt;strong&gt;
            RDMA zero-copy technology:
           &lt;/strong&gt;
           By bypassing the CPU and directly accessing stored data, it reduces data movement overhead and further improves the efficiency of parallel storage systems. In scenarios where multiple clients are accessing data concurrently, RDMA technology can significantly reduce CPU load, ensuring high system performance.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*orZMIhSq2T-KHSErMAzZYw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           AI Optimization and Dynamic Load Balancing
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● In-Network Computing: Parallel storage supports a variety of caching technologies to reduce data movement and improve the efficiency of parallel storage systems. For example, in AI training scenarios, data preprocessing can be performed directly in the storage system, reducing GPU latency.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Dynamic Connection Management: The storage network utilizes a 200G InfiniBand network, ensuring high performance while dynamically adjusting connections based on load, ensuring the stability and performance of the parallel storage system in high-concurrency scenarios. For example, in scientific simulations, multiple research teams can access the storage system simultaneously without performance degradation.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Massive Scalability
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           InfiniBand’s hierarchical topology and network management mechanisms enable it to maintain stable performance in AI clusters with thousands or even tens of thousands of nodes.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Solution Design Introduction
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Device Selection
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Network Card:
           &lt;/strong&gt;
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102393&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            MCX653106A-HDAT
           &lt;/a&gt;
           /
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102601&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            MCX755106AS-HEAT
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Switch:
           &lt;/strong&gt;
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102384&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            MQM9790-NS2F
           &lt;/a&gt;
           /
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102324&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            MQM9700-NS2F
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           &lt;strong&gt;
            Connectors:
           &lt;/strong&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Leaf Server:
           &lt;a href=&quot;https://www.naddod.com/products/101509.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            MFA7U10-H010
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Leaf Spine:
           &lt;a href=&quot;https://www.naddod.com/products/nvidia-networking/102807&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            MMA4Z00-NS
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/products/101436.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            MFP7E10-N020
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Network Topology
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The leaf servers use MFA7U10-H010 400G to 2x200G AOCs, with the 200G side connecting two different servers. The leaf spine uses the 800G multi-mode MMA4Z00-NS module.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The appropriate convergence ratio can be set based on the number of storage nodes and compute nodes.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;466&quot; src=&quot;https://miro.medium.com/v2/1*TXGb9NQaqPrJ5OavACzIGg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Solution Advantages:
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Extremely low latency and ultra-high bandwidth are ideal for both frequent small I/O and high throughput in training/inference.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Using the MQM9790-NS2F switch to build a 200G HDR downlink network provides enhanced networking capabilities and lower costs.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Leaf servers are downgraded to 400G and connected to 2x200G network cards, making them compatible with NVIDIA CX6 network cards and reducing the connection failure domain (compared to 800G to 4x200G, the failure domain is reduced by half), enhancing network availability.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           ● Using 400G to 2x200G AOCs for downlink simplifies deployment, avoids end-face contamination (data center link failures), and ensures leaf server connection reliability.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;394&quot; src=&quot;https://miro.medium.com/v2/1*gLhYT8YV7RWc_8BLK9RC-g.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For NVIDIA Blackwell and Rubin GPUs, robust storage is essential to realizing their full potential. Traditional Ethernet networks struggle in large-scale AI and HPC applications, while InfiniBand-based parallel storage offers extremely high bandwidth, ultra-low latency, and robust scalability. For next-generation AI infrastructure, InfiniBand is key to sustained performance and ROI. NADDOD offers a full-stack InfiniBand product portfolio, including
           &lt;a href=&quot;https://www.naddod.com/collections/nvidia-networking/infiniband-switching&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand switches
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/collections/nvidia-networking/infiniband-adapters&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand NICs
           &lt;/a&gt;
           , and InfiniBand cables and optical modules supporting multiple speeds, from
           &lt;a href=&quot;https://www.naddod.com/collections/infiniband-hdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            HDR InfiniBand modules and cables
           &lt;/a&gt;
           to
           &lt;a href=&quot;https://www.naddod.com/collections/infiniband-ndr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NDR InfiniBand modules and cables
           &lt;/a&gt;
           ,
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-800g-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            XDR InfiniBand modules and cables
           &lt;/a&gt;
           . All products undergo rigorous NVIDIA compatibility verification, delivering high-performance, scalable, and cost-effective
           &lt;a href=&quot;https://www.naddod.com/solution/ai-networking/infiniband&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand solutions
           &lt;/a&gt;
           for enterprises, data centers, AI clusters, HPC, and more. Visit NADDOD.com or
           &lt;a href=&quot;https://www.naddod.com/about-us/contact-us&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            contact an expert
           &lt;/a&gt;
           for more information.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Quick Understanding GPU Server Network Card Configuration in AI Era    </title>
<link>https://naddod.medium.com/quick-understanding-gpu-server-network-card-configuration-in-ai-era-fc73d02fb916</link>
<pubDate>Wed, 24 Sep 2025 08:05:55 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the era of Generative AI (GenAI) and large models, our focus extends beyond the computational power of individual GPU cards to encompass the total effective computing power of GPU clusters. While the computing power of a single GPU card can be estimated based on its peak performance, such as the Nvidia A100, which boasts a peak FP16/BF16 dense compute power of 312 TFLOPS, with a single card’s effective computing power approximately ~298 TFLOPS [1, 2].
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           While we are familiar with the use of single GPU cards and individual GPU servers, we are still learning and gathering insights from practice regarding building GPU clusters, determining the scale of GPU clusters, and planning the total effective computing power. In this article, we’ll delve into discussions surrounding GPU cluster network configurations, cluster scale, and overall effective computing power, with a particular focus on the computational network plane.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;390&quot; src=&quot;https://miro.medium.com/v2/1*z9kLp_Q96eUztsPDSk6b1g.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           GPU Server Network Card Configuration
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The scale and total effective computing power of a GPU cluster largely depend on the network configuration and switch equipment used. For each Nvidia GPU server, Nvidia provides recommended GPU cluster network configurations. For example, with the DGX A100 server, the recommended network connection between servers is 200 Gbps per card (meaning each A100 card has a 200 Gbps network connection to communicate with A100 cards in other servers), and a single DGX A100 server is configured with 8 computational network cards (such as InfiniBand 200 Gbps) [1, 2].
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;895&quot; src=&quot;https://miro.medium.com/v2/1*gHguCcpbTA7jkuksddP0Fg.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           So, how is the computational network bandwidth between GPU servers determined?
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Apart from cost considerations, the computational network bandwidth between GPU servers is determined by the PCIe bandwidth supported by the GPU cards. This is because the network cards of GPU servers are connected to GPU cards via PCIe Switches (GPU ←&amp;gt; PCIe Switch ←&amp;gt; NIC). Therefore, the PCIe bandwidth limits the computational network bandwidth.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For example, in the case of the Nvidia DGX A100 server, since a single A100 card supports PCIe Gen4 with a bidirectional bandwidth of 64 GB/s and a unidirectional bandwidth of 32 GB/s, which is equivalent to 256 Gbps. Therefore, configuring a 200 Gbps network card for a single A100 card is sufficient. Thus, for the computational network, the Nvidia DGX A100 server is configured with 8 Mellanox ConnectX-6 InfiniBand network cards (Note: Mellanox ConnectX-7 can also be configured as it also supports 200 Gbps). If a 400 Gbps network card is configured for A100 cards, it would be underutilized due to the PCIe Gen4 bandwidth limitation (resulting in wastage of network card bandwidth).
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;319&quot; src=&quot;https://miro.medium.com/v2/1*yF-o7jrooOYnQ8r0FV7fXQ.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For the Nvidia DGX H100 server, since a single H100 card supports PCIe Gen5 with a bidirectional bandwidth of 128 GB/s and a unidirectional bandwidth of 64 GB/s, which is equivalent to 512 Gbps. Therefore, configuring a 400 Gbps computational network card for a single H100 card is Nvidia’s recommended standard configuration. In terms of the computational network, the Nvidia DGX H100 server is configured with 8 Mellanox ConnectX-7 InfiniBand network cards, providing each H100 card with a 400 Gbps external network connection.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;374&quot; src=&quot;https://miro.medium.com/v2/1*_-98isqJayDlcrnCtyUO0A.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           It is important to note that for the computational network configuration of A800 and H800 servers, the standard configurations recommended by Nvidia DGX are generally not used in China. For example, for A800 servers, there are two common computational network card configurations: the first is 8 x 200 GbE, where each A800 card has a separate 200 GbE network card configuration (resulting in a total of ~1.6 Tbps RoCEv2 computational network connection for 8 A800 cards); the second is 4 x 200 GbE, where every two A800 cards share one 200 GbE network card, with a maximum of 200 GbE network per card and an average of 100 GbE external connection per A800 card. The second method is similar to the design of Nvidia DGX V100. Considering that communication aggregation can be done within A800 servers before communicating with other servers, the impact of these two computational network card configurations on the overall cluster efficiency is generally consistent.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           H800 supports PCIe Gen5, and for H800 servers, the common computational network card configuration is 8 x 400GbE, where each H800 card has a separate 400 GbE network card configuration, providing each H800 card with a 400 GbE external computational network connection, resulting in a total of ~3.2 Tbps RoCEv2 computational network connection for 8 H800 cards.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           While Nvidia has implemented high-speed interconnection between multiple GPUs within a single server using NVLink and NVSwitch, when multiple servers are used to build a cluster, PCIe bandwidth remains the primary performance bottleneck (cluster network bottleneck). This is because the connection between network cards and GPU cards still mainly relies on PCIe Switches. With the widespread adoption of PCIe Gen6 (standard released in 2022) and even PCIe Gen7 (expected standard release in 2025) in the future, the overall performance of GPU clusters will reach a new level. The Nvidia H20, set to be released in 2024, also supports PCIe Gen5.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Here, I would also like to introducing NADDOD, a leading provider of cutting-edge optical communication solutions. Our comprehensive range of offerings includes:
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;a href=&quot;https://www.naddod.com/collections/800g-infiniband-ndr-osfp-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             Up to 800Gbps Optical Transceivers:
            &lt;/a&gt;
            Our transceivers are compatible with a wide range of networking equipment from top brands like Cisco, HP, Brocade, Juniper, H3C, Huawei, Arista, and more. We also offer custom solutions tailored to your specific needs.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;a href=&quot;https://www.naddod.com/collections/800g-infiniband-ndr-osfp-dac&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
             Up to 800Gbps Direct Attach Cables (DAC) and Active Optical Cables (AOC)
            &lt;/a&gt;
            :Our cables come in various lengths, including 0.5m, 1m, 2m, 3m, and 5m, with custom lengths available upon request.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Wave Division Multiplexers (WDM): Choose from our range of WDM solutions, including OADM, FWDM, CWDM, DWDM, and CEx WDM, supporting up to 48 channels in 100GHz grid and 96 channels in 50GHz grid configurations.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Fiber Patch Cords and Pigtails:We offer a variety of fiber patch cords and pigtails with different fiber types (OS2, OM1, OM2, OM3, OM4, OM5) and connectors (LC, SC, FC, ST, MPO, MU, MTRJ) to suit your networking requirements.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            FBT/PLC Splitters, Fiber Optic Connectors/Adaptors/Attenuators: Our range includes splitters, connectors, adaptors, and attenuators to facilitate efficient fiber optic connectivity.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Ethernet Switches and POE Solutions: We provide Ethernet switches, POE switches, injectors, splitters, and extenders to support your networking infrastructure needs.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;339&quot; src=&quot;https://miro.medium.com/v2/1*vTXyCA91xWcSwVtnb2rC5A.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           All our products are rigorously tested and certified to meet CE, FCC, and RoHS standards, ensuring high quality and compliance. With flexible minimum order quantities (MOQ), reliable performance, competitive pricing, and fast delivery, we offer the perfect solution for your optical communication needs. Partner with us for OEM/ODM cooperation and unlock even more possibilities. Get in touch with us today to learn more about how
           &lt;a href=&quot;https://www.naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD
           &lt;/a&gt;
           can elevate your network infrastructure.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Advantages and Working Principle of RoCE v2 in RDMA Protocol    </title>
<link>https://naddod.medium.com/advantages-and-working-principle-of-roce-v2-in-rdma-protocol-73daae6eceaa</link>
<pubDate>Wed, 24 Sep 2025 07:53:05 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE (RDMA over Converged Ethernet) is an Ethernet-based remote direct memory access protocol designed to achieve high-performance, low-latency data transfer over Ethernet networks. The early versions of RoCE (RoCE v1) had certain limitations, but with the advancement of technology, RoCE v2 emerged to address those shortcomings.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Remote Direct Memory Access (RDMA) is a data transfer mechanism that allows data to be transferred between the memory of one computer to the memory of another computer without involving the host CPU. This approach reduces the overhead of the traditional TCP/IP protocol stack and improves data transfer efficiency. As an RDMA-based protocol, RoCE v2 inherits the advantages of RDMA and optimizes it for Ethernet.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;460&quot; src=&quot;https://miro.medium.com/v2/1*eBlhw71Z5KjAWPdmdNFV1Q.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Basic features of RoCE v2
          &lt;/h2&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            IPv4 and IPv6 Support: RoCE v2 not only supports IPv4 but also incorporates IPv6 support in its protocol design, making it more suitable for the future trends in networking.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Multi-Queue Support: RoCE v2 introduces multi-queue support, enabling the network to handle concurrent requests more effectively, thereby improving network throughput and concurrency performance.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Hardware Independence: RoCE v2 exhibits hardware independence, meaning it can be implemented on Ethernet adapters and switches from different vendors, adapting well to diverse hardware environments.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Network Layer Optimization: RoCE v2 optimizes the network layer, enhancing network stability and performance. It excels in high-load, low-latency application scenarios, such as data centers and high-performance computing.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Hardware Acceleration:Leveraging hardware acceleration techniques, RoCE v2 further improves data transfer efficiency. Hardware acceleration can be implemented on adapters and switches, reducing the burden on the host CPU and lowering transmission latency.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Working Principle of RoCE v2
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Remote Direct Memory Access (RDMA) allows the direct reading and writing of memory in one computer system from another computer system, without involving the host CPU. RDMA reduces the complexity and latency of data transfer by bypassing the traditional TCP/IP protocol stack.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 typically resides at the transport layer of the protocol stack and is built directly on top of the Internet Protocol (IP). Specifically, RoCE v2 utilizes the User Datagram Protocol (UDP) as the transport layer protocol to encapsulate the RDMA protocol, enabling high-performance data transfer over Ethernet networks. The placement of RoCE v2 in the protocol stack allows it to leverage RDMA technology for direct memory access, thereby achieving efficient data transfer.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The core idea of RDMA is to bypass the host CPU, allowing the remote system to directly read and write local memory, thereby reducing the latency of data transfer and the processing overhead on the host. The key aspects of implementing RDMA over Ethernet with RoCE v2 include:
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 requires the use of adapters (network interface cards) and switches that support RDMA functionality. These hardware components possess the capability to handle RDMA requests, enabling efficient memory access within the network.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 Data Transfer Process
          &lt;/h2&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Connection Establishment: RoCE v2 utilizes a control path for establishing connections. The two communicating endpoints exchange control information to establish an RDMA connection, including configuration details of the adapters and switches.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Data Transfer: Once the connection is established, the data transfer phase begins. On the data path, RoCE v2 leverages RDMA capabilities directly, bypassing the host CPU, to copy data from the source memory to the destination memory, enabling zero-copy data transfer.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Connection Termination: After the data transfer is completed, a disconnect request is sent through the control path to terminate the RDMA connection.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;367&quot; src=&quot;https://miro.medium.com/v2/1*eAyUdNSl_ssA62yagdwEgA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The control path is primarily responsible for establishing and managing RDMA connections, including connection initialization, handshake process, and error handling. The control path uses the UDP protocol and carries RDMA-related information in the UDP header to perform these tasks. This process typically involves the collaboration of adapters, switches, and operating systems.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The data path is used for actual data transfer and is crucial for achieving high performance in RoCE v2. In the data path, RoCE v2 utilizes adapter hardware offloading techniques to transfer data directly from the source memory to the destination memory, without the need for intermediate buffering. This reduces the multi-layer copying and processing overheads in the traditional TCP/IP protocol stack. Such hardware offloading mechanism significantly lowers transmission latency and improves data transfer efficiency.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Comparison between RoCE v2 and Traditional Networks
          &lt;/h2&gt;
          &lt;ol&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            Performance Advantage
           &lt;/li&gt;
          &lt;/ol&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 provides lower latency and higher throughput compared to traditional networks, making it an ideal choice for performance-sensitive applications such as high-performance computing and big data analytics.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           2. RDMA Support
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 is a protocol that supports RDMA, enabling direct data transfer between memories without involving the host CPU. This reduces the processing overhead of data transfer and improves efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           3. Protocol Stack
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 uses the UDP/IP protocol stack, while traditional networks use the TCP/IP protocol stack. Since RoCE v2 operates directly on Ethernet, it avoids some of the overhead associated with the TCP/IP protocol, resulting in improved performance.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           4. Applicability
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 is primarily used in data center environments, particularly for applications that require low latency and high throughput. Traditional networks are more widely used in enterprise, internet, and general communication settings.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           5. Use Cases
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           RoCE v2 is commonly employed for communication between high-performance computing, storage, and networking devices within large-scale data centers, while traditional networks are more suitable for general enterprise networks and the internet.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           6. Configuration and Management
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Deploying RoCE v2 may require specialized network hardware and configurations, while traditional networks are typically easier to configure and manage due to their use of widely adopted standard protocols.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           7. Compatibility
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Traditional networks have broader compatibility as they utilize the standard TCP/IP protocol stack, while RoCE v2 requires specific hardware and protocol support.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           When applying RoCE v2 in data center networks, it is essential to address the requirements for lossless network transmission and also focus on fine-grained operations and maintenance to meet the demands of latency-sensitive and packet loss-sensitive network environments. Additionally, there are some deployment challenges in RDMA networks, such as PFC storms, deadlock issues, and complex ECN threshold design in multi-tier networks.
           &lt;a href=&quot;https://www.naddod.com/&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD
           &lt;/a&gt;
           ’s experts have conducted research and accumulated knowledge on these issues and look forward to discussing them further with the community.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> 3nm DSP Chip: Driving 1.6T Connectivity in AI Data Centers    </title>
<link>https://naddod.medium.com/https-www-naddod-com-blog-3nm-dsp-chip-driving-1-6t-connectivity-in-ai-data-centers-887330b50531</link>
<pubDate>Fri, 19 Sep 2025 08:56:52 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As workloads in AI, machine learning, and cloud computing surge, the demand for bandwidth and interconnect performance in data centers is escalating. Optical transceivers, the critical link between servers, switches, and storage, have seen their performance become a bottleneck for expansion and efficiency.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           While 5nm DSP (Digital Signal Processor) chips are widely used in current optical modules, their limitations become apparent when tackling 1.6T (1.6 Tbps) speeds and stringent power consumption constraints. This is the precise reason for the emergence of 3nm DSP chips. This isn’t just a manufacturing breakthrough; it’s the core foundation for next-generation optical modules and interconnect architectures.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Path of Chip Process Advancement: The Leap from 5nm to 3nm
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In recent years, the process technology of optical module DSP chips has undergone rapid iterations, from 26nm, to 7nm and 5nm, and now to 3nm. Each upgrade has brought a qualitative leap in optical module performance. According to TSMC’s technical data, the 3nm process offers approximately 10–15% performance improvements and 25–30% power reduction compared to the 5nm process.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;420&quot; src=&quot;https://miro.medium.com/v2/1*ZCY1GfCl6uvV2P_mDt6vDA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In the data center scenario, this technological advancement is of great significance. With the leap in bandwidth from 400G to 800G and then to 1.6T, optical modules present unprecedented challenges for signal processing, energy efficiency, and the thermal design of DSP chips. The introduction of the 3nm process enables higher chip integration and lower power consumption within a smaller area, laying the technical foundation for the large-scale deployment of 1.6T optical modules.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           By 2025, multiple optical module manufacturers have launched 1.6T modules equipped with 3nm DSP chips. This leap not only improves product performance but also redefines the future possibilities of AI data center network architecture.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Why do optical modules require 3nm chips?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           AI data centers are placing increasingly stringent demands on optical modules: higher bandwidth, lower power consumption, and smaller size. The emergence of 3nm DSP chips precisely meets these requirements. The DSP (digital signal processor) chip in an optical module is responsible for encoding, decoding, and error correction of transmitted signals, and is a key component that impacts optical module performance, power consumption, and cost.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           High-Speed Requirements
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           1.6T optical modules typically use high-speed 200G/lane channels, doubling the signal rate compared to 800G modules with 100G/lane. Higher speeds mean more severe signal attenuation, crosstalk, and noise, requiring DSP chips with stronger equalization and error correction capabilities. The 3nm DSP has significantly improved computing power in modulation and demodulation, clock recovery, forward error correction (FEC), etc., and can meet the stringent requirements of high-speed
           &lt;a href=&quot;https://www.naddod.com/blog/how-is-pam4-transforming-optical-networking&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            PAM4
           &lt;/a&gt;
           signal transmission.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Power Consumption and Heat Dissipation
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The power consumption and heat generation of 5nm DSP pose higher challenges to the design of 1.6T optical modules. In contrast, using 3nm DSP chips effectively reduces power consumption by approximately 30% while providing comparable performance. This allows 1.6T optical modules to operate within reasonable thermal design limits, ensuring acceptable PUE (power usage effectiveness) for the data center.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           High Reliability Requirements
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           AI and HPC tasks place extremely high demands on reliability. Large-scale model training often requires weeks or even months of continuous operation, and any link instability results in significant waste of computing power. The 3nm DSP chip not only improves FEC efficiency, but also maintains a low bit error rate (BER) during long-term operation, ensuring the continuity and stability of data transmission, which is crucial for the efficient operation of AI clusters. 3nm chips are no longer just an “option”, but almost a “necessity”.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Advantages of 1.6T 3nm Chips
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As AI data centers move towards the 1.6T era, the emergence of 3nm DSP chips is a key driving force. Compared to the previous generation 5nm DSP, it not only offers significant advantages in bandwidth and power consumption, but also provides a solid guarantee for interconnect quality and stability in large-scale parallel computing environments.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Lower Bit Error Rate
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           At the ultra-high 1.6T data rate, signal integrity faces severe challenges, placing higher demands on technologies such as equalization, forward error correction (FEC), and clock recovery. The 3nm DSP chip, with its powerful signal processing capabilities, effectively ensures signal integrity at high symbol rates, significantly reducing bit error rates.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;563&quot; src=&quot;https://miro.medium.com/v2/1*QpyqEWGyj0N_n7KYCymcgA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           According to actual test data, NADDOD 1.6T OSFP DR8 optical module, using a 3nm DSP, achieved a bit error rate of 1E-10 to 1E-11, demonstrating an ultra-low bit error rate. Currently, NADDOD is actively preparing and verifying Broadcom’s 3nm solution, which is expected to have better performance. This outstanding performance provides reliable support for long-distance data transmission and complex AI clusters. For more details, see:
           &lt;a href=&quot;https://www.naddod.com/blog/naddod-1-6t-transceiver-full-interoperability-with-nvidia-mms4a00&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD 1.6T OSFP224 Transceivers Achieve Full Interoperability in Connectivity Tests with NVIDIA MMS4A00
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Significant Improvement in Energy Efficiency
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Compared to the 5nm process, the 3nm process can reduce power consumption by approximately 30–35% while maintaining equivalent performance. This improvement is directly reflected in the overall power consumption of optical modules.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Take
           &lt;a href=&quot;https://www.naddod.com/products/102514.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            NADDOD 1.6T OSFP DR8
           &lt;/a&gt;
           optical module as an example. This module utilizes Broadcom’s 3nm DSP and its own independently developed silicon photonics chip, keeping power consumption below 25W, approximately 7W lower than comparable 5nm solutions. This power consumption reduction not only reduces the operating costs of the data center, but more importantly, it alleviates the heat dissipation pressure during high-density deployment, making it possible to miniaturize and increase the density of equipment.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;258&quot; src=&quot;https://miro.medium.com/v2/1*jC5pJDOZ9wgBBAVMhnnUZQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Imagine that there are dozens or even hundreds of 1.6T modules in a switch chassis. If each module could save 5–10W of power, the power consumption and heat dissipation burden of the entire system would be significantly reduced, significantly lowering operating costs.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Higher Integration and Miniaturized Design
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The 3nm process enables the integration of more functional modules within the same chip area. This leaves more space for optical engines, lasers, and drivers. It helps module manufacturers integrate more complex electro-optical architectures in OSFP packaging. At the same time, 3nm chips have greater potential for coupling with
           &lt;a href=&quot;https://www.naddod.com/blog/what-is-silicon-photonics&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            silicon photonics
           &lt;/a&gt;
           , laying the foundation for larger-scale on-chip optical interconnection in the future.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Future-oriented Scalability
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           1.6T is not the end of the road for data centers. The industry is already exploring the possibility of 3.2T optical modules, which build on the evolutionary path from 200G/lane to 400G/lane.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The 3nm DSP chip design leaves room for future speed upgrades, such as faster SerDes channels, more advanced modulation methods, and lower-latency FEC. This means that 1.6T modules using 3nm DSPs can not only meet current AI training needs, but also enable a smooth transition to next-generation hyperscale cluster networks.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The emergence of 3nm DSPs is not only a natural evolution of chip manufacturing processes, but also marks a new era in data center optical interconnects. Compared to 5nm, 3nm offers significant advantages in power consumption, signal integrity, and bandwidth density, making 1.6T optical modules a viable solution.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With the increasing demand for interconnects in AI, HPC, and hyperscale data centers, 3nm DSPs will become the core driver of future optical modules and lay the foundation for network architectures with higher bandwidth and lower energy consumption. NADDOD also offers a full range of products, including
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-800g-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand XDR
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/collections/infiniband-ndr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            InfiniBand NDR
           &lt;/a&gt;
           , to meet the needs of clusters of varying sizes and diverse network architectures, ensuring the feasibility of customer network solutions and the consistency of cluster deployments.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> OSFP vs. OSFP-XD: Choosing the Right 1.6T Transceiver Form Factor    </title>
<link>https://naddod.medium.com/osfp-vs-osfp-xd-choosing-the-right-1-6t-transceiver-form-factor-baee19156e67</link>
<pubDate>Fri, 19 Sep 2025 02:43:04 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*3W_9zle-z9uG394yzr_oRw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Driven by the growing demand for AI, high-performance computing (HPC), and big data processing, data centers and communications networks are facing unprecedented bandwidth pressure. Against this backdrop, 1.6T optical modules have rapidly become a trend in the communications industry, representing the core technology for next-generation high-speed data transmission. The packaging form of optical modules directly impacts their size, heat dissipation, electrical connectivity, and optical performance.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Among the various 1.6T optical module packaging standards, OSFP (Octal Small Form-Factor Pluggable) and OSFP-XD (eXtended Density) are two key technology options. Each has its own design focus, aiming to meet the differentiated performance, power consumption, and density requirements of various application scenarios. Understanding the characteristics of these two packaging options is key to choosing the one that best suits your network architecture.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Introduction of 1.6T OSFP
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           OSFP is a compact form factor specification designed for high-speed optical modules. It connects to host devices through standardized interfaces, defining the layout for power, control, and high-speed signal channels to ensure device compatibility. The OSFP form factor also specifies uniform standards for dimensions, thermal management, and hot-pluggability, enabling high-density deployment as well as simplified module replacement and maintenance.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The OSFP specification has been widely adopted in
           &lt;a href=&quot;https://www.naddod.com/collections/400g-osfp-modules&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            400G OSFP transceivers
           &lt;/a&gt;
           and
           &lt;a href=&quot;https://www.naddod.com/collections/800g-qsfp-dd-or-osfp&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            800G OSFP transceivers
           &lt;/a&gt;
           . In 2022, the OSFP MSA introduced the OSFP1600 specification (also referred to as 1.6T OSFP or OSFP 224G). This standard is fully backward compatible with existing 400G/800G OSFP modules and delivers 1.6T total bandwidth via eight 200G host electrical interfaces. NADDOD provides high-quality
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-transceivers-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T OSFP optical transceivers
           &lt;/a&gt;
           , offering customers reliable options and flexible deployment paths when building high-speed networks and AI clusters.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The 1.6T OSFP module integrates an advanced heat sink design to effectively dissipate the heat generated by high-speed signal transmission, while also improving electrical and mechanical reliability.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;700&quot; src=&quot;https://miro.medium.com/v2/1*ZJVX_2YuuUufWuRKVNqIpg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Introduction of 1.6T OSFP-XD
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           OSFP-XD (OSFP eXtended Density) is a package standard that expands the number of channels based on the existing OSFP. By increasing the number of channels from 8 to 16, it achieves a single-port 1.6T rate and provides space for subsequent upgrades to higher rates (such as 3.2T).
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The design goals of OSFP-XD focus on high-power compatibility, copper cable support, and optimized port density. Compared to OSFP, it supports higher power requirements, ensuring heat dissipation and energy efficiency for future higher-speed optical modules. Furthermore, OSFP-XD is compatible with passive copper cable interconnects, meeting the needs of short-reach connections and diverse deployments. In terms of port density, it significantly increases system capacity within limited rack space, enabling data centers to expand with more ports and higher bandwidth without increasing floor space.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           With these features, OSFP-XD is ideally suited for applications requiring long-term expansion plans, including AI training clusters, HPC, and hyperscale data centers. As a future-oriented packaging solution, it not only addresses the current 1.6T bandwidth requirement but also lays the foundation for the development of 3.2T and higher speeds.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;330&quot; src=&quot;https://miro.medium.com/v2/1*-CP2oEoDd-7zhMeuLrZ8vQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Comparison between OSFP and OSFP-XD
          &lt;/h2&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Appearance
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The OSFP-XD package increases the number of channels in the traditional OSFP module, achieving higher port density and bandwidth expansion potential, while slightly extending its length. This design doubles the number of high-speed electrical signals within the module, significantly improving data transmission capacity.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;202&quot; src=&quot;https://miro.medium.com/v2/1*oacCBEKI7aSWt5Sqdd3M9g.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Thermal Design and Heat Dissipation
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The OSFP form factor meets the thermal design requirements of 1.6T optical modules using a standard heat sink and is suitable for optical devices with standard power consumption. The OSFP-XD structure allows for greater headroom for higher power consumption, supporting up to 40W of heat dissipation, thus meeting the requirements of future 1600-ZR and 3200G modules.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Channels and Bandwidth
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           OSFP is based on 8 channels and achieves 1.6T bandwidth at a 200G channel rate. OSFP-XD, with 16 channels, achieves 1.6T bandwidth at current 100G channels, leaving room for future upgrades to 3.2T bandwidth at 200G channels. This difference provides OSFP-XD with greater flexibility in channel expansion.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Power Consumption
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           OSFP modules typically operate in the lower power range, maintaining power density consistent with 400G and 800G applications. OSFP-XD supports higher power consumption levels and enables higher port density within limited space, improving overall system power efficiency.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           Signal Integrity
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Both OSFP and OSFP-XD form factors must meet the signal integrity requirements of high-speed SerDes. OSFP accommodates 8-lane configurations, with insertion loss and return loss specifications consistent with 400G/800G. OSFP-XD with its 16-lane layout, increases the need for high-frequency signal path optimization, requiring further improvements in PCB design and connector precision to ensure link stability.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           How Should Enterprises Choose Between OSFP and OSFP-XD Form Factors?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           When choosing a 1.6T form factor, enterprises need to consider their network architecture, business scale, and future expansion plans. Most 400G/800G switches currently on the market use an 8-lane design, and OSFP continues this architecture, ensuring compatibility with existing equipment. It is suitable for deployments in medium-sized data centers or existing 400G/800G systems, meeting the requirements for stable interconnection and continuous operation. If an enterprise’s goal is to upgrade an existing network or build an AI training cluster that needs immediate deployment, OSFP based on a mature ecosystem is a more reliable choice. NADDOD’s
           &lt;a href=&quot;https://www.naddod.com/products/102514.html&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T OSFP 2xDR4 transceiver
           &lt;/a&gt;
           offers comprehensive interface specifications and guaranteed compatibility, supporting optical-to-electrical conversion and efficient data transmission, providing a reliable solution for these mainstream application scenarios.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In contrast, OSFP-XD’s 16-channel design makes it incompatible with current mainstream 400G/800G equipment. Therefore, it is primarily used for module performance testing, experimental verification, or cutting-edge research projects. OSFP-XD is more suitable for companies conducting technical pre-research and evaluating potential high-density, high-bandwidth deployment scenarios when planning network architectures for the next 3–5 years. It can support experimental applications requiring high port density and higher bandwidth, but is not suitable as the primary choice for current production networks. When selecting OSFP-XD, companies should carefully consider compatibility with existing equipment and optical module technologies, as well as potential early technical risks.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;362&quot; src=&quot;https://miro.medium.com/v2/1*HhIhWmJVjCHoM78ASh8puQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Choosing between OSFP and OSFP-XD form factors requires comprehensive consideration. Different form factors vary in port count, channel layout, and module support capabilities, so the decision should be based on network architecture, business scale, and long-term development strategy. The correct form factor selection impacts not only the immediate efficiency of the optical module but also the long-term maintenance and return on investment of the entire data center or high-performance computing environment. NADDOD is committed to providing industry-leading high-speed optical interconnect solutions. Our
           &lt;a href=&quot;https://www.naddod.com/collections/1600g-800g-infiniband-xdr&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            1.6T InfiniBand XDR solution
           &lt;/a&gt;
           is ready for deployment. Contact our technical experts today to learn more about the upgrade options and products that best suit your network architecture.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> xAI Colossus 2: Ushering in the Gigawatt Era of AI Supercomputing    </title>
<link>https://naddod.medium.com/https-www-naddod-com-blog-xai-colossus-2-ushering-in-the-gigawatt-era-of-ai-supercomputing-4b6aac75425c</link>
<pubDate>Thu, 18 Sep 2025 08:23:24 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The rapid development of Artificial Intelligence (AI) is marked by escalating computational power, with each leap promising a new revolution. From early lab models to today’s unprecedented AI superclusters, the boundaries of compute are constantly being pushed. Every frontier AI model — be it ChatGPT, Claude, Gemini, or Grok — is backed by a colossal cluster of tens of thousands of GPUs. Now, Elon Musk’s xAI is building Colossus 2, a project poised to redefine the scale and speed of AI infrastructure.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This article offers an in-depth look at the evolution of xAI’s Colossus supercomputer, from the stunning debut of Colossus 1 to the gigawatt-scale breakthrough of Colossus 2. We will analyze its technical architecture, energy strategy, and funding backdrop, anticipating its profound impact on the global AI landscape.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           From Colossus 1 to Colossus 2: A Leap in Speed ​​and Scale
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Before diving into Colossus 2, we must first look back at
           &lt;a href=&quot;https://www.naddod.com/blog/xai-colossus-100-000-gpu-supercluster-powered-by-spectrum-x&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            Colossus 1
           &lt;/a&gt;
           . This AI training cluster astounded the industry with its incredible construction speed. It was built from the ground up in just 122 days, a stark contrast to the four years often required for traditional data centers.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Colossus 1 was initially equipped with approximately 200,000 NVIDIA H100/H200 GPUs and about 30,000 GB200 NVL72 systems, with a total estimated power consumption of around 300 megawatts (MW). This made it the world’s largest, fully operational, single coherent AI training cluster at the time. The successful construction of Colossus 1 established xAI’s reputation in AI infrastructure, demonstrating its powerful capability for rapid deployment and execution.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;378&quot; src=&quot;https://miro.medium.com/v2/1*r_nfGCyGFP0sTZUdNRuEmA.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Source: SemiAnalysis Datacenter Industry Model
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           However, in the fierce competition of the AI domain, stagnation means falling behind. To achieve another massive leap, xAI launched the even more ambitious Colossus 2 project. The project commenced on March 7, 2025, with xAI acquiring a one-million-square-foot warehouse and two adjacent one-hundred-acre plots in Memphis. Just six months later, 119 air-cooled chillers were already deployed on-site, providing roughly 200 MW of cooling capacity. This astonishing pace of construction once again highlights xAI’s leading position in AI infrastructure development.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Colossus 2: Entering the Gigawatt Era
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           In 2025, xAI officially launched the Colossus 2 project. The speed of its construction is astonishing: in just six months, xAI deployed 119 air-cooled chillers on site, providing approximately 200MW of cooling capacity, providing a solid guarantee for the subsequent operation of hundreds of thousands of servers. This efficiency far exceeded the industry average — large companies like Oracle and OpenAI typically take 15 months or longer to complete projects of similar scale.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Colossus 2’s hardware configuration was equally impressive. The project initially deployed 200,000 NVIDIA H100 GPUs. NVIDIA CEO Jensen Huang praised xAI’s feat of doubling the cluster size in 92 days as “unprecedented.” In the future, the cluster will gradually expand to a scale of one million GPUs and be equipped with the latest generation of H200 and Blackwell GB200 chips, with each chip having a computing power of up to 20 PFLOPS.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;382&quot; src=&quot;https://miro.medium.com/v2/1*cU3gcuhf0E21Dqodt4jtyg.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           This powerful hardware is the cornerstone of NVIDIA’s AI supercomputers. To learn more about how NVIDIA builds AI supercomputers, please refer to the blog post:
           &lt;a href=&quot;https://www.naddod.com/blog/how-nvidia-builds-ai-supercomputers-with-superpod&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            From H100, GH200 to GB200: How NVIDIA Builds AI Supercomputers with SuperPod.
           &lt;/a&gt;
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           To ensure performance, Colossus 2 utilizes cutting-edge technologies: Supermicro liquid-cooled racks significantly improve thermal efficiency, while NVIDIA’s Spectrum-X Ethernet platform guarantees up to 95% network utilization and zero packet loss. This close partnership demonstrates that NVIDIA is not only a chip supplier for xAI, but also a core strategic partner in the AI ​​race.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;344&quot; src=&quot;https://miro.medium.com/v2/1*aGStYZKACMZYNFqoDiU9VQ.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           The Value and Significance of Colossus 2
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The value of Colossus 2 goes far beyond its status as the world’s largest AI supercomputer. It represents a new direction and new model for the development of AI infrastructure.
          &lt;/p&gt;
          &lt;ul&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             A New Era of Gigawatt-scale Energy Consumption:
            &lt;/strong&gt;
            Colossus 2 will be the first AI supercluster to operate at gigawatt scale, consuming an astonishing amount of energy equivalent to powering a small city.This highlights the immense energy demands of the next generation of AI models. To address this challenge, xAI has adopted innovative energy solutions, including using a large fleet of Tesla Megapacks to provide efficient backup power and partnering with Solaris Energy Infrastructure, a gas turbine leasing company, to quickly acquire the required power.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Accelerating the Race for Artificial General Intelligence (AGI):
            &lt;/strong&gt;
            The massive scale and rapid construction of Colossus 2 represent a significant step forward for xAI in the AGI race. The ultimate goal is to train xAI’s flagship chatbot, Grok, and scale to one million GPUs. This aggressive, no-cost strategy aims to surpass competitors and accelerate the realization of AGI.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Unique AI Model Research Approach:
            &lt;/strong&gt;
            In addition to pursuing massive computing power, xAI also has its own unique AI model research methodology. According to a SemiAnalysis report, xAI is using a unique reinforcement learning (RL) approach, which could enable it to surpass OpenAI, Anthropic, and Google in cutting-edge AI research. This demonstrates that xAI is pursuing breakthroughs not only in hardware but also in underlying model training methods.
           &lt;/li&gt;
           &lt;li data-selectable-paragraph=&quot;&quot;&gt;
            &lt;strong&gt;
             Funding Challenges and Strategic Layout:
            &lt;/strong&gt;
            Building an AI cluster of the scale of Colossus 2 would require capital expenditures reaching tens of billions of dollars. To address this massive funding requirement, funding from the Middle East is a very likely option. Reports indicate that xAI is raising a multi-billion dollar round of funding, with Saudi Arabia’s Public Investment Fund (PIF) playing a key role. SemiAnalysis believes a win-win agreement is likely: xAI would build a new, large-scale data center in Saudi Arabia, with Saudi Arabia providing the necessary funding.
           &lt;/li&gt;
          &lt;/ul&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Future Outlook: Computing Power Becomes Core Competitiveness
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Colossus 2’s ultimate goal is to scale to one million GPUs, enabling it to surpass Meta Superintelligence and Anthropic in total data center capacity for a single training cluster by the third quarter of 2025, once again becoming the world’s largest single data center. The realization of this vision will further solidify xAI’s leading position in the AI ​​race.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The ultimate winner in this AI computing power arms race may not be the company with the best code, but rather the one that can control computing power like an oil field, trade it like natural gas contracts, and build a moat like a regulated utility. The deployment of Colossus 2 will intensify competition among AI infrastructure providers and profoundly impact the future of the global AI industry.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;468&quot; src=&quot;https://miro.medium.com/v2/1*ModKpUxcwdmuj0QkBksRLw.png&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/figure&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Conclusion
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           From the rapid construction of Colossus 1 to the gigawatt scale of Colossus 2, xAI has achieved a cross-cutting development in less than two years. Colossus 2 is not just a supercomputer; it is an AI energy factory representing the future, redefining the boundaries of AI infrastructure through its scale, speed, and capital integration. In this global AI arms race, the strategic significance of Colossus 2 transcends technology itself, shaping the future direction and structure of artificial intelligence development.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
<item>
<title> Detailed Explanation of the Internal Structure of Optical Transceivers    </title>
<link>https://naddod.medium.com/detailed-explanation-of-the-internal-structure-of-optical-transceivers-ad473b66794d</link>
<pubDate>Wed, 17 Sep 2025 01:44:40 -0000</pubDate>
<description>
&lt;html&gt;
 &lt;body&gt;
  &lt;article&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;section&gt;
      &lt;div&gt;
       &lt;div&gt;
       &lt;/div&gt;
       &lt;div&gt;
        &lt;div&gt;
         &lt;div&gt;
          &lt;div&gt;
           &lt;div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           As a key device in the optical communication system, the optical transceiver acts as a transmission medium between network devices, which is used to send and receive data. At present, there are many articles on optical transceivers on the market, but only a relatively small part mentions the internal structure of optical transceivers. This article will introduce the internal structure of optical transceivers in detail, so that you can understand the structure of optical transceiver components more clearly.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Internal structure of optical transceiver
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           The
           &lt;a href=&quot;https://www.naddod.com/collection/optical-transceivers&quot; rel=&quot;noopener ugc nofollow&quot; target=&quot;_blank&quot;&gt;
            optical transceiver
           &lt;/a&gt;
           is mainly composed of three parts: the housing, the optical components, and the integrated circuit board. When you remove the metal housing of the optical transceiver, you will find that the internal components are connected to each other. The following section will focus on the optical device and the integrated circuit board.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Optical Devices
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Optical devices are the core components of an optical transceiver. Different types of optical transceivers use different optical devices.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           1.TOSA and ROSA in common optical transceivers
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           For common optical transceivers, there are two kinds of optical devices, TOSA and ROSA, which have opposite roles.
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           (1) What is TOSA?
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           TOSA is an optical transmitting component, whose main function is to convert electrical signals into optical signals. It consists of a light source (semiconductor light-emitting diode or laser diode), optical interface, monitoring photodiode, metal or plastic housing, and electrical interface. However, TOSA composition structure is not a layer of the same, different transmission distance or application of optical transceivers, TOSA may exist other components, such as filters.
          &lt;/p&gt;
          &lt;div&gt;
           &lt;div&gt;
            &lt;h2&gt;
             Get NADDOD’s stories in your inbox
            &lt;/h2&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;p&gt;
             Join Medium for free to get updates from this writer.
            &lt;/p&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;span&gt;
             &lt;div&gt;
              &lt;div&gt;
               &lt;input placeholder=&quot;Enter your email&quot; type=&quot;text&quot; value=&quot;&quot;/&gt;
              &lt;/div&gt;
             &lt;/div&gt;
            &lt;/span&gt;
            &lt;div&gt;
             &lt;div&gt;
              &lt;button&gt;
               Subscribe
              &lt;/button&gt;
             &lt;/div&gt;
            &lt;/div&gt;
            &lt;div&gt;
             &lt;button&gt;
              Subscribe
             &lt;/button&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;div&gt;
            &lt;div&gt;
            &lt;/div&gt;
           &lt;/div&gt;
          &lt;/div&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Today, most optical transceivers are using laser diodes (LD) as the light source, unlike semiconductor light-emitting diodes (LED), which have lower power consumption, higher output power and higher coupling efficiency. However, there are still low-rate and short-range transmission using semiconductor light-emitting diodes because of their low cost and long service life. The following figure shows the structure of TOSA using laser diodes.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;395&quot; src=&quot;https://miro.medium.com/v2/1*DRJ5RuEw7i3UxgHLME4UhQ.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            TOSA
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           (2) What is ROSA?
          &lt;/p&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           A ROSA is an optical receiver assembly whose primary function is to convert the optical signal transmitted from a TOSA into an electrical signal. a ROSA consists of a photodiode, an optical interface, a metal or plastic housing, and an electrical interface. As with the TOSA, the specific components of the ROSA depend on the specific function and application of the optical transceiver, which may also have other components such as amplifiers designed to recover input signals that have been degraded due to long-distance transmission. In this case, the preamplifier converts the current signal into a voltage signal and amplifies it to a high voltage gain, while the postamplifier equalizes the signal output from the preamplifier to an amplitude level suitable for output to subsequent digital circuits.
          &lt;/p&gt;
          &lt;figure&gt;
           &lt;div role=&quot;button&quot; tabindex=&quot;0&quot;&gt;
            &lt;div&gt;
             &lt;img height=&quot;473&quot; src=&quot;https://miro.medium.com/v2/1*IkF2J8ewYHg1rol2U74AiQ.jpeg&quot; width=&quot;700&quot;/&gt;
            &lt;/div&gt;
           &lt;/div&gt;
           &lt;figcaption data-selectable-paragraph=&quot;&quot;&gt;
            ROSA
           &lt;/figcaption&gt;
          &lt;/figure&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Such a pair of ROSA and TOSA components combine to form the main components of the optical transceiver for transmitting and receiving signals.
          &lt;/p&gt;
          &lt;h3 data-selectable-paragraph=&quot;&quot;&gt;
           2.BOSA in BiDi optical transceiver
          &lt;/h3&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           BOSA refers to the optical bi-directional transceiver assembly. As the name suggests, BOSA is related to BiDi optical transceivers, and BOSA has emerged with the development of optical transceiver manufacturing technology. Since optical transceivers tend to develop in small size, optical transceivers need to integrate TOSA and ROSA in the coupling process.BOSA, on the other hand, integrates TOSA, ROSA and WDM filter together, and uses WDM technology to couple two wavelengths into the same fiber for transmission.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           Integrated Circuit Board
          &lt;/h2&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           1. What is PCBA?
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           PCBA is an integrated circuit board, also known as a printed circuit board, SMT placement and DIP plug-in are the methods often used in the PCBA production process. Simply put, PCBA is actually a thin board with integrated circuits and other electronic components, which can fix integrated circuits and other electronic components to ensure the quality and functionality of electronic devices.
          &lt;/p&gt;
          &lt;h2 data-selectable-paragraph=&quot;&quot;&gt;
           2. The difference between PCBA and PCB
          &lt;/h2&gt;
          &lt;p data-selectable-paragraph=&quot;&quot;&gt;
           Briefly, the main difference between PCBA and PCB is that PCB belongs to the bare board (empty board), PCBA is the finished board. PCB is the printed circuit board, is the carrier of the electrical connection of electronic components, PCBA is the PCB processing process. That is to say, PCB is an empty printed circuit board, there are no parts on it, after SMT placement or DIP plug-in processes, etc., before the PCBA.
          &lt;/p&gt;
         &lt;/div&gt;
        &lt;/div&gt;
       &lt;/div&gt;
      &lt;/div&gt;
     &lt;/section&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/article&gt;
 &lt;/body&gt;
&lt;/html&gt;

</description>
</item>
</channel>
</rss>
