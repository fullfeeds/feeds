<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" version="2.0">
<channel><title>Ahead of AI</title>
<lastBuildDate>Sat, 01 Nov 2025 17:56:17 -0000</lastBuildDate>
<item>
<title> Understanding the 4 Main Approaches to LLM Evaluation (From Scratch) </title>
<link>https://magazine.sebastianraschka.com/p/llm-evaluation-4-approaches</link>
<pubDate>Sun, 05 Oct 2025 11:12:32 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;div dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   &lt;strong&gt;
    How do we actually evaluate LLMs?
   &lt;/strong&gt;
   &lt;br/&gt;
   &lt;span&gt;
    It’s a simple question, but one that tends to open up a much bigger discussion.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   When advising or collaborating on projects, one of the things I get asked most often is how to choose between different models and how to make sense of the evaluation results out there. (And, of course, how to measure progress when fine-tuning or developing our own.)
  &lt;/p&gt;
  &lt;p&gt;
   Since this comes up so often, I thought it might be helpful to share a short overview of the main evaluation methods people use to compare LLMs. Of course, LLM evaluation is a very big topic that can’t be exhaustively covered in a single resource, but I think that having a clear mental map of these main approaches makes it much easier to interpret benchmarks, leaderboards, and papers.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    I originally planned to include these evaluation techniques in my upcoming book,
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://mng.bz/Nwr7&quot; rel=&quot;&quot;&gt;
     Build a Reasoning Model (From Scratch)
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , but they ended up being a bit outside the main scope. (The book itself focuses more on verifier-based evaluation.) So I figured that sharing this as a longer article with from-scratch code examples would be nice.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    In
   &lt;/span&gt;
   &lt;a href=&quot;https://mng.bz/Nwr7&quot; rel=&quot;&quot;&gt;
    Build A Reasoning Model (From Scratch)
   &lt;/a&gt;
   &lt;span&gt;
    , I am taking a hands-on approach to building a reasoning LLM from scratch.
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    If you liked “Build A Large Language Model (From Scratch)”, this book is written in a similar style in terms of building everything from scratch in pure PyTorch.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Q_QP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Q_QP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png 424w, https://substackcdn.com/image/fetch/$s_!Q_QP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png 848w, https://substackcdn.com/image/fetch/$s_!Q_QP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png 1272w, https://substackcdn.com/image/fetch/$s_!Q_QP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:590,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:673404,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;590&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Q_QP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Q_QP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png 424w, https://substackcdn.com/image/fetch/$s_!Q_QP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png 848w, https://substackcdn.com/image/fetch/$s_!Q_QP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png 1272w, https://substackcdn.com/image/fetch/$s_!Q_QP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb34a8123-ef28-48bb-8ea2-28404b08d013_2461x997.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Reasoning is one of the most exciting and important recent advances in improving LLMs, but it’s also one of the easiest to misunderstand if you only hear the term reasoning and read about it in theory. So,
     &lt;/span&gt;
     &lt;a href=&quot;https://mng.bz/Nwr7&quot; rel=&quot;&quot;&gt;
      in this book
     &lt;/a&gt;
     &lt;span&gt;
      , I am taking a hands-on approach to building a reasoning LLM from scratch.
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    The book is currently in early-access with &amp;gt;100 pages already online, and I have just finished another 30 pages that are currently being added by the layout team. If you joined the early access program (a big
   &lt;/span&gt;
   &lt;em&gt;
    thank you
   &lt;/em&gt;
   &lt;span&gt;
    for your support!), you should receive an email when those go live.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    PS: There’s a lot happening on the LLM research front right now. I’m still catching up on my growing list of bookmarked papers and plan to highlight some of the most interesting ones in the next article.
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   But now, let’s discuss the four main LLM evaluation methods along with their from-scratch code implementations to better understand their advantages and weaknesses.
  &lt;/p&gt;
  &lt;h1&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Understanding the main evaluation methods for LLMs
    &lt;/strong&gt;
   &lt;/em&gt;
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    There are four common ways of evaluating trained LLMs in practice:
   &lt;/span&gt;
   &lt;em&gt;
    multiple choice
   &lt;/em&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;em&gt;
    verifiers
   &lt;/em&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;em&gt;
    leaderboards
   &lt;/em&gt;
   &lt;span&gt;
    , and
   &lt;/span&gt;
   &lt;em&gt;
    LLM judges
   &lt;/em&gt;
   &lt;span&gt;
    , as shown in Figure 1 below. Research papers, marketing materials, technical reports, and model cards (a term for LLM-specific technical reports) often include results from two or more of these categories.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!nwaB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nwaB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png 424w, https://substackcdn.com/image/fetch/$s_!nwaB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png 848w, https://substackcdn.com/image/fetch/$s_!nwaB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png 1272w, https://substackcdn.com/image/fetch/$s_!nwaB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:363,&quot;width&quot;:1050,&quot;resizeWidth&quot;:645,&quot;bytes&quot;:92153,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;222.9857142857143&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!nwaB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nwaB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png 424w, https://substackcdn.com/image/fetch/$s_!nwaB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png 848w, https://substackcdn.com/image/fetch/$s_!nwaB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png 1272w, https://substackcdn.com/image/fetch/$s_!nwaB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc26764a9-6a26-4467-bb03-74b6cd1ed72b_1050x363.png 1456w&quot; width=&quot;645&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 1: An overview of the 4 different evaluations models covered in this article.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h6&gt;
  &lt;/h6&gt;
  &lt;p&gt;
   &lt;span&gt;
    Furthermore the four categories introduced here fall into two groups:
   &lt;/span&gt;
   &lt;em&gt;
    benchmark-based evaluation
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    judgment-based evaluation
   &lt;/em&gt;
   &lt;span&gt;
    , as shown in the figure above.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (There are also other measures, such as
   &lt;/span&gt;
   &lt;em&gt;
    training loss,
   &lt;/em&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    perplexity
   &lt;/em&gt;
   &lt;span&gt;
    , and
   &lt;/span&gt;
   &lt;em&gt;
    rewards
   &lt;/em&gt;
   &lt;span&gt;
    , but they are usually used internally during model development.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The following subsections provide brief overviews and examples of each of the four  methods.
  &lt;/p&gt;
  &lt;h1&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Method 1: Evaluating answer-choice accuracy
    &lt;/strong&gt;
   &lt;/em&gt;
  &lt;/h1&gt;
  &lt;p&gt;
   We begin with a benchmark‑based method: multiple‑choice question answering.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Historically, one of the most widely used evaluation methods is multiple-choice benchmarks such as
   &lt;/span&gt;
   &lt;em&gt;
    MMLU
   &lt;/em&gt;
   &lt;span&gt;
    (short for Massive Multitask Language Understanding,
   &lt;/span&gt;
   &lt;a href=&quot;https://huggingface.co/datasets/cais/mmlu&quot; rel=&quot;&quot;&gt;
    https://huggingface.co/datasets/cais/mmlu
   &lt;/a&gt;
   &lt;span&gt;
    ). To illustrate this approach, figure 2 shows a representative task from the MMLU dataset.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!WmmA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!WmmA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 424w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 848w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 1272w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:608,&quot;width&quot;:1040,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:146320,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;608&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!WmmA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!WmmA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 424w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 848w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 1272w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 1456w&quot; width=&quot;1040&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 2: Evaluating an LLM on MMLU by comparing its multiple-choice prediction with the correct answer from the dataset.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Figure 2 shows just a single example from the MMLU dataset. The complete MMLU dataset consists of 57 subjects (from high school math to biology) with about 16 thousand multiple-choice questions in total, and performance is measured in terms of accuracy (the fraction of correctly answered questions), for example 87.5% if 14,000 out of 16,000 questions are answered correctly.
  &lt;/p&gt;
  &lt;p&gt;
   Multiple-choice benchmarks, such as MMLU, test an LLM’s knowledge recall in a straightforward, quantifiable way similar to standardized tests, many school exams, or theoretical driving tests.
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    &lt;span&gt;
     Note that figure 2 shows a simplified version of multiple-choice evaluation, where the model’s predicted answer letter is compared directly to the correct one. Two other popular methods exist that involve
    &lt;/span&gt;
    &lt;em&gt;
     log-probability scoring
    &lt;/em&gt;
    &lt;span&gt;
     . I implemented them
    &lt;/span&gt;
    &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/tree/main/chF/02_mmlu&quot; rel=&quot;&quot;&gt;
     here on GitHub
    &lt;/a&gt;
    &lt;span&gt;
     . (As this builds on the concepts explained here, I recommended checking this out after completing this article.)
    &lt;/span&gt;
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   The following subsections illustrate how the MMLU scoring shown in figure 2 can be implemented in code.
  &lt;/p&gt;
  &lt;h2&gt;
   &lt;em&gt;
    &lt;strong&gt;
     1.2 Loading the model
    &lt;/strong&gt;
   &lt;/em&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   First, before we can evaluate it on MMLU, we have to load the pre-trained model. Here, we are going to use a from-scratch implementation of Qwen3 0.6B in pure PyTorch, which requires only about 1.5 GB of RAM.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that the Qwen3 model implementation details are not important here; we simply treat it as an LLM we want to evaluate. However, if you are curious, a from-scratch implementation walkthrough can be found in my previous
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/qwen3-from-scratch&quot; rel=&quot;&quot;&gt;
    Understanding and Implementing Qwen3 From Scratch
   &lt;/a&gt;
   &lt;span&gt;
    article, and the source code is also available
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/blob/main/reasoning_from_scratch/qwen3.py&quot; rel=&quot;&quot;&gt;
    here on GitHub
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Instead of copy &amp; pasting the many lines of Qwen3 source code, we import it from my
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/blob/main/reasoning_from_scratch&quot; rel=&quot;&quot;&gt;
    reasoning_from_scratch
   &lt;/a&gt;
   &lt;span&gt;
    Python library, which can be installed via
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;pip install reasoning_from_scratch&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   or
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;&lt;code&gt;uv add reasoning_from_scratch&lt;/code&gt;&lt;/code&gt;&lt;/pre&gt;
  &lt;h4&gt;
   &lt;strong&gt;
    Code block 1: Loading a pre-trained model
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;pre&gt;&lt;code&gt;from pathlib import Path
import torch
from reasoning_from_scratch.ch02 import get_device
from reasoning_from_scratch.qwen3 import (
    download_qwen3_small, Qwen3Tokenizer,
    Qwen3Model, QWEN_CONFIG_06_B
)

device = get_device()

# Set matmul precision to &quot;high&quot; to 
# enable Tensor Cores on compatible GPUs
torch.set_float32_matmul_precision(&quot;high&quot;)

# Uncomment the following line 
# if you encounter device compatibility issues
# device = &quot;cpu&quot;

# Use the base model by default
WHICH_MODEL = &quot;base&quot;

if WHICH_MODEL == &quot;base&quot;:
    # Download and initialize the base version of Qwen3-0.6B
    download_qwen3_small(
        kind=&quot;base&quot;, tokenizer_only=False, out_dir=&quot;qwen3&quot;
    )
    tokenizer_path = Path(&quot;qwen3&quot;) / &quot;tokenizer-base.json&quot;
    model_path = Path(&quot;qwen3&quot;) / &quot;qwen3-0.6B-base.pth&quot;
    tokenizer = Qwen3Tokenizer(tokenizer_file_path=tokenizer_path)

elif WHICH_MODEL == &quot;reasoning&quot;:
    # Download and initialize the reasoning-tuned version of Qwen3-0.6B
    download_qwen3_small(
        kind=&quot;reasoning&quot;, tokenizer_only=False, out_dir=&quot;qwen3&quot;
    )
    tokenizer_path = Path(&quot;qwen3&quot;) / &quot;tokenizer-reasoning.json&quot;
    model_path = Path(&quot;qwen3&quot;) / &quot;qwen3-0.6B-reasoning.pth&quot;
    tokenizer = Qwen3Tokenizer(
        tokenizer_file_path=tokenizer_path,
        apply_chat_template=True,
        add_generation_prompt=True,
        add_thinking=True,
    )

else:
    raise ValueError(f&quot;Invalid choice: WHICH_MODEL={WHICH_MODEL}&quot;)

# Load the model weights and move to device
model = Qwen3Model(QWEN_CONFIG_06_B)
model.load_state_dict(torch.load(model_path))
model.to(device)

# Optionally enable model compilation for potential performance gains
USE_COMPILE = False
if USE_COMPILE:
    torch._dynamo.config.allow_unspec_int_on_nn_module = True
    model = torch.compile(model)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2&gt;
   &lt;em&gt;
    &lt;strong&gt;
     1.3 Checking the generated answer letter
    &lt;/strong&gt;
   &lt;/em&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   In this section, we implement the simplest and perhaps most intuitive MMLU scoring method, which relies on checking whether a generated multiple-choice answer letter matches the correct answer. This is similar to what was illustrated earlier in Figure 2, which is shown below again for convenience.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!WmmA!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!WmmA!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 424w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 848w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 1272w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:608,&quot;width&quot;:1040,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:146320,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;608&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!WmmA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!WmmA!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 424w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 848w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 1272w, https://substackcdn.com/image/fetch/$s_!WmmA!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d5f7998-21be-4144-bfc2-57b2e0a4b1c4_1040x608.png 1456w&quot; title=&quot;&quot; width=&quot;1040&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 3: Evaluating an LLM on MMLU by comparing its multiple-choice prediction with the correct answer from the dataset.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   For this, we will work with an example from the MMLU dataset:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;example = {
    &quot;question&quot;: (
        &quot;How many ways are there to put 4 distinguishable&quot;
        &quot; balls into 2 indistinguishable boxes?&quot;
    ),
    “choices”: [&quot;7&quot;, &quot;11&quot;, &quot;16&quot;, &quot;8&quot;],
    “answer”: &quot;D&quot;,
}&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Next, we define a function to format the LLM prompts.
  &lt;/p&gt;
  &lt;h4&gt;
   &lt;strong&gt;
    Code block 2: Loading a pre-trained model
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;pre&gt;&lt;code&gt;def format_prompt(example):
    return (
        f&quot;{example[&#x27;question&#x27;]}\n&quot;
        f&quot;A. {example[&#x27;choices&#x27;][0]}\n&quot;
        f&quot;B. {example[&#x27;choices&#x27;][1]}\n&quot;
        f&quot;C. {example[&#x27;choices&#x27;][2]}\n&quot;
        f&quot;D. {example[&#x27;choices&#x27;][3]}\n&quot;
        &quot;Answer: &quot;
    )
# Trailing space in &quot;Answer: &quot; encourages a single-letter next token&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Let’s execute the function on the MMLU example to get an idea of what the formatted LLM input looks like:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;prompt = format_prompt(example)
print(prompt)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The output is:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    How many ways are there to put 4 distinguishable balls into 2
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    indistinguishable boxes?
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;How many ways are there to put 4 distinguishable balls into 2
indistinguishable boxes?
A. 7
B. 11
C. 16
D. 8
Answer: &lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    The model prompt, as shown above, provides the model with a list of the different answer choices and ends with an
   &lt;/span&gt;
   &lt;code&gt;
    “Answer: “
   &lt;/code&gt;
   &lt;span&gt;
    text that encourages the model to generate the correct answer.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While it is not strictly necessary, it can sometimes also be helpful to provide additional questions along with the correct answers as input, so that the model can observe how it is expected to solve the task. (For example, cases where 5 examples are provided are also known as 5-shot MMLU.) However, for current generations of LLMs, where even the base models are quite capable, this is not required.
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;h4&gt;
    &lt;strong&gt;
     Loading different MMLU samples
    &lt;/strong&gt;
   &lt;/h4&gt;
   &lt;p&gt;
    &lt;span&gt;
     You can load examples from the MMLU dataset directly via the datasets library (which can be installed
    &lt;/span&gt;
    &lt;code&gt;
     via pip install datasets
    &lt;/code&gt;
    &lt;span&gt;
     or
    &lt;/span&gt;
    &lt;code&gt;
     uv add datasets
    &lt;/code&gt;
    &lt;span&gt;
     ):
    &lt;/span&gt;
   &lt;/p&gt;
   &lt;pre&gt;&lt;code&gt;from datasets import load_dataset

configs = get_dataset_config_names(&quot;cais/mmlu&quot;)
dataset = load_dataset(&quot;cais/mmlu&quot;, &quot;high_school_mathematics&quot;)
# Inspect the first example from the test set:
example = dataset[&quot;test&quot;][0]
print(example)&lt;/code&gt;&lt;/pre&gt;
   &lt;p&gt;
    &lt;span&gt;
     Above, we used the
    &lt;/span&gt;
    &lt;code&gt;
     “high_school_mathematics”
    &lt;/code&gt;
    &lt;span&gt;
     subset; to get a list of the other subsets, use the following code:
    &lt;/span&gt;
   &lt;/p&gt;
   &lt;pre&gt;&lt;code&gt;from datasets import get_dataset_config_names

subsets = get_dataset_config_names(”cais/mmlu”)
print(subsets)&lt;/code&gt;&lt;/pre&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   Next, we tokenize the prompt and wrap it in a PyTorch tensor object as input to the LLM:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;prompt_ids = tokenizer.encode(prompt)
prompt_fmt = torch.tensor(prompt_ids, device=device)
# Add batch dimension:
prompt_fmt = prompt_fmt.unsqueeze(0)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Then, with all that setup out of the way, we define the main scoring function below, which generates a few tokens (here, 8 tokens by default) and extracts the first instance of letter A/B/C/D that the model prints.
  &lt;/p&gt;
  &lt;h4&gt;
   &lt;strong&gt;
    Code block 3: Extracting the generated letter
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;pre&gt;&lt;code&gt;from reasoning_from_scratch.ch02_ex import (
    generate_text_basic_stream_cache
)

def predict_choice(
    model, tokenizer, prompt_fmt, max_new_tokens=8
):
    pred = None
    for t in generate_text_basic_stream_cache(
        model=model,
        token_ids=prompt_fmt,
        max_new_tokens=max_new_tokens,
        eos_token_id=tokenizer.eos_token_id,
    ):
        answer = tokenizer.decode(t.squeeze(0).tolist())
        for letter in answer:
            letter = letter.upper()
            # stop as soon as a letter appears
            if letter in &quot;ABCD&quot;:
                pred = letter
                break
        if pred:
            break
    return pred&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   We can then check the generated letter using the function from the code block above as follows:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;pred1 = predict_choice(model, tokenizer, prompt_fmt)
print(
    f&quot;Generated letter: {pred1}\n&quot;
    f&quot;Correct? {pred1 == example[&#x27;answer&#x27;]}&quot;
)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The result is:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;Generated letter: C
Correct? False&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    As we can see, the generated answer is incorrect (
   &lt;/span&gt;
   &lt;code&gt;
    False
   &lt;/code&gt;
   &lt;span&gt;
    ) in this case.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    This was just one of the 270 examples from the
   &lt;/span&gt;
   &lt;code&gt;
    high_school_mathematics
   &lt;/code&gt;
   &lt;span&gt;
    subset in MMLU. The screenshot (Figure 4) below show’s the performance of the base model and reasoning variant when executed on the complete subset. The code for this is available
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/blob/main/chF/02_mmlu/1_letter_matching.py&quot; rel=&quot;&quot;&gt;
    here on GitHub
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!HSuI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!HSuI!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png 424w, https://substackcdn.com/image/fetch/$s_!HSuI!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png 848w, https://substackcdn.com/image/fetch/$s_!HSuI!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png 1272w, https://substackcdn.com/image/fetch/$s_!HSuI!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:917,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:571085,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;917&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!HSuI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!HSuI!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png 424w, https://substackcdn.com/image/fetch/$s_!HSuI!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png 848w, https://substackcdn.com/image/fetch/$s_!HSuI!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png 1272w, https://substackcdn.com/image/fetch/$s_!HSuI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F098958b6-4a5b-4070-a350-8f318abcede2_1815x1143.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 4: Base and reasoning model performance on the MMLU
     &lt;/span&gt;
     &lt;code&gt;
      high_school_mathematics
     &lt;/code&gt;
     &lt;span&gt;
      subset
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Assuming the questions have an equal answer probability, a random guesser (with uniform probability choosing A, B, C, or D) is expected to achieve 25% probability. So the both the base and reasoning model are not very good.
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;h4&gt;
    &lt;strong&gt;
     Multiple-choice answer formats
    &lt;/strong&gt;
   &lt;/h4&gt;
   &lt;p&gt;
    Note that this section implemented a simplified version of multiple-choice evaluation for illustration purposes, where the model’s predicted answer letter is compared directly to the correct one. In practice, more widely used variations exist, such as log-probability scoring, where we measure how likely the model considers each candidate answer rather than just checking the final letter choice. (We discuss probability-based scoring in chapter 4.) For reasoning models, evaluation can also involve assessing the likelihood of generating the correct answer when it is provided as input.
   &lt;/p&gt;
   &lt;div&gt;
    &lt;figure&gt;
     &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!MM--!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
      &lt;div&gt;
       &lt;picture&gt;
        &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!MM--!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png 424w, https://substackcdn.com/image/fetch/$s_!MM--!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png 848w, https://substackcdn.com/image/fetch/$s_!MM--!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png 1272w, https://substackcdn.com/image/fetch/$s_!MM--!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png 1456w&quot; type=&quot;image/webp&quot;&gt;
         &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:857,&quot;width&quot;:1389,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:255774,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;857&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!MM--!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!MM--!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png 424w, https://substackcdn.com/image/fetch/$s_!MM--!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png 848w, https://substackcdn.com/image/fetch/$s_!MM--!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png 1272w, https://substackcdn.com/image/fetch/$s_!MM--!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5fb712c-10f1-4240-b3e6-2cbf7ccfc356_1389x857.png 1456w&quot; title=&quot;&quot; width=&quot;1389&quot;/&gt;
        &lt;/source&gt;
       &lt;/picture&gt;
      &lt;/div&gt;
     &lt;/a&gt;
     &lt;figcaption&gt;
      &lt;span&gt;
       Figure 5: Other MMLU scoring methods are described and shared on
      &lt;/span&gt;
      &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/tree/main/chF/02_mmlu&quot; rel=&quot;&quot;&gt;
       GitHub here
      &lt;/a&gt;
     &lt;/figcaption&gt;
    &lt;/figure&gt;
   &lt;/div&gt;
   &lt;p&gt;
    However, regardless of which MMLU scoring variant we use, the evaluation still amounts to checking whether the model selects from the predefined answer options.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   &lt;strong&gt;
    A limitation of multiple‑choice benchmarks like MMLU is that they only measure an LLM’s ability to select from predefined options and thus is not very useful for evaluating reasoning capabilities
   &lt;/strong&gt;
   &lt;span&gt;
    besides checking if and how much knowledge the model has forgotten compared to the base model. It does not capture free-form writing ability or real-world utility.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Still, multiple-choice benchmarks remain simple and useful diagnostics: for example, a high MMLU score doesn’t necessarily mean the model is strong in practical use, but a low score can highlight potential knowledge gaps.
  &lt;/p&gt;
  &lt;h1&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Method 2: Using verifiers to check answers
    &lt;/strong&gt;
   &lt;/em&gt;
  &lt;/h1&gt;
  &lt;p&gt;
   Related to multiple-choice question answering discussed in the previous section, verification-based approaches quantify the LLMs capabilities via an accuracy metric. However, in contrast to multiple-choice benchmarks, verification methods allow LLMs to provide a free-form answer. We then extract the relevant answer portion and use a so-called verifier to compare the answer portion to the correct answer provided in the dataset, as illustrated in Figure 6 below.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!RL2q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!RL2q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png 424w, https://substackcdn.com/image/fetch/$s_!RL2q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png 848w, https://substackcdn.com/image/fetch/$s_!RL2q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png 1272w, https://substackcdn.com/image/fetch/$s_!RL2q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0304413b-aa78-4488-a157-841d6399d3e6_975x986.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:986,&quot;width&quot;:975,&quot;resizeWidth&quot;:656,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;663.4010256410256&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!RL2q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!RL2q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png 424w, https://substackcdn.com/image/fetch/$s_!RL2q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png 848w, https://substackcdn.com/image/fetch/$s_!RL2q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png 1272w, https://substackcdn.com/image/fetch/$s_!RL2q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0304413b-aa78-4488-a157-841d6399d3e6_975x986.png 1456w&quot; width=&quot;656&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 6: Evaluating an LLM with a verification-based method in free-form question answering. The model generates a free-form answer (which may include multiple steps) and a final boxed answer, which is extracted and compared against the correct answer from the dataset.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h6&gt;
  &lt;/h6&gt;
  &lt;p&gt;
   When we compare the extracted answer with the provided answer, as shown in figure above, we can employ external tools, such as code interpreters or calculator-like tools/software.
  &lt;/p&gt;
  &lt;p&gt;
   The downside is that this method can only be applied to domains that can be easily (and ideally deterministically) verified, such as math and code. Also, this approach can introduce additional complexity and dependencies, and it may shift part of the evaluation burden from the model itself to the external tool.
  &lt;/p&gt;
  &lt;p&gt;
   However, because it allows us to generate an unlimited number of math problem variations programmatically and benefits from step-by-step reasoning, it has become a cornerstone of reasoning model evaluation and development.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    I wrote a comprehensive 35-page on this topic in my “Build a Reasoning Model (From Scratch)” book, so I am skipping the code implementation here. (I submitted the chapter last week. If you have the early access version, you’ll receive an email when it goes live and will be able to read it then. In the meantime, you can find the step-by-step code
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/blob/main/ch03/01_main-chapter-code/ch03_main.ipynb&quot; rel=&quot;&quot;&gt;
    here on GitHub
   &lt;/a&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!P2NG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!P2NG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png 424w, https://substackcdn.com/image/fetch/$s_!P2NG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png 848w, https://substackcdn.com/image/fetch/$s_!P2NG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png 1272w, https://substackcdn.com/image/fetch/$s_!P2NG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:656,&quot;width&quot;:1406,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:143170,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;656&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!P2NG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!P2NG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png 424w, https://substackcdn.com/image/fetch/$s_!P2NG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png 848w, https://substackcdn.com/image/fetch/$s_!P2NG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png 1272w, https://substackcdn.com/image/fetch/$s_!P2NG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F74860b8a-d12a-4781-8362-d5eb06feb389_1406x656.png 1456w&quot; width=&quot;1406&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 7: Excerpt from the verification-based evaluation approach available
     &lt;/span&gt;
     &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/blob/main/ch03/01_main-chapter-code/ch03_main.ipynb&quot; rel=&quot;&quot;&gt;
      here on GitHub
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Method 3: Comparing models using preferences and leaderboards
    &lt;/strong&gt;
   &lt;/em&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   So far, we have covered two methods that offer easily quantifiable metrics such as model accuracy. However, none of the aforementioned methods evaluate LLMs in a more holistic way, including judging the style of the responses. In this section, as illustrated in Figure 8 below, we discuss a judgment-based method, namely, LLM leaderboards.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!nm_T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nm_T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png 424w, https://substackcdn.com/image/fetch/$s_!nm_T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png 848w, https://substackcdn.com/image/fetch/$s_!nm_T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png 1272w, https://substackcdn.com/image/fetch/$s_!nm_T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:457,&quot;width&quot;:1114,&quot;resizeWidth&quot;:639,&quot;bytes&quot;:100702,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;262.1391382405745&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!nm_T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nm_T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png 424w, https://substackcdn.com/image/fetch/$s_!nm_T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png 848w, https://substackcdn.com/image/fetch/$s_!nm_T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png 1272w, https://substackcdn.com/image/fetch/$s_!nm_T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36c6779c-5eb0-4cfd-88fd-b1b68f44b227_1114x457.png 1456w&quot; width=&quot;639&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 8: A mental model of the topics covered in this book with a focus on the judgment- and benchmark-based evaluation methods covered in this appendix. Having already covered benchmark-based approaches (multiple choice, verifiers) in the previous section, we now introduce judgment-based approaches to measure LLM performance, with this subsection focusing on leaderboards.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h6&gt;
  &lt;/h6&gt;
  &lt;p&gt;
   The leaderboard method described here is a judgment-based approach where models are ranked not by accuracy values or other fixed benchmark scores but by user (or other LLM) preferences on their outputs.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    A popular leaderboard is
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://lmarena.ai/&quot; rel=&quot;&quot;&gt;
     LM Arena
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    (formerly
   &lt;/span&gt;
   &lt;em&gt;
    Chatbot Arena
   &lt;/em&gt;
   &lt;span&gt;
    ), where users compare responses from two user-selected or anonymous models and vote for the one they prefer, as shown in Figure 9.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!m7Un!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!m7Un!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png 424w, https://substackcdn.com/image/fetch/$s_!m7Un!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png 848w, https://substackcdn.com/image/fetch/$s_!m7Un!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png 1272w, https://substackcdn.com/image/fetch/$s_!m7Un!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1110,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1110&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!m7Un!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!m7Un!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png 424w, https://substackcdn.com/image/fetch/$s_!m7Un!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png 848w, https://substackcdn.com/image/fetch/$s_!m7Un!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png 1272w, https://substackcdn.com/image/fetch/$s_!m7Un!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1a1816e-790d-421c-a364-48efe66ae622_1600x1220.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 9: Example of a judgment-based leaderboard interface (LM Arena). Two LLMs are given the same prompt, their responses are shown side by side, and users vote for the preferred answer.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   These preference votes, which are collected as shown in the figure above, are then aggregated across all users into a leaderboard that ranks different models by user preference. A current snapshot of the LM Arena leaderboard (accessed on October 3, 2025) is shown below in Figure 10.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!KhbK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!KhbK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png 424w, https://substackcdn.com/image/fetch/$s_!KhbK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png 848w, https://substackcdn.com/image/fetch/$s_!KhbK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png 1272w, https://substackcdn.com/image/fetch/$s_!KhbK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:818,&quot;width&quot;:1115,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:143874,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;818&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!KhbK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!KhbK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png 424w, https://substackcdn.com/image/fetch/$s_!KhbK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png 848w, https://substackcdn.com/image/fetch/$s_!KhbK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png 1272w, https://substackcdn.com/image/fetch/$s_!KhbK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2eb7522f-34c1-4d0a-8f54-d5276a77e740_1115x818.png 1456w&quot; width=&quot;1115&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 10: Screenshot of the LM Arena leaderboard that shows the current leading LLMs based on user preferences on text tasks
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In the remainder of this section, we will implement a simple example of a leaderboard.
  &lt;/p&gt;
  &lt;p&gt;
   To create a concrete example, consider users prompting different LLMs in a setup similar to Figure 9. The list below represents pairwise votes where the first model is the winner:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;votes = [
    (&quot;GPT-5&quot;, &quot;Claude-3&quot;),
    (&quot;GPT-5&quot;, &quot;Llama-4&quot;),
    (&quot;Claude-3&quot;, &quot;Llama-3&quot;),
    (&quot;Llama-4&quot;, &quot;Llama-3&quot;),
    (&quot;Claude-3&quot;, &quot;Llama-3&quot;),
    (&quot;GPT-5&quot;, &quot;Llama-3&quot;),
]&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the list above, each tuple in the votes list represents a pairwise preference between two models, written as
   &lt;/span&gt;
   &lt;code&gt;
    (winner, loser)
   &lt;/code&gt;
   &lt;span&gt;
    . So,
   &lt;/span&gt;
   &lt;code&gt;
    (“GPT-5”, “Claude-3”)
   &lt;/code&gt;
   &lt;span&gt;
    means that a user preferred GPT-5 over a Claude-3 model answer.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the remainder of this section, we will turn the
   &lt;/span&gt;
   &lt;code&gt;
    votes
   &lt;/code&gt;
   &lt;span&gt;
    list into a leaderboard. For this, we will use the popular
   &lt;/span&gt;
   &lt;a href=&quot;https://en.wikipedia.org/wiki/Elo_rating_system&quot; rel=&quot;&quot;&gt;
    Elo rating system
   &lt;/a&gt;
   &lt;span&gt;
    , which was originally developed for ranking chess players.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Before we look at the concrete code implementation, in short, it works as follows. Each model starts with a baseline score. Then, after each comparison and the preference vote, the model’s rating is updated. Specifically, if a user prefers a current model over a highly ranked model, the current model will get a relatively large ranking update and rank higher in the leaderboard. Vice versa, if the current model loses against a lowly ranked model, it increases the rating only a little. (And if the current model loses, it is updated in a similar fashion, but with ranking points getting subtracted instead of added.)
  &lt;/p&gt;
  &lt;p&gt;
   The code to turn these pairwise rankings into a leaderboard is shown in the code block below.
  &lt;/p&gt;
  &lt;h4&gt;
   &lt;strong&gt;
    Code block 4: Constructing a leaderboard
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;pre&gt;&lt;code&gt;def elo_ratings(vote_pairs, k_factor=32,
                initial_rating=1000):
    # Initialize all models with the same base rating
    ratings = {
        model: initial_rating
        for pair in vote_pairs
        for model in pair
    }

    # Update ratings after each match
    for winner, loser in vote_pairs:

        # Expected score for the current winner
        expected_winner = 1.0 / (
            1.0 + 10 ** (
                (ratings[loser] - ratings[winner])
                / 400.0
            )
        )

        # k_factor determines sensitivity of updates
        ratings[winner] = (
            ratings[winner]
            + k_factor * (1 - expected_winner)
        )
        ratings[loser] = (
            ratings[loser]
            + k_factor * (0 - (1 - expected_winner))
        )

    return ratings&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;code&gt;
    elo_ratings
   &lt;/code&gt;
   &lt;span&gt;
    function defined above takes the votes as input and turns it into a leaderboard, as follows:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;ratings = elo_ratings(votes, k_factor=32, initial_rating=1000)
for model in sorted(ratings, key=ratings.get, reverse=True):
    print(f&quot;{model:8s} : {ratings[model]:.1f}&quot;)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   This results in the following leaderboard ranking, where the higher the score, the better:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;GPT-5 : 1043.7
Claude-3 : 1015.2
Llama-4 : 1000.7
Llama-3 : 940.4&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   So, how does this work? For each pair, we compute the expected score of the winner using the following formula:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;expected_winner = 1 / (1 + 10 ** ((rating_loser - rating_winner) / 400))&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    This value
   &lt;/span&gt;
   &lt;code&gt;
    expected_winner
   &lt;/code&gt;
   &lt;span&gt;
    is the model’s predicted chance to win in a no-draw setting based on the current ratings. It determines how large the rating update is.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    First, each model starts at
   &lt;/span&gt;
   &lt;code&gt;
    initial_rating = 1000
   &lt;/code&gt;
   &lt;span&gt;
    . If the two ratings (winner and loser) are equal, we have
   &lt;/span&gt;
   &lt;code&gt;
    expected_winner = 0.5
   &lt;/code&gt;
   &lt;span&gt;
    , which indicates an even match. In this case, the updates are:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;rating_winner + k_factor * (1 - 0.5) = rating_winner + 16

rating_loser + k_factor * (0 - (1 - 0.5)) = rating_loser - 16&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Now, if a heavy favorite (a model with a high rating) wins, we have
   &lt;/span&gt;
   &lt;code&gt;
    expected_winner ≈ 1
   &lt;/code&gt;
   &lt;span&gt;
    . The favorite gains only a small amount and the loser loses only a little:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;rating_winner + 32 * (1 - 0.99) = rating_winner + 0.32

rating_loser + 32 * (0 - (1 - 0.99)) = rating_loser - 0.32&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    However, if an underdog (a model with a low rating) wins, we have
   &lt;/span&gt;
   &lt;code&gt;
    expected_winner ≈ 0
   &lt;/code&gt;
   &lt;span&gt;
    , and the winner gets almost the full
   &lt;/span&gt;
   &lt;code&gt;
    k_factor
   &lt;/code&gt;
   &lt;span&gt;
    points while the loser loses about the same magnitude:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;rating_winner + 32 * (1 - 0.01) = rating_winner + 31.68

rating_loser + 32 * (0 - (1 - 0.01)) = rating_loser - 31.68&lt;/code&gt;&lt;/pre&gt;
  &lt;blockquote&gt;
   &lt;h4&gt;
    &lt;strong&gt;
     Order matters
    &lt;/strong&gt;
   &lt;/h4&gt;
   &lt;p&gt;
    The Elo approach updates ratings after each match (model comparisons), so later results build on ratings that have already been updated. This means the same set of outcomes, when presented in a different order, can end with slightly different final scores. This effect is usually mild, but it can happen especially when an upset happens early versus late.
   &lt;/p&gt;
   &lt;p&gt;
    &lt;span&gt;
     To reduce this order effect, we can shuffle the votes pairs and run the
    &lt;/span&gt;
    &lt;code&gt;
     elo_ratings
    &lt;/code&gt;
    &lt;span&gt;
     function multiple times and average the ratings.
    &lt;/span&gt;
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   Leaderboard approaches such as the one described above provide a more dynamic view of model quality than static benchmark scores. However, the results can be influenced by user demographics, prompt selection, and voting biases. Benchmarks and leaderboards can also be gamed, and users may select responses based on style rather than correctness. Finally, compared to automated benchmark harnesses, leaderboards do not provide instant feedback on newly developed variants, which makes them harder to use during active model development.
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;h4&gt;
    &lt;strong&gt;
     Other ranking methods
    &lt;/strong&gt;
   &lt;/h4&gt;
   &lt;p&gt;
    The LM Arena originally used the Elo method described in this section but recently transitioned to a statistical approach based on the Bradley–Terry model. The main advantage of the Bradley-Terry model is that, being statistically grounded, it allows the construction of confidence intervals to express uncertainty in the rankings. Also, in contrast to the Elo ratings, the Bradley-Terry model estimates all ratings jointly using a statistical fit over the entire dataset, which makes it immune to order effects.
   &lt;/p&gt;
   &lt;p&gt;
    &lt;span&gt;
     To keep the reported scores in a familiar range, the Bradley-Terry model is fitted to produce values comparable to Elo. Even though the leaderboard no longer officially uses Elo ratings, the term “Elo” remains widely used by LLM researchers and practitioners when comparing models. A code example showing the Elo rating is available
    &lt;/span&gt;
    &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/tree/main/chF/03_leaderboards&quot; rel=&quot;&quot;&gt;
     here on GitHub
    &lt;/a&gt;
    &lt;span&gt;
     .
    &lt;/span&gt;
   &lt;/p&gt;
   &lt;div&gt;
    &lt;figure&gt;
     &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!hCOf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
      &lt;div&gt;
       &lt;picture&gt;
        &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!hCOf!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png 424w, https://substackcdn.com/image/fetch/$s_!hCOf!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png 848w, https://substackcdn.com/image/fetch/$s_!hCOf!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png 1272w, https://substackcdn.com/image/fetch/$s_!hCOf!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png 1456w&quot; type=&quot;image/webp&quot;&gt;
         &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:688,&quot;width&quot;:1301,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:206507,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;688&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!hCOf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!hCOf!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png 424w, https://substackcdn.com/image/fetch/$s_!hCOf!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png 848w, https://substackcdn.com/image/fetch/$s_!hCOf!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png 1272w, https://substackcdn.com/image/fetch/$s_!hCOf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57300005-93c2-4984-b099-475e4c7d6aac_1301x688.png 1456w&quot; width=&quot;1301&quot;/&gt;
        &lt;/source&gt;
       &lt;/picture&gt;
      &lt;/div&gt;
     &lt;/a&gt;
     &lt;figcaption&gt;
      &lt;span&gt;
       Figure 11: A comparison of Elo and Bradley-Terry rankings; the source code is available
      &lt;/span&gt;
      &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/tree/main/chF/03_leaderboards&quot; rel=&quot;&quot;&gt;
       here on GitHub
      &lt;/a&gt;
      &lt;span&gt;
       .
      &lt;/span&gt;
     &lt;/figcaption&gt;
    &lt;/figure&gt;
   &lt;/div&gt;
  &lt;/blockquote&gt;
  &lt;h1&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Method 3: Judging responses with other LLMs
    &lt;/strong&gt;
   &lt;/em&gt;
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the early days, LLMs were evaluated using statistical and heuristics-based methods, including a measure called
   &lt;/span&gt;
   &lt;em&gt;
    BLEU
   &lt;/em&gt;
   &lt;span&gt;
    , which is a crude measure of how well generated text matches reference text. The problem with such metrics is that they require exact word matches and don’t account for synonyms, word changes, and so on.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   One solution to this problem, if we want to judge the written answer text as a whole, is to use relative rankings and leaderboard-based approaches as discussed in the previous section. However, a downside of leaderboards is the subjective nature of the preference-based comparisons as it involves human feedback (as well as the challenges that are associated with collecting this feedback).
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    A related method is to use another LLM with a pre-defined grading
   &lt;/span&gt;
   &lt;em&gt;
    rubric
   &lt;/em&gt;
   &lt;span&gt;
    (i.e., an evaluation guide) to compare an LLM’s response to a reference response and judge the response quality based on a pre-defined rubric, as illustrated in Figure 12.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!o8kH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!o8kH!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png 424w, https://substackcdn.com/image/fetch/$s_!o8kH!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png 848w, https://substackcdn.com/image/fetch/$s_!o8kH!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png 1272w, https://substackcdn.com/image/fetch/$s_!o8kH!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1295,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1295&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!o8kH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!o8kH!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png 424w, https://substackcdn.com/image/fetch/$s_!o8kH!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png 848w, https://substackcdn.com/image/fetch/$s_!o8kH!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png 1272w, https://substackcdn.com/image/fetch/$s_!o8kH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4b646f80-a08d-40f4-ae65-eb7b850c7ccd_1503x1337.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;strong&gt;
      Figure F12: Example of an LLM-judge evaluation. The model to be evaluated generates an answer, which is then scored by a separate judge LLM according to a rubric and a provided reference answer.
     &lt;/strong&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    In practice, the judge-based approach shown in Figure 12 works well when the judge LLM is strong. Common setups use leading proprietary LLMs via an API (e.g., the GPT-5 API), though specialized judge models also exist. (E.g., one of the many examples is
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2405.08029&quot; rel=&quot;&quot;&gt;
    Phudge
   &lt;/a&gt;
   &lt;span&gt;
    ; ultimately, most of these specialized models are just smaller models fine-tuned to have similar scoring behavior as proprietary GPT models.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   One of the reasons why judges work so well is also that evaluating an answer is often easier than generating one.
  &lt;/p&gt;
  &lt;p&gt;
   To implement a judge-based model evaluation as shown in Figue 12 programmatically in Python, we could either load one of the larger Qwen3 models in PyTorch and prompt it with a grading rubric and the model answer we want to evaluate.
  &lt;/p&gt;
  &lt;p&gt;
   Alternatively, we can use other LLMs through an API, for example the ChatGPT or Ollama API.
  &lt;/p&gt;
  &lt;p&gt;
   As we already know how to load Qwen3 models in PyTorch, to make it more interesting, in the remainder of the section, we will implement the judge-based evaluation shown in Figure 12 using the Ollama API in Python.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Specifically, we will use the 20-billion parameter gpt-oss open-weight model by OpenAI as it offers a good balance between capabilities and efficiency. For more information about gpt-oss, please see my
   &lt;/span&gt;
   &lt;em&gt;
    From GPT-2 to gpt-oss: Analyzing the Architectural Advances
   &lt;/em&gt;
   &lt;span&gt;
    article:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2&gt;
   3.1 Implementing a LLM-as-a-judge approach in Ollama
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://ollama.com&quot; rel=&quot;&quot;&gt;
    Ollama
   &lt;/a&gt;
   &lt;span&gt;
    is an efficient open-source application for running LLMs on a laptop. It serves as a wrapper around the open-source
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/ggerganov/llama.cpp&quot; rel=&quot;&quot;&gt;
    llama.cpp
   &lt;/a&gt;
   &lt;span&gt;
    library, which implements LLMs in pure C/C++ to maximize efficiency. However, note that Ollama is only a tool for generating text using LLMs (inference) and does not support training or fine-tuning LLMs.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    To execute the following code, please install Ollama by visiting the official website at
   &lt;/span&gt;
   &lt;a href=&quot;https://ollama.com&quot; rel=&quot;&quot;&gt;
    https://ollama.com
   &lt;/a&gt;
   &lt;span&gt;
    and follow the provided instructions for your operating system:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     For macOS and Windows users: Open the downloaded Ollama application. If prompted to install command-line usage, select “yes.”
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     For Linux users: Use the installation command available on the Ollama website.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Before implementing the model evaluation code, let’s first download the gpt-oss model and verify that Ollama is functioning correctly by using it from the command line terminal.
  &lt;/p&gt;
  &lt;p&gt;
   Execute the following command on the command line (not in a Python session) to try out the 20 billion parameter gpt-oss model:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;ollama run gpt-oss:20b&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The first time you execute this command, the 20 billion parameter gpt-oss model, which takes up 14 GB of storage space, will be automatically downloaded. The output looks as follows:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;$ ollama run gpt-oss:20b
pulling manifest 
pulling b112e727c6f1: 100% ▕██████████████████████▏  13 GB                         
pulling fa6710a93d78: 100% ▕██████████████████████▏ 7.2 KB                         
pulling f60356777647: 100% ▕██████████████████████▏  11 KB                         
pulling d8ba2f9a17b3: 100% ▕██████████████████████▏   18 B                         
pulling 55c108d8e936: 100% ▕██████████████████████▏  489 B                         
verifying sha256 digest 
writing manifest 
removing unused layers 
success&lt;/code&gt;&lt;/pre&gt;
  &lt;blockquote&gt;
   &lt;h4&gt;
    &lt;strong&gt;
     Alternative Ollama models
    &lt;/strong&gt;
   &lt;/h4&gt;
   &lt;p&gt;
    Note that the gpt-oss:20b in the ollama run gpt-oss:20b command refers to the 20 billion parameter gpt-oss model. Using Ollama with the gpt-oss:20b model requires approximately 13 GB of RAM. If your machine does not have sufficient RAM, you can try using a smaller model, such as the 4 billion parameter qwen3:4b model via ollama run qwen3:4b, which only requires around 4 GB of RAM.
   &lt;/p&gt;
   &lt;p&gt;
    For more powerful computers, you can also use the larger 120-billion parameter gpt-oss model by replacing gpt-oss:20b with gpt-oss:120b. However, keep in mind that this model requires significantly more computational resources.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   Once the model download is complete, we are presented with a command-line interface that allows us to interact with the model. For example, try asking the model, “What is 1+2?”:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt; What is 1+2?

Thinking...

User asks: “What is 1+2?” This is simple: answer 3. Provide explanation? Possibly ask for simple

arithmetic. Provide answer: 3.

...done thinking.

1 + 2 = **3**&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    You can end this ollama run gpt-oss:20b session using the input
   &lt;/span&gt;
   &lt;code&gt;
    /bye
   &lt;/code&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   You can end this ollama run gpt-oss:20b session using the input /bye.
  &lt;/p&gt;
  &lt;p&gt;
   In the remainder of this section, we will use the ollama API. This approach requires that Ollama is running in the background. There are three different options to achieve this:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    1. Run the
   &lt;/span&gt;
   &lt;code&gt;
    ollama serve
   &lt;/code&gt;
   &lt;span&gt;
    command in the terminal (recommended). This runs the Ollama backend as a server, usually on
   &lt;/span&gt;
   &lt;code&gt;
    http://localhost:11434
   &lt;/code&gt;
   &lt;span&gt;
    . Note that it doesn’t load a model until it’s called through the API (later in this section).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    2. Run the
   &lt;/span&gt;
   &lt;code&gt;
    ollama run gpt-oss:20b
   &lt;/code&gt;
   &lt;span&gt;
    command similar to earlier, but keep it open and don’t exit the session via
   &lt;/span&gt;
   &lt;code&gt;
    /bye
   &lt;/code&gt;
   &lt;span&gt;
    . As discussed earlier, this opens a minimal convenience wrapper around a local Ollama server. Behind the scenes, it uses the same server API as ollama serve.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   3. Ollama desktop app. Opening the desktop app runs the same backend automatically and provides a graphical interface on top of it as shown in Figure 12 earlier.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!NeHY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!NeHY!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png 424w, https://substackcdn.com/image/fetch/$s_!NeHY!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png 848w, https://substackcdn.com/image/fetch/$s_!NeHY!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png 1272w, https://substackcdn.com/image/fetch/$s_!NeHY!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/475023aa-2070-4533-9776-fba77b711052_911x1313.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1313,&quot;width&quot;:911,&quot;resizeWidth&quot;:606,&quot;bytes&quot;:474020,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;873.4116355653128&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!NeHY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!NeHY!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png 424w, https://substackcdn.com/image/fetch/$s_!NeHY!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png 848w, https://substackcdn.com/image/fetch/$s_!NeHY!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png 1272w, https://substackcdn.com/image/fetch/$s_!NeHY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F475023aa-2070-4533-9776-fba77b711052_911x1313.png 1456w&quot; width=&quot;606&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 13: Two different options to keep the Ollama server (/application) running so we can use it via the Ollama API in Python.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;h4&gt;
    &lt;strong&gt;
     Ollama server IP address
    &lt;/strong&gt;
   &lt;/h4&gt;
   &lt;p&gt;
    &lt;span&gt;
     Ollama runs locally on our machine by starting a local server-like process. When running ollama serve in the terminal, as described above, you may encounter an error message saying
    &lt;/span&gt;
    &lt;code&gt;
     Error: listen tcp 127.0.0.1:11434: bind: address already in use
    &lt;/code&gt;
    &lt;span&gt;
     .
    &lt;/span&gt;
   &lt;/p&gt;
   &lt;p&gt;
    &lt;span&gt;
     If that’s the case, try use the command
    &lt;/span&gt;
    &lt;code&gt;
     OLLAMA_HOST=127.0.0.1:11435 ollama serve
    &lt;/code&gt;
    &lt;span&gt;
     (and if this address is also in use, try to increment the numbers by one until you find an address not in use.)
    &lt;/span&gt;
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   The following code verifies that the Ollama session is running properly before we use Ollama to evaluate the test set responses generated in the previous section:
  &lt;/p&gt;
  &lt;h4&gt;
   Code block 5: Checking if Ollama is running
  &lt;/h4&gt;
  &lt;pre&gt;&lt;code&gt;import psutil

def check_if_running(process_name):
    running = False
    for proc in psutil.process_iter([&quot;name&quot;]):
        if process_name in proc.info[&quot;name&quot;]:
            running = True
            break
    return running

ollama_running = check_if_running(&quot;ollama&quot;)

if not ollama_running:
    raise RuntimeError(
        &quot;Ollama not running. &quot;
        &quot;Launch ollama before proceeding.&quot;
    )
print(&quot;Ollama running:&quot;, check_if_running(&quot;ollama&quot;))&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Ensure that the output from executing the previous code displays Ollama running:
   &lt;/span&gt;
   &lt;code&gt;
    True
   &lt;/code&gt;
   &lt;span&gt;
    . If it shows
   &lt;/span&gt;
   &lt;code&gt;
    False
   &lt;/code&gt;
   &lt;span&gt;
    , please verify that the
   &lt;/span&gt;
   &lt;code&gt;
    ollama serve
   &lt;/code&gt;
   &lt;span&gt;
    command or the Ollama application is actively running (see Figure 13).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the remainder of this article, we will interact with the local gpt-oss model, running on our machine, through the Ollama REST API using Python. The following
   &lt;/span&gt;
   &lt;code&gt;
    query_model
   &lt;/code&gt;
   &lt;span&gt;
    function demonstrates how to use the API:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h4&gt;
   &lt;strong&gt;
    Code block 6: Querying a local Ollama model
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;pre&gt;&lt;code&gt;import json
import urllib.request


def query_model(
    prompt,
    model=&quot;gpt-oss:20b&quot;,
    # If you used 
    # OLLAMA_HOST=127.0.0.1:11435 ollama serve
    # update the address below
    url=&quot;http://localhost:11434/api/chat&quot;
):
    # Create the data payload as a dictionary:
    data = {
        &quot;model&quot;: model,
        &quot;messages&quot;: [
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: prompt}
        ],
        # Settings required for deterministic responses:
        &quot;options&quot;: {
            &quot;seed&quot;: 123,
            &quot;temperature&quot;: 0,
            &quot;num_ctx&quot;: 2048
        }
    }

    # Convert the dictionary to JSON and encode it to bytes
    payload = json.dumps(data).encode(&quot;utf-8&quot;)

    # Create a POST request and add headers
    request = urllib.request.Request(  
        url,
        data=payload,
        method=&quot;POST&quot;
    )
    request.add_header(&quot;Content-Type&quot;, &quot;application/json&quot;)

    response_data = &quot;&quot;

    # Send the request and capture the streaming response
    with urllib.request.urlopen(request) as response:
        while True:
            line = response.readline().decode(&quot;utf-8&quot;)
            if not line:
                break
            # Parse each line into JSON
            response_json = json.loads(line)
            response_data += response_json[&quot;message&quot;][&quot;content&quot;]

    return response_data&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Here’s an example of how to use the
   &lt;/span&gt;
   &lt;code&gt;
    query_model
   &lt;/code&gt;
   &lt;span&gt;
    function that we just implemented:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;ollama_model = &quot;gpt-oss:20b&quot;
result = query_model(&quot;What is 1+2?&quot;, ollama_model)
print(result)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The resulting response is “3”. (It differs from what we’d get if we ran Ollama run or the Ollama application due to different default settings.)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Using the
   &lt;/span&gt;
   &lt;code&gt;
    query_model
   &lt;/code&gt;
   &lt;span&gt;
    function, we can evaluate the responses generated by our model with a prompt that includes a grading rubric asking the gpt-oss model to rate our target model’s responses on a scale from 1 to 5 based on a correct answer as a reference.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The prompt we use for this is shown below:
  &lt;/p&gt;
  &lt;h4&gt;
   &lt;strong&gt;
    Code block 7: Setting up the prompt template including grading rubric
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;pre&gt;&lt;code&gt;def rubric_prompt(instruction, reference_answer, model_answer):
    rubric = (
        &quot;You are a fair judge assistant. You will be &quot;
        &quot;given an instruction, a reference answer, and &quot;
        &quot;a candidate answer to evaluate, according to &quot;
        &quot;the following rubric:\n\n&quot;
        &quot;1: The response fails to address the &quot;
        &quot;instruction, providing irrelevant, incorrect, &quot;
        &quot;or excessively verbose content.\n&quot;
        &quot;2: The response partially addresses the &quot;
        &quot;instruction but contains major errors, &quot;
        &quot;omissions, or irrelevant details.\n&quot;
        &quot;3: The response addresses the instruction to &quot;
        &quot;some degree but is incomplete, partially &quot;
        &quot;correct, or unclear in places.\n&quot;
        &quot;4: The response mostly adheres to the &quot;
        &quot;instruction, with only minor errors, &quot;
        &quot;omissions, or lack of clarity.\n&quot;
        &quot;5: The response fully adheres to the &quot;
        &quot;instruction, providing a clear, accurate, and &quot;
        &quot;relevant answer in a concise and efficient &quot;
        &quot;manner.\n\n&quot;
        &quot;Now here is the instruction, the reference &quot;
        &quot;answer, and the response.\n&quot;
    )

    prompt = (
        f&quot;{rubric}\n&quot;
        f&quot;Instruction:\n{instruction}\n\n&quot;
        f&quot;Reference Answer:\n{reference_answer}\n\n&quot;
        f&quot;Answer:\n{model_answer}\n\n&quot;
        f&quot;Evaluation: &quot;
    )
    return prompt&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;code&gt;
    model_answer
   &lt;/code&gt;
   &lt;span&gt;
    in the
   &lt;/span&gt;
   &lt;code&gt;
    rubric_prompt
   &lt;/code&gt;
   &lt;span&gt;
    is intended to represent the response produced by our own model in practice. For illustration purposes, we hardcode a plausible model answer here rather than generating it dynamically. (However, feel free to use the Qwen3 model we loaded at the beginning of this article to generate a real
   &lt;/span&gt;
   &lt;code&gt;
    model_answer
   &lt;/code&gt;
   &lt;span&gt;
    ).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Next, let’s generate the rendered prompt for the Ollama model:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;rendered_prompt = rubric_prompt(
    instruction=(
        &quot;If all birds can fly, and a penguin is a bird, &quot;
        &quot;can a penguin fly?&quot;
    ),
    reference_answer=(
        &quot;Yes, according to the premise that all birds can fly, &quot;
        &quot;a penguin can fly.&quot;
    ),
    model_answer=(
        &quot;Yes – under those premises a penguin would be able to fly.&quot;
    )
)
print(rendered_prompt)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The output is as follows:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;You are a fair judge assistant. You will be given an instruction, a
reference answer, and a candidate answer to evaluate, according to the
following rubric:

1: The response fails to address the instruction, providing irrelevant,
incorrect, or excessively verbose content.
2: The response partially addresses the instruction but contains major
errors, omissions, or irrelevant details.
3: The response addresses the instruction to some degree but is
incomplete, partially correct, or unclear in places.
4: The response mostly adheres to the instruction, with only minor
errors, omissions, or lack of clarity.
5: The response fully adheres to the instruction, providing a clear,
accurate, and relevant answer in a concise and efficient manner.

Now here is the instruction, the reference answer, and the response.

Instruction:
If all birds can fly, and a penguin is a bird, can a penguin fly?

Reference Answer:
Yes, according to the premise that all birds can fly, a penguin can
fly.

Answer:
Yes – under those premises a penguin would be able to fly.

Evaluation: &lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Ending the prompt in
   &lt;/span&gt;
   &lt;code&gt;
    “Evaluation: “
   &lt;/code&gt;
   &lt;span&gt;
    incentivizes the model to generate the answer. Let’s see how the gpt-oss:20b model judges the response:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;result = query_model(rendered_prompt, ollama_model)
print(result)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The response is as follows:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;**Score: 5**

The candidate answer directly addresses the question, correctly applies the given premises, and concisely states that a penguin would be able to fly. It is accurate, relevant, and clear.&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    As we can see, the answer receives the highest score, which is reasonable, as it is indeed correct. While this was a simple example stepping through the process manually, we could take this idea further and implement a for-loop that iteratively queries the model (for example, the Qwen3 model we loaded earlier) with questions from an evaluation dataset and evaluate it via gpt-oss and calculate the average score. You can find an implementation of such a script where we evaluate the Qwen3 model on the MATH-500 dataset
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/tree/main/chF/04_llm-judge&quot; rel=&quot;&quot;&gt;
    here on GitHub
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!e5fC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e5fC!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png 424w, https://substackcdn.com/image/fetch/$s_!e5fC!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png 848w, https://substackcdn.com/image/fetch/$s_!e5fC!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png 1272w, https://substackcdn.com/image/fetch/$s_!e5fC!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/04869674-1018-4da5-b287-100920e21b9b_1332x864.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:864,&quot;width&quot;:1332,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:327005,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;864&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!e5fC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e5fC!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png 424w, https://substackcdn.com/image/fetch/$s_!e5fC!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png 848w, https://substackcdn.com/image/fetch/$s_!e5fC!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png 1272w, https://substackcdn.com/image/fetch/$s_!e5fC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F04869674-1018-4da5-b287-100920e21b9b_1332x864.png 1456w&quot; title=&quot;&quot; width=&quot;1332&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 14: A comparison of the Qwen3 0.6 base and reasoning variants on the first 10 examples in MATH-500 evaluated by gpt-oss:20b as a judge. You can find the code
     &lt;/span&gt;
     &lt;a href=&quot;https://github.com/rasbt/reasoning-from-scratch/tree/main/chF/04_llm-judge&quot; rel=&quot;&quot;&gt;
      here on GitHub
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;h3&gt;
    &lt;strong&gt;
     Scoring intermediate reasoning steps with process reward models
    &lt;/strong&gt;
   &lt;/h3&gt;
   &lt;p&gt;
    &lt;span&gt;
     Related to symbolic verifiers and LLM judges, there is a class of learned models called
    &lt;/span&gt;
    &lt;em&gt;
     process reward models
    &lt;/em&gt;
    &lt;span&gt;
     (PRMs). Like judges, PRMs can evaluate reasoning traces beyond just the final answer, but unlike general judges, they focus specifically on the intermediate steps of reasoning. And unlike verifiers, which check correctness symbolically and usually only at the outcome level, PRMs provide step-by-step reward signals during training in reinforcement learning. We can categorize PRMs as “step-level judges,” which are predominantly developed for training, not pure evaluation. (In practice, PRMs are difficult to train reliably at scale. For example, DeepSeek R1 did not adopt PRMs and instead combined verifiers for the reasoning training.)
    &lt;/span&gt;
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   Judge-based evaluations offer advantages over preference-based leaderboards, including scalability and consistency, as they do not rely on large pools of human voters. (Technically, it is possible to outsource the preference-based rating behind leaderboards to LLM judges as well). However, LLM judges also share similar weaknesses with human voters: results can be biased by model preferences, prompt design, and answer style. Also, there is a strong dependency on the choice of judge model and rubric, and they lack the reproducibility of fixed benchmarks.
  &lt;/p&gt;
  &lt;h1&gt;
   Conclusion
  &lt;/h1&gt;
  &lt;p&gt;
   In this article, we covered four different evaluation approaches: multiple choice, verifiers, leaderboards, and LLM judges.
  &lt;/p&gt;
  &lt;p&gt;
   I know this was a long article, but I hope you found it useful for getting an overview of how LLMs are evaluated. A from-scratch approach like this can be verbose, but it is a great way to understand how these methods work under the hood, which in turn helps us identify weaknesses and areas for improvement.
  &lt;/p&gt;
  &lt;p&gt;
   That being said, you are probably wondering, “What is the best way to evaluate an LLM?” Unfortunately, there is no single best method since, as we have seen, each comes with different trade-offs. In short:
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Multiple-choice
   &lt;/strong&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Relatively quick and cheap to run at scale
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Standardized and reproducible across papers (or model cards)
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Measures basic knowledge recall
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Does not reflect how LLMs are used in the real world
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Verifiers
   &lt;/strong&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Standardized, objective grading for domains with ground truth
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Allows free-form answers (with some constraints on final answer formatting)
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Can also score intermediate steps if using process verifiers or process reward models
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Requires verifiable domains (for example, math or code), and building good verifiers can be tricky
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Outcome-only verifiers evaluate only the final answer, not reasoning quality
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Arena-style leaderboards (human pairwise preference)
   &lt;/strong&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Directly answers “Which model do people prefer?” on real prompts
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Allows free-form answers and implicitly accounts for style, helpfulness, and safety
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Expensive and time-intensive for humans
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Does not measure correctness, only preference
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Nonstationary populations can affect stability
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    LLM-as-a-judge
   &lt;/strong&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Scalable across many tasks
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (+) Allows free-form answers
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Dependent on the judge’s capability (ensembles can make this more robust)
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    (-) Depends on rubric choice
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   While I am usually not a big fan of radar plots, one can be helpful here to visualize these different evaluation areas, as shown below.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!vxlv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vxlv!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png 424w, https://substackcdn.com/image/fetch/$s_!vxlv!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png 848w, https://substackcdn.com/image/fetch/$s_!vxlv!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png 1272w, https://substackcdn.com/image/fetch/$s_!vxlv!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1209,&quot;width&quot;:1456,&quot;resizeWidth&quot;:478,&quot;bytes&quot;:270638,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/175225406?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;396.9107142857143&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!vxlv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vxlv!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png 424w, https://substackcdn.com/image/fetch/$s_!vxlv!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png 848w, https://substackcdn.com/image/fetch/$s_!vxlv!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png 1272w, https://substackcdn.com/image/fetch/$s_!vxlv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faacd21b7-bf77-4cdc-bdb4-efe254a7e1b1_1586x1317.png 1456w&quot; title=&quot;&quot; width=&quot;478&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 15: A radar chart showing conceptually that we ideally want to pay attention to different areas when evaluating an LLM to identify its strengths and weaknesses.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   For instance, a strong multiple-choice rating suggests that the model has solid general knowledge. Combine that with a strong verifier score, and the model is likely also answering technical questions correctly. However, if the model performs poorly on LLM-as-a-judge and leaderboard evaluations, it may struggle to write or articulate responses effectively and could benefit from some RLHF.
  &lt;/p&gt;
  &lt;p&gt;
   So, the best evaluation combines multiple areas. But ideally it also uses data that directly aligns with your goals or business problems. For example, suppose you are implementing an LLM to assist with legal or law-related tasks. It makes sense to run the model on standard benchmarks like MMLU as a quick sanity check, but ultimately you will want to tailor the evaluations to your target domain, such as law. You can find public benchmarks online that serve as good starting points, but in the end, you will want to test with your own proprietary data. Only then can you be reasonably confident that the model has not already seen the test data during training.
  &lt;/p&gt;
  &lt;p&gt;
   In any case, model evaluation is a very big and important topic. I hope this article was useful in explaining how the main approaches work, and that you took away a few useful insights for the next time you look at model evaluations or run them yourself.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    As always,
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    Happy tinkering!
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    This magazine is a personal passion project, and your support helps keep it alive.
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you’d like to support my work, please consider my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch)
    &lt;/a&gt;
    &lt;span&gt;
     book or its follow-up,
    &lt;/span&gt;
    &lt;a href=&quot;https://mng.bz/Nwr7&quot; rel=&quot;&quot;&gt;
     Build a Reasoning Model (From Scratch)
    &lt;/a&gt;
    &lt;span&gt;
     . (I’m confident you’ll get a lot out of these; they explain how LLMs work in depth you won’t find elsewhere.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    Thanks for reading, and for helping support independent research!
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!RCl_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!RCl_!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp 424w, https://substackcdn.com/image/fetch/$s_!RCl_!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp 848w, https://substackcdn.com/image/fetch/$s_!RCl_!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp 1272w, https://substackcdn.com/image/fetch/$s_!RCl_!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;Build a Large Language Model (From Scratch)&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;Build a Large Language Model (From Scratch)&quot;,&quot;title&quot;:&quot;Build a Large Language Model (From Scratch)&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;726&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!RCl_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!RCl_!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp 424w, https://substackcdn.com/image/fetch/$s_!RCl_!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp 848w, https://substackcdn.com/image/fetch/$s_!RCl_!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp 1272w, https://substackcdn.com/image/fetch/$s_!RCl_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F27a118a0-5da6-4486-b1f0-3743754d0a77_8106x4044.webp 1456w&quot; title=&quot;Build a Large Language Model (From Scratch)&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;em&gt;
      &lt;span&gt;
       Build a Large Language Model (From Scratch) is now available on
      &lt;/span&gt;
      &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
       Amazon
      &lt;/a&gt;
      &lt;span&gt;
       . Build a Reasoning Model (From Scratch) is in
      &lt;/span&gt;
      &lt;a href=&quot;https://mng.bz/Nwr7&quot; rel=&quot;&quot;&gt;
       Early Access at Manning
      &lt;/a&gt;
      &lt;span&gt;
       .
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you read the book and have a few minutes to spare, I’d really appreciate a
   &lt;/span&gt;
   &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
    brief review
   &lt;/a&gt;
   &lt;span&gt;
    . It helps us authors a lot!
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Your support means a great deal! Thank you!
   &lt;/strong&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Understanding and Implementing Qwen3 From Scratch </title>
<link>https://magazine.sebastianraschka.com/p/qwen3-from-scratch</link>
<pubDate>Sat, 06 Sep 2025 11:10:21 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;div dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   &lt;span&gt;
    Previously, I compared the most notable open-weight architectures of 2025 in
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison&quot; rel=&quot;&quot;&gt;
    The Big LLM Architecture Comparison
   &lt;/a&gt;
   &lt;span&gt;
    . Then, I zoomed in and discussed the various architecture components in
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the&quot; rel=&quot;&quot;&gt;
    From GPT-2 to gpt-oss: Analyzing the Architectural Advances
   &lt;/a&gt;
   &lt;span&gt;
    on a conceptual level.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Since all good things come in threes, before covering some of the noteworthy research highlights of this summer, I wanted to now dive into these architectures hands-on, in code. By following along, you will understand how it actually works under the hood and gain building blocks you can adapt for your own experiments or projects.
  &lt;/p&gt;
  &lt;div data-component-name=&quot;VideoEmbedPlayer&quot;&gt;
   &lt;div&gt;
    &lt;div aria-label=&quot;Video player&quot; role=&quot;region&quot;&gt;
     &lt;div&gt;
      &lt;video controlslist=&quot;nodownload&quot; crossorigin=&quot;anonymous&quot; poster=&quot;https://substack-video.s3.amazonaws.com/video_upload/post/172832845/ca5edee4-5320-47fc-a73a-0269f4850125/transcoded-00001.png?refresh=Sat Sep 06 2025 11:29:51 GMT+0000 (Coordinated Universal Time)&quot; src=&quot;blob:https://magazine.sebastianraschka.com/fd02a7e5-8261-4712-9203-4c71b38f5099&quot;&gt;
      &lt;/video&gt;
     &lt;/div&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For this, I picked Qwen3 (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2505.09388&quot; rel=&quot;&quot;&gt;
    initially released in May
   &lt;/a&gt;
   &lt;span&gt;
    and updated in July) because it is one of the most widely liked and used open-weight model families as of this writing.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The reasons why Qwen3 models are so popular are, in my view, as follows:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      A developer- and commercially friendly open-source (
     &lt;/span&gt;
     &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-0.6B/blob/main/LICENSE&quot; rel=&quot;&quot;&gt;
      Apache License v2.0
     &lt;/a&gt;
     &lt;span&gt;
      ) without any strings attached beyond the original open-source license terms (some other open-weight LLMs impose additional usage limits)
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      The performance is really good; for example, as of this writing, the open-weight 235B-Instruct variant is ranked 8 on the
     &lt;/span&gt;
     &lt;a href=&quot;https://lmarena.ai/leaderboard/text&quot; rel=&quot;&quot;&gt;
      LMArena leaderboard
     &lt;/a&gt;
     &lt;span&gt;
      , tied with the proprietary Claude Opus 4. The only 2 other open-weight LLMs that rank higher are DeepSeek 3.1 (3x larger) and Kimi K2 (4x larger). On September 5th,
     &lt;/span&gt;
     &lt;a href=&quot;https://x.com/Alibaba_Qwen/status/1963991502440562976&quot; rel=&quot;&quot;&gt;
      Qwen3 released a 1T parameter “max” variant
     &lt;/a&gt;
     &lt;span&gt;
      on their platform that beats Kimi K2, DeepSeek 3.1, and Claude Opus 4 on all major benchmarks; however, this model is closed-source for now.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     There are many different model sizes available for different compute budgets and use-cases, from 0.6B dense models to 480B parameter Mixture-of-Experts models.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   This is going to be a long article due to the from-scratch code in pure PyTorch. While the code sections may look verbose, I hope that they help explain the building blocks better than conceptual figures alone!
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Tip 1:
   &lt;/strong&gt;
   &lt;span&gt;
    If you are reading this article in your email inbox, the narrow line width may cause code snippets to wrap awkwardly. For a better experience, I recommend
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/qwen3-from-scratch&quot; rel=&quot;&quot;&gt;
    opening it in your web browser
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Tip 2:
   &lt;/strong&gt;
   &lt;span&gt;
    You can use the table of contents on the left side of the website for easier navigation between sections.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;br/&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!_APX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_APX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png 424w, https://substackcdn.com/image/fetch/$s_!_APX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png 848w, https://substackcdn.com/image/fetch/$s_!_APX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png 1272w, https://substackcdn.com/image/fetch/$s_!_APX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:652,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:892224,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/172832845?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;652&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!_APX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_APX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png 424w, https://substackcdn.com/image/fetch/$s_!_APX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png 848w, https://substackcdn.com/image/fetch/$s_!_APX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png 1272w, https://substackcdn.com/image/fetch/$s_!_APX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb3d5266-b65a-45ea-9f8f-98d7d8038b8e_3432x1536.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 1: Preview of the Qwen3 Dense and Mixture-of-Experts architectures discussed and (re)implemented in pure PyTorch in this article.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> From GPT-2 to gpt-oss: Analyzing the Architectural Advances </title>
<link>https://magazine.sebastianraschka.com/p/from-gpt-2-to-gpt-oss-analyzing-the</link>
<pubDate>Sat, 09 Aug 2025 11:23:07 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;div dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   OpenAI just released their new open-weight LLMs this week: gpt-oss-120b and gpt-oss-20b, their first open-weight models since GPT-2 in 2019. And yes, thanks to some clever optimizations, they can run locally (but more about this later).
  &lt;/p&gt;
  &lt;p&gt;
   This is the first time since GPT-2 that OpenAI has shared a large, fully open-weight model. Earlier GPT models showed how the transformer architecture scales. The 2022 ChatGPT release then made these models mainstream by demonstrating concrete usefulness for writing and knowledge (and later coding) tasks. Now they have shared some long-awaited weight model, and the architecture has some interesting details.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
   I spent the past few days reading through the code and technical reports to summarize the most interesting details. (Just days after, OpenAI also announced GPT-5, which I will briefly discuss in the context of the gpt-oss models at the end of this article.)
  &lt;/p&gt;
  &lt;p&gt;
   Below is a quick preview of what the article covers. For easier navigation, I recommend using the Table of Contents on the left of on the article page.
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Model architecture comparisons with GPT-2
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     MXFP4 optimization to fit gpt-oss models onto single GPUs
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Width versus depth trade-offs (gpt-oss vs Qwen3)
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Attention bias and sinks
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Benchmarks and comparisons with GPT-5
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   I hope you find it informative!
  &lt;/p&gt;
  &lt;h2&gt;
   &lt;strong&gt;
    1. Model Architecture Overview
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Before we discuss the architecture in more detail, let&#x27;s start with an overview of the two models, gpt-oss-20b and gpt-oss-120b, shown in Figure 1 below.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!rlW0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:681,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:243817,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;681&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!rlW0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 424w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 848w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1272w, https://substackcdn.com/image/fetch/$s_!rlW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F83b5bdbd-7b2b-4c52-9783-0d4de94a0e5a_1589x743.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 1: The two gpt-oss models side by side.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you have looked at recent LLM architecture diagrams before, or read my previous
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison&quot; rel=&quot;&quot;&gt;
    Big Architecture Comparison
   &lt;/a&gt;
   &lt;span&gt;
    article, you may notice that there is nothing novel or unusual at first glance. This is not surprising, since leading LLM developers tend to use the same base architecture and then apply smaller tweaks. This is pure speculation on my part, but I think this is because
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     There is significant rotation of employees between these labs.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      We still have not found anything better than the transformer architecture. Even though state space models and text diffusion models exist, as far as I know no one has shown that they perform as well as transformers at this scale. (Most of the comparisons I found focus only on benchmark performance. It is still unclear how well the models handle real-world, multi-turn writing and coding tasks. At the time of writing, the highest-ranking non-purely-transformer-based model on the
     &lt;/span&gt;
     &lt;a href=&quot;https://lmarena.ai/leaderboard/text&quot; rel=&quot;&quot;&gt;
      LM Arena
     &lt;/a&gt;
     &lt;span&gt;
      is Jamba, which is a transformer–state space model hybrid, at rank 96.)
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Most of the gains likely come from data and algorithm tweaks rather than from major architecture changes.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   That being said, there are still many interesting aspects of their design choices. Some are shown in the figure above (while others are not, but we will discuss them later as well). In the rest of this article, I will highlight these features and compare them to other architectures, one at a time.
  &lt;/p&gt;
  &lt;p&gt;
   I should also note that I am not affiliated with OpenAI in any way. My information comes from reviewing the released model code and reading their technical reports. If you want to learn how to use these models locally, the best place to start is OpenAI&#x27;s official model hub pages:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://huggingface.co/openai/gpt-oss-20b&quot; rel=&quot;&quot;&gt;
      https://huggingface.co/openai/gpt-oss-20b
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://huggingface.co/openai/gpt-oss-120b&quot; rel=&quot;&quot;&gt;
      https://huggingface.co/openai/gpt-oss-120b
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   The 20B model can run on a consumer GPU with up to 16 GB of RAM. The 120B model can run on a single H100 with 80 GB of RAM or newer hardware. I will return to this later, as there are some important caveats.
  &lt;/p&gt;
  &lt;h2&gt;
   &lt;strong&gt;
    2. Coming From GPT-2
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Before we jump into comparisons between gpt-oss and a more recent architecture, let&#x27;s hop into the time machine and take a side-by-side look at GPT-2 (Figure 2) to see just how far things have come.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!AsnD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:788,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:267271,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;788&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!AsnD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 424w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 848w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1272w, https://substackcdn.com/image/fetch/$s_!AsnD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff07debb6-0111-4ff4-a0c9-d4ce27120b72_1531x829.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 2: A side-by-side comparison between gpt-oss-20b and GPT-2 XL 1.5B.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    Both gpt-oss and GPT-2 are decoder-only LLMs built on the transformer architecture introduced in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
    Attention Is All You Need (2017)
   &lt;/a&gt;
   &lt;span&gt;
    paper. Over the years, many details have evolved.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    However, these changes are not unique to gpt-oss. And as we will see later, they appear in many other LLMs. Since I discussed many of these aspects in the previous
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison&quot; rel=&quot;&quot;&gt;
    Big Architecture Comparison
   &lt;/a&gt;
   &lt;span&gt;
    article, I will try to keep each subsection brief and focused.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    1.1 Removing Dropout
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1207.0580&quot; rel=&quot;&quot;&gt;
    Dropout (2012)
   &lt;/a&gt;
   &lt;span&gt;
    is a traditional technique to prevent overfitting by randomly &quot;dropping out&quot; (i.e., setting to zero) a fraction of the layer activations or attention scores (Figure 3) during training. However, dropout is rarely used in modern LLMs, and most models after GPT-2 have dropped it (no pun intended).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!BS-w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:850,&quot;width&quot;:845,&quot;resizeWidth&quot;:554,&quot;bytes&quot;:130475,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;557.2781065088758&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!BS-w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 424w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 848w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1272w, https://substackcdn.com/image/fetch/$s_!BS-w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb25371fe-9ee0-4a65-a5aa-46f3bca4b349_845x850.png 1456w&quot; width=&quot;554&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 3: An illustration of dropout applied to the attention score matrix.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   I assume that dropout was originally used in GPT-2 because it was inherited from the original transformer architecture. Researchers likely noticed that it does not really improve LLM performance (I observed the same in my small-scale GPT-2 replication runs). This is likely because LLMs are typically trained for only a single epoch over massive datasets, which is in contrast to the multi-hundred-epoch training regimes for which dropout was first introduced. So, since LLMs see each token only once during training, there is little risk of overfitting.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Interestingly, while Dropout is kind of ignored in LLM architecture design for many years, I found a
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2505.24788&quot; rel=&quot;&quot;&gt;
    2025 research paper
   &lt;/a&gt;
   &lt;span&gt;
    with small scale LLM experiments (Pythia 1.4B) that confirms that Dropout results in worse downstream performance in these single-epoch regimes.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    1.2 RoPE Replaces Absolute Positional Embeddings
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   In transformer-based LLMs, positional encoding is necessary because of the attention mechanism. By default, attention treats the input tokens as if they have no order. In the original GPT architecture, absolute positional embeddings addressed this by adding a learned embedding vector for each position in the sequence (Figure 4), which is then added to the token embeddings.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!YCov!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:533,&quot;width&quot;:1195,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:123823,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;533&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!YCov!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 424w, https://substackcdn.com/image/fetch/$s_!YCov!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 848w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1272w, https://substackcdn.com/image/fetch/$s_!YCov!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F62fdfc32-605c-4065-abc6-689756d53b87_1195x533.png 1456w&quot; width=&quot;1195&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 4: Illustration of absolute positional embeddings.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    RoPE (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2104.09864&quot; rel=&quot;&quot;&gt;
    Rotary Position Embedding
   &lt;/a&gt;
   &lt;span&gt;
    ) introduced a different approach: instead of adding position information as separate embeddings, it encodes position by rotating the query and key vectors in a way that depends on each token&#x27;s position. (RoPE is an elegant idea but also a bit of a tricky topic to explain. I plan to cover separately in more detail one day.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While first introduced in 2021, RoPE became widely adopted with the release of the original Llama model in 2023 and has since become a staple in modern LLMs.
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    1.3 Swish/SwiGLU Replaces GELU
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   Early GPT architectures used GELU. Why now use Swish over GELU? Swish is considered computationally slightly cheaper, and in my opinion, that all there is to it. Depending on which paper you look at, you will find that one is slightly better than the other in terms of modeling performance. In my opinion, these small differences are probably within a standard error, and your mileage will vary based on hyperparameter sensitivity.
  &lt;/p&gt;
  &lt;p&gt;
   Activation functions used to be a hot topic of debate until the deep learning community largely settled on ReLU more than a decade ago. Since then, researchers have proposed and tried many ReLU-like variants with smoother curves, and GELU and Swish (Figure 5) are the ones that stuck.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!WIz6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:775,&quot;width&quot;:1407,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:237022,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;775&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!WIz6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 424w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 848w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1272w, https://substackcdn.com/image/fetch/$s_!WIz6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8aa7b74a-6520-4124-99fd-6df60b9e0e7e_1407x775.png 1456w&quot; width=&quot;1407&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 5: Comparison between Swish and GELU activations, which are both smoother versions or ReLU.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Early GPT architectures used GELU, which is defined as
   &lt;/span&gt;
   &lt;code&gt;
    0.5x * [1 + erf(x / sqrt(2))]
   &lt;/code&gt;
   &lt;span&gt;
    . Here,
   &lt;/span&gt;
   &lt;code&gt;
    erf
   &lt;/code&gt;
   &lt;span&gt;
    (short for error function) is the integral of a Gaussian and it is computed using polynomial approximations of the Gaussian integral, which makes it more computationally expensive than simpler functions like the sigmoid used in Swish, where Swish is simply
   &lt;/span&gt;
   &lt;code&gt;
    x * sigmoid(x)
   &lt;/code&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In practice, Swish is computationally slightly cheaper than GELU, and that&#x27;s probably the main reason it replaced GELU in most newer models. Depending on which paper we look at, one might be somewhat better in terms of modeling performance. But I&#x27;d say these gains are often within standard error, and the winner will depend heavily on hyperparameter tuning.
  &lt;/p&gt;
  &lt;p&gt;
   Swish is used in most architectures today. However, GELU is not entirely forgotten; for example, Google&#x27;s Gemma models still use GELU.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    What&#x27;s more notable, though, is that the feed forward module (a small multi-layer perceptron) is replaced by a gated &quot;GLU&quot; counterpart, where GLU stands for gated linear unit and was proposed in a
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/pdf/2002.05202&quot; rel=&quot;&quot;&gt;
    2020 paper
   &lt;/a&gt;
   &lt;span&gt;
    . Concretely, the 2 fully connected layers are replaced by 3 fully connected layers that are used as shown in Figure 6 below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!8gzt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:844,&quot;width&quot;:1005,&quot;resizeWidth&quot;:655,&quot;bytes&quot;:190423,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;550.0696517412936&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!8gzt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 424w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 848w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1272w, https://substackcdn.com/image/fetch/$s_!8gzt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F10a8befb-b407-45a4-b38b-486c3d7a65d6_1005x844.png 1456w&quot; width=&quot;655&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 6: A comparison between Swish and GELU and their gated counterparts, SwiGLU and GEGLU.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    At first glance, it may appear that the GEGLU/SwiGLU variants may be better than the regular feed forward layers because there are simply more parameters due to the extra layer. But this is deceiving because in practice, the
   &lt;/span&gt;
   &lt;code&gt;
    W
   &lt;/code&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;code&gt;
    V
   &lt;/code&gt;
   &lt;span&gt;
    weight layers in SwiGLU/GEGLU are usually chosen to be half the size each of the
   &lt;/span&gt;
   &lt;code&gt;
    W_1
   &lt;/code&gt;
   &lt;span&gt;
    layer in a traditional feed forward layer.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   To illustrate this better, consider the concrete code implementations of the regular and GLU variants:
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!_JVz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:776,&quot;width&quot;:1039,&quot;resizeWidth&quot;:687,&quot;bytes&quot;:267148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;513.1010587102984&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_JVz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 424w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 848w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1272w, https://substackcdn.com/image/fetch/$s_!_JVz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc0fc02c2-0a64-4137-b988-c1f7381da13c_1039x776.png 1456w&quot; width=&quot;687&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 7: Regular feed forward module (top) and SwiGLU variant (bottom) next to each other.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   So, suppose we have an embedding dimension of 1024. In the regular feed forward case, this would then be
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     fc1: 1024 × 4096 = 4,194,304
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     fc2: 1024 × 4096 = 4,194,304
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   That is fc1 + fc2 = 8,388,608 parameters.
  &lt;/p&gt;
  &lt;p&gt;
   For the GLU variant, we have
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     fc1: 1024 × 2048 = 2,097,152
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     fc2: 1024 × 2048 = 2,097,152
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     fc3: 2048 × 1024 = 2,097,152
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   I.e., 3 × 2,097,152 = 6,291,456 weight parameters.
  &lt;/p&gt;
  &lt;p&gt;
   So, overall, using the GLU variants results in fewer parameters, and they perform better as well. The reason for this better performance is that these GLU variants provide an additional multiplicative interaction, which improves expressivity (the same reason deep &amp; slim neural nets perform better than shallow &amp; wide neural nets, provided they are trained well).
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    1.4 Mixture-of-Experts Replaces Single FeedForward Module
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   In addition to upgrading the feed forward module to a SwiGLU, as discussed in the previous section, gpt-oss replaces the single feed forward module with multiple feed forward modules, using only a subset for each token generation step. This approach is known as a Mixture-of-Experts (MoE) and illustrated in Figure 8 below.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!SYqb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:640,&quot;width&quot;:1307,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:120915,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;640&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!SYqb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 424w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 848w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1272w, https://substackcdn.com/image/fetch/$s_!SYqb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffa3367a4-914a-49e0-8c94-016969397ab3_1307x640.png 1456w&quot; width=&quot;1307&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 8: The feed forward module is replaced by a Mixture-of-Expert (MoE) module.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, replacing
   &lt;/span&gt;
   &lt;em&gt;
    a single
   &lt;/em&gt;
   &lt;span&gt;
    feed forward module with
   &lt;/span&gt;
   &lt;em&gt;
    multiple
   &lt;/em&gt;
   &lt;span&gt;
    feed forward modules (as done in a MoE setup) substantially increases the model&#x27;s total parameter count. However, the key trick is that we don&#x27;t use (&quot;activate&quot;) all experts for every token. Instead, a router selects only a small subset of experts per token.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Because only a few experts are active at a time, MoE modules are often referred to as
   &lt;/span&gt;
   &lt;em&gt;
    sparse
   &lt;/em&gt;
   &lt;span&gt;
    , in contrast to
   &lt;/span&gt;
   &lt;em&gt;
    dense
   &lt;/em&gt;
   &lt;span&gt;
    modules that always use the full parameter set. However, the large total number of parameters via an MoE increases the capacity of the LLM, which means it can take up more knowledge during training. The sparsity keeps inference efficient, though, as we don&#x27;t use all the parameters at the same time.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   (Fun fact: In most MoE models, expert weights account for more than 90% of the total model parameters.)
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    1.5 Grouped Query Attention Replaces Multi-Head Attention
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   As mentioned in my previous articles, Grouped Query Attention (GQA) has emerged in recent years as a more compute- and parameter-efficient alternative to Multi-Head Attention (MHA).
  &lt;/p&gt;
  &lt;p&gt;
   In MHA, each head has its own set of keys and values. GQA reduces memory usage by grouping multiple heads to share the same key and value projections.
  &lt;/p&gt;
  &lt;p&gt;
   For example, as shown in Figure 9, if there are 2 key–value groups and 4 attention heads, heads 1 and 2 might share one set of keys and values, while heads 3 and 4 share another. This grouping decreases the total number of key and value computations, leading to lower memory usage and improved efficiency — without noticeably affecting modeling performance, according to ablation studies.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Kohq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:588,&quot;width&quot;:1237,&quot;resizeWidth&quot;:637,&quot;bytes&quot;:83420,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;302.7938561034762&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Kohq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 424w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 848w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1272w, https://substackcdn.com/image/fetch/$s_!Kohq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa2347cc2-3685-4547-b31b-be7bbfb21201_1237x588.png 1456w&quot; width=&quot;637&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 9: A comparison between MHA and GQA. Here, the group size is 2, where a key and value pair is shared among 2 queries.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   So, the core idea behind GQA is to reduce the number of key and value heads by sharing them across multiple query heads. This (1) lowers the model&#x27;s parameter count and (2) reduces the memory bandwidth usage for key and value tensors during inference since fewer keys and values need to be stored and retrieved from the KV cache.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (If you are curious how GQA looks in code, see my
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb&quot; rel=&quot;&quot;&gt;
    GPT-2 to Llama 3 conversion guide
   &lt;/a&gt;
   &lt;span&gt;
    for a version without KV cache and my KV-cache variant
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py&quot; rel=&quot;&quot;&gt;
    here
   &lt;/a&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    While GQA is mainly a computational-efficiency workaround for MHA, ablation studies (such as those in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2305.13245&quot; rel=&quot;&quot;&gt;
    original GQA paper
   &lt;/a&gt;
   &lt;span&gt;
    and the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2307.09288&quot; rel=&quot;&quot;&gt;
    Llama 2 paper
   &lt;/a&gt;
   &lt;span&gt;
    ) show it performs comparably to standard MHA in terms of LLM modeling performance.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    1.6 Sliding Window Attention
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    Sliding-window attention (Figure 10 below) was first introduced in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2004.05150&quot; rel=&quot;&quot;&gt;
    LongFormer paper (2020)
   &lt;/a&gt;
   &lt;span&gt;
    and later popularized by Mistral. Interestingly, gpt-oss applies it in every second layer. You can think of it as a variation of multi-head attention, or in this case grouped query attention (GQA), where the attention context is restricted to a smaller window, reducing both memory usage and compute costs.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!wwFe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:225815,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;721&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!wwFe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wwFe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe79d48c3-2bdf-41bb-9e18-45f128cd5c01_1600x792.jpeg 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 10: Comparison between regular attention (left) and sliding window attention (right).
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Concretely, gpt-oss alternates between GQA layers that attend to the full context and GQA layers with a sliding window limited to 128 tokens.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    As I discussed in my
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison&quot; rel=&quot;&quot;&gt;
    previous article
   &lt;/a&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2408.00118&quot; rel=&quot;&quot;&gt;
    Gemma 2 (2024)
   &lt;/a&gt;
   &lt;span&gt;
    used a similar 1:1 ratio.
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2503.19786&quot; rel=&quot;&quot;&gt;
    Gemma 3
   &lt;/a&gt;
   &lt;span&gt;
    earlier this year went much further and shifted to a 5:1 ratio, which means only one full-attention layer for every five sliding-window (local) attention layers.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   According to the Gemma ablation studies, sliding-window attention has minimal impact on modeling performance, as shown in the figure below. Note that the window size in Gemma 2 was 4096 tokens, which Gemma 3 reduced to 1024. In gpt-oss, the window is just 128 tokens, which is remarkably small.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    And as a fun fact, the
   &lt;/span&gt;
   &lt;a href=&quot;https://openai.com/index/introducing-gpt-oss/&quot; rel=&quot;&quot;&gt;
    official announcement article
   &lt;/a&gt;
   &lt;span&gt;
    notes that sliding-window attention was apparently already used in GPT-3:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    The models use alternating dense and locally banded sparse attention patterns, similar to GPT-3
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   Who knew?
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    1.7 RMSNorm Replaces LayerNorm
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    Finally, the last small tweak, coming from GPT-2, is replacing
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot; rel=&quot;&quot;&gt;
    LayerNorm (2016)
   &lt;/a&gt;
   &lt;span&gt;
    by
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1910.07467&quot; rel=&quot;&quot;&gt;
    RMSNorm (2019)
   &lt;/a&gt;
   &lt;span&gt;
    , which has been a common trend in recent years.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Akin to swapping GELU with Swish and SwiGLU, RMSNorm is one of these smaller but sensible efficiency improvements. RMSNorm is similar to LayerNorm in its purpose to normalize layer activations, as shown in Figure 11 below.
  &lt;/p&gt;
  &lt;p&gt;
   You might recall that not too long ago, BatchNorm was the go-to choice for this task. It has since fallen out of favor, largely because it is harder to parallelize efficiently (due to the mean and variance batch statistics) and performs poorly with small batch sizes.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!H32R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:599,&quot;width&quot;:1367,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:274255,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;599&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!H32R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 424w, https://substackcdn.com/image/fetch/$s_!H32R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 848w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1272w, https://substackcdn.com/image/fetch/$s_!H32R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ac713f9-4f15-4104-9b67-eff1c4f29f95_1367x599.png 1456w&quot; width=&quot;1367&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 11: A comparison between LayerNorm (left) and RMSNorm (right) for a small linear layer.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As we can see in Figure 11 above, both LayerNorm and RMSNorm scale the layer outputs to be in a reasonable range.
  &lt;/p&gt;
  &lt;p&gt;
   LayerNorm subtracts the mean and divides by the standard deviation such that the layer outputs have a zero mean and unit variance (variance of 1 and standard deviation of one).
  &lt;/p&gt;
  &lt;p&gt;
   RMSNorm divides the inputs by the root-mean-square. This doesn&#x27;t force zero mean and unit variance, but the mean and variance are in a reasonable range: -1 to 1 for the mean and 0 to 1 for the variance. In this particular example shown in Figure 11, the mean is 0.77 and the variance is 0.41.
  &lt;/p&gt;
  &lt;p&gt;
   Both LayerNorm and RMSNorm stabilize activation scales and improve optimization, but RMSNorm is often preferred in large-scale LLMs because it is cheaper to compute. Unlike LayerNorm, RMSNorm has no bias (shift) term and reduces the expensive mean and variance computations to a single root-mean-square operation. This reduces the number of cross-feature reductions from two to one, which lowers communication overhead on GPUs and improving training efficiency.
  &lt;/p&gt;
  &lt;p&gt;
   Figure 12 shows what this looks like in code:
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!m5aM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/de968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:919,&quot;resizeWidth&quot;:589,&quot;bytes&quot;:259430,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;442.23068552774754&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!m5aM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 424w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 848w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1272w, https://substackcdn.com/image/fetch/$s_!m5aM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fde968991-5068-40d9-87bb-98b887f5f384_919x690.png 1456w&quot; width=&quot;589&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 12: Code implementations of LayerNorm and RMSNorm showing that RMSNorm is computationally simpler.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    1.8 The GPT-2 Legacy
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   I still think that GPT-2 is an excellent beginner architecture when learning about LLMs. It&#x27;s simple enough to understand without getting lost in layers of optimization tricks, but still complex enough to give you a solid grasp of how modern transformer models work.
  &lt;/p&gt;
  &lt;p&gt;
   By starting with GPT-2, you can focus on the fundamentals (attention mechanisms, positional embeddings, normalization, and the overall training pipeline) without being overwhelmed by the extra features and tweaks found in newer architectures.
  &lt;/p&gt;
  &lt;p&gt;
   In fact, I think it&#x27;s worth the time to learn about and even implement GPT-2 first before trying to stack newer changes on top. You will not only have an easier time understanding those changes, but you will likely also appreciate them more, because you will get a better understanding of what limitations or problems they try to solve.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For instance, starting with my GPT-2 code I recently implemented the
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3&quot; rel=&quot;&quot;&gt;
    Qwen3 architecture from scratch
   &lt;/a&gt;
   &lt;span&gt;
    , which is super similar to gpt-oss, which brings us to the next topic: Comparing gpt-oss to a more recent architecture.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2&gt;
   &lt;strong&gt;
    3. Comparing gpt-oss To A Recent Architecture (Qwen3)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Now that we have walked through the evolution from GPT-2 to GPT OSS, we can take the next step and compare GPT OSS to a more recent architecture, Qwen3, which was released three months earlier in May 2025.
  &lt;/p&gt;
  &lt;p&gt;
   The reason I am selecting Qwen3 here is that it is among the top open-weight models as of the time of writing. Additionally, one of the Qwen3 MoE models is more or less directly comparable to GPT OSS due to its relatively similar overall size in terms of trainable parameters.
  &lt;/p&gt;
  &lt;p&gt;
   Figure 13 below compares gpt-oss-20b to a Qwen3 model of comparable size.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!5K75!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:741,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:268927,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;741&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!5K75!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 424w, https://substackcdn.com/image/fetch/$s_!5K75!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 848w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1272w, https://substackcdn.com/image/fetch/$s_!5K75!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F115d3e6c-4c29-459c-a7e1-195bda61963a_1603x816.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 13: A gpt-oss and Qwen3 model of comparable size side by side.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As we can see, gpt-oss 20B and Qwen3 30B-A3B are very similar in their architecture components. The primary difference here, aside from the dimensions, is that gpt-oss employs sliding window attention, as discussed earlier in section 1.6 (not shown in this figure), whereas Qwen3 does not.
  &lt;/p&gt;
  &lt;p&gt;
   Let&#x27;s walk through the noteworthy details one by one in the following subsections.
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    3.1 Width Versus Depth
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   If we look at the two models closely, we see that Qwen3 is a much deeper architecture with its 48 transformer blocks instead of 24 (Figure 14).
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!G1hj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:696,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:307435,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;696&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!G1hj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 424w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 848w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1272w, https://substackcdn.com/image/fetch/$s_!G1hj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8681af3c-a62a-4224-94f3-89dcbe8284a0_1704x815.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 14: Qwen3 has twice as many transformer blocks as gpt-oss-20b.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   On the other hand, gpt-oss is a much wider architecture:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     An embedding dimension of 2880 instead of 2048
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     An intermediate expert (feed forward) projection dimension of 5760 instead of 768
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   It&#x27;s also worth noting that gpt-oss uses twice as many attention heads, but this doesn&#x27;t directly increase the model&#x27;s width. The width is determined by the embedding dimension.
  &lt;/p&gt;
  &lt;p&gt;
   Does one approach offer advantages over the other given a fixed number of parameters? As a rule of thumb, deeper models have more flexibility but can be harder to train due to instability issues, due to exploding and vanishing gradients (which RMSNorm and shortcut connections aim to mitigate).
  &lt;/p&gt;
  &lt;p&gt;
   Wider architectures have the advantage of being faster during inference (with a higher tokens/second throughput) due to better parallelization at a higher memory cost.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    When it comes to modeling performance, there&#x27;s unfortunately no good apples-to-apples comparison I am aware of (where parameter size and datasets are kept constant) except for an ablation study in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2408.00118&quot; rel=&quot;&quot;&gt;
    Gemma 2 paper (Table 9)
   &lt;/a&gt;
   &lt;span&gt;
    , which found that for a 9B parameter architecture, a wider setup is slightly better than a deeper setup. Across 4 benchmarks, the wider model achieved a 52.0 average score, and the deeper model achieved a 50.8 average score.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    3.2 Few Large Versus Many Small Experts
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   As shown in Figure 14 above, it&#x27;s also noteworthy that gpt-oss has a surprisingly small number of experts (32 instead of 128), and only uses 4 instead of 8 active experts per token. However, each expert is much larger than the experts in Qwen3.
  &lt;/p&gt;
  &lt;p&gt;
   This is interesting because the recent trends and developments point towards more, smaller models as being beneficial. This change, at a constant total parameter size, is nicely illustrated in Figure 15 below from the DeepSeekMoE paper.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!qYc3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:609,&quot;width&quot;:1131,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:219481,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;609&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qYc3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 424w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 848w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1272w, https://substackcdn.com/image/fetch/$s_!qYc3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5941e19e-ead5-47be-9c41-8afee9124c6d_1131x609.png 1456w&quot; width=&quot;1131&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 15: An annotated figure from &quot;DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models&quot;,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.06066&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.06066
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Notably, unlike DeepSeek&#x27;s models, neither gpt-oss nor Qwen3 uses shared experts, though.
  &lt;/p&gt;
  &lt;p&gt;
   To be fair, the small number of experts in gpt-oss could be a side effect of the 20B size. Looking at the 120B mode below, they indeed increased the number of experts (and transformer blocks) while keeping everything else fixed, as shown in Figure 16 below.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!w8-R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:291088,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;726&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!w8-R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 424w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 848w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1272w, https://substackcdn.com/image/fetch/$s_!w8-R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F901e0e7c-c007-4d32-a489-183c633ec44b_1679x837.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 16: The two gpt-oss architectures side by side, where the larger 120B model only scales the number of transformer blocks and number of experts.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The boring explanation for the fact that the 20B and 120B models are so similar is probably that the 120B model was the main focus. And the easiest way to create a smaller model was to make it a bit shorter (fewer transformer blocks) and to reduce the number of experts, because that&#x27;s where most of the parameters are. However, one might speculate whether they started training the 120B model, and then chopped some of the transformer blocks and experts for continued pre-training (instead of starting from random weights).
  &lt;/p&gt;
  &lt;p&gt;
   In any case, it&#x27;s because it&#x27;s quite unusual to only scale those two (transformer blocks and number of experts). For instance, when looking at Qwen3 MoE models of multiple sizes (Figure 17 below), they were scaled more proportionally to each other over many more aspects..
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!0h6T!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:903,&quot;width&quot;:1120,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:210100,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;903&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!0h6T!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 424w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 848w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1272w, https://substackcdn.com/image/fetch/$s_!0h6T!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9faf6dc8-5876-47c2-8965-212195773ed9_1120x903.png 1456w&quot; width=&quot;1120&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     Figure 17: Architecture differences in the various Qwen3 models.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    3.3 Attention Bias and Attention Sinks
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   Both gpt-oss and Qwen3 use grouped query attention. The main difference is that gpt-oss restricts the context size via sliding window attention in each second layer, as mentioned earlier.
  &lt;/p&gt;
  &lt;p&gt;
   However, there&#x27;s one interesting detail that caught my eye. It seems that gpt-oss uses bias units for the attention weights, as shown in the figure below.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!U3bl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:441,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:176606,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;441&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!U3bl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 424w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 848w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1272w, https://substackcdn.com/image/fetch/$s_!U3bl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1f1c450c-a6ad-4bc7-9e0e-6596c75fadda_1606x486.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 18: gpt-oss models use bias units in the attention layers. See code example
     &lt;/span&gt;
     &lt;a href=&quot;https://github.com/huggingface/transformers/blob/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py#L228-L243&quot; rel=&quot;&quot;&gt;
      here
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    I haven&#x27;t seen these bias units being used since the GPT-2 days, and they are commonly regarded as redundant. Indeed, I found a
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2302.08626&quot; rel=&quot;&quot;&gt;
    recent paper
   &lt;/a&gt;
   &lt;span&gt;
    that shows mathematically that this is at least true for the key transformation (k_proj). Furthermore, the empirical results show that there is little difference between with and without bias units (see Figure 19 below).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!FT2j!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FT2j!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png 424w, https://substackcdn.com/image/fetch/$s_!FT2j!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png 848w, https://substackcdn.com/image/fetch/$s_!FT2j!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png 1272w, https://substackcdn.com/image/fetch/$s_!FT2j!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:182,&quot;width&quot;:397,&quot;resizeWidth&quot;:279,&quot;bytes&quot;:20470,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;127.90428211586902&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!FT2j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FT2j!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png 424w, https://substackcdn.com/image/fetch/$s_!FT2j!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png 848w, https://substackcdn.com/image/fetch/$s_!FT2j!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png 1272w, https://substackcdn.com/image/fetch/$s_!FT2j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73888178-f1d1-4490-a310-d61ae113b0a9_397x182.png 1456w&quot; width=&quot;279&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 19: Table from
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/pdf/2302.08626&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/pdf/2302.08626
     &lt;/a&gt;
     &lt;span&gt;
      showing the average test loss when the models were trained from scratch with and without bias units.
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Another detail you may have noticed is the definition of
   &lt;/span&gt;
   &lt;code&gt;
    sinks
   &lt;/code&gt;
   &lt;span&gt;
    in the code screenshot in Figure 18. In general models, attention sinks are special &quot;always-attended&quot; tokens placed at the start of the sequence to stabilize attention, which is especially useful in long-context scenarios. I.e., if the context gets very long, this special attended token at the beginning is still attended to, and it can learn to store some generally useful information about the entire sequence. (I think it was originally proposed in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2309.17453&quot; rel=&quot;&quot;&gt;
    Efficient Streaming Language Models with Attention Sinks
   &lt;/a&gt;
   &lt;span&gt;
    paper.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the gpt-oss implementation,
   &lt;/span&gt;
   &lt;em&gt;
    attention sinks
   &lt;/em&gt;
   &lt;span&gt;
    are not actual tokens in the input sequence. Instead, they are learned per-head bias logits that are appended to the attention scores (Figure 20). The goal is the same as with the above-mentioned attention sinks, but without modifying the tokenized inputs.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Qwo6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:988,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:202184,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;684&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Qwo6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 424w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 848w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1272w, https://substackcdn.com/image/fetch/$s_!Qwo6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542cfc4c-ccfb-48b9-b285-ecbe8d2c0e4e_988x684.png 1456w&quot; width=&quot;988&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 20: The use of attention sinks in gpt-oss; based on the Hugging Face code
     &lt;/span&gt;
     &lt;a href=&quot;https://github.com/huggingface/transformers/blame/369c99d0cea403b77bd0aef818527106453fd9fc/src/transformers/models/gpt_oss/modular_gpt_oss.py&quot; rel=&quot;&quot;&gt;
      here
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    3.4 License
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   Lastly, and similar to Qwen3, the gpt-oss models are Apache 2.0 open-source license, which is great (it&#x27;s the same license that I prefer for my own open-source projects). This means that the models can be distilled into other models or used in commercial products without restriction.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Open-weight vs. open-source LLMs.
   &lt;/strong&gt;
   &lt;span&gt;
    This distinction has been debated for years, but it is worth clarifying to avoid confusion about this release and its artifacts. Some model developers release only the model weights and inference code (for example, Llama, Gemma, gpt-oss), while others (for example, OLMo) release everything including training code, datasets, and weights as true open source.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    By that stricter definition, gpt-oss is an
   &lt;/span&gt;
   &lt;em&gt;
    open-weight
   &lt;/em&gt;
   &lt;span&gt;
    model (just like Qwen3) because it includes the weights and inference code but not the training code or datasets. However, the terminology is used inconsistently across the industry.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    I assume the &quot;oss&quot; in &quot;gpt-oss&quot; stands for
   &lt;/span&gt;
   &lt;em&gt;
    open source software
   &lt;/em&gt;
   &lt;span&gt;
    ; however, I am positively surprised that OpenAI itself clearly describes gpt-oss as an open-weight model in their official
   &lt;/span&gt;
   &lt;a href=&quot;https://openai.com/index/introducing-gpt-oss/&quot; rel=&quot;&quot;&gt;
    announcement article
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2&gt;
   &lt;strong&gt;
    4 Other Interesting Tidbits
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   While the previous sections described how the architecture has evolved since GPT-2 and discussed its similarities to Qwen3 (and most other recent models), there are still a few additional but noteworthy details I have not mentioned, yet. These are points that did not fit neatly into the earlier sections but are still worth mentioning.
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    4.1 Training Overview
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    Unfortunately, there is not much information about the training set sizes and algorithms available. I added the most interesting puzzle pieces from the
   &lt;/span&gt;
   &lt;a href=&quot;https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf&quot; rel=&quot;&quot;&gt;
    model card report
   &lt;/a&gt;
   &lt;span&gt;
    (1) and
   &lt;/span&gt;
   &lt;a href=&quot;https://openai.com/index/introducing-gpt-oss/&quot; rel=&quot;&quot;&gt;
    announcement post
   &lt;/a&gt;
   &lt;span&gt;
    (2) below:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    The gpt-oss models were trained using our most advanced pre-training and post-training techniques [...] (1)
   &lt;/p&gt;
   &lt;p&gt;
    [...] required 2.1million H100-hours to complete, with gpt-oss-20b needing almost 10x fewer. (1)
   &lt;/p&gt;
   &lt;p&gt;
    [...] including a supervised fine-tuning stage and a high-compute RL stage [...] (2)
   &lt;/p&gt;
   &lt;p&gt;
    We trained the models on a mostly English, text-only dataset, with a focus on STEM, coding, and general knowledge. (2)
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, we know that the gpt-oss models are reasoning models. The training compute of 2.1 million H100 GPU hours is roughly on par with the 2.788 million H800 GPU hours that the ~5.6x larger
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2412.19437&quot; rel=&quot;&quot;&gt;
    DeepSeek V3
   &lt;/a&gt;
   &lt;span&gt;
    model was trained for. Unfortunately, there is no information about the Qwen3 training time available yet.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Interestingly, the GPT-oss training hour estimate includes both the supervised learning for instruction following and the reinforcement learning for reasoning, whereas DeepSeek V3 is just a pre-trained base model on top of which DeepSeek R1 was trained separately.
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    4.2 Reasoning Efforts
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   As mentioned in the previous section, the gpt-oss models are reasoning models. However, what&#x27;s particularly interesting is that they were trained so that users can easily control the degree of reasoning via inference time scaling.
  &lt;/p&gt;
  &lt;p&gt;
   Concretely, gpt-oss models can receive &quot;Reasoning effort: low/medium/high&quot; instructions as part of their system prompt, which directly affects the response length and accuracy, as shown in Figure 21.
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!LsLL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1219,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:175317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;548&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!LsLL!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 424w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 848w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1272w, https://substackcdn.com/image/fetch/$s_!LsLL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1a51339c-49a4-48c6-b60f-ce841b604692_1219x548.png 1456w&quot; width=&quot;1219&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 21: Response length and quality of gpt-oss models under different reasoning efforts (annotated figure from the
     &lt;/span&gt;
     &lt;a href=&quot;https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf&quot; rel=&quot;&quot;&gt;
      model card
     &lt;/a&gt;
     &lt;span&gt;
      )
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This level of adjustability is useful because it lets us balance cost, compute, and accuracy. For example, if the task is simple, such as answering a straightforward knowledge question or fixing a small typo, we can skip extended reasoning. This saves time and resources while avoiding unnecessarily long responses and verbose reasoning traces.
  &lt;/p&gt;
  &lt;p&gt;
   It is somewhat unfortunate that OpenAI did not release the base models prior to reinforcement learning-based reasoning training, unlike Qwen3 or OLMo. Base models are particularly valuable starting points for researchers working on reasoning methods (which is one reason I currently like working with Qwen3 Base). My guess is that OpenAI&#x27;s decision was driven more by industry and production use cases than by research considerations.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that the original Qwen3 models also have a toggle for enabling/disabling thinking (reasoning) modes (via a
   &lt;/span&gt;
   &lt;code&gt;
    enable_thinking=True/False
   &lt;/code&gt;
   &lt;span&gt;
    setting in the tokenizer that simply adds &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags to disable the reasoning behavior). However, the Qwen3 team updated their models in the last few weeks and moved away from the hybrid model towards dedicated Instruct/Thinking/Coder variants.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The reason was that the hybrid mode resulted in lower performance compared to the individual models:
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    &lt;span&gt;
     After discussing with the community and reflecting on the matter, we have decided to abandon the hybrid thinking mode. We will now train the Instruct and Thinking models separately to achieve the best possible quality.
    &lt;/span&gt;
    &lt;a href=&quot;https://www.actuia.com/en/news/alibaba-launches-qwen3-235b-a22b-instruct-2507-and-breaks-away-from-hybrid-reasoning/?utm_source=chatgpt.com&quot; rel=&quot;&quot;&gt;
     Source
    &lt;/a&gt;
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    4.3 MXFP4 Optimization: A Small But Important Detail
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   One interesting surprise is that OpenAI released the gpt-oss models with an MXFP4 quantization scheme for the MoE experts.
  &lt;/p&gt;
  &lt;p&gt;
   Quantization formats used to be a niche topic, mostly relevant to mobile or embedded AI, but that&#x27;s changed with the push toward bigger models. In this case, the MXFP4 optimization allows the model to run on single GPU devices.
  &lt;/p&gt;
  &lt;p&gt;
   Here’s what that looks like in practice:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     The large model (think 120B) fits on a single 80GB H100 or newer GPU. Not consumer hardware, but hey, it&#x27;s much cheaper to rent a 1-H100 machine than a multi-H100 machine. Plus, we don&#x27;t have to worry about distributing the model across GPUs and adding communication overhead. It&#x27;s really nice that AMD MI300X cards are supported from day 1 as well!
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     The smaller 20B model even fits into 16 GB of VRAM; the caveat is that it has to be a RTX 50-series GPU or newer to support MXFP4.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Note that the models will also run on older hardware but without MXFP4 support and will thus consume more RAM. Without MXFP4 optimization, the models in bfloat16 will consume more like 48 GB (gpt-oss-20b) and 240 GB (gpt-oss-120b).
  &lt;/p&gt;
  &lt;h3&gt;
   &lt;strong&gt;
    4.4 Benchmarks
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    The models are still a bit too new for independent benchmarks. Checking the
   &lt;/span&gt;
   &lt;a href=&quot;https://lmarena.ai/leaderboard&quot; rel=&quot;&quot;&gt;
    LM Arena leaderboard
   &lt;/a&gt;
   &lt;span&gt;
    , I found that gpt-oss is not listed, yet. So, Qwen3-Instruct remains the top open-weight model, according to users on the LM Arena, for now (Figure 22).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!u2e3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!u2e3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png 424w, https://substackcdn.com/image/fetch/$s_!u2e3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png 848w, https://substackcdn.com/image/fetch/$s_!u2e3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png 1272w, https://substackcdn.com/image/fetch/$s_!u2e3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:928,&quot;width&quot;:1246,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:199404,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;928&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!u2e3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!u2e3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png 424w, https://substackcdn.com/image/fetch/$s_!u2e3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png 848w, https://substackcdn.com/image/fetch/$s_!u2e3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png 1272w, https://substackcdn.com/image/fetch/$s_!u2e3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86a8d9a1-db1c-4a8c-bcef-e2b61fd99893_1246x928.png 1456w&quot; width=&quot;1246&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 22: Current view of the
     &lt;/span&gt;
     &lt;a href=&quot;https://lmarena.ai/leaderboard&quot; rel=&quot;&quot;&gt;
      LM Arena Leaderboard
     &lt;/a&gt;
     &lt;span&gt;
      (as of 8 Aug 2025)
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Looking at a reasoning benchmarks provide in the gpt-oss announcement post, we can see that the gpt-ossmodels are on par with OpenAI&#x27;s proprietary models as well as Qwen3 (Figure 23).
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!ueCy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ueCy!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png 424w, https://substackcdn.com/image/fetch/$s_!ueCy!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png 848w, https://substackcdn.com/image/fetch/$s_!ueCy!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png 1272w, https://substackcdn.com/image/fetch/$s_!ueCy!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e665447b-5092-4e88-b27a-768998945b02_978x588.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:588,&quot;width&quot;:978,&quot;resizeWidth&quot;:669,&quot;bytes&quot;:100836,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;402.22085889570553&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!ueCy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ueCy!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png 424w, https://substackcdn.com/image/fetch/$s_!ueCy!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png 848w, https://substackcdn.com/image/fetch/$s_!ueCy!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png 1272w, https://substackcdn.com/image/fetch/$s_!ueCy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe665447b-5092-4e88-b27a-768998945b02_978x588.png 1456w&quot; width=&quot;669&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 23: The main benchmark charts are from the official gpt-oss
     &lt;/span&gt;
     &lt;a href=&quot;https://openai.com/index/gpt-oss-model-card/&quot; rel=&quot;&quot;&gt;
      announcement post
     &lt;/a&gt;
     &lt;span&gt;
      . The &quot;no tools&quot; gpt-oss-120b data is taken from the official
     &lt;/span&gt;
     &lt;a href=&quot;https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf&quot; rel=&quot;&quot;&gt;
      model card paper
     &lt;/a&gt;
     &lt;span&gt;
      , and the Qwen3 numbers are taken from the official
     &lt;/span&gt;
     &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507&quot; rel=&quot;&quot;&gt;
      Qwen3 repository
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   However, this should be caveated by the fact that gpt-oss-120b is almost half the size of the Qwen3 A235B-A22B-Thinking-2507 model and can run on a single GPU.
  &lt;/p&gt;
  &lt;p&gt;
   Benchmark performance, however, does not always reflect real-world usability. In my limited use over the past few days, I have found gpt-oss to be quite capable. That said, as others have observed, it does seem to have a relatively high tendency to hallucinate (a point also mentioned in its model card).
  &lt;/p&gt;
  &lt;p&gt;
   This may stem from its heavy training focus on reasoning tasks such as math, puzzles, and code, which could have led to some &quot;general knowledge forgetting.&quot; Still, because gpt-oss was designed with tool use in mind, this limitation may become less relevant over time. Tool integration in open-source LLMs is still in its early stages, but as it matures, I expect that we increasingly let models consult external sources (like search engines) when answering factual or knowledge-based queries.
  &lt;/p&gt;
  &lt;p&gt;
   If that happens, it could be sensible to prioritize reasoning capacity over memorization. This is much like in human learning in school (or in life in general), where problem-solving skills often matter more than memorizing facts.
  &lt;/p&gt;
  &lt;h2&gt;
   &lt;strong&gt;
    5 gpt-oss and GPT-5
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   OpenAI had a busy week and released the long-awaited GPT-5 model shortly after gpt-oss. The GPT-5 release was interesting. And if there&#x27;s one thing I have to say here, it&#x27;s that I am really surprised by how good their open-source models really are compared to their best product offering in terms of benchmark performance (Figure 24).
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!IDPE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!IDPE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png 424w, https://substackcdn.com/image/fetch/$s_!IDPE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png 848w, https://substackcdn.com/image/fetch/$s_!IDPE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png 1272w, https://substackcdn.com/image/fetch/$s_!IDPE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1144,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:2040548,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/170506328?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1144&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!IDPE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!IDPE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png 424w, https://substackcdn.com/image/fetch/$s_!IDPE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png 848w, https://substackcdn.com/image/fetch/$s_!IDPE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png 1272w, https://substackcdn.com/image/fetch/$s_!IDPE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5552d62b-4c0b-4b19-adcb-59a0a162ad72_5923x4653.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption&gt;
     &lt;span&gt;
      Figure 24: The main benchmark charts are from the official GPT-5
     &lt;/span&gt;
     &lt;a href=&quot;https://openai.com/index/introducing-gpt-5/&quot; rel=&quot;&quot;&gt;
      announcement post
     &lt;/a&gt;
     &lt;span&gt;
      . The gpt-oss data is taken from the official
     &lt;/span&gt;
     &lt;a href=&quot;https://cdn.openai.com/pdf/419b6906-9da6-406c-a19d-1bb078ac7637/oai_gpt-oss_model_card.pdf&quot; rel=&quot;&quot;&gt;
      model card paper
     &lt;/a&gt;
     &lt;span&gt;
      and
     &lt;/span&gt;
     &lt;a href=&quot;https://openai.com/index/introducing-gpt-oss/&quot; rel=&quot;&quot;&gt;
      announcement post
     &lt;/a&gt;
     &lt;span&gt;
      , and the Qwen3 numbers are taken from the official
     &lt;/span&gt;
     &lt;a href=&quot;https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct&quot; rel=&quot;&quot;&gt;
      Qwen3-Coder repository
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   All in all, even though some people called the release overhyped, I am glad that we have a new set of really strong open weight models that are not too far behind the best proprietary ones. Of course, benchmarks often do not accurately reflect real-world use, and it is still too early to tell based on the limited usage. But I think these are good times for people who like to work with open-weight and local (or privately hosted) models.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    This magazine is a personal passion project, and your support helps keep it alive. If you would like to contribute, there are a few great ways:
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
        Grab a copy of my book
       &lt;/a&gt;
      &lt;/strong&gt;
      &lt;span&gt;
       . Build a Large Language Model (From Scratch) walks you through building an LLM step by step, from tokenizer to training.
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot; rel=&quot;&quot;&gt;
        Check out the video course
       &lt;/a&gt;
      &lt;/strong&gt;
      &lt;span&gt;
       . There’s now a 17-hour video course based on the book, available from Manning. It follows the book closely, section by section, and works well both as a standalone or as a code-along resource. The video course is ad-free (unlike the YouTube version) and has a cleaner, more structured format. It also contains 5 additional hours of pre-requisite video material created by Abhinav Kimothi.
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;a href=&quot;https://magazine.sebastianraschka.com/subscribe&quot; rel=&quot;&quot;&gt;
        Subscribe
       &lt;/a&gt;
      &lt;/strong&gt;
      &lt;span&gt;
       . A paid subscription helps to make my writing sustainable and gives you access to additional contents.
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;em&gt;
    Thanks for reading, and for helping support independent research!
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;figure&gt;
    &lt;a data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!wVLk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;878&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;div&gt;
    &lt;div&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Big LLM Architecture Comparison </title>
<link>https://magazine.sebastianraschka.com/p/the-big-llm-architecture-comparison</link>
<pubDate>Sat, 19 Jul 2025 04:11:10 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   It has been seven years since the original GPT architecture was developed. At first glance, looking back at GPT-2 (2019) and forward to DeepSeek-V3 and Llama 4 (2024-2025), one might be surprised at how structurally similar these models still are.
  &lt;/p&gt;
  &lt;p&gt;
   Sure, positional embeddings have evolved from absolute to rotational (RoPE), Multi-Head Attention has largely given way to Grouped-Query Attention, and the more efficient SwiGLU has replaced activation functions like GELU. But beneath these minor refinements, have we truly seen groundbreaking changes, or are we simply polishing the same architectural foundations?
  &lt;/p&gt;
  &lt;p&gt;
   Comparing LLMs to determine the key ingredients that contribute to their good (or not-so-good) performance is notoriously challenging: datasets, training techniques, and hyperparameters vary widely and are often not well documented.
  &lt;/p&gt;
  &lt;p&gt;
   However, I think that there is still a lot of value in examining the structural changes of the architectures themselves to see what LLM developers are up to in 2025. (A subset of them are shown in Figure 1 below.)
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!iCn-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!iCn-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 424w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 848w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1272w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1016,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:1563062,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;1016&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!iCn-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!iCn-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 424w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 848w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1272w, https://substackcdn.com/image/fetch/$s_!iCn-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ae4aa85-3e22-486c-9bd9-27edc4acbf8b_3000x2093.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 1: A subset of the architectures covered in this article.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   So, in this article, rather than writing about benchmark performance or training algorithms, I will focus on the architectural developments that define today&#x27;s flagship open models.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (As you may remember,
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-multimodal-llms&quot; rel=&quot;&quot;&gt;
    I wrote about multimodal LLMs
   &lt;/a&gt;
   &lt;span&gt;
    not too long ago; in this article, I will focus on the text capabilities of recent models and leave the discussion of multimodal capabilities for another time.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Tip:
   &lt;/strong&gt;
   &lt;span&gt;
    This is a fairly comprehensive article, so I recommend using the navigation bar to access the table of contents (just hover over the left side of the Substack page).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   1. DeepSeek V3/R1
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    As you have probably heard more than once by now,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2501.12948&quot; rel=&quot;&quot;&gt;
    DeepSeek R1
   &lt;/a&gt;
   &lt;span&gt;
    made a big impact when it was released in January 2025. DeepSeek R1 is a reasoning model built on top of the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2412.19437&quot; rel=&quot;&quot;&gt;
    DeepSeek V3 architecture
   &lt;/a&gt;
   &lt;span&gt;
    , which was introduced in December 2024.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While my focus here is on architectures released in 2025, I think it’s reasonable to include DeepSeek V3, since it only gained widespread attention and adoption following the launch of DeepSeek R1 in 2025.
  &lt;/p&gt;
  &lt;p&gt;
   If you are interested in the training of DeepSeek R1 specifically, you may also find my article from earlier this year useful:
  &lt;/p&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-reasoning-llms&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In this section, I’ll focus on two key architectural techniques introduced in DeepSeek V3 that improved its computational efficiency and distinguish it from many other LLMs:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Multi-Head Latent Attention (MLA)
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Mixture-of-Experts (MoE)
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1.1 Multi-Head Latent Attention (MLA)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Before discussing Multi-Head Latent Attention (MLA), let&#x27;s briefly go over some background to motivate why it&#x27;s used. For that, let&#x27;s start with Grouped-Query Attention (GQA), which has become the new standard replacement for a more compute- and parameter-efficient alternative to Multi-Head Attention (MHA) in recent years.
  &lt;/p&gt;
  &lt;p&gt;
   So, here&#x27;s a brief GQA summary. Unlike MHA, where each head also has its own set of keys and values, to reduce memory usage, GQA groups multiple heads to share the same key and value projections.
  &lt;/p&gt;
  &lt;p&gt;
   For example, as further illustrated in Figure 2 below, if there are 2 key-value groups and 4 attention heads, then heads 1 and 2 might share one set of keys and values, while heads 3 and 4 share another. This reduces the total number of key and value computations, which leads to lower memory usage and improved efficiency (without noticeably affecting the modeling performance, according to ablation studies).
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!uVhV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!uVhV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 424w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 848w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1272w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:474,&quot;width&quot;:1023,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;474&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!uVhV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!uVhV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 424w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 848w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1272w, https://substackcdn.com/image/fetch/$s_!uVhV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F647caf83-cd3d-46f8-8bd0-0946bd896ea1_1023x474.png 1456w&quot; width=&quot;1023&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 2: A comparison between MHA and GQA. Here, the group size is 2, where a key and value pair is shared among 2 queries.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   So, the core idea behind GQA is to reduce the number of key and value heads by sharing them across multiple query heads. This (1) lowers the model&#x27;s parameter count and (2) reduces the memory bandwidth usage for key and value tensors during inference since fewer keys and values need to be stored and retrieved from the KV cache.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (If you are curious how GQA looks in code, see my
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-llama2-to-llama3.ipynb&quot; rel=&quot;&quot;&gt;
    GPT-2 to Llama 3 conversion guide
   &lt;/a&gt;
   &lt;span&gt;
    for a version without KV cache and my KV-cache variant
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py&quot; rel=&quot;&quot;&gt;
    here
   &lt;/a&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    While GQA is mainly a computational-efficiency workaround for MHA, ablation studies (such as those in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2305.13245&quot; rel=&quot;&quot;&gt;
    original GQA paper
   &lt;/a&gt;
   &lt;span&gt;
    and the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2307.09288&quot; rel=&quot;&quot;&gt;
    Llama 2 paper
   &lt;/a&gt;
   &lt;span&gt;
    ) show it performs comparably to standard MHA in terms of LLM modeling performance.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Now, Multi-Head Latent Attention (MLA) offers a different memory-saving strategy that also pairs particularly well with KV caching. Instead of sharing key and value heads like GQA, MLA compresses the key and value tensors into a lower-dimensional space before storing them in the KV cache.
  &lt;/p&gt;
  &lt;p&gt;
   At inference time, these compressed tensors are projected back to their original size before being used, as shown in the Figure 3 below. This adds an extra matrix multiplication but reduces memory usage.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!jagJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!jagJ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 424w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 848w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1272w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:806,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;806&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!jagJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!jagJ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 424w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 848w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1272w, https://substackcdn.com/image/fetch/$s_!jagJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feb9a75be-2848-4b99-af3d-4c48bdd0181a_1550x858.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 3: Comparison between MLA (used in DeepSeek V3 and R1) and regular MHA.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   (As a side note, the queries are also compressed, but only during training, not inference.)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    By the way, MLA is not new in DeepSeek V3, as its
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2405.04434&quot; rel=&quot;&quot;&gt;
    DeepSeek-V2 predecessor
   &lt;/a&gt;
   &lt;span&gt;
    also used (and even introduced) it. Also, the V2 paper contains a few interesting ablation studies that may explain why the DeepSeek team chose MLA over GQA (see Figure 4 below).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!efDX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!efDX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 424w, https://substackcdn.com/image/fetch/$s_!efDX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 848w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1272w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:856,&quot;width&quot;:903,&quot;resizeWidth&quot;:644,&quot;bytes&quot;:288103,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;610.4806201550388&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!efDX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!efDX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 424w, https://substackcdn.com/image/fetch/$s_!efDX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 848w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1272w, https://substackcdn.com/image/fetch/$s_!efDX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2b7e646a-16c1-4245-9a3f-55a41f3070c2_903x856.png 1456w&quot; width=&quot;644&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 4: Annotated tables from the DeepSeek-V2 paper, https://arxiv.org/abs/2405.04434
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As shown in Figure 4 above, GQA appears to perform worse than MHA, whereas MLA offers better modeling performance than MHA, which is likely why the DeepSeek team chose MLA over GQA. (It would have been interesting to see the &quot;KV Cache per Token&quot; savings comparison between MLA and GQA as well!)
  &lt;/p&gt;
  &lt;p&gt;
   To summarize this section before we move on to the next architecture component, MLA is a clever trick to reduce KV cache memory use while even slightly outperforming MHA in terms of modeling performance.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1.2 Mixture-of-Experts (MoE)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The other major architectural component in DeepSeek worth highlighting is its use of Mixture-of-Experts (MoE) layers. While DeepSeek did not invent MoE, it has seen a resurgence this year, and many of the architectures we will cover later also adopt it.
  &lt;/p&gt;
  &lt;p&gt;
   You are likely already familiar with MoE, but a quick recap may be helpful.
  &lt;/p&gt;
  &lt;p&gt;
   The core idea in MoE is to replace each FeedForward module in a transformer block with multiple expert layers, where each of these expert layers is also a FeedForward module. This means that we swap a single FeedForward block for multiple FeedForward blocks, as illustrated in the Figure 5 below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!e3O4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e3O4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 424w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 848w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 1272w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:918,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;918&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!e3O4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e3O4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 424w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 848w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 1272w, https://substackcdn.com/image/fetch/$s_!e3O4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4efdd493-d8c8-4038-ad24-20d6db86eae8_1600x1009.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 5: An illustration of the Mixture-of-Experts (MoE) module in DeepSeek V3/R1 (right) compared to an LLM with a standard FeedForward block (left).
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The FeedForward block inside a transformer block (shown as the dark gray block in the figure above) typically contains a large number of the model&#x27;s total parameters. (Note that the transformer block, and thereby the FeedForward block, is repeated many times in an LLM; in the case of DeepSeek-V3, 61 times.)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, replacing
   &lt;/span&gt;
   &lt;em&gt;
    a single
   &lt;/em&gt;
   &lt;span&gt;
    FeedForward block with
   &lt;/span&gt;
   &lt;em&gt;
    multiple
   &lt;/em&gt;
   &lt;span&gt;
    FeedForward blocks (as done in a MoE setup) substantially increases the model&#x27;s total parameter count. However, the key trick is that we don&#x27;t use (&quot;activate&quot;) all experts for every token. Instead, a router selects only a small subset of experts per token. (In the interest of time, or rather article space, I&#x27;ll cover the router in more detail another time.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Because only a few experts are active at a time, MoE modules are often referred to as
   &lt;/span&gt;
   &lt;em&gt;
    sparse
   &lt;/em&gt;
   &lt;span&gt;
    , in contrast to
   &lt;/span&gt;
   &lt;em&gt;
    dense
   &lt;/em&gt;
   &lt;span&gt;
    modules that always use the full parameter set. However, the large total number of parameters via an MoE increases the capacity of the LLM, which means it can take up more knowledge during training. The sparsity keeps inference efficient, though, as we don&#x27;t use all the parameters at the same time.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   For example, DeepSeek-V3 has 256 experts per MoE module and a total of 671 billion parameters. Yet during inference, only 9 experts are active at a time (1 shared expert plus 8 selected by the router). This means just 37 billion parameters are used per inference step as opposed to all 671 billion.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    One notable feature of DeepSeek-V3&#x27;s MoE design is the use of a shared expert. This is an expert that is always active for every token. This idea is not new and was already introduced in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2401.06066&quot; rel=&quot;&quot;&gt;
    DeepSeek 2024 MoE
   &lt;/a&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2201.05596&quot; rel=&quot;&quot;&gt;
    2022 DeepSpeedMoE paper
   &lt;/a&gt;
   &lt;span&gt;
    s.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!i4ms!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!i4ms!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 424w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 848w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1272w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:569,&quot;width&quot;:1039,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;569&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!i4ms!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!i4ms!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 424w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 848w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1272w, https://substackcdn.com/image/fetch/$s_!i4ms!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3d93c441-a6d2-4257-bd80-2d3590c4001c_1039x569.png 1456w&quot; width=&quot;1039&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 6: An annotated figure from &quot;DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models&quot;, https://arxiv.org/abs/2401.06066
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    The benefit of having a shared expert was first noted in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2201.05596&quot; rel=&quot;&quot;&gt;
    DeepSpeedMoE paper
   &lt;/a&gt;
   &lt;span&gt;
    , where they found that it boosts overall modeling performance compared to no shared experts. This is likely because common or repeated patterns don&#x27;t have to be learned by multiple individual experts, which leaves them with more room for learning more specialized patterns.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1.3 DeepSeek Summary
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   To summarize, DeepSeek-V3 is a massive 671-billion-parameter model that, at launch, outperformed other open-weight models, including the 405B Llama 3. Despite being larger, it is much more efficient at inference time thanks to its Mixture-of-Experts (MoE) architecture, which activates only a small subset of (just 37B) parameters per token.
  &lt;/p&gt;
  &lt;p&gt;
   Another key distinguishing feature is DeepSeek-V3&#x27;s use of Multi-Head Latent Attention (MLA) instead of Grouped-Query Attention (GQA). Both MLA and GQA are inference-efficient alternatives to standard Multi-Head Attention (MHA), particularly when using KV caching. While MLA is more complex to implement, a study in the DeepSeek-V2 paper has shown it delivers better modeling performance than GQA.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   2. OLMo 2
  &lt;/h1&gt;
  &lt;p&gt;
   The OLMo series of models by the non-profit Allen Institute for AI is noteworthy due to its transparency in terms of training data and code, as well as the relatively detailed technical reports.
  &lt;/p&gt;
  &lt;p&gt;
   While you probably won’t find OLMo models at the top of any benchmark or leaderboard, they are pretty clean and, more importantly, a great blueprint for developing LLMs, thanks to their transparency.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    And while OLMo models are popular because of their transparency, they are not that bad either. In fact, at the time of release in January (before Llama 4, Gemma 3, and Qwen 3),
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2501.00656&quot; rel=&quot;&quot;&gt;
    OLMo 2
   &lt;/a&gt;
   &lt;span&gt;
    models were sitting at the Pareto frontier of compute to performance, as shown in Figure 7 below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!7DYj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!7DYj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 424w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 848w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1272w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:823,&quot;width&quot;:1027,&quot;resizeWidth&quot;:666,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;533.7078870496592&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!7DYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!7DYj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 424w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 848w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1272w, https://substackcdn.com/image/fetch/$s_!7DYj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbb5c7281-eded-4319-9ae2-7b07478a86b2_1027x823.png 1456w&quot; width=&quot;666&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 7: Modeling benchmark performance (higher is better) vs pre-training cost (FLOPs; lower is better) for different LLMs. This is an annotated figure from the OLMo 2 paper, https://arxiv.org/abs/2501.00656
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As mentioned earlier in this article, I aim to focus only on the LLM architecture details (not training or data) to keep it at a manageable length. So, what were the interesting architectural design choices in OLMo2 ? It mainly comes down to normalizations: the placement of RMSNorm layers as well as the addition of a QK-norm, which I will discuss below.
  &lt;/p&gt;
  &lt;p&gt;
   Another thing worth mentioning is that OLMo 2 still uses traditional Multi-Head Attention (MHA) instead of MLA or GQA.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.1 Normalization Layer Placement
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Overall, OLMo 2 largely follows the architecture of the original GPT model,  similar to other contemporary LLMs. However, there are some noteworthy deviations. Let&#x27;s start with the normalization layers.
  &lt;/p&gt;
  &lt;p&gt;
   Similar to Llama, Gemma, and most other LLMs, OLMo 2 switched from LayerNorm to RMSNorm.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    But since RMSNorm is old hat (it&#x27;s basically a simplified version of LayerNorm with fewer trainable parameters), I will skip the discussion of RMSNorm vs LayerNorm. (Curious readers can find an RMSNorm code implementation in my
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch05/07_gpt_to_llama/converting-gpt-to-llama2.ipynb&quot; rel=&quot;&quot;&gt;
    GPT-2 to Llama conversion guide
   &lt;/a&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    However, it&#x27;s worth discussing the placement of the RMSNorm layer. The original transformer (from the &quot;
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
    Attention is all you need
   &lt;/a&gt;
   &lt;span&gt;
    &quot; paper) placed the two normalization layers in the transformer block
   &lt;/span&gt;
   &lt;em&gt;
    after
   &lt;/em&gt;
   &lt;strong&gt;
   &lt;/strong&gt;
   &lt;span&gt;
    the attention module and the FeedForward module, respectively.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This is also known as Post-LN or Post-Norm.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    GPT and most other LLMs that came after placed the normalization layers
   &lt;/span&gt;
   &lt;em&gt;
    before
   &lt;/em&gt;
   &lt;span&gt;
    the attention and FeedForward modules, which is known as Pre-LN or Pre-Norm. A comparison between Post- and Pre-Norm is shown in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!wYj9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!wYj9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 424w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 848w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1272w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:789,&quot;width&quot;:1444,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;789&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!wYj9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!wYj9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 424w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 848w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1272w, https://substackcdn.com/image/fetch/$s_!wYj9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F61a4560f-d97f-4c78-a7a3-765babb45bec_1444x789.png 1456w&quot; width=&quot;1444&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 8: A comparison of Post-Norm, Pre-Norm, and OLMo 2&#x27;s flavor of Post-Norm.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2002.04745&quot; rel=&quot;&quot;&gt;
    2020, Xiong et al.
   &lt;/a&gt;
   &lt;span&gt;
    showed that Pre-LN results in more well-behaved gradients at initialization. Furthermore, the researchers mentioned that Pre-LN even works well without careful learning rate warm-up, which is otherwise a crucial tool for Post-LN.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Now, the reason I am mentioning that is that OLMo 2 adopted a form of Post-LN (but with RMSNorm instead of LayerNorm, so I am calling it
   &lt;/span&gt;
   &lt;em&gt;
    Post-Norm
   &lt;/em&gt;
   &lt;span&gt;
    ).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In OLMo 2, instead of placing the normalization layers before the attention and FeedForward layers, they place them after, as shown in the figure above. However, notice that in contrast to the original transformer architecture, the normalization layers are still inside the residual layers (skip connections).
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, why did they move the position of the normalization layers?
   &lt;/span&gt;
   &lt;strong&gt;
   &lt;/strong&gt;
   &lt;span&gt;
    The reason is that it helped with training stability, as shown in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!ebW0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ebW0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 424w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 848w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1272w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:407,&quot;width&quot;:1289,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;407&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!ebW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ebW0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 424w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 848w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1272w, https://substackcdn.com/image/fetch/$s_!ebW0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F186190ec-2ae5-430d-b0d4-a63486e0f3fb_1289x407.png 1456w&quot; width=&quot;1289&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 9: A plot showing the training stability for Pre-Norm (like in GPT-2, Llama 3, and many others) versus OLMo 2&#x27;s flavor of Post-Norm.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Unfortunately this figure shows the results of the reordering together with QK-Norm, which is a separate concept. So, it’s hard to tell how much the normalization layer reordering contributed by itself.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.2 QK-Norm
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Since the previous section already mentioned the QK-norm, and other LLMs we discuss later, such as Gemma 2 and Gemma 3, also use QK-norm, let&#x27;s briefly discuss what this is.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    QK-Norm is essentially yet another RMSNorm layer. It&#x27;s placed inside the Multi-Head Attention (MHA) module and applied to the queries (q) and keys (k) before applying RoPE. To illustrate this, below is an excerpt of a Grouped-Query Attention (GQA) layer I wrote for my
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3&quot; rel=&quot;&quot;&gt;
    Qwen3 from-scratch implementation
   &lt;/a&gt;
   &lt;span&gt;
    (the QK-norm application in GQA is similar to MHA in OLMo):
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;class GroupedQueryAttention(nn.Module):
    def __init__(
        self, d_in, num_heads, num_kv_groups,
        head_dim=None, qk_norm=False, dtype=None
    ):
        # ...

        if qk_norm:
            self.q_norm = RMSNorm(head_dim, eps=1e-6)
            self.k_norm = RMSNorm(head_dim, eps=1e-6)
        else:
            self.q_norm = self.k_norm = None

    def forward(self, x, mask, cos, sin):
        b, num_tokens, _ = x.shape

        # Apply projections
        queries = self.W_query(x) 
        keys = self.W_key(x)
        values = self.W_value(x) 

        # ...

        # Optional normalization
        if self.q_norm:
            queries = self.q_norm(queries)
        if self.k_norm:
            keys = self.k_norm(keys)

        # Apply RoPE
        queries = apply_rope(queries, cos, sin)
        keys = apply_rope(keys, cos, sin)

        # Expand K and V to match number of heads
        keys = keys.repeat_interleave(self.group_size, dim=1)
        values = values.repeat_interleave(self.group_size, dim=1)

        # Attention
        attn_scores = queries @ keys.transpose(2, 3)
        # ...
&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    As mentioned earlier, together with Post-Norm, QK-Norm stabilizes the training. Note that QK-Norm was not invented by OLMo 2 but goes back to the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2302.05442&quot; rel=&quot;&quot;&gt;
    2023 Scaling Vision Transformers paper
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.3 OLMo 2 Summary
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   In short, the noteworthy OLMo 2 architecture design decisions are primarily the RMSNorm placements: RMSNorm after instead of before the attention and FeedForward modules (a flavor of Post-Norm), as well as the addition of RMSNorm for the queries and keys inside the attention mechanism (QK-Norm), which both, together, help stabilize the training loss.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Below is a figure that further compares OLMo 2 to Llama 3 side by side; as one can see, the architectures are otherwise relatively similar except for the fact that OLMo 2 still uses the traditional MHA instead of GQA. (However, the
   &lt;/span&gt;
   &lt;a href=&quot;https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct&quot; rel=&quot;&quot;&gt;
    OLMo 2 team released a 32B variant
   &lt;/a&gt;
   &lt;span&gt;
    3 months later that uses GQA.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!S6Y9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!S6Y9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 424w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 848w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1272w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:737,&quot;width&quot;:1329,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:153520,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;737&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!S6Y9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 424w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 848w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1272w, https://substackcdn.com/image/fetch/$s_!S6Y9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fa42974-cfae-45cc-9cd9-fc1d9607d386_1329x737.png 1456w&quot; width=&quot;1329&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 10: An architecture comparison between Llama 3 and OLMo 2.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   3. Gemma 3
  &lt;/h1&gt;
  &lt;p&gt;
   Google&#x27;s Gemma models have always been really good, and I think they have always been a bit underhyped compared to other popular models, like the Llama series.
  &lt;/p&gt;
  &lt;p&gt;
   One of the distinguishing aspects of Gemma is the rather large vocabulary size (to support multiple languages better), and the stronger focus on the 27B size (versus 8B or 70B). But note that Gemma 2 also comes in smaller sizes: 1B, 4B, and 12B.
  &lt;/p&gt;
  &lt;p&gt;
   The 27B size hits a really nice sweet spot: it&#x27;s much more capable than an 8B model but not as resource-intensive as a 70B model, and it runs just fine locally on my Mac Mini.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, what else is interesting in
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2503.19786&quot; rel=&quot;&quot;&gt;
    Gemma 3
   &lt;/a&gt;
   &lt;span&gt;
    ? As discussed earlier, other models like Deepseek-V3/R1 use a Mixture-of-Experts (MoE) architecture to reduce memory requirements at inference, given a fixed model size. (The MoE approach is also used by several other models we will discuss later.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Gemma 3 uses a different &quot;trick&quot; to reduce computational costs, namely sliding window attention.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3.1 Sliding Window Attention
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    With sliding window attention (originally introduced in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2004.05150&quot; rel=&quot;&quot;&gt;
    LongFormer paper in 2020
   &lt;/a&gt;
   &lt;span&gt;
    and also already used by
   &lt;/span&gt;
   &lt;a href=&quot;http://arxiv.org/abs/2408.00118&quot; rel=&quot;&quot;&gt;
    Gemma 2
   &lt;/a&gt;
   &lt;span&gt;
    ), the Gemma 3 team was able to reduce the memory requirements in the KV cache by a substantial amount, as shown in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!LQA4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!LQA4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 424w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 848w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1272w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:302,&quot;width&quot;:665,&quot;resizeWidth&quot;:555,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;252.04511278195488&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!LQA4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!LQA4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 424w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 848w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1272w, https://substackcdn.com/image/fetch/$s_!LQA4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5363ce6-0ec8-49e6-b296-9836c248e159_665x302.png 1456w&quot; width=&quot;555&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 11: An annotated figure from Gemma 3 paper (https://arxiv.org/abs/2503.19786) showing the KV cache memory savings via sliding window attention.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, what is sliding window attention? If we think of regular self-attention as a
   &lt;/span&gt;
   &lt;em&gt;
    global
   &lt;/em&gt;
   &lt;span&gt;
    attention mechanism, since each sequence element can access every other sequence element, then we can think of sliding window attention as
   &lt;/span&gt;
   &lt;em&gt;
    local
   &lt;/em&gt;
   &lt;span&gt;
    attention, because here we restrict the context size around the current query position. This is illustrated in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!tTJ5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!tTJ5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 424w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 848w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1272w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:721,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;721&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!tTJ5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 424w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 848w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1272w, https://substackcdn.com/image/fetch/$s_!tTJ5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff32c2d74-ec34-43ef-86bc-bcce832426b3_1600x792.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 12: A comparison between regular attention (left) and sliding window attention (right).
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Please note that sliding window attention can be used with both Multi-Head Attention and Grouped-Query Attention; Gemma 3 uses grouped-query attention.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    As mentioned above, sliding window attention is also referred to as
   &lt;/span&gt;
   &lt;em&gt;
    local
   &lt;/em&gt;
   &lt;span&gt;
    attention because the local window surrounds and moves with the current query position. In contrast, regular attention is
   &lt;/span&gt;
   &lt;em&gt;
    global
   &lt;/em&gt;
   &lt;span&gt;
    as each token can access all other tokens.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Now, as briefly mentioned above, the Gemma 2 predecessor architecture also used sliding window attention before. The difference in Gemma 3 is that they adjusted the ratio between global (regular) and local (sliding) attention.
  &lt;/p&gt;
  &lt;p&gt;
   For instance, Gemma 2 uses a hybrid attention mechanism that combines sliding window (local) and global attention in a 1:1 ratio. Each token can attend to a 4k-token window of nearby context.
  &lt;/p&gt;
  &lt;p&gt;
   Where Gemma 2 used sliding window attention in every other layer, Gemma 3 now has a 5:1 ratio, meaning there&#x27;s only 1 full attention layer for every 5 sliding windows (local) attention layers; moreover, the sliding window size was reduced from 4096 (Gemma 2) to just 1024 (Gemma 3). This shifts the model&#x27;s focus towards more efficient, localized computations.
  &lt;/p&gt;
  &lt;p&gt;
   According to their ablation study, the use of sliding window attention has minimal impact on modeling performance, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!YSZb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!YSZb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 424w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 848w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1272w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:434,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;434&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!YSZb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!YSZb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 424w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 848w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1272w, https://substackcdn.com/image/fetch/$s_!YSZb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F788a5023-1ba3-4372-89cd-4ebeff132255_1600x477.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 13: An annotated figure from Gemma 3 paper (https://arxiv.org/abs/2503.19786) showing that sliding window attention has little to no impact on the LLM-generated output perplexity.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While sliding window attention is the most notable architecture aspect of Gemma 3, I want to also briefly go over the placement of the normalization layers as a follow-up to the previous OLMo 2 section.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3.2 Normalization Layer Placement in Gemma 3
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   A small but interesting tidbit to highlight is that Gemma 3 uses RMSNorm in both a Pre-Norm and Post-Norm setting around its grouped-query attention module.
  &lt;/p&gt;
  &lt;p&gt;
   This is similar to Gemma 2 but still worth highlighting, as it differs from (1) the Post-Norm used in the original transformer (“Attention is all you need”), (2) the Pre-Norm, which was popularized by GPT-2 and used in many other architectures afterwards, and (3) the Post-Norm flavor in OLMo 2 that we saw earlier.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!A1BM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!A1BM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 424w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 848w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1272w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:855,&quot;width&quot;:1068,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;855&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!A1BM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!A1BM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 424w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 848w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1272w, https://substackcdn.com/image/fetch/$s_!A1BM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F752ec4b7-2720-44f8-958b-0ca85b3a96f1_1068x855.png 1456w&quot; width=&quot;1068&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 14: An architecture comparison between OLMo2 and Gemma 3; note the additional normalization layers in Gemma 3.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   I think this normalization layer placement is a relatively intuitive approach as it gets the best of both worlds: Pre-Norm and Post-Norm. In my opinion, a bit of extra normalization can&#x27;t hurt. In the worst case, if the extra normalization is redundant, this adds a bit of inefficiency through redundancy. In practice, since RMSNorm is relatively cheap in the grand scheme of things, this shouldn&#x27;t have any noticeable impact, though.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3.3 Gemma 3 Summary
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Gemma 3 is a well-performing open-weight LLM that, in my opinion, is a bit underappreciated in the open-source circles. The most interesting part is the use of sliding window attention to improve efficiency (it will be interesting to combine it with MoE in the future).
  &lt;/p&gt;
  &lt;p&gt;
   Also, Gemma 3 has a unique normalization layer placement, placing RMSNorm layers both before and after the attention and FeedForward modules.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3.4 Bonus: Gemma 3n
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    A few months after the Gemma 3 release, Google shared
   &lt;/span&gt;
   &lt;a href=&quot;https://developers.googleblog.com/en/introducing-gemma-3n/&quot; rel=&quot;&quot;&gt;
    Gemma 3n
   &lt;/a&gt;
   &lt;span&gt;
    , which is a Gemma 3 model that has been optimized for small-device efficiency with the goal of running on phones.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   One of the changes in Gemma 3n to achieve better efficiency is the so-called Per-Layer Embedding (PLE) parameters layer. The key idea here is to keep only a subset of the model&#x27;s parameters in GPU memory. Token-layer specific embeddings, such as those for text, audio, and vision modalities, are then streamed from the CPU or SSD on demand.
  &lt;/p&gt;
  &lt;p&gt;
   The figure below illustrates the PLE memory savings, listing 5.44 billion parameters for a standard Gemma 3 model. This likely refers to the Gemma 3 4-billion variant.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Su7d!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Su7d!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 424w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 848w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1272w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b05999d6-88ca-4739-8b0b-266b48da288b_662x483.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:483,&quot;width&quot;:662,&quot;resizeWidth&quot;:606,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;442.1419939577039&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Su7d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Su7d!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 424w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 848w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1272w, https://substackcdn.com/image/fetch/$s_!Su7d!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb05999d6-88ca-4739-8b0b-266b48da288b_662x483.png 1456w&quot; width=&quot;606&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 15: An annotated figure from Google&#x27;s Gemma 3n blog (https://developers.googleblog.com/en/introducing-gemma-3n/) illustrating the PLE memory savings.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The 5.44 vs. 4 billion parameter discrepancy is because Google has an interesting way of reporting parameter counts in LLMs. They often exclude embedding parameters to make the model appear smaller, except in cases like this, where it is convenient to include them to make the model appear larger. This is not unique to Google, as this approach has become a common practice across the field.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Another interesting trick is the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2310.07707&quot; rel=&quot;&quot;&gt;
    MatFormer
   &lt;/a&gt;
   &lt;span&gt;
    concept (short for Matryoshka Transformer). For instance, Gemma 3n uses a single shared LLM (transformer) architecture that can be sliced into smaller, independently usable models. Each slice is trained to function on its own, so at inference time, we can run just the part you need (instead of the large model).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   4. Mistral Small 3.1
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://mistral.ai/news/mistral-small-3-1&quot; rel=&quot;&quot;&gt;
    Mistral Small 3.1 24B
   &lt;/a&gt;
   &lt;span&gt;
    , which was released in March shortly after Gemma 3, is noteworthy for outperforming Gemma 3 27B on several benchmarks (except for math) while being faster.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The reasons for the lower inference latency of Mistral Small 3.1 over Gemma 3 are likely due to their custom tokenizer, as well as shrinking the KV cache and layer count. Otherwise, it&#x27;s a standard architecture as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!ZZnO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ZZnO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 424w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 848w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 1272w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:791,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;791&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ZZnO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 424w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 848w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 1272w, https://substackcdn.com/image/fetch/$s_!ZZnO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa861f68d-c50c-471c-bae1-2308c135bb54_1460x793.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 16: An architecture comparison between Gemma 3 27B and Mistral 3.1 Small 24B.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Interestingly, earlier Mistral models had utilized sliding window attention, but they appear to have abandoned it in Mistral Small 3.1. So, since Mistral uses regular Grouped-Query Attention instead of Grouped-Query Attention with a sliding window as in Gemma 3, maybe there are additional inference compute savings due to being able to use more optimized code (i.e., FlashAttention). For instance, I speculate that while sliding window attention reduces memory usage, it doesn&#x27;t necessarily reduce inference latency, which is what Mistral Small 3.1 is focused on.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   5. Llama 4
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    The extensive introductory discussion on Mixture-of-Experts (MoE) earlier in this article pays off again.
   &lt;/span&gt;
   &lt;a href=&quot;https://ai.meta.com/blog/llama-4-multimodal-intelligence/&quot; rel=&quot;&quot;&gt;
    Llama 4
   &lt;/a&gt;
   &lt;span&gt;
    has also adopted an MoE approach and otherwise follows a relatively standard architecture that is very similar to DeepSeek-V3, as shown in the figure below. (Llama 4 includes native multimodal support, similar to models like Gemma and Mistral. However, since this article focuses on language modeling, we only focus on the text model.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!ShdO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ShdO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 424w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 848w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1272w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:749,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;749&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!ShdO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ShdO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 424w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 848w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1272w, https://substackcdn.com/image/fetch/$s_!ShdO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F17518ff9-1f60-4aca-b654-034dabe20626_1600x823.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 17: An architecture comparison between DeepSeek V3 (671-billion parameters) and Llama 4 Maverick (400-billion parameters).
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While the Llama 4 Maverick architecture looks very similar to DeepSeek-V3 overall, there are some interesting differences worth highlighting.
  &lt;/p&gt;
  &lt;p&gt;
   First, Llama 4 uses Grouped-Query Attention similar to its predecessors, whereas DeepSeek-V3 uses Multi-Head Latent Attention, which we discussed at the beginning of this article. Now, both DeepSeek-V3 and Llama 4 Maverick are very large architectures, with DeepSeek-V3 being approximately 68% larger in its total parameter count. However, with 37 billion active parameters, DeepSeek-V3 has more than twice as many active parameters as Llama 4 Maverick (17B).
  &lt;/p&gt;
  &lt;p&gt;
   Llama 4 Maverick uses a more classic MoE setup with fewer but larger experts (2 active experts with 8,192 hidden size each) compared to DeepSeek-V3 (9 active experts with 2,048 hidden size each). Also, DeepSeek uses MoE layers in each transformer block (except the first 3), whereas Llama 4 alternates MoE and dense modules in every other transformer block.
  &lt;/p&gt;
  &lt;p&gt;
   Given the many small differences between architectures, it is difficult to determine their exact impact on final model performance. The main takeaway, however, is that MoE architectures have seen a significant rise in popularity in 2025.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   6. Qwen3
  &lt;/h1&gt;
  &lt;p&gt;
   The Qwen team consistently delivers high-quality open-weight LLMs. When I helped co-advising the LLM efficiency challenge at NeurIPS 2023, I remember that the top winning solutions were all Qwen2-based.
  &lt;/p&gt;
  &lt;p&gt;
   Now, Qwen3 is another hit model series at the top of the leaderboards for their size classes. There are 7 dense models: 0.6B, 1.7B, 4B, 8B, 14B, and 32B. And there are 2 MoE models: 30B-A3B, and 235B-A22B.
  &lt;/p&gt;
  &lt;p&gt;
   (By the way, note that the missing whitespace in &quot;Qwen3&quot; is not a typo; I simply try to preserve the original spelling the Qwen developers chose.)
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    6.1 Qwen3 (Dense)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Let&#x27;s discuss the dense model architecture first. As of this writing, the 0.6B model may well be the smallest current-generation open-weight model out there. And based on my personal experience, it performs really well given its small size. It has great token/sec throughput and a low memory footprint if you are planning to run it locally. But what&#x27;s more, it&#x27;s also easy to train locally (for educational purposes) due to its small size.
  &lt;/p&gt;
  &lt;p&gt;
   So, Qwen3 0.6B has replaced Llama 3 1B for me for most purposes. A comparison between these two architectures is shown below.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!e8cD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e8cD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 424w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 848w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 1272w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:807,&quot;width&quot;:1331,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;807&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!e8cD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e8cD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 424w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 848w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 1272w, https://substackcdn.com/image/fetch/$s_!e8cD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5185aaf2-0130-42c3-8705-2b58c17e6c66_1331x807.png 1456w&quot; width=&quot;1331&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 18: An architecture comparison between Qwen3 0.6B and Llama 3 1B; notice that Qwen3 is a deeper architecture with more layers, whereas Llama 3 is a wider architecture with more attention heads.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you are interested in a human-readable Qwen3 implementation without external third-party LLM library dependencies, I recently implemented
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3&quot; rel=&quot;&quot;&gt;
    Qwen3 from scratch (in pure PyTorch)
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The computational performance numbers in the figure above are based on my from-scratch PyTorch implementations when run on an A100 GPU. As one can see, Qwen3 has a smaller memory footprint as it is a smaller architecture overall, but also uses smaller hidden layers and fewer attention heads. However, it uses more transformer blocks than Llama 3, which leads to a slower runtime (lower tokens/sec generation speed).
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    6.2 Qwen3 (MoE)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   As mentioned earlier, Qwen3 also comes in two MoE flavors: 30B-A3B and 235B-A22B. Why do some architectures, like Qwen3, come as regular (dense) and MoE (sparse) variants?
  &lt;/p&gt;
  &lt;p&gt;
   As mentioned at the beginning of this article, MoE variants help reduce inference costs for large base models. Offering both dense and MoE versions gives users flexibility depending on their goals and constraints.
  &lt;/p&gt;
  &lt;p&gt;
   Dense models are typically more straightforward to fine-tune, deploy, and optimize across various hardware.
  &lt;/p&gt;
  &lt;p&gt;
   On the other hand, MoE models are optimized for scaling inference. For instance, at a fixed inference budget, they can achieve a higher overall model capacity (i.e., knowledge uptake during training due to being larger) without proportionally increasing inference costs.
  &lt;/p&gt;
  &lt;p&gt;
   By releasing both types, the Qwen3 series can support a broader range of use cases: dense models for robustness, simplicity, and fine-tuning, and MoE models for efficient serving at scale.
  &lt;/p&gt;
  &lt;p&gt;
   To round up this section, let&#x27;s look at Qwen3 235B-A22B (note that the A22B stands for &quot;22B active parameters) to DeepSeek-V3, which has almost twice as many active parameters (37B).
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!cVH6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!cVH6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 424w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 848w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 1272w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:744,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:231906,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/168650848?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;744&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!cVH6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!cVH6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 424w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 848w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 1272w, https://substackcdn.com/image/fetch/$s_!cVH6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F102089a8-f0b5-4c1d-aeda-752ccb015d4b_1467x750.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 19: An architecture comparison between DeepSeek-V3 and Qwen3 235B-A22B.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    As shown in the figure above, the DeepSeek-V3 and Qwen3 235B-A22B architectures are remarkably similar. What&#x27;s noteworthy, though, is that the Qwen3 model moved away from using a shared expert (earlier Qwen models, such as
   &lt;/span&gt;
   &lt;a href=&quot;https://qwenlm.github.io/blog/qwen2.5-max/&quot; rel=&quot;&quot;&gt;
    Qwen2.5-MoE
   &lt;/a&gt;
   &lt;span&gt;
    did use a shared expert).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Unfortunately, the Qwen3 team did not disclose any reason as to why they moved away from shared experts. If I had to guess, it was perhaps simply not necessary for training stability for their setup when they increased the experts from 2 (in Qwen2.5-MoE) to 8 (in Qwen3). And then they were able to save the extra compute/memory cost by using only 8 instead of 8+1 experts. (However, this doesn&#x27;t explain why DeepSeek-V3 is still keeping their shared expert.)
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   7. SmolLM3
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://huggingface.co/blog/smollm3&quot; rel=&quot;&quot;&gt;
    SmolLM3
   &lt;/a&gt;
   &lt;span&gt;
    is perhaps not as nearly as popular as the other LLMs covered in this article, but I thought it is still an interesting model to include as it offers really good modeling performance at a relatively small and convenient 3-billion parameter model size that sits between the 1.7B and 4B Qwen3 model, as shown in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Moreover, it also shared a lot of the training details, similar to OLMo, which is rare and always appreciated!
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!vPTQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vPTQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 424w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 848w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1272w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ebfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:519,&quot;width&quot;:743,&quot;resizeWidth&quot;:592,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;413.5235531628533&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vPTQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 424w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 848w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1272w, https://substackcdn.com/image/fetch/$s_!vPTQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febfdfd6e-6aee-4894-8a57-307a25bc2c0a_743x519.png 1456w&quot; width=&quot;592&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 20: An annotated figure from the SmolLM3 announcement post, https://huggingface.co/blog/smollm3, comparing the SmolLM3 win rate to Qwen3 1.7B and 4B as well as Llama 3 3B and Gemma 3 4B.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As shown in the architecture comparison figure below, the SmolLM3 architecture looks fairly standard. The perhaps most interesting aspect is its use of NoPE (No Positional Embeddings), though.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!5Iki!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!5Iki!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 424w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 848w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 1272w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:777,&quot;width&quot;:1431,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;777&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!5Iki!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!5Iki!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 424w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 848w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 1272w, https://substackcdn.com/image/fetch/$s_!5Iki!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fed76de6d-bc7a-4e75-8fc5-9efa626d8d92_1431x777.png 1456w&quot; width=&quot;1431&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 21: A side-by-side architecture comparison between Qwen3 4B and SmolLM3 3B.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    7.1 No Positional Embeddings (NoPE)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    NoPE is, in LLM contexts, an older idea that goes back to a 2023 paper (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2305.19466&quot; rel=&quot;&quot;&gt;
    The Impact of Positional Encoding on Length Generalization in Transformers
   &lt;/a&gt;
   &lt;span&gt;
    ) to remove explicit positional information injection (like through classic absolute positional embedding layers in early GPT architectures or nowadays RoPE).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In transformer-based LLMs, positional encoding is typically necessary because self-attention treats tokens independently of order. Absolute position embeddings solve this by adding an additional embedding layer that adds information to the token embeddings.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!lLgK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!lLgK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 424w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 848w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1272w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1190,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;548&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!lLgK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!lLgK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 424w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 848w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1272w, https://substackcdn.com/image/fetch/$s_!lLgK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd79f412c-269f-4236-9cbe-81f1a5944ae1_1190x548.png 1456w&quot; width=&quot;1190&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 22: A modified figure from my Build A Large Language Model (From Scratch) book (https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167) illustrating absolute positional embeddings.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   RoPE, on the other hand, solves this by rotating the query and key vectors relative to their token position.
  &lt;/p&gt;
  &lt;p&gt;
   In NoPE layers, however, no such positional signal is added at all: not fixed, not learned, not relative. Nothing.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Even though there is no positional embedding, the model still knows which tokens come before, thanks to the causal attention mask. This mask prevents each token from attending to future ones. As a result, a token at position
   &lt;/span&gt;
   &lt;em&gt;
    t
   &lt;/em&gt;
   &lt;span&gt;
    can only see tokens at positions
   &lt;/span&gt;
   &lt;em&gt;
    ≤ t
   &lt;/em&gt;
   &lt;span&gt;
    , which preserves the autoregressive ordering.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   So while there is no positional information that is explicitly added, there is still an implicit sense of direction baked into the model&#x27;s structure, and the LLM, in the regular gradient-descent-based training, can learn to exploit it if it finds it beneficial for the optimization objective. (Check out the NoPE paper&#x27;s theorems for more information.)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, overall, the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2305.19466&quot; rel=&quot;&quot;&gt;
    NoPE paper
   &lt;/a&gt;
   &lt;span&gt;
    not only found that no positional information injection is necessary, but it also found that NoPE has better length generalization, which means that LLM answering performance deteriorates less with increased sequence length, as shown in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!I9j6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!I9j6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 424w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 848w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1272w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:800,&quot;width&quot;:1364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;800&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!I9j6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!I9j6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 424w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 848w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1272w, https://substackcdn.com/image/fetch/$s_!I9j6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7a57872-2af0-465b-ab5f-2caecc0aac8f_1364x800.png 1456w&quot; width=&quot;1364&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 23: An annotated figure from the NoPE paper (https://arxiv.org/abs/2305.19466) showing better length generalization with NoPE.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Note that the experiments shown above were conducted with a relatively small GPT-style model of approximately 100 million parameters and relatively small context sizes. It is unclear how well these findings generalize to larger, contemporary LLMs.
  &lt;/p&gt;
  &lt;p&gt;
   For this reason, the SmolLM3 team likely only &quot;applied&quot; NoPE (or rather omitted RoPE) in every 4th layer.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   8. Kimi 2
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://moonshotai.github.io/Kimi-K2/&quot; rel=&quot;&quot;&gt;
    Kimi 2
   &lt;/a&gt;
   &lt;span&gt;
    recently made big waves in the AI community due to being an open-weight model with an incredibly good performance. According to benchmarks, it&#x27;s on par with the best proprietary models like Google&#x27;s Gemini, Anthropic&#x27;s Claude, and OpenAI&#x27;s ChatGPT models.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    A notable aspect is its use of a variant of the relatively new
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/KellerJordan/Muon&quot; rel=&quot;&quot;&gt;
    Muon
   &lt;/a&gt;
   &lt;span&gt;
    optimizer over AdamW. As far as I know, this is the first time Muon was used over AdamW for any production model of this size (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2502.16982&quot; rel=&quot;&quot;&gt;
    previously
   &lt;/a&gt;
   &lt;span&gt;
    , it has only been shown to scale up to 16B). This resulted in very nice training loss curves, which probably helped catapult this model to the top of the aforementioned benchmarks.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While people commented that the loss was exceptionally smooth (due to the lack of spikes), I think it&#x27;s not exceptionally smooth (e.g., see the OLMo 2 loss curve in the figure below; also, the L2 norm of the gradient would probably be a better metric to track training stability). However, what&#x27;s remarkable is how well the loss curve decays.
  &lt;/p&gt;
  &lt;p&gt;
   However, as mentioned in the introduction of this article, training methodologies are a topic for another time.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!_Zh8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_Zh8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 424w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 848w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1272w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:684,&quot;width&quot;:759,&quot;resizeWidth&quot;:612,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;551.5256916996047&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_Zh8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 424w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 848w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1272w, https://substackcdn.com/image/fetch/$s_!_Zh8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d5d4e4-7dc4-49ab-87be-a885ba78447e_759x684.png 1456w&quot; width=&quot;612&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The model itself is 1 trillion parameters large, which is truly impressive.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    It may be the biggest LLM of this generation as of this writing (given the constraints that Llama 4 Behemoth is not released, proprietary LLMs don&#x27;t count, and Google&#x27;s 1.6 trillion
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2101.03961&quot; rel=&quot;&quot;&gt;
    Switch Transformer
   &lt;/a&gt;
   &lt;span&gt;
    is an encoder-decoder architecture from a different generation).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   It&#x27;s also coming full circle as Kimi 2 uses the DeepSeek-V3 architecture we covered at the beginning of this article except they made it larger, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!B3em!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!B3em!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 424w, https://substackcdn.com/image/fetch/$s_!B3em!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 848w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1272w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b721c5ef-057b-405b-9293-f11e161d9230_1599x816.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:743,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;743&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!B3em!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!B3em!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 424w, https://substackcdn.com/image/fetch/$s_!B3em!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 848w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1272w, https://substackcdn.com/image/fetch/$s_!B3em!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb721c5ef-057b-405b-9293-f11e161d9230_1599x816.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure 25: An architecture comparison between DeepSeek V3 and Kimi K2.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As shown in the figure above, Kimi 2.5 is basically the same as DeepSeek V3, except that it uses more experts in the MoE modules and fewer heads in the Multi-head Latent Attention (MLA) module.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Kimi 2 is not coming out of nowhere. The earlier Kimi 1.5 model discussed in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2501.12599&quot; rel=&quot;&quot;&gt;
    Kimi k1.5: Scaling Reinforcement Learning with LLMs paper
   &lt;/a&gt;
   &lt;span&gt;
    , was impressive as well. However, it had the bad luck that the DeepSeek R1 model paper was published on exactly the same date on January 22nd. Moreover, as far as I know, the Kimi 1.5 weights were never publicly shared.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   So, most likely the Kimi K2 team took these lessons to heart and shared Kimi K2 as an open-weight model, before DeepSeek R2 was released. As of this writing, Kimi K2 is the most impressive open-weight model.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    After all these years, LLM releases remain exciting, and I am curious to see what&#x27;s next!
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    This magazine is a personal passion project, and your support helps keep it alive. If you would like to contribute, there are a few great ways:
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
        Grab a copy of my book
       &lt;/a&gt;
      &lt;/strong&gt;
      &lt;span&gt;
       . Build a Large Language Model (From Scratch) walks you through building an LLM step by step, from tokenizer to training.
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;a href=&quot;https://www.manning.com/livevideo/master-and-build-large-language-models&quot; rel=&quot;&quot;&gt;
        Check out the video course
       &lt;/a&gt;
      &lt;/strong&gt;
      &lt;span&gt;
       . There’s now a 17-hour video course based on the book, available from Manning. It follows the book closely, section by section, and works well both as a standalone or as a code-along resource. The video course is ad-free (unlike the YouTube version) and has a cleaner, more structured format. It also contains 5 additional hours of pre-requisite video material created by Abhinav Kimothi.
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;a href=&quot;https://magazine.sebastianraschka.com/subscribe&quot; rel=&quot;&quot;&gt;
        Subscribe
       &lt;/a&gt;
      &lt;/strong&gt;
      &lt;span&gt;
       . A paid subscription helps to make my writing sustainable and gives you access to additional contents.
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;em&gt;
    Thanks for reading, and for helping support independent research!
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!wVLk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;878&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!wVLk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 424w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 848w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!wVLk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffde977ef-c20a-487b-8f1d-5fdbada6585c_1468x885.jpeg 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> LLM Research Papers: The 2025 List (January to June) </title>
<link>https://magazine.sebastianraschka.com/p/llm-research-papers-2025-list-one</link>
<pubDate>Tue, 01 Jul 2025 04:11:45 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   As some of you know, I keep a running list of research papers I (want to) read and reference.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    About six months ago, I shared
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list&quot; rel=&quot;&quot;&gt;
    my 2024 list
   &lt;/a&gt;
   &lt;span&gt;
    , which many readers found useful. So, I was thinking about doing this again. However, this time, I am incorporating that one piece of feedback kept coming up:
   &lt;/span&gt;
   &lt;em&gt;
    &quot;Can you organize the papers by topic instead of date?&quot;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The categories I came up with are:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Reasoning Models
    &lt;/p&gt;
    &lt;p&gt;
     - 1a. Training Reasoning Models
    &lt;/p&gt;
    &lt;p&gt;
     - 1b. Inference-Time Reasoning Strategies
    &lt;/p&gt;
    &lt;p&gt;
     - 1c. Evaluating LLMs and/or Understanding Reasoning
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Other Reinforcement Learning Methods for LLMs
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Other Inference-Time Scaling Methods
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Efficient Training &amp; Architectures
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Diffusion-Based Language Models
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Multimodal &amp; Vision-Language Models
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Data &amp; Pre-training Datasets
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   Also, as LLM research continues to be shared at a rapid pace, I have decided to break the list into bi-yearly updates. This way, the list stays digestible, timely, and hopefully useful for anyone looking for solid summer reading material.
  &lt;/p&gt;
  &lt;p&gt;
   Please note that this is just a curated list for now. In future articles, I plan to revisit and discuss some of the more interesting or impactful papers in larger topic-specific write-ups. Stay tuned!
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   Announcement:
  &lt;/h4&gt;
  &lt;p&gt;
   &lt;span&gt;
    It&#x27;s summer! And that means internship season, tech interviews, and lots of learning.
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    To support those brushing up on intermediate to advanced machine learning and AI topics,
   &lt;/span&gt;
   &lt;strong&gt;
    I have made all 30 chapters of my Machine Learning Q and AI book freely available for the summer:
   &lt;/strong&gt;
   &lt;br/&gt;
   &lt;br/&gt;
   &lt;span&gt;
    🔗
   &lt;/span&gt;
   &lt;a href=&quot;https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents&quot; rel=&quot;&quot;&gt;
    https://sebastianraschka.com/books/ml-q-and-ai/#table-of-contents
   &lt;/a&gt;
   &lt;br/&gt;
   &lt;br/&gt;
   &lt;span&gt;
    Whether you are just curious and want to learn something new or prepping for interviews, hopefully this comes in handy.
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;br/&gt;
   &lt;span&gt;
    Happy reading, and best of luck if you are interviewing!
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1. Reasoning Models
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   This year, my list is very reasoning model-heavy. So, I decided to subdivide it into 3 categories: Training, inference-time scaling, and more general understanding/evaluation.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1a. Training Reasoning Models
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   This subsection focuses on training strategies specifically designed to improve reasoning abilities in LLMs. As you may see, much of the recent progress has centered around reinforcement learning (with verifiable rewards), which I covered in more detail in a previous article.
  &lt;/p&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!A5c7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!A5c7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 424w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 848w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 1272w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:477,&quot;width&quot;:941,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:97188,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166943621?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;477&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!A5c7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!A5c7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 424w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 848w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 1272w, https://substackcdn.com/image/fetch/$s_!A5c7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbac0fd9e-2095-43c3-95be-b9a5b35f0147_941x477.png 1456w&quot; width=&quot;941&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Annotated figure from Reinforcement Pre-Training,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.08007&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.08007
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Jan, Towards System 2 Reasoning in LLMs: Learning How to Think With Meta Chain-of-Thought,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2501.04682&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2501.04682
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Jan, The Lessons of Developing Process Reward Models in Mathematical Reasoning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2501.07301&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2501.07301
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jan, Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2501.09686&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2501.09686
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Jan, Reasoning Language Models: A Blueprint,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2501.11223&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2501.11223
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Jan, Kimi k1.5: Scaling Reinforcement Learning with LLMs,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs//2501.12599&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs//2501.12599
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Jan, DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2501.12948&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2501.12948
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Feb, Competitive Programming with Large Reasoning Models,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.06807&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.06807
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Feb, Demystifying Long Chain-of-Thought Reasoning in LLMs, Demystifying Long Chain-of-Thought Reasoning in LLMs,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.03373&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.03373
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Feb, LIMO: Less is More for Reasoning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.03387&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.03387
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Feb, Teaching Language Models to Critique via Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.03492&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.03492
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Feb, Training Language Models to Reason Efficiently,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.04463&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.04463
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Feb, Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.06781&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.06781
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Feb, On the Emergence of Thinking in LLMs I: Searching for the Right Intuition,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.06773&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.06773
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Feb, LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.07374&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.07374
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Feb, Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.08127&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.08127
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Feb, Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging - An Open Recipe,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.09056&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.09056
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Feb, Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.14768&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.14768
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Feb, SWE-RL: Advancing LLM Reasoning via Reinforcement Learning on Open Software Evolution,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.18449&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.18449
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Mar, Learning from Failures in Multi-Attempt Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.04808&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.04808
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Mar, The First Few Tokens Are All You Need: An Efficient and Effective Unsupervised Prefix Fine-Tuning Method for Reasoning Models,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.02875&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.02875
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Mar, R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.05592&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.05592
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Mar, LMM-R1: Empowering 3B LMMs with Strong Reasoning Abilities Through Two-Stage Rule-Based RL,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.07536&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.07536
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Mar, Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.09516&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.09516
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Mar, Towards Hierarchical Multi-Step Reward Models for Enhanced Reasoning in Large Language Models,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.13551&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.13551
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Mar, Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn&#x27;t,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.16219&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.16219
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Mar, ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.19470&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.19470
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Mar, Understanding R1-Zero-Like Training: A Critical Perspective,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.20783&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.20783
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Mar, RARE: Retrieval-Augmented Reasoning Modeling,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.23513&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.23513
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Mar, Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2503.24290&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2503.24290
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Mar, JudgeLRM: Large Reasoning Models as a Judge,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.00050&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.00050
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Apr, Concise Reasoning via Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.05185&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.05185
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Apr, VL-Rethinker: Incentivizing Self-Reflection of Vision-Language Models with Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.08837&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.08837
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Apr, Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.08672&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.08672
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Apr, Leveraging Reasoning Model Answers to Enhance Non-Reasoning Model Capability,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.09639&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.09639
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Apr, Learning to Reason under Off-Policy Guidance,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.14945&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.14945
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Apr, Tina: Tiny Reasoning Models via LoRA,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.15777&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.15777
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Apr, Reinforcement Learning for Reasoning in Large Language Models with One Training Example,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.20571&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.20571
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Apr, Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2504.21233&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2504.21233
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 May, Llama-Nemotron: Efficient Reasoning Models,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.00949&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.00949
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 May, RM-R1: Reward Modeling as Reasoning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.02387&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.02387
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 May, Absolute Zero: Reinforced Self-play Reasoning with Zero Data,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.03335&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.03335
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 May, INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.07291&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.07291
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 May, MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.07608&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.07608
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 May, Qwen3 Technical Report,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.09388&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.09388
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 May, Beyond &#x27;Aha!&#x27;: Toward Systematic Meta-Abilities Alignment in Large Reasoning Models,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.10554&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.10554
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 May, AdaptThink: Reasoning Models Can Learn When to Think,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.13417&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.13417
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 May, Thinkless: LLM Learns When to Think,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.13379&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.13379
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 May, General-Reasoner: Advancing LLM Reasoning Across All Domains,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.14652&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.14652
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 May, Learning to Reason via Mixture-of-Thought for Logical Reasoning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.15817&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.15817
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 May, RL Tango: Reinforcing Generator and Verifier Together for Language Reasoning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.15034&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.15034
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 May, QwenLong-L1: Towards Long-Context Large Reasoning Models with Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://www.arxiv.org/abs/2505.17667&quot; rel=&quot;&quot;&gt;
      https://www.arxiv.org/abs/2505.17667
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 May, Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.19914&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.19914
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 May, Learning to Reason without External Rewards,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.19590&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.19590
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 May, Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.22954&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.22954
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 May, Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.24726&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.24726
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 May, ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2505.24864&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2505.24864
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jun, Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.01939&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.01939
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Jun, Rewarding the Unlikely: Lifting GRPO Beyond Distribution Sharpening,
     &lt;/span&gt;
     &lt;a href=&quot;https://www.arxiv.org/abs/2506.02355&quot; rel=&quot;&quot;&gt;
      https://www.arxiv.org/abs/2506.02355
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Jun, Reinforcement Pre-Training,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.08007&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.08007
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jun, RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.08672&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.08672
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jun, Reinforcement Learning Teachers of Test Time Scaling,
     &lt;/span&gt;
     &lt;a href=&quot;https://www.arxiv.org/abs/2506.08388&quot; rel=&quot;&quot;&gt;
      https://www.arxiv.org/abs/2506.08388
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jun, Magistral,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.10910&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.10910
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jun, Spurious Rewards: Rethinking Training Signals in RLVR,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.10947&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.10947
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jun, AlphaEvolve: A coding agent for scientific and algorithmic discovery,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.13131&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.13131
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun, Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes Correct Reasoning in Base LLMs,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.14245&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.14245
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Jun, Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.18777&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.18777
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Jun, Bridging Offline and Online Reinforcement Learning for LLMs,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2506.21495&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2506.21495
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1b. Inference-Time Reasoning Strategies
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   This part of the list covers methods that improve reasoning dynamically at test time, without requiring retraining. Often, these papers are focused on trading of computational performance for modeling performance.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Understanding and Coding the KV Cache in LLMs from Scratch </title>
<link>https://magazine.sebastianraschka.com/p/coding-the-kv-cache-in-llms</link>
<pubDate>Tue, 17 Jun 2025 03:55:34 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   KV caches are one of the most critical techniques for efficient inference in LLMs in production. KV caches are an important component for compute-efficient LLM inference in production. This article explains how they work conceptually and in code with a from-scratch, human-readable implementation.
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    &lt;span&gt;
     It&#x27;s been a while since I shared a technical tutorial explaining fundamental LLM concepts. As I am currently recovering from an injury and working on a bigger LLM research-focused article, I thought I&#x27;d share a tutorial article on a topic several readers asked me about (as it was
    &lt;/span&gt;
    &lt;strong&gt;
     not
    &lt;/strong&gt;
    &lt;span&gt;
     included in my
    &lt;/span&gt;
    &lt;em&gt;
     Building a Large Language Model From Scratch
    &lt;/em&gt;
    &lt;span&gt;
     book).
    &lt;/span&gt;
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   Happy reading!
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Overview
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   In short, a KV cache stores intermediate key (K) and value (V) computations for reuse during inference (after training), which results in a substantial speed-up when generating text. The downside of a KV cache is that it adds more complexity to the code, increases memory requirements (the main reason I initially didn&#x27;t include it in the book), and can&#x27;t be used during training. However, the inference speed-ups are often well worth the trade-offs in code complexity and memory when using LLMs in production.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    What Is a KV Cache?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Imagine the LLM is generating some text. Concretely, suppose the LLM is given the following prompt: &quot;Time&quot;. As you may already know, LLMs generate one word (or token) at a time, and the two following text generation steps may look as illustrated in the figure below:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!pooO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!pooO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png 424w, https://substackcdn.com/image/fetch/$s_!pooO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png 848w, https://substackcdn.com/image/fetch/$s_!pooO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png 1272w, https://substackcdn.com/image/fetch/$s_!pooO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:760,&quot;width&quot;:768,&quot;resizeWidth&quot;:527,&quot;bytes&quot;:73550,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;521.5104166666666&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!pooO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!pooO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png 424w, https://substackcdn.com/image/fetch/$s_!pooO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png 848w, https://substackcdn.com/image/fetch/$s_!pooO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png 1272w, https://substackcdn.com/image/fetch/$s_!pooO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4249e23e-7945-4c8f-a11f-2fd921ff0672_768x760.png 1456w&quot; width=&quot;527&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     The diagram illustrates how an LLM generates text one token at a time. Starting with the prompt &quot;Time&quot;, the model generates the next token &quot;flies.&quot; In the next step, the full sequence &quot;Time flies&quot; is reprocessed to generate the token &quot;fast&quot;.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Note that there is some redundancy in the generated LLM text outputs, as highlighted in the next figure:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!As0Z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!As0Z!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png 424w, https://substackcdn.com/image/fetch/$s_!As0Z!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png 848w, https://substackcdn.com/image/fetch/$s_!As0Z!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png 1272w, https://substackcdn.com/image/fetch/$s_!As0Z!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/da5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:618,&quot;width&quot;:617,&quot;resizeWidth&quot;:429,&quot;bytes&quot;:45491,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;429.69529983792546&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!As0Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!As0Z!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png 424w, https://substackcdn.com/image/fetch/$s_!As0Z!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png 848w, https://substackcdn.com/image/fetch/$s_!As0Z!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png 1272w, https://substackcdn.com/image/fetch/$s_!As0Z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fda5df468-5b21-4b1f-9ccb-b144dfb2a293_617x618.png 1456w&quot; width=&quot;429&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     This figure highlights the repeated context (&quot;Time flies&quot;) that must be reprocessed by the LLM at each generation step. Since the LLM does not cache intermediate key/value states, it re-encodes the full sequence every time a new token (e.g., &quot;fast&quot;) is generated.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    When we implement an LLM text generation function, we typically only use the last generated token from each step. However, the visualization above highlights one of the main inefficiencies on a conceptual level. This inefficiency (or redundancy) becomes more clear if we zoom in on the attention mechanism itself. (If you are curious about attention mechanisms, you can read more in Chapter 3 of my
   &lt;/span&gt;
   &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
    Build a Large Language Model (From Scratch)
   &lt;/a&gt;
   &lt;span&gt;
    book or my
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention&quot; rel=&quot;&quot;&gt;
    Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs
   &lt;/a&gt;
   &lt;span&gt;
    article).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    The following figure shows an excerpt of an attention mechanism computation that is at the core of an LLM. Here, the input tokens (&quot;Time&quot; and &quot;flies&quot;) are encoded as 3-dimensional vectors (in reality, these vectors are much larger, but this would make it challenging to fit them into a small figure). The matrices
   &lt;/span&gt;
   &lt;em&gt;
    W
   &lt;/em&gt;
   &lt;span&gt;
    are the weight matrices of the attention mechanism that transform these inputs into key, value, and query vectors.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The figure below shows an excerpt of the underlying attention score computation with the key and value vectors highlighted:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!4BB1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!4BB1!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png 424w, https://substackcdn.com/image/fetch/$s_!4BB1!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png 848w, https://substackcdn.com/image/fetch/$s_!4BB1!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png 1272w, https://substackcdn.com/image/fetch/$s_!4BB1!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:445,&quot;width&quot;:945,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:76215,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;445&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!4BB1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!4BB1!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png 424w, https://substackcdn.com/image/fetch/$s_!4BB1!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png 848w, https://substackcdn.com/image/fetch/$s_!4BB1!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png 1272w, https://substackcdn.com/image/fetch/$s_!4BB1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3748127c-532e-4169-8e12-1fb48e263dbd_945x445.png 1456w&quot; width=&quot;945&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      This figure illustrates how the LLM derives key (
     &lt;/span&gt;
     &lt;code&gt;
      k
     &lt;/code&gt;
     &lt;span&gt;
      ) and value (
     &lt;/span&gt;
     &lt;code&gt;
      v
     &lt;/code&gt;
     &lt;span&gt;
      ) vectors from token embeddings during attention computation. Each input token (e.g., &quot;Time&quot; and &quot;flies&quot;) is projected using learned matrices
     &lt;/span&gt;
     &lt;code&gt;
      W_k
     &lt;/code&gt;
     &lt;span&gt;
      and
     &lt;/span&gt;
     &lt;code&gt;
      W_v
     &lt;/code&gt;
     &lt;span&gt;
      to obtain its corresponding key and value vectors.
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   As mentioned earlier, LLMs generate one word (or token) at a time. Suppose the LLM generated the word &quot;fast&quot; so that the prompt for the next round becomes &quot;Time flies fast&quot;. This is illustrated in the next figure below:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!gBu0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!gBu0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png 424w, https://substackcdn.com/image/fetch/$s_!gBu0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png 848w, https://substackcdn.com/image/fetch/$s_!gBu0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png 1272w, https://substackcdn.com/image/fetch/$s_!gBu0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:877,&quot;width&quot;:1259,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:186341,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;877&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!gBu0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!gBu0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png 424w, https://substackcdn.com/image/fetch/$s_!gBu0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png 848w, https://substackcdn.com/image/fetch/$s_!gBu0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png 1272w, https://substackcdn.com/image/fetch/$s_!gBu0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F06c2f011-ce16-4832-a3aa-4927703fb752_1259x877.png 1456w&quot; width=&quot;1259&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      This diagram shows how the LLM recomputes key and value vectors for previously seen tokens (&quot;Time&quot; and &quot;flies&quot;) during each generation step. When generating the third token (&quot;fast&quot;), the model recomputes the same
     &lt;/span&gt;
     &lt;code&gt;
      k(1)/v(1)
     &lt;/code&gt;
     &lt;span&gt;
      and
     &lt;/span&gt;
     &lt;code&gt;
      k(2)/v(2)
     &lt;/code&gt;
     &lt;span&gt;
      vectors again, rather than reusing them. This repeated computation highlights the inefficiency of not using a KV cache during autoregressive decoding.
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   As we can see, based on comparing the previous 2 figures, the keys and value vectors for the first two tokens are exactly the same, and it would be wasteful to recompute them in each next-token text generation round.
  &lt;/p&gt;
  &lt;p&gt;
   Now, the idea of the KV cache is to implement a caching mechanism that stores the previously generated key and value vectors for reuse, which helps us to avoid these unnecessary recomputations.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    How LLMs Generate Text (Without and With a KV Cache)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    After we went over the basic concept in the previous section, let&#x27;s go into a bit more detail before we look at a concrete code implementation. If we have a text generation process
   &lt;/span&gt;
   &lt;em&gt;
    without
   &lt;/em&gt;
   &lt;span&gt;
    KV cache for &quot;Time flies fast&quot;, we can think of it as follows:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!z-sX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!z-sX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png 424w, https://substackcdn.com/image/fetch/$s_!z-sX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png 848w, https://substackcdn.com/image/fetch/$s_!z-sX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png 1272w, https://substackcdn.com/image/fetch/$s_!z-sX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:194,&quot;width&quot;:741,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:17718,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;194&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!z-sX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!z-sX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png 424w, https://substackcdn.com/image/fetch/$s_!z-sX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png 848w, https://substackcdn.com/image/fetch/$s_!z-sX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png 1272w, https://substackcdn.com/image/fetch/$s_!z-sX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5fdd4b41-96e6-40c2-baf3-aedfeee8d1de_741x194.png 1456w&quot; width=&quot;741&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Notice the redundancy: tokens &quot;Time&quot; and &quot;flies&quot; are recomputed at every new generation step. The KV cache resolves this inefficiency by storing and reusing previously computed key and value vectors:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Initially, the model computes and caches key and value vectors for the input tokens.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     For each new token generated, the model only computes key and value vectors for that specific token.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Previously computed vectors are retrieved from the cache to avoid redundant computations.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   The table below summarizes the computation and caching steps and states:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!qQBU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qQBU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png 424w, https://substackcdn.com/image/fetch/$s_!qQBU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png 848w, https://substackcdn.com/image/fetch/$s_!qQBU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png 1272w, https://substackcdn.com/image/fetch/$s_!qQBU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:202,&quot;width&quot;:736,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:20360,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;202&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!qQBU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qQBU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png 424w, https://substackcdn.com/image/fetch/$s_!qQBU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png 848w, https://substackcdn.com/image/fetch/$s_!qQBU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png 1272w, https://substackcdn.com/image/fetch/$s_!qQBU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8f20643e-0942-4b5f-98ec-051d030d127a_736x202.png 1456w&quot; width=&quot;736&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    The benefits here are that
   &lt;/span&gt;
   &lt;code&gt;
    &quot;Time&quot;
   &lt;/code&gt;
   &lt;span&gt;
    is computed once and reused twice, and
   &lt;/span&gt;
   &lt;code&gt;
    &quot;flies&quot;
   &lt;/code&gt;
   &lt;span&gt;
    is computed once and reused once. (It&#x27;s a short text example for simplicity, but it should be intuitive to see that the longer the text, the more we get to reuse already computed keys and values, which increases the generation speed.)n speed.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The following figure illustrates generation step 3 with and without a KV cache side by side.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!PjfC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!PjfC!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png 424w, https://substackcdn.com/image/fetch/$s_!PjfC!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png 848w, https://substackcdn.com/image/fetch/$s_!PjfC!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png 1272w, https://substackcdn.com/image/fetch/$s_!PjfC!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:926,&quot;width&quot;:841,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:134180,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;926&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!PjfC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!PjfC!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png 424w, https://substackcdn.com/image/fetch/$s_!PjfC!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png 848w, https://substackcdn.com/image/fetch/$s_!PjfC!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png 1272w, https://substackcdn.com/image/fetch/$s_!PjfC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78382a83-f634-4cfa-92b9-bbea30c61a60_841x926.png 1456w&quot; width=&quot;841&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Comparing text generation with and without a KV cache. In the top panel (without cache), key and value vectors are recomputed for each token step, which results in redundant operations. In the bottom panel (with cache), previously computed keys and values are retrieved from the KV cache to avoid recomputation for faster generation.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   So, if we want to implement a KV cache in code, all we have to do is compute the keys and values as usual but then store them so that we can retrieve them in the next round. The next section illustrates this with a concrete code example.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Implementing a KV Cache from Scratch
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   There are many ways to implement a KV cache, with the main idea being that we only compute the key and value tensors for the newly generated tokens in each generation step.
  &lt;/p&gt;
  &lt;p&gt;
   I opted for a simple one that emphasizes code readability. I think it&#x27;s easiest to just scroll through the code changes to see how it&#x27;s implemented.
  &lt;/p&gt;
  &lt;p&gt;
   There are two files I shared on GitHub, which are self-contained Python scripts that implement an LLM with and without KV cache from scratch:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch04/03_kv-cache/gpt_ch04.py&quot; rel=&quot;&quot;&gt;
      gpt_ch04.py
     &lt;/a&gt;
     &lt;span&gt;
      : Self-contained code taken from Chapters 3 and 4 of my
     &lt;/span&gt;
     &lt;em&gt;
      Build a Large Language Model (From Scratch)
     &lt;/em&gt;
     &lt;span&gt;
      book to implement the LLM and run the simple text generation function
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch04/03_kv-cache/gpt_with_kv_cache.py&quot; rel=&quot;&quot;&gt;
      gpt_with_kv_cache.py
     &lt;/a&gt;
     &lt;span&gt;
      : The same as above, but with the necessary changes made to implement the KV cache.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   To read through the KV cache-relevant code modifications, you can either:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    a. Open the
   &lt;/span&gt;
   &lt;code&gt;
    gpt_with_kv_cache.py
   &lt;/code&gt;
   &lt;span&gt;
    file and look out for the
   &lt;/span&gt;
   &lt;code&gt;
    # NEW
   &lt;/code&gt;
   &lt;span&gt;
    sections that mark the new changes:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!KVf4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!KVf4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png 424w, https://substackcdn.com/image/fetch/$s_!KVf4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png 848w, https://substackcdn.com/image/fetch/$s_!KVf4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png 1272w, https://substackcdn.com/image/fetch/$s_!KVf4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:881,&quot;width&quot;:982,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;881&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!KVf4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!KVf4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png 424w, https://substackcdn.com/image/fetch/$s_!KVf4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png 848w, https://substackcdn.com/image/fetch/$s_!KVf4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png 1272w, https://substackcdn.com/image/fetch/$s_!KVf4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3338a3b0-c3ad-4d37-9d3f-15db18db51ff_982x881.png 1456w&quot; width=&quot;982&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   b. Check out the two code files via a file diff tool of your choice to compare the changes:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!qPKJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qPKJ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png 424w, https://substackcdn.com/image/fetch/$s_!qPKJ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png 848w, https://substackcdn.com/image/fetch/$s_!qPKJ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png 1272w, https://substackcdn.com/image/fetch/$s_!qPKJ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:854,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;854&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!qPKJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qPKJ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png 424w, https://substackcdn.com/image/fetch/$s_!qPKJ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png 848w, https://substackcdn.com/image/fetch/$s_!qPKJ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png 1272w, https://substackcdn.com/image/fetch/$s_!qPKJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fccfd3b14-df6e-4988-b986-851b4b207eae_1468x861.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In additoin, to summarize the implementation details, there&#x27;s a short walkthrough in the following subsections.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1. Registering the Cache Buffers
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    Inside the
   &lt;/span&gt;
   &lt;code&gt;
    MultiHeadAttention
   &lt;/code&gt;
   &lt;span&gt;
    constructor, we add two non-persistent buffers,
   &lt;/span&gt;
   &lt;code&gt;
    cache_k
   &lt;/code&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;code&gt;
    cache_v
   &lt;/code&gt;
   &lt;span&gt;
    , which will hold concatenated keys and values across steps:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;self.register_buffer(&quot;cache_k&quot;, None, persistent=False)
self.register_buffer(&quot;cache_v&quot;, None, persistent=False)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    (I made a YouTube video if you want to learn more about buffers:
   &lt;/span&gt;
   &lt;a href=&quot;https://youtu.be/PetlIokI9Ao&quot; rel=&quot;&quot;&gt;
    Understanding PyTorch Buffers
   &lt;/a&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2. Forward pass with
   &lt;/strong&gt;
   &lt;code&gt;
    use_cache
   &lt;/code&gt;
   &lt;strong&gt;
    flag
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    Next, we extend the
   &lt;/span&gt;
   &lt;code&gt;
    forward
   &lt;/code&gt;
   &lt;span&gt;
    method of the
   &lt;/span&gt;
   &lt;code&gt;
    MultiHeadAttention
   &lt;/code&gt;
   &lt;span&gt;
    class to accept a
   &lt;/span&gt;
   &lt;code&gt;
    use_cache
   &lt;/code&gt;
   &lt;span&gt;
    argument:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;def forward(self, x, use_cache=False):
    b, num_tokens, d_in = x.shape
​
    keys_new = self.W_key(x)  # Shape: (b, num_tokens, d_out)
    values_new = self.W_value(x)
    queries = self.W_query(x)
    #...
​
    if use_cache:
        if self.cache_k is None:
            self.cache_k, self.cache_v = keys_new, values_new
        else:
            self.cache_k = torch.cat([self.cache_k, keys_new], dim=1)
            self.cache_v = torch.cat([self.cache_v, values_new], dim=1)
        keys, values = self.cache_k, self.cache_v
    else:
        keys, values = keys_new, values_new&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The storage and retrieval of keys and values here implements the core idea of the KV cache.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Storing
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Concretely, after the cache is initialized via the if
   &lt;/span&gt;
   &lt;code&gt;
    self.cache_k is None: ...
   &lt;/code&gt;
   &lt;span&gt;
    , we add the newly generated keys and values via
   &lt;/span&gt;
   &lt;code&gt;
    self.cache_k = torch.cat(...)
   &lt;/code&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;code&gt;
    self.cache_v = torch.cat(...)
   &lt;/code&gt;
   &lt;span&gt;
    to the cache, respectively.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Retrieving
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Then,
   &lt;/span&gt;
   &lt;code&gt;
    keys, values = self.cache_k, self.cache_v
   &lt;/code&gt;
   &lt;span&gt;
    retrieves the stored values and keys from the cache.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   And that&#x27;s basically it: the core store &amp; retrieve mechanism of a KV cache. The following sections, 3 and 4, just take care of minor implementation details.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3. Clearing the Cache
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    When generating text, we have to remember to reset both the keys and value buffers between two separate text-generation calls. Otherwise, the queries of a new prompt will attend to stale keys left over from the previous sequence, which causes the model to rely on irrelevant context and produce incoherent output. To prevent this, we add a
   &lt;/span&gt;
   &lt;code&gt;
    reset_kv_cache
   &lt;/code&gt;
   &lt;span&gt;
    method to the
   &lt;/span&gt;
   &lt;code&gt;
    MultiHeadAttention
   &lt;/code&gt;
   &lt;span&gt;
    class that we can use between text-generation calls later:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;def reset_cache(self):
    self.cache_k, self.cache_v = None, None&lt;/code&gt;&lt;/pre&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4. Propagating
   &lt;/strong&gt;
   &lt;code&gt;
    use_cache
   &lt;/code&gt;
   &lt;strong&gt;
    in the Full Model
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    With the changes to the
   &lt;/span&gt;
   &lt;code&gt;
    MultiHeadAttention
   &lt;/code&gt;
   &lt;span&gt;
    class in place, we now modify the
   &lt;/span&gt;
   &lt;code&gt;
    GPTModel
   &lt;/code&gt;
   &lt;span&gt;
    class. First, we add a position tracking for the token indices to the instructor:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;self.current_pos = 0&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   This is a simple counter that remembers how many tokens the model has already cached during an incremental generation session.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Then, we replace the one-liner block call with an explicit loop, passing
   &lt;/span&gt;
   &lt;code&gt;
    use_cache
   &lt;/code&gt;
   &lt;span&gt;
    through each transformer block:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;def forward(self, in_idx, use_cache=False):
    # ...
 
    if use_cache:
        pos_ids = torch.arange(
            self.current_pos, self.current_pos + seq_len,            
            device=in_idx.device, dtype=torch.long
        )
        self.current_pos += seq_len
    else:
        pos_ids = torch.arange(
            0, seq_len, device=in_idx.device, dtype=torch.long
        )
    
    pos_embeds = self.pos_emb(pos_ids).unsqueeze(0)
    x = tok_embeds + pos_embeds
    # ...
    for blk in self.trf_blocks:
        x = blk(x, use_cache=use_cache)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    What happens above if we set
   &lt;/span&gt;
   &lt;code&gt;
    use_cache=True
   &lt;/code&gt;
   &lt;span&gt;
    is that we start at the
   &lt;/span&gt;
   &lt;code&gt;
    self.current_pos
   &lt;/code&gt;
   &lt;span&gt;
    and count
   &lt;/span&gt;
   &lt;code&gt;
    seq_len
   &lt;/code&gt;
   &lt;span&gt;
    steps. Then, bump the counter so the next decoding call continues where we left off.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The reason for the
   &lt;/span&gt;
   &lt;code&gt;
    self.current_pos
   &lt;/code&gt;
   &lt;span&gt;
    tracking is that new queries must line up directly after the keys and values that are already stored. Without using a counter, every new step would start at position 0 again, so the model would treat the new tokens as if they overlapped the earlier ones. (Alternatively, we could also keep track via an
   &lt;/span&gt;
   &lt;code&gt;
    offset = block.att.cache_k.shape[1]
   &lt;/code&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The above change then also requires a small modification to the
   &lt;/span&gt;
   &lt;code&gt;
    TransformerBlock
   &lt;/code&gt;
   &lt;span&gt;
    class to accept the
   &lt;/span&gt;
   &lt;code&gt;
    use_cache
   &lt;/code&gt;
   &lt;span&gt;
    argument:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;def forward(self, x, use_cache=False):
    # ...
    self.att(x, use_cache=use_cache)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Lastly, we add a model-level reset to
   &lt;/span&gt;
   &lt;code&gt;
    GPTModel
   &lt;/code&gt;
   &lt;span&gt;
    to clear all block caches at once for our convenience:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;def reset_kv_cache(self):
    for blk in self.trf_blocks:
        blk.att.reset_cache()
    self.current_pos = 0&lt;/code&gt;&lt;/pre&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    5. Using the Cache in Generation
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;span&gt;
    With the changes to the
   &lt;/span&gt;
   &lt;code&gt;
    GPTModel
   &lt;/code&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;code&gt;
    TransformerBlock
   &lt;/code&gt;
   &lt;span&gt;
    , and
   &lt;/span&gt;
   &lt;code&gt;
    MultiHeadAttention
   &lt;/code&gt;
   &lt;span&gt;
    , finally, here&#x27;s how we use the KV cache in a simple text generation function:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;def generate_text_simple_cached(
        model, idx, max_new_tokens, use_cache=True
    ):
    model.eval()
​
    ctx_len = model.pos_emb.num_embeddings  # max sup. len., e.g. 1024
    if use_cache:
        # Init cache with full prompt
        model.reset_kv_cache()
        with torch.no_grad():
            logits = model(idx[:, -ctx_len:], use_cache=True)
​
        for _ in range(max_new_tokens):
            # a) pick the token with the highest log-probability 
            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)
            # b) append it to the running sequence
            idx = torch.cat([idx, next_idx], dim=1)
            # c) feed model only the new token
            with torch.no_grad():
                logits = model(next_idx, use_cache=True)
    else:
        for _ in range(max_new_tokens):
            with torch.no_grad():
                logits = model(idx[:, -ctx_len:], use_cache=False)
            next_idx = logits[:, -1].argmax(dim=-1, keepdim=True)
            idx = torch.cat([idx, next_idx], dim=1)
​
    return idx&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that we only feed the model the new token in c) via
   &lt;/span&gt;
   &lt;code&gt;
    logits = model(next_idx, use_cache=True)
   &lt;/code&gt;
   &lt;span&gt;
    . Without caching, we feed the model the whole input
   &lt;/span&gt;
   &lt;code&gt;
    logits = model(idx[:, -ctx_len:], use_cache=False)
   &lt;/code&gt;
   &lt;span&gt;
    as it has no stored keys and values to reuse.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    A Simple Performance Comparison
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   After covering the KV cache on a conceptual level, the big question is how well it actually performs in practice on a small example. To give the implementation a try, we can run the two aforementioned code files as Python scripts, which will run the small 124 M parameter LLM to generate 200 new tokens (given a 4-token prompt &quot;Hello, I am&quot; to start with):
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/requirements.txt
​
python gpt_ch04.py
​
python gpt_with_kv_cache.py&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   On a Mac Mini with M4 chip (CPU), the results are as follows:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!LWIV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!LWIV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png 424w, https://substackcdn.com/image/fetch/$s_!LWIV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png 848w, https://substackcdn.com/image/fetch/$s_!LWIV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png 1272w, https://substackcdn.com/image/fetch/$s_!LWIV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:160,&quot;width&quot;:743,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:11924,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;160&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!LWIV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!LWIV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png 424w, https://substackcdn.com/image/fetch/$s_!LWIV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png 848w, https://substackcdn.com/image/fetch/$s_!LWIV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png 1272w, https://substackcdn.com/image/fetch/$s_!LWIV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d75a600-22e3-4edf-b642-44700dc7a2db_743x160.png 1456w&quot; title=&quot;&quot; width=&quot;743&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   So, as we can see, we already get a ~5x speed-up with a small 124 M parameter model and a short 200-token sequence length. (Note that this implementation is optimized for code readability and not optimized for CUDA or MPS runtime speed, which would require pre-allocating tensors instead of reinstating and concatenating them.)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Note:
   &lt;/strong&gt;
   &lt;span&gt;
    The model generates &quot;gibberish&quot; in both cases, i.e., text that looks like this:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    Output text: Hello, I am Featureiman Byeswickattribute argue logger Normandy Compton analogous bore ITVEGIN ministriesysics Kle functional recountrictionchangingVirgin embarrassedgl ...
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   This is because we haven&#x27;t trained the model yet. The next chapter trains the model, and you can use the KV cache on the trained model (however, the KV cache is only meant to be used during inference) to generate coherent text. Here, we are using the untrained model to keep the code simple(r).
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    What&#x27;s more important, though, is that both the
   &lt;/span&gt;
   &lt;code&gt;
    gpt_ch04.py
   &lt;/code&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;code&gt;
    gpt_with_kv_cache.py
   &lt;/code&gt;
   &lt;span&gt;
    implementations produce exactly the same text. This tells us that the KV cache is implemented correctly -- it is easy to make indexing mistakes that can lead to divergent results.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Thanks for reading Ahead of AI! Subscribe for free to receive new posts and support my work.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    KV cache Advantages and Disadvantages
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   As sequence length increases, the benefits and downsides of a KV cache become more pronounced in the following ways:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      [Good]
     &lt;/span&gt;
     &lt;strong&gt;
      Computational efficiency increases
     &lt;/strong&gt;
     &lt;span&gt;
      : Without caching, the attention at step
     &lt;/span&gt;
     &lt;em&gt;
      t
     &lt;/em&gt;
     &lt;span&gt;
      must compare the new query with
     &lt;/span&gt;
     &lt;em&gt;
      t
     &lt;/em&gt;
     &lt;span&gt;
      previous keys, so the cumulative work scales quadratically, O(n²). With a cache, each key and value is computed once and then reused, reducing the total per-step complexity to linear, O(n).
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      [Bad]
     &lt;/span&gt;
     &lt;strong&gt;
      Memory usage increases linearly
     &lt;/strong&gt;
     &lt;span&gt;
      : Each new token appends to the KV cache. For long sequences and larger LLMs, the cumulative KV cache grows larger, which can consume a significant or even prohibitive amount of (GPU) memory. As a workaround, we can truncate the KV cache, but this adds even more complexity (but again, it may well be worth it when deploying LLMs.)
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Optimizing the KV Cache Implementation
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   While my conceptual implementation of a KV cache above helps with clarity and is mainly geared towards code readability and educational purposes, deploying it in real-world scenarios (especially with larger models and longer sequence lengths) requires more careful optimization.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Common Pitfalls When Scaling the Cache
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;strong&gt;
      Memory fragmentation and repeated allocations
     &lt;/strong&gt;
     &lt;span&gt;
      : Continuously concatenating tensors via
     &lt;/span&gt;
     &lt;code&gt;
      torch.cat
     &lt;/code&gt;
     &lt;span&gt;
      , as shown earlier, leads to performance bottlenecks due to frequent memory allocation and reallocation.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;strong&gt;
      Linear growth in memory usage
     &lt;/strong&gt;
     &lt;span&gt;
      : Without proper handling, the KV cache size becomes impractical for very long sequences.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Tip 1: Pre-allocate Memory
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;p&gt;
   Rather than concatenating tensors repeatedly, we could pre-allocate a sufficiently large tensor based on the expected maximum sequence length. This ensures consistent memory use and reduces overhead. In pseudo-code, this may look like as follows:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;# Example pre-allocation for keys and values
max_seq_len = 1024  # maximum expected sequence length
cache_k = torch.zeros(
    (batch_size, num_heads, max_seq_len, head_dim), device=device
)
cache_v = torch.zeros(
    (batch_size, num_heads, max_seq_len, head_dim), device=device
)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   During inference, we can then simply write into slices of these pre-allocated tensors.
  &lt;/p&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Tip 2: Truncate Cache via Sliding Window
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;p&gt;
   &lt;span&gt;
    To avoid blowing up our GPU memory, we can implement a sliding window approach with dynamic truncation. Via the sliding window, we maintain only the last
   &lt;/span&gt;
   &lt;code&gt;
    window_size
   &lt;/code&gt;
   &lt;span&gt;
    tokens in the cache:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;# Sliding window cache implementation
window_size = 512
cache_k = cache_k[:, :, -window_size:, :]
cache_v = cache_v[:, :, -window_size:, :]&lt;/code&gt;&lt;/pre&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Optimizations in Practice
   &lt;/strong&gt;
  &lt;/h4&gt;
  &lt;p&gt;
   &lt;span&gt;
    You can find these optimizations in the
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch04/03_kv-cache/gpt_with_kv_cache_optimized.py&quot; rel=&quot;&quot;&gt;
    gpt_with_kv_cache_optimized.py
   &lt;/a&gt;
   &lt;span&gt;
    file.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   On a Mac Mini with an M4 chip (CPU), with a 200-token generation and a window size equal to the LLM&#x27;s context length (to guarantee the same results and thus a fair comparison) below, the code runtimes compare as follows:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!9fY3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!9fY3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png 424w, https://substackcdn.com/image/fetch/$s_!9fY3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png 848w, https://substackcdn.com/image/fetch/$s_!9fY3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png 1272w, https://substackcdn.com/image/fetch/$s_!9fY3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:197,&quot;width&quot;:742,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:17636,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;197&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!9fY3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!9fY3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png 424w, https://substackcdn.com/image/fetch/$s_!9fY3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png 848w, https://substackcdn.com/image/fetch/$s_!9fY3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png 1272w, https://substackcdn.com/image/fetch/$s_!9fY3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F14878ff2-28f2-496f-a5ad-03aa2865640c_742x197.png 1456w&quot; width=&quot;742&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Unfortunately, the speed advantages disappear on CUDA devices as this is a tiny model, and the device transfer and communication outweigh the benefits of a KV cache for this small model.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Conclusion
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Although caching introduces additional complexity and memory considerations, the noticeable gains in efficiency typically outweigh these trade-offs, especially in production environments.
  &lt;/p&gt;
  &lt;p&gt;
   Remember, while I prioritized code clarity and readability over efficiency here, the takeaway is that practical implementations often require thoughtful optimizations, such as pre-allocating memory or applying a sliding-window cache to manage memory growth effectively. In that sense, I hope this article turned out to be informative.
  &lt;/p&gt;
  &lt;p&gt;
   Feel free to experiment with these techniques, and happy coding!
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Bonus: KV Caches in Qwen3 and Llama 3
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    After adding KV caches to my from-scratch implementations of Qwen3 (0.6 B) and Llama 3 (1 B), I ran additional experiments comparing the model runtimes with and without KV cache. Note that I opted for the torch.cat approach mentioned above rather than pre-allocating the KV cache tensors  as described in the
   &lt;/span&gt;
   &lt;em&gt;
    Optimizing the KV Cache Implementation
   &lt;/em&gt;
   &lt;span&gt;
    section. Since Llama 3 and Qwen3 have very large supported context sizes (131k and 41k tokens, respectively), the pre-allocated tensors consume ~8 GB of additional memory, which is quite expensive.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Moreover, because I am using the more memory-efficient
   &lt;/span&gt;
   &lt;code&gt;
    torch.cat
   &lt;/code&gt;
   &lt;span&gt;
    approach to creating the tensors on the fly, I moved the KV cache outside the model to compile the model with
   &lt;/span&gt;
   &lt;code&gt;
    torch.compile
   &lt;/code&gt;
   &lt;span&gt;
    for a computational efficiency boost.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The codes can be found here:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/qwen3.py&quot; rel=&quot;&quot;&gt;
      qwen3.py
     &lt;/a&gt;
     &lt;span&gt;
      |
     &lt;/span&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/11_qwen3&quot; rel=&quot;&quot;&gt;
      README
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/pkg/llms_from_scratch/llama3.py&quot; rel=&quot;&quot;&gt;
      llama3.py
     &lt;/a&gt;
     &lt;span&gt;
      |
     &lt;/span&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama&quot; rel=&quot;&quot;&gt;
      README
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   And the performances are shown below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!-pE8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-pE8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png 424w, https://substackcdn.com/image/fetch/$s_!-pE8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png 848w, https://substackcdn.com/image/fetch/$s_!-pE8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png 1272w, https://substackcdn.com/image/fetch/$s_!-pE8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:616,&quot;width&quot;:931,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:122287,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;616&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!-pE8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-pE8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png 424w, https://substackcdn.com/image/fetch/$s_!-pE8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png 848w, https://substackcdn.com/image/fetch/$s_!-pE8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png 1272w, https://substackcdn.com/image/fetch/$s_!-pE8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe5d3a0bf-4a1a-439a-a83c-9046e1a57515_931x616.png 1456w&quot; width=&quot;931&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!O7SH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!O7SH!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png 424w, https://substackcdn.com/image/fetch/$s_!O7SH!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png 848w, https://substackcdn.com/image/fetch/$s_!O7SH!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png 1272w, https://substackcdn.com/image/fetch/$s_!O7SH!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:598,&quot;width&quot;:876,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:106128,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/166106178?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;598&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!O7SH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!O7SH!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png 424w, https://substackcdn.com/image/fetch/$s_!O7SH!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png 848w, https://substackcdn.com/image/fetch/$s_!O7SH!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png 1272w, https://substackcdn.com/image/fetch/$s_!O7SH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49ce3862-820f-4337-91b7-6b4603ec8f86_876x598.png 1456w&quot; width=&quot;876&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As we can see, on CPUs, the KV cache results in the most substantial speed-up. And compilation boosts that performance even further. However, on a GPU, the best performance can be achieved with the regular compiled model, which is likely because we don’t pre-allocate the tensors on the GPU, and the models are relatively small.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. To support me as an independent researcher, please consider purchasing a copy of my book,
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     , or signing up for a
    &lt;/span&gt;
    &lt;a href=&quot;https://magazine.sebastianraschka.com/subscribe&quot; rel=&quot;&quot;&gt;
     paid subscription
    &lt;/a&gt;
    &lt;span&gt;
     .
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Build a Large Language Model (From Scratch) now
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      available on Amazon
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Your support means a great deal! Thank you!
   &lt;/strong&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Coding LLMs from the Ground Up: A Complete Course </title>
<link>https://magazine.sebastianraschka.com/p/coding-llms-from-the-ground-up</link>
<pubDate>Sat, 10 May 2025 04:03:17 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   I wrote a lot about reasoning models in recent months (4 articles in a row)! Next to everything &quot;agentic,&quot; reasoning is one of the biggest LLM topics of 2025.
  &lt;/p&gt;
  &lt;p&gt;
   This month, however, I wanted to share more fundamental or &quot;foundational&quot; content with you on how to code LLMs, which is one of the best ways to understand how LLMs work.
  &lt;/p&gt;
  &lt;p&gt;
   Why? Many people really liked and benefited from the abbreviated LLM workshop I shared last year:
  &lt;/p&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;
   So, I thought this ~5× longer and more detailed content (~15 hours in total) would be even more useful.
  &lt;/p&gt;
  &lt;p&gt;
   Also, I&#x27;m sadly dealing with a bad neck injury and haven&#x27;t really been able to work on a computer for the past 3 weeks. I am currently trying a conservative treatment before considering the suggested surgical route. This is the worst timing as I just started to get back on track before life threw another curveball.
  &lt;/p&gt;
  &lt;p&gt;
   So, during my recovery, I thought sharing these videos I recorded in the last couple of months would be a nice in-between content.
  &lt;/p&gt;
  &lt;p&gt;
   I hope you find this useful, and thanks for your support!
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     PS: The videos originally started as supplementary content for my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . But it turns out they also work pretty well as standalone content.
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Why build from scratch?
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   It&#x27;s probably the best and most efficient way to learn how LLMs really work. Plus, many readers have told me they had a lot of fun doing it.
  &lt;/p&gt;
  &lt;p&gt;
   To offer an analogy: if you are into cars and want to understand how they work, following a tutorial that walks you through building one from the ground up is a great way to learn. Of course, we probably wouldn&#x27;t want to start by building a Formula 1 race car since it would be prohibitively expensive and overly complex for a first project. Instead, it makes more sense to start with something simpler, like a go-kart.
  &lt;/p&gt;
  &lt;p&gt;
   Building a go-kart still teaches you how the steering works, how the motor functions, and more. You can even take it to the track and practice (and have a lot of fun with it) before stepping into a professional race car (or joining a company or team that is focused on building one). After all, the best race drivers often got their start by building and tinkering with their own go-karts (think Michael Schumacher and Ayrton Senna). By doing that, they not only developed a great feel for the car but could also provide valuable feedback to their mechanics, which gave them an edge over the other drivers.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   References
  &lt;/h3&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      Build an LLM from Scratch book (
     &lt;/span&gt;
     &lt;a href=&quot;https://mng.bz/M96o&quot; rel=&quot;&quot;&gt;
      Manning
     &lt;/a&gt;
     &lt;span&gt;
      |
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      Amazon
     &lt;/a&gt;
     &lt;span&gt;
      )
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot; rel=&quot;&quot;&gt;
      Build an LLM from Scratch GitHub repository
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   1 - Set up your code environment (0:21:01)
  &lt;/h4&gt;
  &lt;p&gt;
   This is a supplementary video explaining how to set up a Python environment using uv.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In particular, we are using “
   &lt;/span&gt;
   &lt;code&gt;
    uv pip”
   &lt;/code&gt;
   &lt;span&gt;
    , which is explained in
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/setup/01_optional-python-setup-preferences/README.md&quot; rel=&quot;&quot;&gt;
    this document
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Alternatively, the native “
   &lt;/span&gt;
   &lt;code&gt;
    uv add”
   &lt;/code&gt;
   &lt;span&gt;
    syntax (mentioned but not explicitly covered in this video) is described
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/setup/01_optional-python-setup-preferences/native-uv.md&quot; rel=&quot;&quot;&gt;
    here
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;yAcWnfsZhzo&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2-yAcWnfsZhzo&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Note / Tip:
   &lt;/strong&gt;
   &lt;span&gt;
    The installation may cause issues on certain versions of Windows. If you are on a Windows machine and have troubles with the installation (likely due to a TensorFlow dependency to load the original GPT-2 model weights from OpenAI in video 5), please don’t worry about it and feel free to skip the TensorFlow installation (you can do this by removing the TensorFlow line from the requirements file.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    To provide an alternative, I converted the GPT-2 model weights from a TensorFlow tensor format to PyTorch tensors and shared them on the Hugging Face model hub, which you can use as an alternative to the weight loading portion in video 5:
   &lt;/span&gt;
   &lt;a href=&quot;https://huggingface.co/rasbt/gpt2-from-scratch-pytorch&quot; rel=&quot;&quot;&gt;
    https://huggingface.co/rasbt/gpt2-from-scratch-pytorch
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In any case, you don’t have to worry about this weight-loading code until the end of video 5.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   2 - Working with text data (1:28:01)
  &lt;/h4&gt;
  &lt;p&gt;
   This video goes over text data preparations steps (tokenization, byte pair encoding, data loaders, etc.) for LLM training.
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;341Rb8fJxY0&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2-341Rb8fJxY0&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
    &lt;iframe allow=&quot;autoplay; fullscreen&quot; allowautoplay=&quot;true&quot; allowfullscreen=&quot;true&quot; frameborder=&quot;0&quot; gesture=&quot;media&quot; height=&quot;409&quot; loading=&quot;lazy&quot; src=&quot;https://www.youtube-nocookie.com/embed/341Rb8fJxY0?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0&quot; width=&quot;728&quot;&gt;
    &lt;/iframe&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   3 - Coding attention mechanisms (2:15:40)
  &lt;/h4&gt;
  &lt;p&gt;
   &lt;span&gt;
    This is a supplementary video explaining how attention mechanisms (self-attention, causal attention, multi-head attention) work by coding them from scratch.
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;span&gt;
    You can think of it as building the engine of a car (before adding the frame, seats, and wheels).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;-Ll8DtpNtvk&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2--Ll8DtpNtvk&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
    &lt;iframe allow=&quot;autoplay; fullscreen&quot; allowautoplay=&quot;true&quot; allowfullscreen=&quot;true&quot; frameborder=&quot;0&quot; gesture=&quot;media&quot; height=&quot;409&quot; loading=&quot;lazy&quot; src=&quot;https://www.youtube-nocookie.com/embed/-Ll8DtpNtvk?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0&quot; width=&quot;728&quot;&gt;
    &lt;/iframe&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   4 - Set up your code environment (0:21:01)
  &lt;/h4&gt;
  &lt;p&gt;
   This video covers how to code an LLM architecture from scratch.
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;YSAkgEarBGE&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2-YSAkgEarBGE&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
    &lt;iframe allow=&quot;autoplay; fullscreen&quot; allowautoplay=&quot;true&quot; allowfullscreen=&quot;true&quot; frameborder=&quot;0&quot; gesture=&quot;media&quot; height=&quot;409&quot; loading=&quot;lazy&quot; src=&quot;https://www.youtube-nocookie.com/embed/YSAkgEarBGE?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0&quot; width=&quot;728&quot;&gt;
    &lt;/iframe&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   5 - Pretraining on Unlabeled Data (2:36:44)
  &lt;/h4&gt;
  &lt;p&gt;
   This video explains how to pretrain a LLM from scratch.
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;Zar2TJv-sE0&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2-Zar2TJv-sE0&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
    &lt;iframe allow=&quot;autoplay; fullscreen&quot; allowautoplay=&quot;true&quot; allowfullscreen=&quot;true&quot; frameborder=&quot;0&quot; gesture=&quot;media&quot; height=&quot;409&quot; loading=&quot;lazy&quot; src=&quot;https://www.youtube-nocookie.com/embed/Zar2TJv-sE0?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0&quot; width=&quot;728&quot;&gt;
    &lt;/iframe&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   6 - Finetuning for Classification (2:15:29)
  &lt;/h4&gt;
  &lt;p&gt;
   This is a video explaining how to fine-tune an LLM as a classifier (here using a spam classification example) as a gentle introduction to finetuning, before instruction finetuning the LLM in the next video.
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;5PFXJYme4ik&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2-5PFXJYme4ik&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
    &lt;iframe allow=&quot;autoplay; fullscreen&quot; allowautoplay=&quot;true&quot; allowfullscreen=&quot;true&quot; frameborder=&quot;0&quot; gesture=&quot;media&quot; height=&quot;409&quot; loading=&quot;lazy&quot; src=&quot;https://www.youtube-nocookie.com/embed/5PFXJYme4ik?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0&quot; width=&quot;728&quot;&gt;
    &lt;/iframe&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;h4 class=&quot;header-anchor-post&quot;&gt;
   7 - Instruction Finetuning (1:46:04)
  &lt;/h4&gt;
  &lt;p&gt;
   Finally, this video explains how to instruction finetune the LLM.
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;4yNswvhPWCQ&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2-4yNswvhPWCQ&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
    &lt;iframe allow=&quot;autoplay; fullscreen&quot; allowautoplay=&quot;true&quot; allowfullscreen=&quot;true&quot; frameborder=&quot;0&quot; gesture=&quot;media&quot; height=&quot;409&quot; loading=&quot;lazy&quot; src=&quot;https://www.youtube-nocookie.com/embed/4yNswvhPWCQ?rel=0&amp;autoplay=0&amp;showinfo=0&amp;enablejsapi=0&quot; width=&quot;728&quot;&gt;
    &lt;/iframe&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Happy viewing &amp; tinkering!
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
  &lt;/h2&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   Bonus: LLMs Then And Now (From 2018 to 2025)
  &lt;/h2&gt;
  &lt;p&gt;
   As a big thank you to the paid subscribers, I want to share a 2.5h (non-coding) bonus video I recorded earlier in April, approximately 2 days after the Llama 4 release. In this talk, I discuss the current LLM landscape in 2025 with a focus on what and how things have changed since GPT-2 in 2018.
  &lt;/p&gt;
  &lt;p&gt;
   Thanks for your support, as an independent and self-employed researcher, this really means a lot to me!
  &lt;/p&gt;
  &lt;p&gt;
   Hopefully, things will improve in the next few weeks/months as I have lots of ideas for upcoming articles and can’t wait to work on them!
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The State of Reinforcement Learning for LLM Reasoning </title>
<link>https://magazine.sebastianraschka.com/p/the-state-of-llm-reasoning-model-training</link>
<pubDate>Sat, 19 Apr 2025 04:02:44 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   A lot has happened this month, especially with the releases of new flagship models like GPT-4.5 and Llama 4. But you might have noticed that reactions to these releases were relatively muted. Why? One reason could be that GPT-4.5 and Llama 4 remain conventional models, which means they were trained without explicit reinforcement learning for reasoning.
  &lt;/p&gt;
  &lt;p&gt;
   Meanwhile, competitors such as xAI and Anthropic have added more reasoning capabilities and features into their models. For instance, both the xAI Grok and Anthropic Claude interfaces now include a &quot;thinking&quot; (or &quot;extended thinking&quot;) button for certain models that explicitly toggles reasoning capabilities.
  &lt;/p&gt;
  &lt;p&gt;
   In any case, the muted response to GPT-4.5 and Llama 4 (non-reasoning) models suggests we are approaching the limits of what scaling model size and data alone can achieve.
  &lt;/p&gt;
  &lt;p&gt;
   However, OpenAI&#x27;s recent release of the o3 reasoning model demonstrates there is still considerable room for improvement when investing compute strategically, specifically via reinforcement learning methods tailored for reasoning tasks. (According to OpenAI staff during the recent livestream, o3 used 10× more training compute compared to o1.)
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!DWHh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!DWHh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 424w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 848w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 1272w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:908,&quot;width&quot;:1456,&quot;resizeWidth&quot;:693,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;432.1730769230769&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!DWHh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!DWHh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 424w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 848w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 1272w, https://substackcdn.com/image/fetch/$s_!DWHh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb06be25e-c103-4554-ba16-6e84ed6a5f1c_1600x998.png 1456w&quot; width=&quot;693&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Source: OpenAI livestream (https://openai.com/live/) on April 16, 2025
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   While reasoning alone isn&#x27;t a silver bullet, it reliably improves model accuracy and problem-solving capabilities on challenging tasks (so far). And I expect reasoning-focused post-training to become standard practice in future LLM pipelines.
  &lt;/p&gt;
  &lt;p&gt;
   So, in this article, let&#x27;s explore the latest developments in reasoning via reinforcement learning.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!2SLQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!2SLQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 424w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 848w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 1272w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1016,&quot;width&quot;:1456,&quot;resizeWidth&quot;:713,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;497.532967032967&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!2SLQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 424w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 848w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 1272w, https://substackcdn.com/image/fetch/$s_!2SLQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F739d9d44-4d24-4d82-bbe8-a68ae5bedd2c_1600x1116.png 1456w&quot; width=&quot;713&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      This article focuses on reinforcement learning training methods used to develop and improve reasoning models
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Because it is a relatively long article, I am providing a Table of Contents overview below. To navigate the table of contents, please use the slider on the left-hand side in the web view.
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Understanding reasoning models
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     RLHF basics: where it all started
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     A brief introduction to PPO: RL&#x27;s workhorse algorithm
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     RL algorithms: from PPO to GRPO
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     RL reward modeling: from RLHF to RLVR
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     How the DeepSeek-R1 reasoning models were trained
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Lessons from recent RL papers on training reasoning models
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Noteworthy research papers on training reasoning models
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Tip:
   &lt;/strong&gt;
   &lt;span&gt;
    If you are already familiar with reasoning basics, RL, PPO, and GRPO, please feel free to directly jump ahead to the “Lessons from recent RL papers on training reasoning models” section, which contains summaries of interesting insights from recent reasoning research papers.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Understanding reasoning models
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The big elephant in the room is, of course, the definition of reasoning. In short, reasoning is about inference and training techniques that make LLMs better at handling complex tasks.
  &lt;/p&gt;
  &lt;p&gt;
   To provide a bit more detail on how this is achieved (so far), I&#x27;d like to define reasoning as follows:
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    Reasoning, in the context of LLMs, refers to the model&#x27;s ability to produce intermediate steps before providing a final answer. This is a process that is often described as chain-of-thought (CoT) reasoning. In CoT reasoning, the LLM explicitly generates a structured sequence of statements or computations that illustrate how it arrives at its conclusion.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   And below is a figure along with the definition.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!mB5l!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!mB5l!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png 424w, https://substackcdn.com/image/fetch/$s_!mB5l!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png 848w, https://substackcdn.com/image/fetch/$s_!mB5l!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png 1272w, https://substackcdn.com/image/fetch/$s_!mB5l!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:858,&quot;width&quot;:1164,&quot;resizeWidth&quot;:595,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;438.58247422680415&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!mB5l!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!mB5l!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png 424w, https://substackcdn.com/image/fetch/$s_!mB5l!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png 848w, https://substackcdn.com/image/fetch/$s_!mB5l!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png 1272w, https://substackcdn.com/image/fetch/$s_!mB5l!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d4e5157-5a68-4a37-807d-9d6a2f933084_1164x858.png 1456w&quot; width=&quot;595&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      A simplified illustration of how an LLM might tackle a multi-step reasoning task. Rather than just recalling a fact, the model needs to combine several intermediate reasoning steps to arrive at the correct conclusion. The intermediate reasoning steps may or may not be shown to the user, depending on the implementation.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   If you are new to reasoning models and would like a more comprehensive introduction, I recommend my previous articles:
  &lt;/p&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/first-look-at-reasoning-from-scratch&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-reasoning-llms&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Now, as hinted at the beginning of this section, the reasoning abilities of LLMs can be improved in two ways, as nicely illustrated in a figure from an OpenAI blog post:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Vxu-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Vxu-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png 424w, https://substackcdn.com/image/fetch/$s_!Vxu-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png 848w, https://substackcdn.com/image/fetch/$s_!Vxu-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png 1272w, https://substackcdn.com/image/fetch/$s_!Vxu-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:974,&quot;width&quot;:1456,&quot;resizeWidth&quot;:702,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;469.60714285714283&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Vxu-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Vxu-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png 424w, https://substackcdn.com/image/fetch/$s_!Vxu-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png 848w, https://substackcdn.com/image/fetch/$s_!Vxu-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png 1272w, https://substackcdn.com/image/fetch/$s_!Vxu-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb229cbf-7e4c-4633-ae1f-50266d791abc_1600x1070.png 1456w&quot; width=&quot;702&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Accuracy improvements can be achieved through increased training or test-time compute, where test-time compute is synonymous with inference-time compute and inference-time scaling. Source: Annotated figure from https://openai.com/index/learning-to-reason-with-llms/
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In my previous article:
  &lt;/p&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    I solely focused on the test-time compute methods.
   &lt;/span&gt;
   &lt;strong&gt;
    In this article, I finally want to take a closer look at the training methods.
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    RLHF basics: where it all started
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The reinforcement learning (RL) training methods used to build and improve reasoning models are more or less related to the reinforcement learning with human feedback (RLHF) methodology that is used to develop and align conventional LLMs. So, I want to start with a small recap of how RLHF works before discussing reasoning-specific modification based on RL-based training.
  &lt;/p&gt;
  &lt;p&gt;
   Conventional LLMs typically undergo a 3-step training procedure:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Pre-training
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Supervised fine-tuning
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Alignment (typically via RLHF)
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   The &quot;original&quot; LLM alignment method is RLHF, which is part of the standard repertoire when developing LLMs following the InstructGPT paper, which described the recipe that was used to develop the first ChatGPT model.
  &lt;/p&gt;
  &lt;p&gt;
   The original goal of RLHF is to align LLMs with human preferences. For instance, suppose you use an LLM multiple times where the LLM generates multiple answers for a given prompt. RLHF guides the LLM towards generating more of the style of answer that you prefer. (Often, RLHF is also used to safety-tune LLMs: to avoid sharing sensitive information, using swear words, and so on.)
  &lt;/p&gt;
  &lt;p&gt;
   If you are new to RLHF, here is an excerpt from a talk I gave a few years ago that explains RLHF in less than 5 minutes:
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;vJ4SsfmeQlk&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2-vJ4SsfmeQlk&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Alternatively, the paragraphs below describe RLHF in text form.
  &lt;/p&gt;
  &lt;p&gt;
   The RLHF pipeline takes a pre-trained model and fine-tunes it in a supervised fashion. This fine-tuning is not the RL part yet but is mainly a prerequisite.
  &lt;/p&gt;
  &lt;p&gt;
   Then, RLHF further aligns the LLM using an algorithm called proximal policy optimization (PPO). (Note that there are other algorithms that can be used instead of PPO; I was specifically saying PPO because that&#x27;s what was originally used in RLHF and is still the most popular one today.)
  &lt;/p&gt;
  &lt;p&gt;
   For simplicity, we will look at the RLHF pipeline in three separate steps:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     RLHF Step 1 (prerequisite): Supervised fine-tuning (SFT) of the pre-trained model
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     RLHF Step 2: Creating a reward model
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     RLHF Step 3: Fine-tuning via proximal policy optimization (PPO)
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   RLHF Step 1, shown below, is a supervised fine-tuning step to create the base model for further RLHF fine-tuning.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!MiKW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!MiKW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png 424w, https://substackcdn.com/image/fetch/$s_!MiKW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png 848w, https://substackcdn.com/image/fetch/$s_!MiKW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png 1272w, https://substackcdn.com/image/fetch/$s_!MiKW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:674,&quot;width&quot;:1456,&quot;resizeWidth&quot;:658,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;304.59615384615387&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!MiKW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!MiKW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png 424w, https://substackcdn.com/image/fetch/$s_!MiKW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png 848w, https://substackcdn.com/image/fetch/$s_!MiKW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png 1272w, https://substackcdn.com/image/fetch/$s_!MiKW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6ef3ddcf-1a49-45a2-950e-49ebf56e80f4_1600x741.png 1456w&quot; width=&quot;658&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Annotated figure from InstructGPT paper, https://arxiv.org/abs/2203.02155
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In RLHF step 1, we create or sample prompts (from a database, for example) and ask humans to write good-quality responses. We then use this dataset to fine-tune the pre-trained base model in a supervised fashion. As mentioned before, this is not technically part of RL training but merely a prerequisite.
  &lt;/p&gt;
  &lt;p&gt;
   In RLHF Step 2, we then use this model from supervised fine-tuning (SFT) to create a reward model, as shown below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!At37!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!At37!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png 424w, https://substackcdn.com/image/fetch/$s_!At37!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png 848w, https://substackcdn.com/image/fetch/$s_!At37!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png 1272w, https://substackcdn.com/image/fetch/$s_!At37!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:764,&quot;width&quot;:1456,&quot;resizeWidth&quot;:658,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;345.2692307692308&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!At37!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!At37!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png 424w, https://substackcdn.com/image/fetch/$s_!At37!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png 848w, https://substackcdn.com/image/fetch/$s_!At37!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png 1272w, https://substackcdn.com/image/fetch/$s_!At37!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4aca23a9-df94-4054-a8ef-5d109d8daa47_1600x840.png 1456w&quot; width=&quot;658&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Annotated figure from InstructGPT paper, https://arxiv.org/abs/2203.02155
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As depicted in the figure above, for each prompt, we generate four responses from the fine-tuned LLM created in the prior step. Human annotators then rank these responses based on their preferences. Although this ranking process is time-consuming, it might be somewhat less labor-intensive than creating the dataset for supervised fine-tuning. This is because ranking responses is likely simpler than writing them.
  &lt;/p&gt;
  &lt;p&gt;
   Upon compiling a dataset with these rankings, we can design a reward model that outputs a reward score for the optimization subsequent stage in RLHF Step 3. The idea here is that the reward model replaces and automates the labor-intensive human ranking to make the training feasible on large datasets.
  &lt;/p&gt;
  &lt;p&gt;
   This reward model (RM) generally originates from the LLM created in the prior supervised fine-tuning (SFT) step. To turn the model from RLHF Step 1 into a reward model, its output layer (the next-token classification layer) is substituted with a regression layer, which features a single output node.
  &lt;/p&gt;
  &lt;p&gt;
   The third step in the RLHF pipeline is to use the reward model (RM) to fine-tune the previous model from supervised fine-tuning (SFT), which is illustrated in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Ak0r!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Ak0r!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png 424w, https://substackcdn.com/image/fetch/$s_!Ak0r!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png 848w, https://substackcdn.com/image/fetch/$s_!Ak0r!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png 1272w, https://substackcdn.com/image/fetch/$s_!Ak0r!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:754,&quot;width&quot;:1456,&quot;resizeWidth&quot;:643,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;332.98214285714283&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Ak0r!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Ak0r!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png 424w, https://substackcdn.com/image/fetch/$s_!Ak0r!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png 848w, https://substackcdn.com/image/fetch/$s_!Ak0r!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png 1272w, https://substackcdn.com/image/fetch/$s_!Ak0r!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc374a894-975e-4c42-9bc8-a8ac1fe06af8_1600x829.png 1456w&quot; width=&quot;643&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Annotated figure from InstructGPT paper, https://arxiv.org/abs/2203.02155
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In RLHF Step 3, the final stage, we are now updating the SFT model using proximal policy optimization (PPO) based on the reward scores from the reward model we created in RLHF Step 2.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    A brief introduction to PPO: RL&#x27;s workhorse algorithm
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   As mentioned earlier, the original RLHF method uses a reinforcement learning algorithm called proximal policy optimization (PPO).
  &lt;/p&gt;
  &lt;p&gt;
   PPO was developed to improve the stability and efficiency of training a policy. (In reinforcement learning, “policy” just means the model we want to train; in this case, policy = LLM.)
  &lt;/p&gt;
  &lt;p&gt;
   One of the key ideas behind PPO is that it limits how much the policy is allowed to change during each update step. This is done using a clipped loss function, which helps prevent the model from making overly large updates that could destabilize training.
  &lt;/p&gt;
  &lt;p&gt;
   On top of that, PPO also includes a KL divergence penalty in the loss. This term compares the current policy (the model being trained) to the original SFT model. This encourages the updates to stay reasonably close. The idea is to preference-tune the model, not to completely re-train, after all.
  &lt;/p&gt;
  &lt;p&gt;
   This is where the “proximal” in proximal policy optimization comes from: the algorithm tries to keep the updates close to the existing model while still allowing for improvement. And to encourage a bit of exploration, PPO also adds an entropy bonus, which this encourages the model to vary the outputs during training.
  &lt;/p&gt;
  &lt;p&gt;
   In the following paragraphs, I want to introduce some more terminology to illustrate PPO on a relatively high level. Still, there&#x27;s a lot of jargon involved, so I tried to summarize the key terminology in the figure below before we continue.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!pmzH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!pmzH!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png 424w, https://substackcdn.com/image/fetch/$s_!pmzH!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png 848w, https://substackcdn.com/image/fetch/$s_!pmzH!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png 1272w, https://substackcdn.com/image/fetch/$s_!pmzH!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1259,&quot;width&quot;:1456,&quot;resizeWidth&quot;:669,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;578.4828296703297&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!pmzH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!pmzH!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png 424w, https://substackcdn.com/image/fetch/$s_!pmzH!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png 848w, https://substackcdn.com/image/fetch/$s_!pmzH!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png 1272w, https://substackcdn.com/image/fetch/$s_!pmzH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9c02ecf0-cb1d-4f62-a160-6d07636b99fd_1600x1384.png 1456w&quot; width=&quot;669&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Illustration of the key terms in RLHF. For instance, several models are involved in PPO, where PPO is an algorithm used in RLHF (and RLHF is one of the most popular LLM alignment methods).
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Below, I aim to illustrate the key steps in PPO via pseudo-code.
  &lt;/p&gt;
  &lt;p&gt;
   In addition, to make it more intuitive, I will also use an analogy: Imagine you are a chef running a small food delivery service. And you are constantly trying out new recipe variations to improve customer satisfaction. Your overall goal is to tweak your recipe (policy) based on customer feedback (reward).
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    1. Compute the ratio of the next-token probabilities from the new vs the old policy:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;ratio = new_policy_prob / old_policy_prob&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   In short, this checks how different our new recipe is from the old one.
  &lt;/p&gt;
  &lt;p&gt;
   Side note: Regarding &quot;new_policy_prob&quot;, we are not using the final updated policy yet. We are using the current version of the policy (i.e., the model we are in the middle of training). However, it&#x27;s a convention to call it &quot;new&quot;. So, even though you&#x27;re still experimenting, we call your current draft the &quot;new policy&quot; as per convention.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    2. Multiply that ratio by how good the action was (called the advantage):
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;raw_score = ratio * advantage&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Here, for simplicity, we may assume the advantage is computed based on the reward signal:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;advantage = actual_reward - expected_reward&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   In the chef analogy, we can think of the advantage as how well the new dish performed:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;advantage = customer_rating - expected_rating&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   For example, if a customer rates the new dish with a 9/10, and the customers normally give us a 7/10, that&#x27;s a +2 advantage.
  &lt;/p&gt;
  &lt;p&gt;
   Note that this is a simplification. In reality, this involves generalized advantage estimation (GAE), which I am omitting here so as not to bloat the article further. However, one important detail to mention is that the expected reward is computed by a so-called &quot;critic&quot; (sometimes also called &quot;value model&quot;), and a reward model computes the actual reward. I.e., the advantage computation involves 2 other models, typically the same size as the original model we are fine-tuning.
  &lt;/p&gt;
  &lt;p&gt;
   In the analogy, we can think of this critic or value model as a friend we ask to try our new dish before serving it to the customers. We also ask our friend to estimate how a customer would rank it (that&#x27;s the expected reward). The reward model is the actual customer then who gives the feedback (i.e., the actual reward).
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    3. Compute a clipped score:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   If the new policy changes too much (e.g., ratio &amp;gt; 1.2 or &amp;lt; 0.8), we clip the ratio, as follows:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;clipped_ratio = clamp(ratio, 0.8, 1.2)
clipped_score = clipped_ratio * advantage&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   In the analogy, imagine that the new recipe got an exceptionally great (or bad) review. We might be tempted to overhaul the entire menu now. But that&#x27;s risky. So, instead, we clip how much our recipe can change for now. (For instance, maybe we made the dish much spicier, and that one customer happened to love spicy food, but that doesn&#x27;t mean everyone else will.)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    4. Then we use the smaller of the raw score and clipped scor
   &lt;/strong&gt;
   &lt;span&gt;
    e:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;if advantage &amp;gt;= 0:
    final_score = min(raw_score, clipped_score)
else:
    final_score = max(raw_score, clipped_score)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Again, this is related to being a bit cautious. For instance, if the advantage is positive (the new behavior is better), we cap the reward. That&#x27;s because we don&#x27;t want to over-trust a good result that might be a coincidence or luck.
  &lt;/p&gt;
  &lt;p&gt;
   If the advantage is negative (the new behavior is worse), we limit the penalty. The idea here is similar. Namely, we don&#x27;t want to overreact to one bad result unless we are really sure.
  &lt;/p&gt;
  &lt;p&gt;
   In short, we use the smaller of the two scores if the advantage is positive (to avoid over-rewarding), and the larger when the advantage is negative (to avoid over-penalizing).
  &lt;/p&gt;
  &lt;p&gt;
   In the analogy, this ensures that if a recipe is doing better than expected, we don&#x27;t over-reward it unless we are confident. And if it&#x27;s underperforming, we don&#x27;t over-penalize it unless it&#x27;s consistently bad.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    5. Calculating the loss:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This final score is what we maximize during training (using gradient descent after flipping the sign of the score to minimize). In addition, we also add a KL penalty term, where β is a hyperparameter for the penalty strength:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;loss = -final_score + β * KL(new_policy || reference_policy)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   In the analogy, we add the penalty to ensure new recipes are not too different from our original style. This prevents you from &quot;reinventing the kitchen&quot; every week. For example, we don&#x27;t want to turn an Italian restaurant into a BBQ place all of a sudden.
  &lt;/p&gt;
  &lt;p&gt;
   This was a lot of information, so I summarized it with a concrete, numeric example in an LLM context via the figure below. But please feel free to skip it if it&#x27;s too complicated; you should be able to follow the rest of the article just fine.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!jibb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!jibb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png 424w, https://substackcdn.com/image/fetch/$s_!jibb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png 848w, https://substackcdn.com/image/fetch/$s_!jibb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png 1272w, https://substackcdn.com/image/fetch/$s_!jibb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1600,&quot;width&quot;:981,&quot;resizeWidth&quot;:668,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1089.500509683996&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!jibb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!jibb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png 424w, https://substackcdn.com/image/fetch/$s_!jibb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png 848w, https://substackcdn.com/image/fetch/$s_!jibb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png 1272w, https://substackcdn.com/image/fetch/$s_!jibb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fec08dc6e-301b-44a3-a6de-a2ae0e5cfaad_981x1600.png 1456w&quot; width=&quot;668&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   I admit that I may have gone overboard with the PPO walkthrough. But once I had written it, it was hard to delete it. I hope some of you will find it useful!
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    That being said,
   &lt;/span&gt;
   &lt;strong&gt;
    the main takeaways that will be relevant in the next section are that there are multiple models involved in PPO:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    1. The policy, which is the LLM that has been trained with SFT and that we want to further align).
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    2. The reward model, which is a model that has been trained to predict the reward (see RLHF step 2).
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    3. The critic, which is a trainable model that estimates the reward.
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    4. A reference model (original policy) that we use to make sure that the policy doesn&#x27;t deviate too much.
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   By the way, you might wonder why we need both a reward model and a critic model. The reward model is usually trained before training the policy with PPO. It&#x27;s to automate the preference labeling by human judges, and it gives the score for the complete responses generated by the policy LLM.
  &lt;/p&gt;
  &lt;p&gt;
   The critic, in contrast, judges partial responses. We use it to create the final response. While the reward model typically remains frozen, the critic model is updated during training to estimate the reward created by the reward model better.
  &lt;/p&gt;
  &lt;p&gt;
   More details about PPO are out of the scope of this article, but interested readers can find the mathematical details in these four papers that predate the InstructGPT paper:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (1)
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1602.01783&quot; rel=&quot;&quot;&gt;
    Asynchronous Methods for Deep Reinforcement Learning
   &lt;/a&gt;
   &lt;span&gt;
    (2016) by Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, and Kavukcuoglu introduces policy gradient methods as an alternative to Q-learning in deep learning-based RL.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (2)
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1707.06347&quot; rel=&quot;&quot;&gt;
    Proximal Policy Optimization Algorithms
   &lt;/a&gt;
   &lt;span&gt;
    (2017) by Schulman, Wolski, Dhariwal, Radford, and Klimov presents a modified proximal policy-based reinforcement learning procedure that is more data-efficient and scalable than the vanilla policy optimization algorithm above.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (3)
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1909.08593&quot; rel=&quot;&quot;&gt;
    Fine-Tuning Language Models from Human Preferences
   &lt;/a&gt;
   &lt;span&gt;
    (2020) by Ziegler, Stiennon, Wu, Brown, Radford, Amodei, Christiano, Irving illustrates the concept of PPO and reward learning to pretrained language models including KL regularization to prevent the policy from diverging too far from natural language.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (4)
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2009.01325&quot; rel=&quot;&quot;&gt;
    Learning to Summarize from Human Feedback
   &lt;/a&gt;
   &lt;span&gt;
    (2022) by Stiennon, Ouyang, Wu, Ziegler, Lowe, Voss, Radford, Amodei, Christiano introduces the popular RLHF three-step procedure that was later also used in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot; rel=&quot;&quot;&gt;
    InstructGPT paper
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    RL algorithms: from PPO to GRPO
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   As mentioned before, PPO was the original algorithm used in RLHF. From a technical standpoint, it works perfectly fine in the RL pipeline that&#x27;s being used to develop reasoning models. However, what DeepSeek-R1 used for their RL pipeline is an algorithm called Group Relative Policy Optimization (GRPO), which was introduced in one of their earlier papers:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.03300&quot; rel=&quot;&quot;&gt;
      DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
     &lt;/a&gt;
     &lt;span&gt;
      (2024)
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   The DeepSeek team introduced GRPO as
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    a variant of Proximal Policy Optimization (PPO) that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   &lt;span&gt;
    So,
   &lt;/span&gt;
   &lt;strong&gt;
    the key motivation here is to improve computational efficiency.
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The efficiency improvements are achieved by dropping the &quot;critic&quot; (value model), i.e., the LLM that computes the value function (i.e., the expected future reward).
  &lt;/p&gt;
  &lt;p&gt;
   Instead of relying on this additional model to compute the estimated reward to compute the advantages, GRPO takes a simpler approach: it samples multiple answers from the policy model itself and uses their relative quality to compute the advantages.
  &lt;/p&gt;
  &lt;p&gt;
   To illustrate the differences between PPO and GRPO, I borrowed a nice figure from the DeepSeekMath paper:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!z_Sr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!z_Sr!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png 424w, https://substackcdn.com/image/fetch/$s_!z_Sr!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png 848w, https://substackcdn.com/image/fetch/$s_!z_Sr!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png 1272w, https://substackcdn.com/image/fetch/$s_!z_Sr!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/aae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:794,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;794&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!z_Sr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!z_Sr!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png 424w, https://substackcdn.com/image/fetch/$s_!z_Sr!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png 848w, https://substackcdn.com/image/fetch/$s_!z_Sr!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png 1272w, https://substackcdn.com/image/fetch/$s_!z_Sr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faae68a20-4c1b-44a2-af8b-693e8efb21e4_1600x872.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Annotated figure from DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models (https://arxiv.org/abs/2402.03300) to illustrate the differences between PPO and GRPO.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    RL reward modeling: from RLHF to RLVR
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   So far, we looked at RLHF as a procedure, and we have introduced two reinforcement learning algorithms commonly used for it: PPO and GRPO.
  &lt;/p&gt;
  &lt;p&gt;
   But if RLHF is already a core part of the LLM alignment toolkit, what does any of this have to do with reasoning?
  &lt;/p&gt;
  &lt;p&gt;
   The connection between RLHF and reasoning comes from how the DeepSeek team applied a similar RL-based approach (with GRPO) to train the reasoning capabilities of their R1 and R1-Zero models.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The difference is that instead of relying on
   &lt;/span&gt;
   &lt;em&gt;
    human preferences and training a reward mode
   &lt;/em&gt;
   &lt;span&gt;
    l, the DeepSeek-R1 team used
   &lt;/span&gt;
   &lt;em&gt;
    verifiable rewards
   &lt;/em&gt;
   &lt;span&gt;
    . This approach is called reinforcement learning with verifiable rewards (RLVR).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Again, it&#x27;s worth emphasizing: In contrast to standard RLHF, RLVR bypasses the need for a reward model.
  &lt;/p&gt;
  &lt;p&gt;
   So, rather than learning what counts as a &quot;good&quot; answer from human-labeled examples, the model gets direct binary feedback (correct or wrong) from a deterministic tool, such as symbolic verifiers or rule-based tools. Think calculators for math problems or compilers for code generation.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!9KP1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!9KP1!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png 424w, https://substackcdn.com/image/fetch/$s_!9KP1!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png 848w, https://substackcdn.com/image/fetch/$s_!9KP1!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png 1272w, https://substackcdn.com/image/fetch/$s_!9KP1!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:826,&quot;width&quot;:1456,&quot;resizeWidth&quot;:525,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;297.83653846153845&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!9KP1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!9KP1!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png 424w, https://substackcdn.com/image/fetch/$s_!9KP1!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png 848w, https://substackcdn.com/image/fetch/$s_!9KP1!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png 1272w, https://substackcdn.com/image/fetch/$s_!9KP1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffb315cfb-fdc6-4e3f-be74-7c8cb00ea09e_1586x900.png 1456w&quot; width=&quot;525&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Example of reinforcement learning with verifiable rewards (RLVR). The model is prompted to solve a math problem and produces an answer. Instead of using a learned reward model, a symbolic verifier (e.g., a calculator) checks the output and provides binary feedback based on correctness.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   One motivation here is to avoid noisy or expensive human or learned rewards by using automatic correctness checks as supervision signals during RL. The other motivation is that by using &quot;cheap&quot; tools like calculators, we can replace the expensive reward model training and the reward model itself. Since the reward model is usually the whole pre-trained model (but with a regression head), RLVR is much more efficient.
  &lt;/p&gt;
  &lt;p&gt;
   So, in short, DeepSeek-R1 used RLVR with GRPO, which eliminates two expensive models in the training procedure: the reward model and the value model (critic), as illustrated in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!g4uq!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!g4uq!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png 424w, https://substackcdn.com/image/fetch/$s_!g4uq!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png 848w, https://substackcdn.com/image/fetch/$s_!g4uq!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png 1272w, https://substackcdn.com/image/fetch/$s_!g4uq!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:828,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;828&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!g4uq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!g4uq!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png 424w, https://substackcdn.com/image/fetch/$s_!g4uq!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png 848w, https://substackcdn.com/image/fetch/$s_!g4uq!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png 1272w, https://substackcdn.com/image/fetch/$s_!g4uq!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F44fa9529-7e3a-44ca-b0f8-2401423bf7ab_1600x910.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Comparison of reinforcement learning setups in LLM training. Traditional RLHF with PPO uses both a reward model (trained on human preferences) and a critic (value model) to guide learning. GRPO eliminates the critic model. RLVR with GRPO goes a step further by also removing the reward model, relying instead on verifiable rewards from symbolic tools like calculators or compilers.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In the next section, I want to briefly go over the DeepSeek-R1 pipeline and discuss the different verifiable rewards that the DeepSeek team used.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    How the DeepSeek-R1 reasoning models were trained
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Now that we have clarified what RLHF and RLVR are, as well as PPO and GRPO, let&#x27;s briefly recap the main insights from the DeepSeek-R1 paper in the context of RL and reasoning.
  &lt;/p&gt;
  &lt;p&gt;
   First, there were three types of models:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     DeepSeek-R1-Zero trained with pure RL
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     DeepSeek-R1 trained with instruction fine-tuning (SFT) and RL
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     DeepSeek-Distill variants created via instruction fine-tuning SFT without RL
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   I created a DeepSeek-R1 pipeline diagram to illustrate how these models relate to each other, as shown below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!abwB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!abwB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png 424w, https://substackcdn.com/image/fetch/$s_!abwB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png 848w, https://substackcdn.com/image/fetch/$s_!abwB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png 1272w, https://substackcdn.com/image/fetch/$s_!abwB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1127,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1127&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!abwB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!abwB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png 424w, https://substackcdn.com/image/fetch/$s_!abwB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png 848w, https://substackcdn.com/image/fetch/$s_!abwB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png 1272w, https://substackcdn.com/image/fetch/$s_!abwB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4f17a2fc-0cbf-446d-bd27-fc8c6ac4117a_1600x1238.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Training pipeline for the DeepSeek-R1 family
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    DeepSeek-R1-Zero
   &lt;/strong&gt;
   &lt;span&gt;
    was trained using the verifiable rewards (RLVR) with GRPO, and this turned out to be sufficient for the model to exhibit reasoning abilities via intermediate-step generation. This showed that it&#x27;s possible to skip the SFT stage. The model improves its reasoning abilities through exploration instead of learning from examples.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    DeepSeek-R1
   &lt;/strong&gt;
   &lt;span&gt;
    is the flagship model, the one with the best performance. The difference compared to DeepSeek-R1-Zero is that they alternated instruction fine-tuning, RLVR, and RLHF.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    DeepSeek-Distill
   &lt;/strong&gt;
   &lt;span&gt;
    variants are meant to be small and more easily deployable models; they were generated by instruction fine-tuning Llama 3 and Qwen 2.5 models using instruction data from the DeepSeek-R1 model. This approach didn&#x27;t use any RL for the reasoning part (however, RLHF was used to create the Llama 3 and Qwen 2.5 base models).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   For more details on explaining the DeepSeek-R1 pipeline, please see my previous article &quot;Understanding Reasoning LLMs&quot;:
  &lt;/p&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-reasoning-llms&quot; rel=&quot;noopener&quot; target=&quot;_blank&quot;&gt;
   &lt;/a&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The main takeaway here is that the DeepSeek team didn&#x27;t use an LLM-based reward model to train DeepSeek-R1-Zero. Instead, they used rule-based rewards for the reasoning training of DeepSeek-R1-Zero and DeepSeek-R1:
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    We do not apply the outcome or process neural reward model in developing DeepSeek-R1-Zero, because we find that the neural reward model may suffer from reward hacking in the large-scale reinforcement learning process [...]
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    To train DeepSeek-R1-Zero, we adopt a rule-based reward system that mainly consists of two types of rewards:
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    (1) Accuracy rewards: The accuracy reward model evaluates whether the response is correct. For example, in the case of math problems with deterministic results, the model is required to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-based verification of correctness. Similarly, for LeetCode problems, a compiler can be used to generate feedback based on predefined test cases.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    (2) Format rewards: In addition to the accuracy reward model, we employ a format reward model that enforces the model to put its thinking process between &#x27;&amp;lt;think&amp;gt;&#x27; and &#x27;&amp;lt;/think&amp;gt;’ tags.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Lessons from recent RL papers on training reasoning models
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   I realize that the introduction (i.e., everything up to this point) turned out to be much longer than I expected. Nonetheless, I think that this lengthy introduction is perhaps necessary to put the following lessons into context.
  &lt;/p&gt;
  &lt;p&gt;
   After going through a large number of recent papers on reasoning models last month, I have put together a summary of the most interesting ideas and insights in this section. (References like “[1]” point to the corresponding papers listed at the end of the article.)
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1. Reinforcement learning further improves distilled models
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   The original DeepSeek-R1 paper demonstrated clearly that supervised fine-tuning (SFT) followed by reinforcement learning (RL) outperforms RL alone.
  &lt;/p&gt;
  &lt;p&gt;
   Given this observation, it&#x27;s intuitive that additional RL should further improve distilled models (as distilled models essentially represent models trained via SFT using reasoning examples generated by a larger model.)
  &lt;/p&gt;
  &lt;p&gt;
   Indeed, the DeepSeek team observed this phenomenon explicitly:
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   Several teams independently verified these observations:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     [8] Using the 1.5B DeepSeek-R1-Distill-Qwen model, researchers demonstrated substantial performance improvements from RL fine-tuning with just 7,000 examples and a modest $42 compute budget. Impressively, this small model surpassed OpenAI’s o1-preview on the AIME24 math benchmark.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     [15] However, another team cautioned that these gains might not always be statistically significant. This suggests that, although RL can improve smaller distilled models, the benchmark results might sometimes be overstating the improvements.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!oCP8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!oCP8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 424w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 848w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 1272w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:888,&quot;width&quot;:1288,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;888&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!oCP8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!oCP8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 424w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 848w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 1272w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 1456w&quot; width=&quot;1288&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility, https://arxiv.org/abs/2504.07086
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2. The problem of long incorrect answers
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   I previously mentioned that RL with verifiable rewards (RLVR) does not strictly require the GRPO algorithm; DeepSeek&#x27;s GRPO simply happens to be efficient and to perform well.
  &lt;/p&gt;
  &lt;p&gt;
   However, [12] showed that vanilla PPO paired with a basic binary correctness reward was sufficient to scale models in reasoning capability and response length.
  &lt;/p&gt;
  &lt;p&gt;
   More interestingly, both PPO and GRPO have a length bias. And several papers explored methods to tackle excessively long incorrect answers:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     [14] Provided an analysis illustrating how PPO inadvertently favors longer responses due to mathematical biases in loss calculations; GRPO may suffer from the same issue.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!bkp0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!bkp0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 424w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 848w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 1272w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:868,&quot;width&quot;:1118,&quot;resizeWidth&quot;:495,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;384.3112701252236&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!bkp0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!bkp0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 424w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 848w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 1272w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 1456w&quot; width=&quot;495&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Concise Reasoning via Reinforcement Learning, https://arxiv.org/abs/2504.05185
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     As a follow-up to the statement above, [7] [10] specifically identified length and difficulty-level biases in GRPO. The modified variant &quot;Dr. GRPO&quot; simplifies advantage calculations by removing length and standard deviation normalization, providing clearer training signals.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     [1] Explicitly penalized lengthy incorrect answers in GRPO while rewarding concise, correct ones.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     [3] [6] Didn’t directly control response length in GRPO but found token-level rewards beneficial, allowing models to better focus on critical reasoning steps.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     [5] Introduced explicit penalties in GRPO for responses exceeding specific lengths, enabling precise length control during inference.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3. Emergent abilities from RL
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   Beyond &quot;AHA&quot; moments mentioned in the DeepSeek-R1 paper, RL has been shown to induce valuable self-verification and reflective reasoning capabilities in models [2] [9]. Interestingly, similar to the AHA moment, these capabilities emerged naturally during training without explicit instruction.
  &lt;/p&gt;
  &lt;p&gt;
   [1] Showed that extending context lengths (up to 128k tokens) further improves the model&#x27;s self-reflection and self-correction capabilities.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4. Generalization beyond specific domains
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   Most research efforts so far has focused on reasoning tasks in math or coding contexts. However, [4] demonstrated successful generalization by training models on logic puzzles. And models trained on logic puzzles also achieved strong performance in mathematical reasoning tasks. This is evidence for RL&#x27;s ability to induce general reasoning behaviors independent of specific domain knowledge.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    5. Extensions to broader domains
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   As a follow-up to the section above, another interesting insight [11] is that reasoning capabilities can naturally extend beyond structured domains like math, code, and logic.
  &lt;/p&gt;
  &lt;p&gt;
   Models successfully applied reasoning to areas including medicine, chemistry, psychology, economics, and education, leveraging generative soft-scoring methods to effectively handle free-form answers.
  &lt;/p&gt;
  &lt;p&gt;
   Notable next steps for reasoning models include:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Integrating existing reasoning models (e.g., o1, DeepSeek-R1) with capabilities such as external tool use and retrieval-augmented generation (RAG); the just-released o3 model from Open AI paves the way here
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Speaking of tool-use and search, [9] showed that giving reasoning models the ability to search induces behaviors such as self-correction and robust generalization across benchmarks, despite minimal training datasets.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Based on the hoops DeepSeek-R1 team went through in terms of maintaining the performance on knowledge-based tasks, I believe adding search abilities to reasoning models is almost a no-brainer.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    6. Is reasoning solely due to RL?
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   The fundamental claim behind DeepSeek-R1 (and R1-Zero) is that RLVR explicitly induces reasoning capabilities. However, recent findings [10] suggest that reasoning behaviors, including the &quot;Aha moment,&quot; might already be present in base models due to pre-training on extensive chain-of-thought data.
  &lt;/p&gt;
  &lt;p&gt;
   My recent comparisons between DeepSeek V3 base and R1 reinforce this observation, as the updated base model also demonstrates reasoning-like behaviors. For instance, the comparison between the original V3 and R1 models clearly shows the difference between a non-reasoning and a reasoning model:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!eRCf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!eRCf!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png 424w, https://substackcdn.com/image/fetch/$s_!eRCf!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png 848w, https://substackcdn.com/image/fetch/$s_!eRCf!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png 1272w, https://substackcdn.com/image/fetch/$s_!eRCf!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:941,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;941&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!eRCf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!eRCf!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png 424w, https://substackcdn.com/image/fetch/$s_!eRCf!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png 848w, https://substackcdn.com/image/fetch/$s_!eRCf!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png 1272w, https://substackcdn.com/image/fetch/$s_!eRCf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4c429ead-c30e-4bb3-9aeb-8f9b7fd09645_1600x1034.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   However, this is no longer true when comparing the updated V3 base model to R1:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!oV_S!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!oV_S!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png 424w, https://substackcdn.com/image/fetch/$s_!oV_S!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png 848w, https://substackcdn.com/image/fetch/$s_!oV_S!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png 1272w, https://substackcdn.com/image/fetch/$s_!oV_S!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:899,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;899&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!oV_S!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!oV_S!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png 424w, https://substackcdn.com/image/fetch/$s_!oV_S!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png 848w, https://substackcdn.com/image/fetch/$s_!oV_S!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png 1272w, https://substackcdn.com/image/fetch/$s_!oV_S!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67ac85a9-1149-4644-9d28-6f694dc725c5_1600x988.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Additionally, [13] identified that self-reflection and self-correction behaviors emerge progressively throughout pre-training across various domains and model sizes. This further complicates the attribution of reasoning capabilities solely to RL methods.
  &lt;/p&gt;
  &lt;p&gt;
   Perhaps the conclusion is that RL definitely turns simple base models into reasoning models. However, it&#x27;s not the only way to induce or improve reasoning abilities. As the DeepSeek-R1 team showed, distillation also improves reasoning. And since distillation, in this paper, meant instruction fine-tuning on chain-of-thought data, it&#x27;s likely that pre-training on data that includes chain-of-thought data induces these abilities as well. (As I explained in my book through hands-on code, pre-training and instruction fine-tuning are based on the same next-token prediction task and loss functions, after all.)
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Noteworthy research papers on training reasoning models
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   After reading through a large number of reasoning papers last month, I tried to summarize the most interesting takeaways in the previous section. However, for those who are curious about the sources with a bit more detail, I also listed 15 relevant papers in this section below as an optional read. (For simplicity, the following summaries are sorted by date.)
  &lt;/p&gt;
  &lt;p&gt;
   Please note that this list is also not comprehensive (I capped it at 15), as this article is already more than too long!
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [1] Scaling Reinforcement Learning (And Context Length)
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 22 Jan,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Kimi k1.5: Scaling Reinforcement Learning with LLMs
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2501.12599
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   It&#x27;s interesting that this paper came out the same day as the DeepSeek-R1 paper! Here, the authors showcase a multi-modal LLM trained with RL. Similar to DeepSeek-R1, they didn&#x27;t use process reward models (PRMs) but employed verifiable rewards. A PRM is a type of reward model used in RL (especially in LLM training) that evaluates not just the final answer but also the reasoning steps that led to it.
  &lt;/p&gt;
  &lt;p&gt;
   Another key idea here is that scaling the context length (up to 128k tokens) helps the model plan, reflect, and self-correct during reasoning. So, in addition to the correctness reward that is similar to DeepSeek-R1 they also have a length reward. Specifically, they promote shorter correct responses, and incorrect long answers get penalized more.
  &lt;/p&gt;
  &lt;p&gt;
   And they propose a method called long2short to distill these long-chain-of-thought skills into more efficient short-CoT models. (It does this by distilling shorter correct responses from the long-CoT model using methods like model merging, shortest rejection sampling, DPO, and a 2nd round of RL with stronger length penalties.)
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!FvXN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FvXN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png 424w, https://substackcdn.com/image/fetch/$s_!FvXN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png 848w, https://substackcdn.com/image/fetch/$s_!FvXN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png 1272w, https://substackcdn.com/image/fetch/$s_!FvXN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:724,&quot;width&quot;:1318,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;724&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!FvXN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FvXN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png 424w, https://substackcdn.com/image/fetch/$s_!FvXN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png 848w, https://substackcdn.com/image/fetch/$s_!FvXN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png 1272w, https://substackcdn.com/image/fetch/$s_!FvXN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F176ae121-dda5-48ac-8bff-fb63d317c3ac_1318x724.png 1456w&quot; width=&quot;1318&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Kimi k1.5: Scaling Reinforcement Learning with LLMs, https://arxiv.org/abs//2501.12599
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [2] Competitive Programming with Large Reasoning Models
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 3 Feb,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Competitive Programming with Large Reasoning Models
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2502.06807
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This paper from OpenAI evaluates their o-models (like o1, o1-ioi, and o3) on competitive programming tasks. While it doesn&#x27;t go into the technical details of how RL was applied, it still offers some interesting takeaways.
  &lt;/p&gt;
  &lt;p&gt;
   First, the models were trained using outcome-based RL, rather than process-based reward models. This is similar to approaches like DeepSeek-R1 and Kimi.
  &lt;/p&gt;
  &lt;p&gt;
   One of the interesting findings is that o3 can learn its own test-time (i.e., inference-time scaling) strategies. For example, it often writes a simple brute-force version of a problem (something that trades efficiency for correctness) and then uses it to verify the outputs of its more optimized solution. This kind of strategy wasn&#x27;t hand-coded; the model figured it out on its own.
  &lt;/p&gt;
  &lt;p&gt;
   So overall, the paper argues that scaling general-purpose RL allows models to develop their own reasoning and verification methods, without needing any human heuristics or domain-specific inference pipelines. In contrast, other (earlier) models like o1-ioi relied on handcrafted test-time strategies like clustering thousands of samples and reranking them, which required a lot of manual design and tuning.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!AFi5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!AFi5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png 424w, https://substackcdn.com/image/fetch/$s_!AFi5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png 848w, https://substackcdn.com/image/fetch/$s_!AFi5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png 1272w, https://substackcdn.com/image/fetch/$s_!AFi5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fe06168b-51b5-4d08-8211-3830870218a7_1208x732.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:732,&quot;width&quot;:1208,&quot;resizeWidth&quot;:674,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;408.4172185430464&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!AFi5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!AFi5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png 424w, https://substackcdn.com/image/fetch/$s_!AFi5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png 848w, https://substackcdn.com/image/fetch/$s_!AFi5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png 1272w, https://substackcdn.com/image/fetch/$s_!AFi5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe06168b-51b5-4d08-8211-3830870218a7_1208x732.png 1456w&quot; width=&quot;674&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Competitive Programming with Large Reasoning Models, https://arxiv.org/abs/2502.06807
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [3] Exploring the Limit of Outcome Reward
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 10 Feb,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2502.06781
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This paper explores how far RL with just binary &quot;correct&quot; or &quot;wrong&quot; feedback (like in DeepSeek-R1) can go for solving math problems. To do this, they start by using Best-of-N sampling to collect positive examples and apply behavior cloning on them, which they show is theoretically enough to optimize the policy.
  &lt;/p&gt;
  &lt;p&gt;
   To deal with the challenge of sparse rewards (especially when long chains of thought include partially correct steps) they add a token-level reward model that learns to assign importance weights to different parts of the reasoning. This helps the model focus on the most critical steps when learning and improves the overall performance.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!yT5L!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!yT5L!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png 424w, https://substackcdn.com/image/fetch/$s_!yT5L!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png 848w, https://substackcdn.com/image/fetch/$s_!yT5L!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png 1272w, https://substackcdn.com/image/fetch/$s_!yT5L!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:758,&quot;width&quot;:1390,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;758&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!yT5L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!yT5L!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png 424w, https://substackcdn.com/image/fetch/$s_!yT5L!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png 848w, https://substackcdn.com/image/fetch/$s_!yT5L!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png 1272w, https://substackcdn.com/image/fetch/$s_!yT5L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3dd40c4-3b95-4ab3-9a74-c282585a8019_1390x758.png 1456w&quot; width=&quot;1390&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Exploring the Limit of Outcome Reward for Learning Mathematical Reasoning, https://arxiv.org/abs/2502.06781
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [4] LLM Reasoning with Rule-Based Reinforcement (On Logic Data)
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 20 Feb,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2502.14768
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   DeepSeek-R1 focused on math and code tasks. This paper trains a 7B model using logic puzzles as the main training data.
  &lt;/p&gt;
  &lt;p&gt;
   The researchers adopt a similar rule-based RL setup as DeepSeek-R1 but make several adjustments:
  &lt;/p&gt;
  &lt;p&gt;
   1. They introduce a strict format reward that penalizes shortcuts and ensures the model separates its reasoning from its final answer using &amp;lt;think&amp;gt; and &amp;lt;answer&amp;gt; tags.
  &lt;/p&gt;
  &lt;p&gt;
   2. They also use a system prompt that explicitly tells the model to first think through the problem step-by-step before giving the final answer.
  &lt;/p&gt;
  &lt;p&gt;
   Even with only 5K synthetic logic problems, the model develops good reasoning skills that generalize well to harder math benchmarks like AIME and AMC.
  &lt;/p&gt;
  &lt;p&gt;
   This is particularly interesting because it shows that logic-based RL training can teach models to reason in ways that transfer beyond the original domain.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!j1cS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!j1cS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!j1cS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!j1cS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!j1cS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!j1cS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!j1cS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!j1cS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!j1cS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!j1cS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F36e013eb-b79e-4645-973a-fc8b134c8020_1600x900.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning, https://arxiv.org/abs/2502.14768
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [5] Controlling How Long A Reasoning Model Thinks
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 6 Mar,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2503.04697
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   One hallmark of reasoning models is that they tend to generate longer outputs because of chain-of-thought reasoning. But by default, there is no explicit way to control how long the responses are.
  &lt;/p&gt;
  &lt;p&gt;
   This paper introduces Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that helps models to adhere to user-specified length constraints while still optimizing for accuracy.
  &lt;/p&gt;
  &lt;p&gt;
   In short, LCPO is similar to GRPO, i.e., &quot;GRPO + Custom Reward for Length Control&quot; implemented as
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;reward = reward_correctness - α * |target_length - actual_length|&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   where the target length is provided as part of the user prompt. This LCPO method above encourages the model to adhere to the provided target length exactly.
  &lt;/p&gt;
  &lt;p&gt;
   In addition, they also introduce an LCPO-Max variant, which, instead of encouraging the model to match the target length exactly, encourages the model to stay below a maximum token length:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;reward = reward_correctness * clip(α * (target_length - actual_length) + δ, 0, 1)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The authors train a 1.5B model called L1 using LCPO, which can adjust its output length based on the prompt. This lets users trade-off between accuracy and compute, depending on the task. Interestingly, the paper also finds that these long-chain models actually become surprisingly good at short reasoning too, even outperforming much larger models like GPT-4o at the same token lengths.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!FL9p!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FL9p!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png 424w, https://substackcdn.com/image/fetch/$s_!FL9p!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png 848w, https://substackcdn.com/image/fetch/$s_!FL9p!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png 1272w, https://substackcdn.com/image/fetch/$s_!FL9p!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:755,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;755&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!FL9p!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FL9p!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png 424w, https://substackcdn.com/image/fetch/$s_!FL9p!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png 848w, https://substackcdn.com/image/fetch/$s_!FL9p!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png 1272w, https://substackcdn.com/image/fetch/$s_!FL9p!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96e79a6d-e9c2-49a9-8242-de47ae444408_1600x830.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning, https://arxiv.org/abs/2503.04697
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [6] Incentivizing the Search Capability in LLMs
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 10 Mar,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2503.05592
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Reasoning models like DeepSeek-R1 that have been trained with RL rely on their internal knowledge. The authors here focus on improving these models on knowledge-based tasks that require more time-sensitive or recent information by adding access to external search systems.
  &lt;/p&gt;
  &lt;p&gt;
   So, this paper improves these models by teaching them to use external search systems during the reasoning process. Instead of relying on test-time strategies or supervised training, the authors use a two-stage reinforcement learning method that helps the model learn how and when to search on its own. The model first learns the search format, and then learns how to use search results to find correct answers.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!lSjH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!lSjH!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png 424w, https://substackcdn.com/image/fetch/$s_!lSjH!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png 848w, https://substackcdn.com/image/fetch/$s_!lSjH!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png 1272w, https://substackcdn.com/image/fetch/$s_!lSjH!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:672,&quot;width&quot;:1198,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;672&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!lSjH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!lSjH!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png 424w, https://substackcdn.com/image/fetch/$s_!lSjH!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png 848w, https://substackcdn.com/image/fetch/$s_!lSjH!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png 1272w, https://substackcdn.com/image/fetch/$s_!lSjH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F22dfffd6-b925-47e1-9fef-2c0aadc1fbd4_1198x672.png 1456w&quot; width=&quot;1198&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from R1-Searcher: Incentivizing the Search Capability in LLMs via Reinforcement Learning, https://arxiv.org/abs/2503.05592
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [7] Open-Source LLM Reinforcement Learning at Scale
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 ​​18 Mar,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     DAPO: An Open-Source LLM Reinforcement Learning System at Scale
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2503.14476
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While this paper is mainly about developing a DeepSeek-R1-like training pipeline and open-sourcing it, it also proposes interesting improvements to the GRPO algorithm that was used in DeepSeek-R1 training.
  &lt;/p&gt;
  &lt;p&gt;
   1. Clip-higher: Increases the upper bound of the PPO clipping range to encourage exploration and prevent entropy collapse during training.
  &lt;/p&gt;
  &lt;p&gt;
   2. Dynamic sampling: Improves training efficiency by filtering out prompts where all sampled responses are either always correct or always wrong.
  &lt;/p&gt;
  &lt;p&gt;
   3. Token-level policy gradient loss: moves from sample-level to token-level loss calculation so that longer responses can have more influence on the gradient update.*
  &lt;/p&gt;
  &lt;p&gt;
   4. Overlong reward shaping: Adds a soft penalty for responses that get truncated for being too long, which reduces reward noise and helps stabilize training.
  &lt;/p&gt;
  &lt;p&gt;
   * Standard GRPO uses a sample-level loss calculation. This involves first averaging the loss over the tokens for each sample and then averaging the loss over the samples. Since the samples have equal weight, the tokens in samples with longer responses may disproportionally contribute less to the overall loss. At the same time, researchers observed that longer responses often contain gibberish before the final answer, and this gibberish wouldn&#x27;t be sufficiently penalized in the original GRPO sample-level loss calculation.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!PcKh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!PcKh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png 424w, https://substackcdn.com/image/fetch/$s_!PcKh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png 848w, https://substackcdn.com/image/fetch/$s_!PcKh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png 1272w, https://substackcdn.com/image/fetch/$s_!PcKh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1025,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1025&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!PcKh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!PcKh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png 424w, https://substackcdn.com/image/fetch/$s_!PcKh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png 848w, https://substackcdn.com/image/fetch/$s_!PcKh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png 1272w, https://substackcdn.com/image/fetch/$s_!PcKh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8d635353-5ff8-4edb-a2cd-67c3f8527b95_1472x1036.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from DAPO: An Open-Source LLM Reinforcement Learning System at Scale, https://arxiv.org/abs/2503.14476
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [8] Reinforcement Learning for Reasoning in Small LLMs
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 20 Mar,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn&#x27;t
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2503.16219
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The original DeepSeek-R1 paper showed that when developing small(er) reasoning models, distillation gives better results than pure RL. In this paper, researchers follow up on this and investigate ways to improve small, distilled reasoning models further with RL.
  &lt;/p&gt;
  &lt;p&gt;
   So, using the 1.5B DeepSeek-R1-Distill-Qwen model, they find that with only 7000 training examples and a $42 compute budget, RL fine-tuning can lead to strong improvements. In this case, the improvements are enough to outperform OpenAI&#x27;s o1-preview on the AIME24 math benchmark, for example.
  &lt;/p&gt;
  &lt;p&gt;
   Furthermore, there were 3 interesting learnings in that paper:
  &lt;/p&gt;
  &lt;p&gt;
   1. Small LLMs can achieve fast reasoning improvements within the first 50–100 training steps using a compact, high-quality dataset. But the performance quickly drops if training continues too long, mainly due to length limits and output instability.
  &lt;/p&gt;
  &lt;p&gt;
   2. Mixing easier and harder problems helps the model produce shorter, more stable responses early in training. However, performance still degrades over time.
  &lt;/p&gt;
  &lt;p&gt;
   3. Using a cosine-shaped reward function helps control output length more effectively and improves training consistency. But this slightly reduces peak performance compared to standard accuracy-based rewards.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!KEk2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!KEk2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png 424w, https://substackcdn.com/image/fetch/$s_!KEk2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png 848w, https://substackcdn.com/image/fetch/$s_!KEk2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png 1272w, https://substackcdn.com/image/fetch/$s_!KEk2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:531,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;531&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!KEk2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!KEk2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png 424w, https://substackcdn.com/image/fetch/$s_!KEk2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png 848w, https://substackcdn.com/image/fetch/$s_!KEk2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png 1272w, https://substackcdn.com/image/fetch/$s_!KEk2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa1f0457a-b764-403a-8d4b-d4281b867181_1600x583.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn&#x27;t, https://arxiv.org/abs/2503.16219
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [9] Learning to Reason with Search
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 25 Mar,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2503.19470
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The ReSearch framework proposed in this paper extends the RL method from the DeepSeek-R1 paper to include search results as part of the reasoning process. The model learns when and how to search based on its ongoing reasoning chain, and it then uses the retrieved information for the next steps of reasoning.
  &lt;/p&gt;
  &lt;p&gt;
   This is all done without supervised data on reasoning steps. The researchers also show that this approach can lead to useful behaviors like self-correction and reflection, and that it generalizes well across multiple benchmarks despite being trained on just one dataset.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!_9nY!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_9nY!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png 424w, https://substackcdn.com/image/fetch/$s_!_9nY!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png 848w, https://substackcdn.com/image/fetch/$s_!_9nY!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png 1272w, https://substackcdn.com/image/fetch/$s_!_9nY!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:700,&quot;width&quot;:1410,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;700&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!_9nY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_9nY!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png 424w, https://substackcdn.com/image/fetch/$s_!_9nY!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png 848w, https://substackcdn.com/image/fetch/$s_!_9nY!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png 1272w, https://substackcdn.com/image/fetch/$s_!_9nY!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93de69b7-cbb3-46e5-ad59-7f48efef0d2d_1410x700.png 1456w&quot; width=&quot;1410&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning, https://arxiv.org/abs/2503.19470
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   PS: How does this method differ from the R1-Searcher discussed earlier?
  &lt;/p&gt;
  &lt;p&gt;
   R1-Searcher uses a two-stage, outcome-based reinforcement learning approach. In the first stage, it teaches the model how to invoke external retrieval; in the second, it learns to use the retrieved information to answer questions.
  &lt;/p&gt;
  &lt;p&gt;
   ReSearch, in contrast, integrates search directly into the reasoning process. It trains the model end-to-end using reinforcement learning, without any supervision on reasoning steps. Behaviors such as reflecting on incorrect queries and correcting them emerge naturally during training here.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [10] Understanding R1-Zero-Like Training
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 26 Mar, Understanding R1-Zero-Like Training: A Critical Perspective, https://arxiv.org/abs/2503.20783
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This paper investigates why DeepSeek-R1-Zero&#x27;s pure RL approach works to improve reasoning.
  &lt;/p&gt;
  &lt;p&gt;
   The authors find that some base models like Qwen2.5 already show strong reasoning and even the &quot;Aha moment&quot; without any RL. So the &quot;Aha moment&quot; might not be induced by RL, but instead inherited from pre-training. This challenges the idea that RL alone is what creates deep reasoning behaviors.
  &lt;/p&gt;
  &lt;p&gt;
   The paper also identifies two biases in GRPO:
  &lt;/p&gt;
  &lt;p&gt;
   1. Response-length bias: GRPO divides the advantage by the length of the response. This makes long incorrect answers get smaller penalties, so the model learns to generate longer bad answers.
  &lt;/p&gt;
  &lt;p&gt;
   2. Difficulty-level bias: GRPO also normalizes by the standard deviation of rewards for each question. Easy or hard questions (with low reward variance) get overweighted.
  &lt;/p&gt;
  &lt;p&gt;
   To fix this, the authors introduce Dr. GRPO, which is a modification of standard GRPO. Here, they get rid of the response length normalization in the advantage computation. Also, they get rid of the question-level standard deviation. This will result in more efficient training and fewer unnecessary long answers. Especially if the model is wrong, generating a long answer is no longer encouraged.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [11] Expanding RL with Verifiable Rewards Across Diverse Domains
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 31 Mar,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2503.23829
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   DeepSeek-R1 and most other reasoning models that followed focused on reward signals from easily verifiable domains like code and math. This paper explores how to extend these methods to more complex areas like medicine, chemistry, psychology, economics, and education, where answers are usually free-form and harder to verify (beyond a simple correct/incorrect).
  &lt;/p&gt;
  &lt;p&gt;
   The authors find that using expert-written reference answers makes evaluation more feasible than expected, even in these broader domains. To provide reward signals, they introduce a generative, soft-scoring method without needing heavy domain-specific annotation.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!cJ-q!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!cJ-q!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png 424w, https://substackcdn.com/image/fetch/$s_!cJ-q!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png 848w, https://substackcdn.com/image/fetch/$s_!cJ-q!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png 1272w, https://substackcdn.com/image/fetch/$s_!cJ-q!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:878,&quot;width&quot;:1432,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;878&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!cJ-q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!cJ-q!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png 424w, https://substackcdn.com/image/fetch/$s_!cJ-q!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png 848w, https://substackcdn.com/image/fetch/$s_!cJ-q!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png 1272w, https://substackcdn.com/image/fetch/$s_!cJ-q!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff831a219-4e96-469d-b7cb-0e0b19c446af_1432x878.png 1456w&quot; width=&quot;1432&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains, https://arxiv.org/abs/2503.23829
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [12] Scaling Up Reinforcement Learning (With a Simple Setup)
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 31 Mar,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2503.24290
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In this paper, the authors explore a minimalist reinforcement learning setup for training LLMs on reasoning tasks. They use vanilla PPO instead of GRPO (which was used in DeepSeek-R1-Zero) and skip the usual KL regularization commonly included in RLHF pipelines.
  &lt;/p&gt;
  &lt;p&gt;
   Interestingly, they find that this simple setup (vanilla PPO and a basic binary reward function based on answer correctness) is sufficient to train models that scale up in both reasoning performance and response length.
  &lt;/p&gt;
  &lt;p&gt;
   Using the same Qwen-32B base as DeepSeek-R1-Zero, their model outperforms it on multiple reasoning benchmarks while requiring only 1/10 the training steps.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!gtil!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!gtil!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png 424w, https://substackcdn.com/image/fetch/$s_!gtil!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png 848w, https://substackcdn.com/image/fetch/$s_!gtil!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png 1272w, https://substackcdn.com/image/fetch/$s_!gtil!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/adc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1197,&quot;width&quot;:1456,&quot;resizeWidth&quot;:676,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;555.75&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!gtil!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!gtil!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png 424w, https://substackcdn.com/image/fetch/$s_!gtil!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png 848w, https://substackcdn.com/image/fetch/$s_!gtil!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png 1272w, https://substackcdn.com/image/fetch/$s_!gtil!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fadc2b1fc-ce73-4ace-a2af-465ef3f86d33_1600x1315.png 1456w&quot; width=&quot;676&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model, https://arxiv.org/abs/2503.24290
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [13] Rethinking Reflection in Pre-Training
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 5 Apr,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Rethinking Reflection in Pre-Training
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2504.04022
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Based on the interesting insights from the DeepSeek-R1 paper, namely applying pure RL to a base model, we think that reasoning abilities in LLMs emerge from RL. This paper provides a bit of a plot twist, saying that self-correction already appears earlier during pre-training.
  &lt;/p&gt;
  &lt;p&gt;
   Concretely, by introducing deliberately flawed chains-of-thought into tasks, the authors measure whether models can identify and correct these errors. They find that both explicit and implicit forms of reflection emerge steadily throughout pre-training. This happens across many domains and model sizes. Even relatively early checkpoints show signs of self-correction, and the ability becomes stronger as pre-training compute increases.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!HU9a!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!HU9a!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png 424w, https://substackcdn.com/image/fetch/$s_!HU9a!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png 848w, https://substackcdn.com/image/fetch/$s_!HU9a!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png 1272w, https://substackcdn.com/image/fetch/$s_!HU9a!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/67dc8709-485e-412a-be2f-56b86389289c_1486x934.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:915,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;915&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!HU9a!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!HU9a!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png 424w, https://substackcdn.com/image/fetch/$s_!HU9a!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png 848w, https://substackcdn.com/image/fetch/$s_!HU9a!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png 1272w, https://substackcdn.com/image/fetch/$s_!HU9a!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F67dc8709-485e-412a-be2f-56b86389289c_1486x934.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Rethinking Reflection in Pre-Training, https://arxiv.org/abs/2504.04022
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [14] Concise Reasoning via Reinforcement Learning
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 7 Apr,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Concise Reasoning via Reinforcement Learning
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2504.05185
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As we all know by now, reasoning models often generate longer responses, which raises compute costs. Now, this new paper shows that this behavior comes from the RL training process, not from an actual need for long answers for better accuracy. The RL loss tends to favor longer responses when the model gets negative rewards, which I think explains the &quot;aha&quot; moments and longer chains of thought that arise from pure RL training.
  &lt;/p&gt;
  &lt;p&gt;
   I.e., if the model gets a negative reward (i.e., the answer is wrong), the math behind PPO causes the average per-token loss becomes smaller when the response is longer. So, the model is indirectly encouraged to make its responses longer. This is true even if those extra tokens don&#x27;t actually help solve the problem.
  &lt;/p&gt;
  &lt;p&gt;
   What does the response length have to do with the loss? When the reward is negative, longer responses can dilute the penalty per individual token, which results in lower (i.e., better) loss values (even though the model is still getting the answer wrong).
  &lt;/p&gt;
  &lt;p&gt;
   So the model &quot;learns&quot; that longer responses reduce the punishment, even though they are not helping correctness.
  &lt;/p&gt;
  &lt;p&gt;
   However, it&#x27;s important to emphasize that this analysis was done for PPO:
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    Of note, our current analysis is not applicable to GRPO, and a precise analysis of such methods is left for future work.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   In addition, the researchers show that a second round of RL (using just a few problems that are sometimes solvable) can shorten responses while preserving or even improving accuracy. This has big implications for deployment efficiency.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!bkp0!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!bkp0!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 424w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 848w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 1272w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:868,&quot;width&quot;:1118,&quot;resizeWidth&quot;:477,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;370.33631484794273&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!bkp0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!bkp0!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 424w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 848w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 1272w, https://substackcdn.com/image/fetch/$s_!bkp0!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50f896de-3318-478b-a00d-341101e9ab8c_1118x868.png 1456w&quot; width=&quot;477&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from Concise Reasoning via Reinforcement Learning, https://arxiv.org/abs/2504.05185
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    [15] A Sober Look at Progress in Language Model Reasoning
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 9 Apr,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    , https://arxiv.org/abs/2504.07086
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This paper takes a closer look at recent claims that RL can improve distilled language models, like those based on DeepSeek-R1.
  &lt;/p&gt;
  &lt;p&gt;
   For instance, I previously discussed the &quot;20 Mar, Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn&#x27;t&quot; paper that found RL is effective for distilled models.
  &lt;/p&gt;
  &lt;p&gt;
   And also the DeepSeek-R1 paper mentioned
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    Additionally, we found that applying RL to these distilled models yields significant further gains. We believe this warrants further exploration and therefore present only the results of the simple SFT-distilled models here.
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   So, while earlier papers reported large performance boosts from RL, this work finds that many of those improvements might just be noise. The authors show that results on small benchmarks like AIME24 are highly unstable: just changing a random seed can shift scores by several percentage points.
  &lt;/p&gt;
  &lt;p&gt;
   When RL models are evaluated under more controlled and standardized setups, the gains turn out to be much smaller than originally reported, and often not statistically significant. However, some models trained with RL do show modest improvements, but these are usually weaker than what supervised fine-tuning achieves, and they often don&#x27;t generalize well to new benchmarks.
  &lt;/p&gt;
  &lt;p&gt;
   So, while RL might help in some cases to improve smaller distilled models, this paper argues that its benefits have been overstated and better evaluation standards are needed to understand what’s actually working.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!oCP8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!oCP8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 424w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 848w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 1272w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:888,&quot;width&quot;:1288,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;888&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!oCP8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!oCP8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 424w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 848w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 1272w, https://substackcdn.com/image/fetch/$s_!oCP8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fad0e98ed-292c-40f4-80ad-0aee7ef3e9aa_1288x888.png 1456w&quot; width=&quot;1288&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility, https://arxiv.org/abs/2504.07086
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. To support me as an independent researcher, please consider purchasing a copy of my book,
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     , or signing up for a
    &lt;/span&gt;
    &lt;a href=&quot;https://magazine.sebastianraschka.com/subscribe&quot; rel=&quot;&quot;&gt;
     paid subscription
    &lt;/a&gt;
    &lt;span&gt;
     .
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Build a Large Language Model (From Scratch) now
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      available on Amazon
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Your support means a great deal! Thank you!
   &lt;/strong&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> First Look at Reasoning From Scratch: Chapter 1 </title>
<link>https://magazine.sebastianraschka.com/p/first-look-at-reasoning-from-scratch</link>
<pubDate>Sat, 29 Mar 2025 04:11:41 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   Hi everyone,
  &lt;/p&gt;
  &lt;p&gt;
   As you know, I&#x27;ve been writing a lot lately about the latest research on reasoning in LLMs. Before my next research-focused blog post, I wanted to offer something special to my paid subscribers as a thank-you for your ongoing support.
  &lt;/p&gt;
  &lt;p&gt;
   So, I&#x27;ve started writing a new book on how reasoning works in LLMs, and here I&#x27;m sharing the first Chapter 1 with you. This ~15-page chapter is an introduction reasoning in the context of LLMs and provides an overview of methods like inference-time scaling and reinforcement learning.
  &lt;/p&gt;
  &lt;p&gt;
   Thanks for your support! I hope you enjoy the chapter, and stay tuned for my next blog post on reasoning research!
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Happy reading,
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;em&gt;
    Sebastian
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Chapter 1: Introduction
   &lt;/strong&gt;
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    Welcome to the next stage of large language models (LLMs):
   &lt;/span&gt;
   &lt;em&gt;
    reasoning
   &lt;/em&gt;
   &lt;span&gt;
    . LLMs have transformed how we process and generate text, but their success has been largely driven by statistical pattern recognition. However, new advances in reasoning methodologies now enable LLMs to tackle more complex tasks, such as solving logical puzzles or multi-step arithmetic. Understanding these methodologies is the central focus of this book.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In this introductory chapter, you will learn:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     What &quot;reasoning&quot; means specifically in the context of LLMs.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     How reasoning differs fundamentally from pattern matching.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     The conventional pre-training and post-training stages of LLMs.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Key approaches to improving reasoning abilities in LLMs.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Why building reasoning models from scratch can improve our understanding of their strengths, limitations, and practical trade-offs.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   After building foundational concepts in this chapter, the following chapters shift toward practical, hands-on coding examples to directly implement reasoning techniques for LLMs.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1.1 What Does &quot;Reasoning&quot; Mean for Large Language Models?
   &lt;/strong&gt;
  &lt;/h2&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The State of LLM Reasoning Model Inference </title>
<link>https://magazine.sebastianraschka.com/p/state-of-llm-reasoning-and-inference-scaling</link>
<pubDate>Sat, 08 Mar 2025 04:11:42 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   Improving the reasoning abilities of large language models (LLMs) has become one of the hottest topics in 2025, and for good reason. Stronger reasoning skills allow LLMs to tackle more complex problems, making them more capable across a wide range of tasks users care about.
  &lt;/p&gt;
  &lt;p&gt;
   In the last few weeks, researchers have shared a large number of new strategies to improve reasoning, including scaling inference-time compute, reinforcement learning, supervised fine-tuning, and distillation. And many approaches combine these techniques for greater effect.
  &lt;/p&gt;
  &lt;p&gt;
   This article explores recent research advancements in reasoning-optimized LLMs, with a particular focus on inference-time compute scaling that have emerged since the release of DeepSeek R1.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!IOSP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!IOSP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png 424w, https://substackcdn.com/image/fetch/$s_!IOSP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png 848w, https://substackcdn.com/image/fetch/$s_!IOSP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png 1272w, https://substackcdn.com/image/fetch/$s_!IOSP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/af9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1260,&quot;width&quot;:1448,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;1260&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!IOSP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!IOSP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png 424w, https://substackcdn.com/image/fetch/$s_!IOSP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png 848w, https://substackcdn.com/image/fetch/$s_!IOSP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png 1272w, https://substackcdn.com/image/fetch/$s_!IOSP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Faf9e2677-652a-4af1-9f57-dc0c253d2198_1448x1260.png 1456w&quot; width=&quot;1448&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;span&gt;
       The four main categories of implementing reasoning models I explained in
      &lt;/span&gt;
      &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-reasoning-llms&quot; rel=&quot;&quot;&gt;
       Understanding Reasoning LLMs
      &lt;/a&gt;
     &lt;/em&gt;
     &lt;span&gt;
      . This article focuses on inference-time-scaling methods.
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Implementing and improving reasoning in LLMs: The four main categories
  &lt;/h1&gt;
  &lt;p&gt;
   Since most readers are likely already familiar with LLM reasoning models, I will keep the definition short: An LLM-based reasoning model is an LLM designed to solve multi-step problems by generating intermediate steps or structured &quot;thought&quot; processes. Unlike simple question-answering LLMs that just share the final answer, reasoning models either explicitly display their thought process or handle it internally, which helps them to perform better at complex tasks such as puzzles, coding challenges, and mathematical problems.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!ZsN9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ZsN9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png 424w, https://substackcdn.com/image/fetch/$s_!ZsN9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png 848w, https://substackcdn.com/image/fetch/$s_!ZsN9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png 1272w, https://substackcdn.com/image/fetch/$s_!ZsN9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:564,&quot;width&quot;:1326,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;564&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!ZsN9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ZsN9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png 424w, https://substackcdn.com/image/fetch/$s_!ZsN9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png 848w, https://substackcdn.com/image/fetch/$s_!ZsN9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png 1272w, https://substackcdn.com/image/fetch/$s_!ZsN9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8abbfe39-f656-4845-b376-18c1e563210a_1326x564.png 1456w&quot; width=&quot;1326&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Side-by-side comparison of a basic LLM’s one-line answer and a reasoning LLM’s explanatory response.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    In general, there are two main strategies to improve reasoning: (1) increasing
   &lt;/span&gt;
   &lt;em&gt;
    training
   &lt;/em&gt;
   &lt;span&gt;
    compute or (2) increasing
   &lt;/span&gt;
   &lt;em&gt;
    inference
   &lt;/em&gt;
   &lt;span&gt;
    compute, also known as
   &lt;/span&gt;
   &lt;em&gt;
    inference-time scaling
   &lt;/em&gt;
   &lt;span&gt;
    or
   &lt;/span&gt;
   &lt;em&gt;
    test-time scalin
   &lt;/em&gt;
   &lt;span&gt;
    g. (Inference compute refers to the processing power required to generate model outputs in response to a user query after training.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!pgyl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!pgyl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png 424w, https://substackcdn.com/image/fetch/$s_!pgyl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png 848w, https://substackcdn.com/image/fetch/$s_!pgyl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png 1272w, https://substackcdn.com/image/fetch/$s_!pgyl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:975,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;975&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!pgyl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!pgyl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png 424w, https://substackcdn.com/image/fetch/$s_!pgyl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png 848w, https://substackcdn.com/image/fetch/$s_!pgyl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png 1272w, https://substackcdn.com/image/fetch/$s_!pgyl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddde6f39-3b88-4962-9d02-2cf767dc82e9_1484x994.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Accuracy improvements can be achieved through increased training or test-time compute, where test-time compute is synonymous with inference-time compute and inference-time scaling. Source: Annotated figure from https://openai.com/index/learning-to-reason-with-llms/
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that the plots shown above make it look like we improve reasoning either via train-time compute OR test-time compute. However, LLMs are usually designed to improve reasoning by
   &lt;/span&gt;
   &lt;strong&gt;
    combining
   &lt;/strong&gt;
   &lt;span&gt;
    heavy train-time compute (extensive training or fine-tuning, often with reinforcement learning or specialized data)
   &lt;/span&gt;
   &lt;strong&gt;
    and
   &lt;/strong&gt;
   &lt;span&gt;
    increased test-time compute (allowing the model to &quot;think longer&quot; or perform extra computation during inference).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!RPhE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!RPhE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png 424w, https://substackcdn.com/image/fetch/$s_!RPhE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png 848w, https://substackcdn.com/image/fetch/$s_!RPhE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png 1272w, https://substackcdn.com/image/fetch/$s_!RPhE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:738,&quot;width&quot;:1456,&quot;resizeWidth&quot;:570,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;288.91483516483515&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!RPhE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!RPhE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png 424w, https://substackcdn.com/image/fetch/$s_!RPhE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png 848w, https://substackcdn.com/image/fetch/$s_!RPhE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png 1272w, https://substackcdn.com/image/fetch/$s_!RPhE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff24fdf8c-61a7-451d-a02d-85f8fc4fce73_1600x811.png 1456w&quot; width=&quot;570&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     The many terms that are used synonymously with inference-time scaling.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    To understand how reasoning models are being developed and improved, I think it remains useful to look at the different techniques separately. In my previous article,
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-reasoning-llms&quot; rel=&quot;&quot;&gt;
    Understanding Reasoning LLMs
   &lt;/a&gt;
   &lt;span&gt;
    , I discussed a finer categorization into four categories, as summarized in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!_2dU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_2dU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png 424w, https://substackcdn.com/image/fetch/$s_!_2dU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png 848w, https://substackcdn.com/image/fetch/$s_!_2dU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png 1272w, https://substackcdn.com/image/fetch/$s_!_2dU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:538,&quot;width&quot;:1456,&quot;resizeWidth&quot;:568,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;209.87912087912088&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!_2dU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_2dU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png 424w, https://substackcdn.com/image/fetch/$s_!_2dU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png 848w, https://substackcdn.com/image/fetch/$s_!_2dU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png 1272w, https://substackcdn.com/image/fetch/$s_!_2dU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb5e5fdf9-e72c-497b-9cf4-b4e3c24f33f1_1600x591.png 1456w&quot; width=&quot;568&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Methods 2-4 in the figure above typically produce models that generate longer responses because they include intermediate steps and explanations in their outputs. Since inference costs scale with response length (e.g., a response twice as long requires twice the compute), these training approaches are inherently linked to inference scaling. However, in this section on inference-time compute scaling, I focus specifically on techniques that explicitly regulate the number of generated tokens, whether through additional sampling strategies, self-correction mechanisms, or other methods.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In this article, I focus on the interesting new research papers and model releases focused on scaling
   &lt;/span&gt;
   &lt;strong&gt;
    inference-time compute scaling
   &lt;/strong&gt;
   &lt;span&gt;
    that followed
   &lt;/span&gt;
   &lt;strong&gt;
    after
   &lt;/strong&gt;
   &lt;span&gt;
    the DeepSeek R1 release on January 22nd, 2025. (Originally, I wanted to cover methods from all categories in this article, but due to the excessive length, I decided to release a separate article focused on train-time compute methods in the future.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!tGdE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!tGdE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png 424w, https://substackcdn.com/image/fetch/$s_!tGdE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png 848w, https://substackcdn.com/image/fetch/$s_!tGdE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png 1272w, https://substackcdn.com/image/fetch/$s_!tGdE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1074,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1074&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!tGdE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!tGdE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png 424w, https://substackcdn.com/image/fetch/$s_!tGdE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png 848w, https://substackcdn.com/image/fetch/$s_!tGdE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png 1272w, https://substackcdn.com/image/fetch/$s_!tGdE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a24019b-8b52-4780-9c15-877892ab647c_1600x1180.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Development process of DeepSeek&#x27;s reasoning models that I discussed in my previous article, Understanding Reasoning LLMs (https://magazine.sebastianraschka.com/p/understanding-reasoning-llms).
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Before we look into Inference-time compute scaling methods and the different areas of progress on the reasoning model with a focus on the inference-time compute scaling category, let me at least provide a brief overview of all the different categories.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    1. Inference-time compute scaling
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This category includes methods that improve model reasoning capabilities at inference time without training or modifying the underlying model weights. The core idea is to trade off increased computational resources for improved performance, which helps with making even fixed models more capable through techniques such as chain-of-thought reasoning, and various sampling procedures.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    While I categorize inference-time compute scaling separately to focus on methods in this context, it is important to note that this technique can be applied to any LLM. For example, OpenAI developed its o1 model using reinforcement learning and then additionally leveraged inference-time compute scaling. Interestingly, as I discussed in my previous article on reasoning models (
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-reasoning-llms&quot; rel=&quot;&quot;&gt;
    Understanding Reasoning LLMs
   &lt;/a&gt;
   &lt;span&gt;
    ), the DeepSeek R1 paper explicitly categorized common inference-time scaling methods (such as Process Reward Model-based and Monte Carlo Tree Search-based approaches) under &quot;unsuccessful attempts.&quot; This suggests that DeepSeek did not explicitly use these techniques beyond the R1 model’s natural tendency to generate longer responses, which serves as an implicit form of inference-time scaling over the V3 base model. However, since explicit inference-time scaling is often implemented at the application layer rather than within the LLM itself, DeepSeek acknowledged that they could easily incorporate it into the R1 deployment or application.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    2. Pure reinforcement learning
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This approach focuses solely on reinforcement learning (RL) to develop or improve reasoning capabilities. It typically involves training models with verifiable reward signals from math or coding domains. While RL allows models to develop more strategic thinking and self-improvement capabilities, it comes with challenges such as reward hacking, instability, and high computational costs.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    3. Reinforcement learning and supervised fine-tuning
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    This hybrid approach combines RL with supervised fine-tuning (SFT) to achieve more stable and generalizable improvements than pure RL. Typically, a model is first trained with SFT on high-quality instruction data and then further refined using RL to optimize specific behaviors
   &lt;/span&gt;
   &lt;strong&gt;
    .
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    4. Supervised fine-tuning and model distillation
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This method improves the reasoning capabilities of a model by instruction fine-tuning it on high-quality labeled datasets (SFT). If this high-quality dataset is generated by a larger LLM, then this methodology is also referred to as &quot;knowledge distillation&quot; or just &quot;distillation&quot; in LLM contexts. However, note that this differs slightly from traditional knowledge distillation in deep learning, which typically involves training a smaller model using not only the outputs (labels) but also the logits of a larger teacher model.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Inference-time compute scaling methods
  &lt;/h1&gt;
  &lt;p&gt;
   The previous section already briefly summarized inference-time compute scaling. Before discussing the recent research in this category, let me describe the inference-time scaling in a bit more detail.
  &lt;/p&gt;
  &lt;p&gt;
   Inference-time scaling improves an LLM&#x27;s reasoning by increasing computational resources (&quot;compute&quot;) during inference. The idea why this can improve reasoning can be given with a simple analogy: humans give better responses when given more time to think, and similarly, LLMs can improve with techniques that encourage more &quot;thought&quot; during generation.
  &lt;/p&gt;
  &lt;p&gt;
   One approach here is prompt engineering, such as chain-of-thought (CoT) prompting, where phrases like &quot;think step by step&quot; guide the model to generate intermediate reasoning steps. This improves accuracy on complex problems but is unnecessary for simple factual queries. Since CoT prompts generate more tokens, they effectively make inference more expensive.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Knds!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Knds!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png 424w, https://substackcdn.com/image/fetch/$s_!Knds!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png 848w, https://substackcdn.com/image/fetch/$s_!Knds!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png 1272w, https://substackcdn.com/image/fetch/$s_!Knds!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:381,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;381&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Knds!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Knds!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png 424w, https://substackcdn.com/image/fetch/$s_!Knds!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png 848w, https://substackcdn.com/image/fetch/$s_!Knds!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png 1272w, https://substackcdn.com/image/fetch/$s_!Knds!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d37faa4-3261-492c-85a4-766926b8c17c_1600x419.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An example of classic CoT prompting from the 2022 Large Language Models are Zero-Shot Reasoners paper (https://arxiv.org/abs/2205.11916).
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Another method involves voting and search strategies, such as majority voting or beam search, which refine responses by selecting the best output.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!O9a-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!O9a-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png 424w, https://substackcdn.com/image/fetch/$s_!O9a-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png 848w, https://substackcdn.com/image/fetch/$s_!O9a-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png 1272w, https://substackcdn.com/image/fetch/$s_!O9a-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:798,&quot;width&quot;:1334,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:367118,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/158620387?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;798&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!O9a-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!O9a-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png 424w, https://substackcdn.com/image/fetch/$s_!O9a-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png 848w, https://substackcdn.com/image/fetch/$s_!O9a-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png 1272w, https://substackcdn.com/image/fetch/$s_!O9a-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ad9742b-993f-4ecd-8f80-2fa41d43164b_1334x798.png 1456w&quot; width=&quot;1334&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Different search-based methods rely on a process-reward-based model to select the best answer. Annotated figure from the LLM Test-Time Compute paper, https://arxiv.org/abs/2408.03314
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   1. &quot;s1: Simple test-time scaling&quot;
  &lt;/h1&gt;
  &lt;p&gt;
   The remainder of this article will be focused on the recent research advances in the inference-time scaling category for improving reasoning capabilities of LLMs. Let me start with a more detailed discussion of a paper that serves as an example of inference-time scaling.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, one of the interesting recent research papers in this category is
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2501.19393&quot; rel=&quot;&quot;&gt;
    s1: Simple Test-Time Scaling
   &lt;/a&gt;
   &lt;span&gt;
    (31 Jan, 2025), which introduces so-called &quot;wait&quot; tokens, which can be considered as a more modern version of the aforementioned &quot;think step by step&quot; prompt modification.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Note that this involves supervised finetuning (SFT) to generate the initial model, so it&#x27;s not a pure inference-time scaling approach. However, the end goal is actively controlling the reasoning behavior through inference-time scaling; hence, I considered this paper for the &quot;1. Inference-time compute scaling&quot; category.
  &lt;/p&gt;
  &lt;p&gt;
   In short, their approach is twofold:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Create a curated SFT dataset with 1k training examples that include reasoning traces.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Control the length of responses by:
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     a) Appending &quot;Wait&quot; tokens to get the LLM to generate longer responses, self-verify, and correct itself, or
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     b) Stopping generation by adding an end-of-thinking token delimiter (&quot;Final Answer:&quot;). They call this length control &quot;budget forcing.&quot;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!qk_K!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qk_K!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png 424w, https://substackcdn.com/image/fetch/$s_!qk_K!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png 848w, https://substackcdn.com/image/fetch/$s_!qk_K!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png 1272w, https://substackcdn.com/image/fetch/$s_!qk_K!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:854,&quot;width&quot;:1154,&quot;resizeWidth&quot;:524,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;387.7781629116118&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!qk_K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qk_K!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png 424w, https://substackcdn.com/image/fetch/$s_!qk_K!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png 848w, https://substackcdn.com/image/fetch/$s_!qk_K!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png 1272w, https://substackcdn.com/image/fetch/$s_!qk_K!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0e7f4d94-9f8f-4353-87ad-78f3cba7b9cd_1154x854.png 1456w&quot; width=&quot;524&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of &quot;wait&quot; token insertion to control the length of the output. Annotated figure from https://arxiv.org/abs/2501.19393.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Budget forcing can be seen as a sequential inference scaling technique because it still generates one token at a time (but just more of it). In contrast, we have parallel techniques like majority voting, which aggregate multiple independent completions.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!kYWF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!kYWF!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png 424w, https://substackcdn.com/image/fetch/$s_!kYWF!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png 848w, https://substackcdn.com/image/fetch/$s_!kYWF!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png 1272w, https://substackcdn.com/image/fetch/$s_!kYWF!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:456,&quot;width&quot;:798,&quot;resizeWidth&quot;:516,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;294.85714285714283&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!kYWF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!kYWF!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png 424w, https://substackcdn.com/image/fetch/$s_!kYWF!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png 848w, https://substackcdn.com/image/fetch/$s_!kYWF!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png 1272w, https://substackcdn.com/image/fetch/$s_!kYWF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7f0c49b-a644-4142-bed0-7d114ecd39c2_798x456.png 1456w&quot; width=&quot;516&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Correlation between response accuracy and length. Annotated figure from https://arxiv.org/abs/2501.19393.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    They found their budget-forcing method more effective than other inference-scaling techniques I&#x27;ve discussed, like majority voting. If there&#x27;s something to criticize or improve, I would&#x27;ve liked to see results for more sophisticated parallel inference-scaling methods, like beam search, lookahead search, or the best compute-optimal search described in Google&#x27;s
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2408.03314&quot; rel=&quot;&quot;&gt;
     Scaling LLM Test-Time Compute Optimally Can Be More Effective Than Scaling Model Parameters
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper last year. Or even a simple comparison with a classic sequential method like chain-of-thought prompting (&quot;Think step by step&quot;).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Anyway, it&#x27;s a really interesting paper and approach!
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    PS: Why &quot;Wait&quot; tokens?
   &lt;/strong&gt;
   &lt;span&gt;
    My guess is the researchers were inspired by the &quot;Aha moment&quot; figure in the DeepSeek-R1 paper, where researchers saw LLMs coming up with something like &quot;
   &lt;/span&gt;
   &lt;em&gt;
    Wait, wait. Wait. That&#x27;s an aha moment I can flag here.
   &lt;/em&gt;
   &lt;span&gt;
    &quot; which showed that pure reinforcement learning can induce reasoning behavior in LLMs.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Interestingly, they also tried other tokens like &quot;
   &lt;/span&gt;
   &lt;em&gt;
    Hmm
   &lt;/em&gt;
   &lt;span&gt;
    &quot; but found that &quot;
   &lt;/span&gt;
   &lt;em&gt;
    Wait
   &lt;/em&gt;
   &lt;span&gt;
    &quot; performed slightly better.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Qd4X!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Qd4X!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png 424w, https://substackcdn.com/image/fetch/$s_!Qd4X!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png 848w, https://substackcdn.com/image/fetch/$s_!Qd4X!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png 1272w, https://substackcdn.com/image/fetch/$s_!Qd4X!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:510,&quot;width&quot;:758,&quot;resizeWidth&quot;:396,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;266.4379947229551&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Qd4X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Qd4X!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png 424w, https://substackcdn.com/image/fetch/$s_!Qd4X!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png 848w, https://substackcdn.com/image/fetch/$s_!Qd4X!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png 1272w, https://substackcdn.com/image/fetch/$s_!Qd4X!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6bdfe7db-8c97-4240-8be0-11efa7abdf7c_758x510.png 1456w&quot; width=&quot;396&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      &quot;
     &lt;/span&gt;
     &lt;em&gt;
      Wait&quot;
     &lt;/em&gt;
     &lt;span&gt;
      vs &quot;
     &lt;/span&gt;
     &lt;em&gt;
      Hmm&quot;
     &lt;/em&gt;
     &lt;span&gt;
      tokens.
     &lt;/span&gt;
     &lt;em&gt;
      Annotated figure from https://arxiv.org/abs/2501.19393.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Other noteworthy research papers on inference-time compute scaling
  &lt;/h1&gt;
  &lt;p&gt;
   Since it&#x27;s been a very active month on the reasoning model research front, I need to keep the summaries of other papers relatively brief to manage a reasonable length for this article. Hence, below are brief summaries of other interesting research articles related to inference-time compute scaling, sorted in ascending order by publication date.
  &lt;/p&gt;
  &lt;p&gt;
   As mentioned earlier, not all of these articles fall neatly into the inference-time compute scaling category, as some of them also involve specific training. However, these papers have in common that controlling inference-time compute is a specific mechanism of action. (Many distilled or SFT methods that I will cover in upcoming articles will lead to longer responses, which can be seen as a form of inference-time compute scaling. However, they do not actively control the length during inference, which makes these methods different from those covered here.)
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;span&gt;
    2.
   &lt;/span&gt;
   &lt;strong&gt;
    Test-Time Preference Optimization
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 22 Jan,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    &lt;span&gt;
     ,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.12895&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2501.12895
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Test-time Preference Optimization (TPO) is an iterative process that aligns LLM outputs with human preferences during inference (this is without altering its underlying model weights). In each iteration, the model:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Generates multiple responses for a given prompt.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Score the responses with a reward model to select the highest- and lowest-scoring ones as &quot;chosen&quot; and &quot;rejected&quot; responses
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Prompt the model to compare and critique the &quot;chosen&quot; and &quot;rejected&quot; responses
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Refine the output by converting the critiques into textual suggestions to update the original model responses
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   By doing steps 1-4 iteratively, the model refines its original responses.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!dmJN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!dmJN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png 424w, https://substackcdn.com/image/fetch/$s_!dmJN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png 848w, https://substackcdn.com/image/fetch/$s_!dmJN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png 1272w, https://substackcdn.com/image/fetch/$s_!dmJN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:745,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;745&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!dmJN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!dmJN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png 424w, https://substackcdn.com/image/fetch/$s_!dmJN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png 848w, https://substackcdn.com/image/fetch/$s_!dmJN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png 1272w, https://substackcdn.com/image/fetch/$s_!dmJN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd2a1bd16-7cf7-4898-8dce-a2d8352f76a8_1600x819.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from &quot;Test-Time Preference Optimization: On-the-Fly Alignment via Iterative Textual Feedback&quot;, https://arxiv.org/abs/2501.12895
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3. Thoughts Are All Over the Place
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 30 Jan,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    &lt;span&gt;
     ,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.18585&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2501.18585
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The researchers explore a phenomenon called &quot;underthinking&quot;, where reasoning models frequently switch between reasoning paths instead of fully focusing on exploring promising ones, which lowers the problem solving accuracy.
  &lt;/p&gt;
  &lt;p&gt;
   To address this &quot;underthinking&quot; issue, they introduce a method called the Thought Switching Penalty (TIP), which modifies the logits of thought-switching tokens to discourage premature reasoning path transitions.
  &lt;/p&gt;
  &lt;p&gt;
   Their approach does not require model fine-tuning and empirically improves accuracy across multiple challenging test sets.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!vvCX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vvCX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png 424w, https://substackcdn.com/image/fetch/$s_!vvCX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png 848w, https://substackcdn.com/image/fetch/$s_!vvCX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png 1272w, https://substackcdn.com/image/fetch/$s_!vvCX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:852,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;852&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!vvCX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vvCX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png 424w, https://substackcdn.com/image/fetch/$s_!vvCX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png 848w, https://substackcdn.com/image/fetch/$s_!vvCX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png 1272w, https://substackcdn.com/image/fetch/$s_!vvCX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7111ccaa-c4c1-4c7c-84f9-74d38df3c663_1528x894.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from &quot;Thoughts Are All Over the Place: On the Underthinking of o1-Like LLMs&quot;, https://arxiv.org/abs/2501.18585
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4. Trading Inference-Time Compute for Adversarial Robustness
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 31 Jan,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Trading Inference-Time Compute for Adversarial Robustness
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    &lt;span&gt;
     ,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2501.18841&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2501.18841
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Increasing inference-time compute improves the adversarial robustness of reasoning LLMs in many cases in terms of reducing the rate of successful attacks. Unlike adversarial training, this method does not need any special training or require prior knowledge of specific attack types.
  &lt;/p&gt;
  &lt;p&gt;
   However, there are some important exceptions. For example, the improvements in settings involving policy ambiguities or loophole exploitation are limited. Additionally, the reasoning-improved robustness increases can be reduced by new attack strategies such as &quot;Think Less&quot; and &quot;Nerd Sniping&quot;.
  &lt;/p&gt;
  &lt;p&gt;
   So, while these findings suggest that scaling inference-time compute can improve LLM safety, this alone is not a complete solution to adversarial robustness.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Gt2_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Gt2_!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png 424w, https://substackcdn.com/image/fetch/$s_!Gt2_!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png 848w, https://substackcdn.com/image/fetch/$s_!Gt2_!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png 1272w, https://substackcdn.com/image/fetch/$s_!Gt2_!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1067,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1067&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Gt2_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Gt2_!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png 424w, https://substackcdn.com/image/fetch/$s_!Gt2_!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png 848w, https://substackcdn.com/image/fetch/$s_!Gt2_!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png 1272w, https://substackcdn.com/image/fetch/$s_!Gt2_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F704acd82-10a8-4879-9bd3-26bb67c3155f_1600x1173.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from &quot;Trading Inference-Time Compute for Adversarial Robustness&quot;, https://arxiv.org/abs/2501.18841
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    5. Chain-of-Associated-Thoughts
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 4 Feb, CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2502.02390&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2502.02390
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The researchers combine classic Monte Carlo Tree Search inference-time scaling with an &quot;associative memory&quot; that serves as the LLM&#x27;s knowledge base during the exploration of reasoning pathways. Using this so-called associative memory, it&#x27;s easier for the LLM to consider earlier reasoning pathways and use dynamically involving information during the response generation.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!AtpC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!AtpC!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png 424w, https://substackcdn.com/image/fetch/$s_!AtpC!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png 848w, https://substackcdn.com/image/fetch/$s_!AtpC!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png 1272w, https://substackcdn.com/image/fetch/$s_!AtpC!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:707,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;707&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!AtpC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!AtpC!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png 424w, https://substackcdn.com/image/fetch/$s_!AtpC!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png 848w, https://substackcdn.com/image/fetch/$s_!AtpC!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png 1272w, https://substackcdn.com/image/fetch/$s_!AtpC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0635fb-c0b4-45df-b8d3-b54254ab92b5_1600x777.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from &quot;CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning&quot;, https://arxiv.org/abs/2502.02390
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    6. Step Back to Leap Forward
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    📄 6 Feb, Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models, https://arxiv.org/abs/2502.0440
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This paper proposes a self-backtracking mechanism that allows LLMs to improve their reasoning by learning when and where to backtrack during training and inference. While training involves teaching the model to recognize and correct suboptimal reasoning paths using a &amp;lt;backtrack&amp;gt; token, the key contribution is an inference-time tree-based search that uses this learned backtracking ability to explore alternative solutions.
  &lt;/p&gt;
  &lt;p&gt;
   What&#x27;s unique is that this exploration does not require without relying on external reward models (unlike the search-based methods that use a process-reward-based model that I mentioned at the beginning of the &quot;1. Inference-time compute scaling methods&quot; section in this article).
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!e6x3!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e6x3!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png 424w, https://substackcdn.com/image/fetch/$s_!e6x3!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png 848w, https://substackcdn.com/image/fetch/$s_!e6x3!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png 1272w, https://substackcdn.com/image/fetch/$s_!e6x3!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:811,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;811&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!e6x3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e6x3!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png 424w, https://substackcdn.com/image/fetch/$s_!e6x3!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png 848w, https://substackcdn.com/image/fetch/$s_!e6x3!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png 1272w, https://substackcdn.com/image/fetch/$s_!e6x3!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1df5fbf3-97f2-4976-b46f-2d5196b6bdc4_1594x888.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from &quot;Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models&quot;, https://arxiv.org/abs/2502.04404
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   I added this paper here as it&#x27;s heavily focused on the proposed backtracking inference-time scaling method, which improves reasoning by dynamically adjusting search depth and breadth rather than fundamentally altering the training paradigm (although, the training with &amp;lt;backtrack&amp;gt; tokens is required).
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    7. Scaling up Test-Time Compute with Latent Reasoning
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 7 Feb, Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2502.05171&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2502.05171
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Instead of improving reasoning by generating more tokens, the researchers propose a model that scales inference-time compute by iterating over a recurrent depth block in latent space. This block functions like a hidden state in RNNs, which allows the model to refine its reasoning without requiring longer token outputs.
  &lt;/p&gt;
  &lt;p&gt;
   However, a key drawback is the lack of explicit reasoning steps, which are (in my opinion) useful for human interpretability and a major advantage of chain-of-thought methods.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!kVPW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!kVPW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png 424w, https://substackcdn.com/image/fetch/$s_!kVPW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png 848w, https://substackcdn.com/image/fetch/$s_!kVPW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png 1272w, https://substackcdn.com/image/fetch/$s_!kVPW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b82da925-5736-44ba-bed1-ea3207b06382_1516x602.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:578,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;578&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!kVPW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!kVPW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png 424w, https://substackcdn.com/image/fetch/$s_!kVPW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png 848w, https://substackcdn.com/image/fetch/$s_!kVPW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png 1272w, https://substackcdn.com/image/fetch/$s_!kVPW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb82da925-5736-44ba-bed1-ea3207b06382_1516x602.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Annotated figure from &quot;Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach&quot;, https://arxiv.org/abs/2502.05171
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    8. Can a 1B LLM Surpass a 405B LLM?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 10 Feb, Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2502.06703&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2502.06703
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Many inference-time scaling techniques depend on sampling, which requires a Process Reward Model (PRM) to select the best solution. This paper systematically analyzes how inference-time compute scaling interacts with PRMs and problem difficulty.
  &lt;/p&gt;
  &lt;p&gt;
   The researchers develop a compute-optimal scaling strategy that adapts to the choice of PRM, policy model, and task complexity. Their results show that with the right inference-time scaling approach, a 1B parameter model can outperform a 405B Llama 3 model that lacks inference-time scaling.
  &lt;/p&gt;
  &lt;p&gt;
   Similarly, they show how a 7B model with inference-time scaling surpasses DeepSeek-R1 while maintaining higher inference efficiency.
  &lt;/p&gt;
  &lt;p&gt;
   These findings highlight how inference-time scaling can significantly improve LLMs, where small LLMs, with the right inference compute budget, can outperform much larger models.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!DiM2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!DiM2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png 424w, https://substackcdn.com/image/fetch/$s_!DiM2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png 848w, https://substackcdn.com/image/fetch/$s_!DiM2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png 1272w, https://substackcdn.com/image/fetch/$s_!DiM2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:952,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;952&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!DiM2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!DiM2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png 424w, https://substackcdn.com/image/fetch/$s_!DiM2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png 848w, https://substackcdn.com/image/fetch/$s_!DiM2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png 1272w, https://substackcdn.com/image/fetch/$s_!DiM2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F3c471e7f-36e7-41a8-a7e0-80bebf3c0f36_1600x1046.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Annotated figure from &quot;Can 1B LLM Surpass 405B LLM? Rethinking Compute-Optimal Test-Time Scaling&quot;, https://arxiv.org/abs/2502.06703
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    9.
   &lt;/strong&gt;
   &lt;span&gt;
    Learning to Reason from Feedback at Test-Time
   &lt;/span&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 16 Feb, Learning to Reason from Feedback at Test-Time,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2502.15771&quot; rel=&quot;&quot;&gt;
     https://www.arxiv.org/abs/2502.12521
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   It&#x27;s a bit hard to classify this as either an inference-time or training-time method, because it optimizes the LLM, changing its weight parameters, at inference-time.
  &lt;/p&gt;
  &lt;p&gt;
   So, this paper explores a way to make LLMs learn from their mistakes during inference time without having to store failed attempts in the prompt (which gets expensive). Instead of the usual method of refining answers by adding previous attempts to the context (sequential revision) or blindly generating new answers (parallel sampling), this approach updates the model&#x27;s weights at inference time.
  &lt;/p&gt;
  &lt;p&gt;
   To do this, the authors introduce OpTune, a small, trainable optimizer that updates the model&#x27;s weights based on the mistakes it made in a previous attempt. This means the model remembers what it did wrong without needing to keep the incorrect answer in the prompt/context.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!nJMD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nJMD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png 424w, https://substackcdn.com/image/fetch/$s_!nJMD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png 848w, https://substackcdn.com/image/fetch/$s_!nJMD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png 1272w, https://substackcdn.com/image/fetch/$s_!nJMD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:652,&quot;width&quot;:1426,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:191851,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/158620387?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;652&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!nJMD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nJMD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png 424w, https://substackcdn.com/image/fetch/$s_!nJMD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png 848w, https://substackcdn.com/image/fetch/$s_!nJMD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png 1272w, https://substackcdn.com/image/fetch/$s_!nJMD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1925302-7fc2-4c7b-91e9-1c0fc4f0609e_1426x652.png 1456w&quot; width=&quot;1426&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Annotated figure from &quot;Learning to Reason from Feedback at Test-Time”
     &lt;/span&gt;
     &lt;strong&gt;
      &lt;span&gt;
       ,
      &lt;/span&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2502.15771&quot; rel=&quot;&quot;&gt;
       https://www.arxiv.org/abs/2502.12521
      &lt;/a&gt;
     &lt;/strong&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    10. Inference-Time Computations for LLM Reasoning and Planning
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 18 Feb, Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights,
    &lt;/span&gt;
    &lt;a href=&quot;https://www.arxiv.org/abs/2502.12521&quot; rel=&quot;&quot;&gt;
     https://www.arxiv.org/abs/2502.12521
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This paper benchmarks various inference-time compute scaling techniques for reasoning and planning tasks with a focus on analyzing their trade-offs between computational cost and performance.
  &lt;/p&gt;
  &lt;p&gt;
   The authors evaluate multiple techniques—such as Chain-of-Thought, Tree-of-Thought, and Reasoning as Planning across eleven tasks spanning arithmetic, logical, commonsense, algorithmic reasoning, and planning.
  &lt;/p&gt;
  &lt;p&gt;
   The main finding is that while scaling inference-time computation can improve reasoning, no single technique consistently outperforms others across all tasks.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Vm7j!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Vm7j!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png 424w, https://substackcdn.com/image/fetch/$s_!Vm7j!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png 848w, https://substackcdn.com/image/fetch/$s_!Vm7j!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png 1272w, https://substackcdn.com/image/fetch/$s_!Vm7j!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/42115dab-1086-4035-9a64-65a83631377e_1600x1023.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:931,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;931&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Vm7j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Vm7j!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png 424w, https://substackcdn.com/image/fetch/$s_!Vm7j!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png 848w, https://substackcdn.com/image/fetch/$s_!Vm7j!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png 1272w, https://substackcdn.com/image/fetch/$s_!Vm7j!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42115dab-1086-4035-9a64-65a83631377e_1600x1023.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Annotated figure from
     &lt;/span&gt;
     &lt;em&gt;
      Inference-Time Computations for LLM Reasoning and Planning: A Benchmark and Insights
     &lt;/em&gt;
     &lt;span&gt;
      , https://www.arxiv.org/abs/2502.12521
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    11. Inner Thinking Transformer
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 19 Feb, Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2502.13842&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2502.13842
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The Inner Thinking Transformer (ITT) dynamically allocates more compute during inference. Instead of using a fixed depth (= using same number of layers) for all tokens as in standard transformer-based LLMs, ITT employs Adaptive Token Routing to allocate more compute to difficult tokens. These difficult tokens pass through the same layer multiple times to undergo additional processing, which increases the inference-compute budget for these difficult tokens.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!-oC7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-oC7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png 424w, https://substackcdn.com/image/fetch/$s_!-oC7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png 848w, https://substackcdn.com/image/fetch/$s_!-oC7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png 1272w, https://substackcdn.com/image/fetch/$s_!-oC7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1090,&quot;width&quot;:1414,&quot;resizeWidth&quot;:626,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;482.56011315417254&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!-oC7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-oC7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png 424w, https://substackcdn.com/image/fetch/$s_!-oC7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png 848w, https://substackcdn.com/image/fetch/$s_!-oC7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png 1272w, https://substackcdn.com/image/fetch/$s_!-oC7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4a6eb47e-fcbe-4c71-8d45-e7d82ae14ba1_1414x1090.png 1456w&quot; width=&quot;626&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from &quot;Inner Thinking Transformer: Leveraging Dynamic Depth Scaling to Foster Adaptive Internal Thinking&quot;, https://arxiv.org/abs/2502.13842
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    12. Test Time Scaling for Code Generation
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 20 Feb, S*: Test Time Scaling for Code Generation,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2502.14382&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2502.14382
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Inference-time scaling can be achieved by parallel scaling (generating multiple answers), sequential scaling (iteratively refining answers), or both as described in the Google paper from Summer 2024 (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2408.03314&quot; rel=&quot;&quot;&gt;
    Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters
   &lt;/a&gt;
   &lt;span&gt;
    ).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   S* is a test-time compute scaling method designed specifically for code generation that improves both parallel scaling (generating multiple solutions) and sequential scaling (iterative debugging).
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!quMS!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!quMS!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png 424w, https://substackcdn.com/image/fetch/$s_!quMS!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png 848w, https://substackcdn.com/image/fetch/$s_!quMS!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png 1272w, https://substackcdn.com/image/fetch/$s_!quMS!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:752,&quot;width&quot;:972,&quot;resizeWidth&quot;:536,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;414.6831275720165&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!quMS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!quMS!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png 424w, https://substackcdn.com/image/fetch/$s_!quMS!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png 848w, https://substackcdn.com/image/fetch/$s_!quMS!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png 1272w, https://substackcdn.com/image/fetch/$s_!quMS!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F94a88f17-b4b1-4642-aeb1-6db29071ef91_972x752.png 1456w&quot; width=&quot;536&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figure from &quot;S*: Test Time Scaling for Code Generation&quot;, https://arxiv.org/abs/2502.14382
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The approach operates in two stages:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Stage 1: Generation
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The model generates multiple code solutions and iteratively refines them using execution results and test cases provided in the problem prompt.
  &lt;/p&gt;
  &lt;p&gt;
   Think of this like a coding competition where a model submits solutions, runs tests, and fixes mistakes:
  &lt;/p&gt;
  &lt;p&gt;
   1. The model generates multiple candidate solutions.
  &lt;/p&gt;
  &lt;p&gt;
   2. Each solution is executed on public test cases (predefined input-output pairs).
  &lt;/p&gt;
  &lt;p&gt;
   3. If a solution fails (incorrect output or crashes), the model analyzes the execution results (errors, outputs) and modifies the code to improve it.
  &lt;/p&gt;
  &lt;p&gt;
   4. This refinement process continues iteratively until the model finds solutions that pass the test cases.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    For example,
   &lt;/strong&gt;
   &lt;span&gt;
    suppose the model is asked to implement a function is_even(n) that returns True for even numbers and False otherwise.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The model’s first attempt might be:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;def is_even(n):
    return n % 2  # ❌ Incorrect: should be `== 0`&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   The model tests this implementation with public test cases:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;Input	        Expected	Model Output	Status
is_even(4)	True	        False	        ❌ Fail
is_even(3)	False	        True	        ❌ Fail&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    After reviewing the results, the model realizes that 4 % 2 returns 0, not True, so it
   &lt;/span&gt;
   &lt;strong&gt;
    modifies
   &lt;/strong&gt;
   &lt;span&gt;
    the function:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;def is_even(n):
    return n % 2 == 0  # ✅ Corrected&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Now the function
   &lt;/span&gt;
   &lt;strong&gt;
    passes all public tests
   &lt;/strong&gt;
   &lt;span&gt;
    , completing the debugging phase.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Stage 2: Selection
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Once multiple solutions have passed public tests, the model must choose the best one (if possible). Here, S* introduces adaptive input synthesis to avoid random picking:
  &lt;/p&gt;
  &lt;p&gt;
   1. The model compares two solutions that both pass public tests.
  &lt;/p&gt;
  &lt;p&gt;
   2. It asks itself: &quot;Can I generate an input that will reveal a difference between these solutions?&quot;
  &lt;/p&gt;
  &lt;p&gt;
   3. It creates a new test input and runs both solutions on it.
  &lt;/p&gt;
  &lt;p&gt;
   4. If one solution produces the correct output while the other fails, the model selects the better one.
  &lt;/p&gt;
  &lt;p&gt;
   5. If both solutions behave identically, the model randomly picks one.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    For example,
   &lt;/strong&gt;
   &lt;span&gt;
    consider two different implementations of
   &lt;/span&gt;
   &lt;code&gt;
    is_perfect_square(n)
   &lt;/code&gt;
   &lt;span&gt;
    :
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;import math

def is_perfect_square_A(n):
    return math.isqrt(n) ** 2 == n&lt;/code&gt;&lt;/pre&gt;
  &lt;pre&gt;&lt;code&gt;def is_perfect_square_B(n):
    return math.sqrt(n).is_integer()&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Both pass the provided test cases for simple examples:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;n = 25
print(is_perfect_square_A(n))  # ✅ True (Correct)
print(is_perfect_square_B(n))  # ✅ True (Correct)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   But when the LLM generates edge cases we can see one of them fail, so the model would select the solution A in this case:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;n = 10**16 + 1
print(is_perfect_square_A(n))  # ✅ False (Correct)
print(is_perfect_square_B(n))  # ❌ True (Incorrect)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   13. Chain of Draft
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 25 Feb, Chain of Draft: Thinking Faster by Writing Less,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2502.18600&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2502.18600
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The researchers observe that while reasoning LLMs often generate verbose step-by-step explanations, humans typically rely on concise drafts that capture only essential information.
  &lt;/p&gt;
  &lt;p&gt;
   Inspired by this, they propose Chain of Draft (CoD), a prompting strategy that reduces verbosity by generating minimal but informative intermediate steps. So, in a sense it&#x27;s a method for inference-time scaling that improves the efficiency of inference-time scaling through generating fewer tokens.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Gaj6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Gaj6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png 424w, https://substackcdn.com/image/fetch/$s_!Gaj6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png 848w, https://substackcdn.com/image/fetch/$s_!Gaj6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png 1272w, https://substackcdn.com/image/fetch/$s_!Gaj6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b869a967-9498-435f-85f2-a38557db14e3_1460x982.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:979,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;979&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Gaj6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Gaj6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png 424w, https://substackcdn.com/image/fetch/$s_!Gaj6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png 848w, https://substackcdn.com/image/fetch/$s_!Gaj6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png 1272w, https://substackcdn.com/image/fetch/$s_!Gaj6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb869a967-9498-435f-85f2-a38557db14e3_1460x982.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated figures from &quot;Chain of Draft: Thinking Faster by Writing Less&quot;,
     &lt;/em&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2502.18600&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2502.18600
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Looking at the results, it seems that CoD is almost as brief as standard prompting, but as accurate as Chain of Thought (CoT) prompting. As I mentioned earlier, in my opinion, one of the advantages of reasoning models is that users can read the reasoning traces to learn and to better evaluate / trust the response. CoD somewhat diminishes the advantage of CoD. However, it might come in very handy where verbose intermediate steps are not needed as it speeds up the generation while maintaining the accuracy of CoT.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   14. Better Feedback and Edit Models
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     📄 6 Mar, Dedicated Feedback and Edit Models Empower Inference-Time Scaling for Open-Ended General-Domain Tasks,
    &lt;/span&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2503.04378&quot; rel=&quot;&quot;&gt;
     https://arxiv.org/abs/2503.04378
    &lt;/a&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Many techniques for scaling inference-time reasoning rely on tasks with verifiable answers (like math and code that can be checked), which makes them difficult to apply to open-ended tasks like writing and general problem-solving.
  &lt;/p&gt;
  &lt;p&gt;
   To address this limitation regarding verifiable answers, the researchers develop a system where one model generates an initial response, another provides feedback (&quot;feedback model&quot;), and a third refines the response based on that feedback (&quot;edit model&quot;).
  &lt;/p&gt;
  &lt;p&gt;
   They train these specialized &quot;feedback&quot; and &quot;edit&quot; models using a large dataset of human-annotated responses and feedback. These models then help improve responses by generating better feedback and making more effective edits during inference time.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!zA8v!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!zA8v!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png 424w, https://substackcdn.com/image/fetch/$s_!zA8v!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png 848w, https://substackcdn.com/image/fetch/$s_!zA8v!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png 1272w, https://substackcdn.com/image/fetch/$s_!zA8v!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:716,&quot;width&quot;:1136,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:119502,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/158620387?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;716&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!zA8v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!zA8v!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png 424w, https://substackcdn.com/image/fetch/$s_!zA8v!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png 848w, https://substackcdn.com/image/fetch/$s_!zA8v!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png 1272w, https://substackcdn.com/image/fetch/$s_!zA8v!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73568387-83fb-4744-bd3d-f5cbcfe53f1d_1136x716.png 1456w&quot; width=&quot;1136&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Conclusion
  &lt;/h1&gt;
  &lt;p&gt;
   Inference-time compute scaling has become one of the hottest research topics this year to improve the reasoning abilities of large language models without requiring modification to model weights.
  &lt;/p&gt;
  &lt;p&gt;
   The many techniques I summarized above range from simple token-based interventions like “Wait” tokens to sophisticated search and optimization-based strategies such as Test-Time Preference Optimization and Chain-of-Associated-Thoughts.
  &lt;/p&gt;
  &lt;p&gt;
   On the big-picture level, one recurring theme is that increasing compute at inference allows even relatively small models to achieve substantial improvements (on reasoning benchmarks) compared to standard approaches.
  &lt;/p&gt;
  &lt;p&gt;
   This suggests that inference strategies can help narrow the performance gap between smaller, more cost-effective models and their larger counterparts.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    The cost caveat
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The caveat is that inference-time scaling increases the inference costs, so whether to use a small model with substantial inference scaling or training a larger model and using it with less or no inference scaling is a math that has to be worked out based on how much use the model gets.
  &lt;/p&gt;
  &lt;p&gt;
   As an example, an o1 model, which uses heavy inference time scaling, is actually still slightly cheaper than a likely larger GPT-4.5 model that likely doesn&#x27;t use inference time scaling.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!nhEn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nhEn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png 424w, https://substackcdn.com/image/fetch/$s_!nhEn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png 848w, https://substackcdn.com/image/fetch/$s_!nhEn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png 1272w, https://substackcdn.com/image/fetch/$s_!nhEn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:732,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;732&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!nhEn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nhEn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png 424w, https://substackcdn.com/image/fetch/$s_!nhEn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png 848w, https://substackcdn.com/image/fetch/$s_!nhEn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png 1272w, https://substackcdn.com/image/fetch/$s_!nhEn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1f749e4-4167-4013-b1c9-651c83bf8d3b_1504x756.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   (It will be interesting to see how well GPT-4.5 will perform with o1- or o3-style inference-time scaling.)
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Which technique?
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   However, inference-time compute scaling is not a silver bullet. While methods like Monte Carlo Tree Search, self-backtracking, and dynamic-depth scaling can substantially improve reasoning performance, the effectiveness also still depends on the task and difficulty. As one of the earlier papers showed, there&#x27;s no inference-time compute scaling technique that performs best across all tasks.
  &lt;/p&gt;
  &lt;p&gt;
   Additionally, many of these approaches trade off response latency for improved reasoning, and slow responses can be annoying to some users. For instance, I usually switch from o1 to GPT4o if I have simple tasks due to the faster response time.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    What&#x27;s next
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Looking ahead, I think we will see many more papers this year centered around the two main branches of &quot;reasoning via inference-time compute scaling&quot; research:
  &lt;/p&gt;
  &lt;p&gt;
   1. Research that is purely centered around developing the best possible model topping the benchmarks.
  &lt;/p&gt;
  &lt;p&gt;
   2. Research that is concerned with balancing cost and performance trade-offs across different reasoning tasks.
  &lt;/p&gt;
  &lt;p&gt;
   Either way, what&#x27;s nice about inference-time compute scaling is that it can be applied to any type of existing LLM to make it better for specific tasks.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Thinking on Demand
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   An interesting trend on the industry side is what I refer to as &quot;thinking on demand&quot;. Following the release of DeepSeek R1, it feels like companies have been rushing to add reasoning capabilities to their offerings.
  &lt;/p&gt;
  &lt;p&gt;
   An interesting development here is that most LLM providers started to add the option for users to enable or disable thinking. An interesting development is that most LLM providers now allow users to enable or disable these &quot;thinking&quot; features. The mechanism is not publicly shared, but it&#x27;s likely the same model with dialed-back inference-time compute scaling.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For instance,
   &lt;/span&gt;
   &lt;a href=&quot;https://www.anthropic.com/news/claude-3-7-sonnet&quot; rel=&quot;&quot;&gt;
    Claude 3.7 Sonnet
   &lt;/a&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;a href=&quot;https://x.ai/blog/grok-3&quot; rel=&quot;&quot;&gt;
    Grok 3
   &lt;/a&gt;
   &lt;span&gt;
    now have a &quot;thinking&quot; that users can enable for their model, whereas OpenAI requires users to switch between models. For example, GPT4o/4.5 and o1/o3-mini if they want to use explicit reasoning models. However, the OpenAI CEO mentioned that GPT4.5 will likely be their last model, which doesn&#x27;t explicitly have a reasoning or &quot;thinking&quot; mode. On the open-source side, even IBM added an explicit &quot;thinking&quot; toggle to their
   &lt;/span&gt;
   &lt;a href=&quot;https://www.ibm.com/new/announcements/ibm-granite-3-2-open-source-reasoning-and-vision&quot; rel=&quot;&quot;&gt;
    Granite models
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Overall, the trend of adding reasoning capabilities whether via inference-time or train-time compute scaling is a major step forward for LLMs in 2025.
  &lt;/p&gt;
  &lt;p&gt;
   In time, I expect that reasoning will no longer be treated as an optional or special feature but will instead become the standard, much as instruction-finetuned or RLHF-tuned models are now the norm over raw pretrained models.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    As mentioned earlier, this article solely focused on inference-time compute length due to its already long lengths, thanks to the very active reasoning research activity.
   &lt;/span&gt;
   &lt;strong&gt;
    In a future article, I plan to cover all the interesting train-time compute scaling methods for reasoning.
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. To support me as an independent researcher, please consider purchasing a copy of my book,
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     , or signing up for a
    &lt;/span&gt;
    &lt;a href=&quot;https://magazine.sebastianraschka.com/subscribe&quot; rel=&quot;&quot;&gt;
     paid subscription
    &lt;/a&gt;
    &lt;span&gt;
     .
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Build a Large Language Model (From Scratch) now
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      available on Amazon
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Your support means a great deal! Thank you!
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Understanding Reasoning LLMs </title>
<link>https://magazine.sebastianraschka.com/p/understanding-reasoning-llms</link>
<pubDate>Wed, 05 Feb 2025 04:11:39 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   This article describes the four main approaches to building reasoning models, or how we can enhance LLMs with reasoning capabilities. I hope this provides valuable insights and helps you navigate the rapidly evolving literature and hype surrounding this topic.
  &lt;/p&gt;
  &lt;p&gt;
   In 2024, the LLM field saw increasing specialization. Beyond pre-training and fine-tuning, we witnessed the rise of specialized applications, from RAGs to code assistants. I expect this trend to accelerate in 2025, with an even greater emphasis on domain- and application-specific optimizations (i.e., &quot;specializations&quot;).
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!QwUc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!QwUc!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 424w, https://substackcdn.com/image/fetch/$s_!QwUc!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 848w, https://substackcdn.com/image/fetch/$s_!QwUc!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 1272w, https://substackcdn.com/image/fetch/$s_!QwUc!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:755,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;755&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!QwUc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!QwUc!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 424w, https://substackcdn.com/image/fetch/$s_!QwUc!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 848w, https://substackcdn.com/image/fetch/$s_!QwUc!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 1272w, https://substackcdn.com/image/fetch/$s_!QwUc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd6ebc5c9-461f-4d3a-889b-b8ea4e14e5ba_1600x830.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Stages 1-3 are the common steps to developing LLMs. Stage 4 specializes LLMs for specific use cases.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The development of reasoning models is one of these specializations. This means we refine LLMs to excel at complex tasks that are best solved with intermediate steps, such as puzzles, advanced math, and coding challenges. However, this specialization does not replace other LLM applications. Because transforming an LLM into a reasoning model also introduces certain drawbacks, which I will discuss later.
  &lt;/p&gt;
  &lt;p&gt;
   To give you a brief glimpse of what&#x27;s covered below, in this article, I will:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Explain the meaning of &quot;reasoning model&quot;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Discuss the advantages and disadvantages of reasoning models
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Outline the methodology behind DeepSeek R1
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Describe the four main approaches to building and improving reasoning models
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Share thoughts on the LLM landscape following the DeepSeek V3 and R1 releases
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Provide tips for developing reasoning models on a tight budget
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   I hope you find this article useful as AI continues its rapid development this year!
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   How do we define &quot;reasoning model&quot;?
  &lt;/h1&gt;
  &lt;p&gt;
   If you work in AI (or machine learning in general), you are probably familiar with vague and hotly debated definitions. The term &quot;reasoning models&quot; is no exception. Eventually, someone will define it formally in a paper, only for it to be redefined in the next, and so on.
  &lt;/p&gt;
  &lt;p&gt;
   In this article, I define &quot;reasoning&quot; as the process of answering questions that require complex, multi-step generation with intermediate steps. For example, factual question-answering like &quot;What is the capital of France?&quot; does not involve reasoning. In contrast, a question like &quot;If a train is moving at 60 mph and travels for 3 hours, how far does it go?&quot; requires some simple reasoning. For instance, it requires recognizing the relationship between distance, speed, and time before arriving at the answer.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!8oZo!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!8oZo!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png 424w, https://substackcdn.com/image/fetch/$s_!8oZo!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png 848w, https://substackcdn.com/image/fetch/$s_!8oZo!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png 1272w, https://substackcdn.com/image/fetch/$s_!8oZo!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:820,&quot;width&quot;:1424,&quot;resizeWidth&quot;:664,&quot;bytes&quot;:327375,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:&quot;https://magazine.sebastianraschka.com/i/156484949?img=https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png&quot;,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;382.35955056179773&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!8oZo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!8oZo!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png 424w, https://substackcdn.com/image/fetch/$s_!8oZo!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png 848w, https://substackcdn.com/image/fetch/$s_!8oZo!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png 1272w, https://substackcdn.com/image/fetch/$s_!8oZo!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff2987079-25f4-45fb-a020-1ac936ed16cb_1424x820.png 1456w&quot; width=&quot;664&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     A regular LLM may only provide a short answer (as shown on the left), whereas reasoning models typically include intermediate steps that reveal part of the thought process. (Note that many LLMs who have not been specifically developed for reasoning tasks can also provide intermediate reasoning steps in their answers.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Most modern LLMs are capable of basic reasoning and can answer questions like, &quot;If a train is moving at 60 mph and travels for 3 hours, how far does it go?&quot; So, today, when we refer to reasoning models, we typically mean LLMs that excel at more complex reasoning tasks, such as solving puzzles, riddles, and mathematical proofs.
  &lt;/p&gt;
  &lt;p&gt;
   Additionally, most LLMs branded as reasoning models today include a &quot;thought&quot; or &quot;thinking&quot; process as part of their response. Whether and how an LLM actually &quot;thinks&quot; is a separate discussion.
  &lt;/p&gt;
  &lt;p&gt;
   Intermediate steps in reasoning models can appear in two ways. First, they may be explicitly included in the response, as shown in the previous figure. Second, some reasoning LLMs, such as OpenAI&#x27;s o1, run multiple iterations with intermediate steps that are not shown to the user.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!DyRP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!DyRP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 424w, https://substackcdn.com/image/fetch/$s_!DyRP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 848w, https://substackcdn.com/image/fetch/$s_!DyRP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 1272w, https://substackcdn.com/image/fetch/$s_!DyRP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:767,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;767&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!DyRP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!DyRP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 424w, https://substackcdn.com/image/fetch/$s_!DyRP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 848w, https://substackcdn.com/image/fetch/$s_!DyRP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 1272w, https://substackcdn.com/image/fetch/$s_!DyRP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F35712d0e-0f40-4855-8d81-4dcea94055ce_1538x810.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &quot;Reasoning&quot; is used at two different levels: 1) processing the input and generating via multiple intermediate steps and 2) providing some sort of reasoning as part of the response to the user.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   When should we use reasoning models?
  &lt;/h1&gt;
  &lt;p&gt;
   Now that we have defined reasoning models, we can move on to the more interesting part: how to build and improve LLMs for reasoning tasks. However, before diving into the technical details, it is important to consider when reasoning models are actually needed.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    When do we need a reasoning model?
   &lt;/strong&gt;
   &lt;span&gt;
    Reasoning models are designed to be good at complex tasks such as solving puzzles, advanced math problems, and challenging coding tasks. However, they are not necessary for simpler tasks like summarization, translation, or knowledge-based question answering. In fact, using reasoning models for everything can be inefficient and expensive. For instance, reasoning models are typically more expensive to use, more verbose, and sometimes more prone to errors due to &quot;overthinking.&quot; Also here the simple rule applies: Use the right tool (or type of LLM) for the task.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The key strengths and limitations of reasoning models are summarized in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!lnf2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!lnf2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 424w, https://substackcdn.com/image/fetch/$s_!lnf2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 848w, https://substackcdn.com/image/fetch/$s_!lnf2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 1272w, https://substackcdn.com/image/fetch/$s_!lnf2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:524,&quot;width&quot;:1352,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;524&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!lnf2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!lnf2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 424w, https://substackcdn.com/image/fetch/$s_!lnf2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 848w, https://substackcdn.com/image/fetch/$s_!lnf2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 1272w, https://substackcdn.com/image/fetch/$s_!lnf2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F46dbe029-ab7d-4278-8dfe-7bc4af79a103_1352x524.png 1456w&quot; width=&quot;1352&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      The key strengths and weaknesses of reasoning models.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   A brief look at the DeepSeek training pipeline
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    Before discussing four main approaches to building and improving reasoning models in the next section, I want to briefly outline the DeepSeek R1 pipeline, as described in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2501.12948&quot; rel=&quot;&quot;&gt;
    DeepSeek R1 technical report
   &lt;/a&gt;
   &lt;span&gt;
    . This report serves as both an interesting case study and a blueprint for developing reasoning LLMs.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Note that DeepSeek did not release a single R1 reasoning model but instead introduced three distinct variants: DeepSeek-R1-Zero, DeepSeek-R1, and DeepSeek-R1-Distill.
  &lt;/p&gt;
  &lt;p&gt;
   Based on the descriptions in the technical report, I have summarized the development process of these models in the diagram below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!z-dr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!z-dr!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png 424w, https://substackcdn.com/image/fetch/$s_!z-dr!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png 848w, https://substackcdn.com/image/fetch/$s_!z-dr!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png 1272w, https://substackcdn.com/image/fetch/$s_!z-dr!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/db19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1101,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:281090,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1101&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!z-dr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!z-dr!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png 424w, https://substackcdn.com/image/fetch/$s_!z-dr!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png 848w, https://substackcdn.com/image/fetch/$s_!z-dr!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png 1272w, https://substackcdn.com/image/fetch/$s_!z-dr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdb19df56-c5bf-4a0c-aafb-4629a39b13f5_1542x1166.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Development process of DeepSeeks three different reasoning models that are discussed in the DeepSeek R1 technical report.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Next, let&#x27;s briefly go over the process shown in the diagram above. More details will be covered in the next section, where we discuss the four main approaches to building and improving reasoning models.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (1) DeepSeek-R1-Zero:
   &lt;/strong&gt;
   &lt;span&gt;
    This model is based on the 671B pre-trained DeepSeek-V3 base model released in December 2024. The research team trained it using reinforcement learning (RL) with two types of rewards. This approach is referred to as &quot;cold start&quot; training because it did not include a supervised fine-tuning (SFT) step, which is typically part of reinforcement learning with human feedback (RLHF).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (2) DeepSeek-R1:
   &lt;/strong&gt;
   &lt;span&gt;
    This is DeepSeek&#x27;s flagship reasoning model, built upon DeepSeek-R1-Zero. The team further refined it with additional SFT stages and further RL training, improving upon the &quot;cold-started&quot; R1-Zero model.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (3) DeepSeek-R1-Distill*:
   &lt;/strong&gt;
   &lt;span&gt;
    Using the SFT data generated in the previous steps, the DeepSeek team fine-tuned Qwen and Llama models to enhance their reasoning abilities. While not distillation in the traditional sense, this process involved training smaller models (Llama 8B and 70B, and Qwen 1.5B–30B) on outputs from the larger DeepSeek-R1 671B model.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   The 4 main ways to build and improve reasoning models
  &lt;/h1&gt;
  &lt;p&gt;
   In this section, I will outline the key techniques currently used to enhance the reasoning capabilities of LLMs and to build specialized reasoning models such as DeepSeek-R1, OpenAI&#x27;s o1 &amp; o3, and others.
  &lt;/p&gt;
  &lt;p&gt;
   Note: The exact workings of o1 and o3 remain unknown outside of OpenAI. However, they are rumored to leverage a combination of both inference and training techniques.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1) Inference-time scaling
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   One way to improve an LLM&#x27;s reasoning capabilities (or any capability in general) is inference-time scaling. This term can have multiple meanings, but in this context, it refers to increasing computational resources during inference to improve output quality.
  &lt;/p&gt;
  &lt;p&gt;
   A rough analogy is how humans tend to generate better responses when given more time to think through complex problems. Similarly, we can apply techniques that encourage the LLM to &quot;think&quot; more while generating an answer. (Although, whether LLMs actually &quot;think&quot; is a different discussion.)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    One straightforward approach to inference-time scaling is clever prompt engineering. A classic example is
   &lt;/span&gt;
   &lt;em&gt;
    chain-of-thought (CoT) prompting
   &lt;/em&gt;
   &lt;span&gt;
    , where phrases like &quot;think step by step&quot; are included in the input prompt. This encourages the model to generate intermediate reasoning steps rather than jumping directly to the final answer, which can often (but not always) lead to more accurate results on more complex problems. (Note that it doesn&#x27;t make sense to employ this strategy for simpler knowledge-based questions, like &quot;What is the capital of France&quot;, which is again a good rule of thumb to find out whether a reasoning model makes sense on your given input query.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!VFAa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!VFAa!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 424w, https://substackcdn.com/image/fetch/$s_!VFAa!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 848w, https://substackcdn.com/image/fetch/$s_!VFAa!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 1272w, https://substackcdn.com/image/fetch/$s_!VFAa!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:381,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;381&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!VFAa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!VFAa!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 424w, https://substackcdn.com/image/fetch/$s_!VFAa!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 848w, https://substackcdn.com/image/fetch/$s_!VFAa!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 1272w, https://substackcdn.com/image/fetch/$s_!VFAa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F523eee5e-afb6-4019-a11b-e0a291d2c286_1600x419.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An example of classic CoT prompting from the 2022 Large Language Models are Zero-Shot Reasoners paper (https://arxiv.org/abs/2205.11916).
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The aforementioned CoT approach can be seen as inference-time scaling because it makes inference more expensive through generating more output tokens.
  &lt;/p&gt;
  &lt;p&gt;
   Another approach to inference-time scaling is the use of voting and search strategies. One simple example is majority voting where we have the LLM generate multiple answers, and we select the correct answer by majority vote. Similarly, we can use beam search and other search algorithms to generate better responses.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    I highly recommend the
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2408.03314&quot; rel=&quot;&quot;&gt;
     Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper that I described in my previous Noteworthy AI Research Papers of 2024 (Part Two) article (https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2) for more details on these different strategies.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!YGJO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!YGJO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 424w, https://substackcdn.com/image/fetch/$s_!YGJO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 848w, https://substackcdn.com/image/fetch/$s_!YGJO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 1272w, https://substackcdn.com/image/fetch/$s_!YGJO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:836,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;836&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!YGJO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!YGJO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 424w, https://substackcdn.com/image/fetch/$s_!YGJO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 848w, https://substackcdn.com/image/fetch/$s_!YGJO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 1272w, https://substackcdn.com/image/fetch/$s_!YGJO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5cb10e5a-738b-4c9e-ba65-5850d4793706_1600x919.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Different search-based methods rely on a process-reward-based model to select the best answer. Annotated figure from the LLM Test-Time Compute paper, https://arxiv.org/abs/2408.03314
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The DeepSeek R1 technical report categorizes common inference-time scaling methods (such as Process Reward Model-based and Monte Carlo Tree Search-based approaches) under &quot;unsuccessful attempts.&quot; This suggests that DeepSeek did not explicitly use these techniques beyond the R1 model&#x27;s natural tendency to generate longer responses, which serves as an implicit form of inference-time scaling compared to the V3 base model.
  &lt;/p&gt;
  &lt;p&gt;
   However, explicit inference-time scaling is often implemented at the application layer rather than within the LLM itself, so DeepSeek may still apply such techniques within their app.
  &lt;/p&gt;
  &lt;p&gt;
   I suspect that OpenAI&#x27;s o1 and o3 models use inference-time scaling, which would explain why they are relatively expensive compared to models like GPT-4o. In addition to inference-time scaling, o1 and o3 were likely trained using RL pipelines similar to those used for DeepSeek R1. More on reinforcement learning in the next two sections below.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2) Pure reinforcement learning (RL)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    One of my personal highlights from the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2501.12948&quot; rel=&quot;&quot;&gt;
    DeepSeek R1 paper
   &lt;/a&gt;
   &lt;span&gt;
    is their discovery that reasoning emerges as a behavior from pure reinforcement learning (RL). Let&#x27;s explore what this means in more detail.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    As outlined earlier, DeepSeek developed three types of R1 models. The first,
   &lt;/span&gt;
   &lt;strong&gt;
    DeepSeek-R1-Zero
   &lt;/strong&gt;
   &lt;span&gt;
    , was built on top of the DeepSeek-V3 base model, a standard pre-trained LLM they released in December 2024. Unlike typical RL pipelines, where supervised fine-tuning (SFT) is applied before RL, DeepSeek-R1-Zero was trained
   &lt;/span&gt;
   &lt;strong&gt;
    exclusively
   &lt;/strong&gt;
   &lt;span&gt;
    with reinforcement learning without an initial SFT stage as highlighted in the diagram below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!_9Z-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_9Z-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png 424w, https://substackcdn.com/image/fetch/$s_!_9Z-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png 848w, https://substackcdn.com/image/fetch/$s_!_9Z-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png 1272w, https://substackcdn.com/image/fetch/$s_!_9Z-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1087,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:285932,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1087&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!_9Z-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_9Z-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png 424w, https://substackcdn.com/image/fetch/$s_!_9Z-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png 848w, https://substackcdn.com/image/fetch/$s_!_9Z-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png 1272w, https://substackcdn.com/image/fetch/$s_!_9Z-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa5bb6ecc-7e46-45fe-abff-1eb02e6b0e3a_1556x1162.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     The development process of DeepSeek-R1-Zero model.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Still, this RL process is similar to the commonly used RLHF approach, which is typically applied to preference-tune LLMs. (I covered RLHF in more detail in my article,
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives&quot; rel=&quot;&quot;&gt;
   &lt;/a&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives&quot; rel=&quot;&quot;&gt;
     LLM Training: RLHF and Its Alternatives
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    .) However, as mentioned above, the key difference in
   &lt;/span&gt;
   &lt;em&gt;
    DeepSeek-R1-Zero
   &lt;/em&gt;
   &lt;span&gt;
    is that they skipped the supervised fine-tuning (SFT) stage for instruction tuning. This is why they refer to it as &quot;pure&quot; RL. (Although, RL in the context of LLMs differs significantly from traditional RL, which is a topic for another time.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   For rewards, instead of using a reward model trained on human preferences, they employed two types of rewards: an accuracy reward and a format reward.
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      The
     &lt;/span&gt;
     &lt;strong&gt;
      accuracy reward
     &lt;/strong&gt;
     &lt;span&gt;
      uses the LeetCode compiler to verify coding answers and a deterministic system to evaluate mathematical responses.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      The
     &lt;/span&gt;
     &lt;strong&gt;
      format reward
     &lt;/strong&gt;
     &lt;span&gt;
      relies on an LLM judge to ensure responses follow the expected format, such as placing reasoning steps inside &amp;lt;think&amp;gt; tags.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Surprisingly, this approach was enough for the LLM to develop basic reasoning skills. The researchers observed an &quot;Aha!&quot; moment, where the model began generating reasoning traces as part of its responses despite not being explicitly trained to do so, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Prn2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Prn2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 424w, https://substackcdn.com/image/fetch/$s_!Prn2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 848w, https://substackcdn.com/image/fetch/$s_!Prn2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 1272w, https://substackcdn.com/image/fetch/$s_!Prn2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:927,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;927&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Prn2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Prn2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 424w, https://substackcdn.com/image/fetch/$s_!Prn2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 848w, https://substackcdn.com/image/fetch/$s_!Prn2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 1272w, https://substackcdn.com/image/fetch/$s_!Prn2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F30f8e37b-ba60-49d2-a95e-9c06b2033ee4_1600x1019.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      A figure from the DeepSeek R1 technical report (https://arxiv.org/abs/2501.12948) showing the emergence of the &quot;Aha&quot; moment.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   While R1-Zero is not a top-performing reasoning model, it does demonstrate reasoning capabilities by generating intermediate &quot;thinking&quot; steps, as shown in the figure above. This confirms that it is possible to develop a reasoning model using pure RL, and the DeepSeek team was the first to demonstrate (or at least publish) this approach.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3) Supervised finetuning and reinforcement learning (SFT + RL)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Next, let&#x27;s look at the development of DeepSeek-R1, DeepSeek’s flagship reasoning model, which serves as a blueprint for building reasoning models. This model improves upon DeepSeek-R1-Zero by incorporating additional supervised fine-tuning (SFT) and reinforcement learning (RL) to improve its reasoning performance.
  &lt;/p&gt;
  &lt;p&gt;
   Note that it is actually common to include an SFT stage before RL, as seen in the standard RLHF pipeline. OpenAI&#x27;s o1 was likely developed using a similar approach.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!19pK!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!19pK!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png 424w, https://substackcdn.com/image/fetch/$s_!19pK!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png 848w, https://substackcdn.com/image/fetch/$s_!19pK!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png 1272w, https://substackcdn.com/image/fetch/$s_!19pK!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/df7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1085,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:299910,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1085&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!19pK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!19pK!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png 424w, https://substackcdn.com/image/fetch/$s_!19pK!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png 848w, https://substackcdn.com/image/fetch/$s_!19pK!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png 1272w, https://substackcdn.com/image/fetch/$s_!19pK!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdf7f99f0-d154-49e5-b60a-4d148e0a61be_1548x1154.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     The development process of DeepSeek-R1 model.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As shown in the diagram above, the DeepSeek team used DeepSeek-R1-Zero to generate what they call &quot;cold-start&quot; SFT data. The term &quot;cold start&quot; refers to the fact that this data was produced by DeepSeek-R1-Zero, which itself had not been trained on any supervised fine-tuning (SFT) data.
  &lt;/p&gt;
  &lt;p&gt;
   Using this cold-start SFT data, DeepSeek then trained the model via instruction fine-tuning, followed by another reinforcement learning (RL) stage. This RL stage retained the same accuracy and format rewards used in DeepSeek-R1-Zero’s RL process. However, they added a consistency reward to prevent language mixing, which occurs when the model switches between multiple languages within a response.
  &lt;/p&gt;
  &lt;p&gt;
   The RL stage was followed by another round of SFT data collection. In this phase, the most recent model checkpoint was used to generate 600K Chain-of-Thought (CoT) SFT examples, while an additional 200K knowledge-based SFT examples were created using the DeepSeek-V3 base model.
  &lt;/p&gt;
  &lt;p&gt;
   These 600K + 200K SFT samples were then used for instruction-finetuning DeepSeek-V3 base before following up with a final round of RL. In this stage, they again used rule-based methods for accuracy rewards for math and coding questions, while human preference labels used for other question types. All in all, this is very similar to regular RLHF except that the SFT data contains (more) CoT examples. And the RL has verifiable rewards in addition to human preference-based rewards.
  &lt;/p&gt;
  &lt;p&gt;
   The final model, DeepSeek-R1 has a noticeable performance boost over DeepSeek-R1-Zero thanks to the additional SFT and RL stages, as shown in the table below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!22Cm!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!22Cm!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 424w, https://substackcdn.com/image/fetch/$s_!22Cm!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 848w, https://substackcdn.com/image/fetch/$s_!22Cm!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 1272w, https://substackcdn.com/image/fetch/$s_!22Cm!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:459,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;459&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!22Cm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!22Cm!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 424w, https://substackcdn.com/image/fetch/$s_!22Cm!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 848w, https://substackcdn.com/image/fetch/$s_!22Cm!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 1272w, https://substackcdn.com/image/fetch/$s_!22Cm!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff7f73f16-db4e-4047-89b0-823f16cefb33_1556x490.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Benchmark comparison of OpenAI A1 and DeepSeek R1 models. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4) Pure supervised finetuning (SFT) and distillation
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   So far, we have covered three key approaches to building and improving reasoning models:
  &lt;/p&gt;
  &lt;p&gt;
   1. Inference-time scaling, a technique that improves reasoning capabilities without training or otherwise modifying the underlying model.
  &lt;/p&gt;
  &lt;p&gt;
   2. Pure reinforcement learning (RL) as in DeepSeek-R1-Zero, which showed that reasoning can emerge as a learned behavior without supervised fine-tuning.
  &lt;/p&gt;
  &lt;p&gt;
   3. Supervised fine-tuning (SFT) plus RL, which led to DeepSeek-R1, DeepSeek’s flagship reasoning model.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    So, what’s left? Model &quot;distillation.&quot;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Surprisingly, DeepSeek also released smaller models trained via a process they call
   &lt;/span&gt;
   &lt;em&gt;
    distillation
   &lt;/em&gt;
   &lt;span&gt;
    . However, in the context of LLMs, distillation does not necessarily follow the classical knowledge distillation approach used in deep learning. Traditionally, in knowledge distillation (as briefly described in Chapter 6 of my
   &lt;/span&gt;
   &lt;a href=&quot;https://amzn.to/40YYowg&quot; rel=&quot;&quot;&gt;
    Machine Learning Q and AI
   &lt;/a&gt;
   &lt;span&gt;
    book), a smaller student model is trained on both the logits of a larger teacher model and a target dataset.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Instead, here distillation refers to instruction fine-tuning smaller LLMs, such as Llama 8B and 70B and Qwen 2.5 models (0.5B to 32B), on an SFT dataset generated by larger LLMs. Specifically, these larger LLMs are DeepSeek-V3 and an intermediate checkpoint of DeepSeek-R1. In fact, the SFT data used for this distillation process is the same dataset that was used to train DeepSeek-R1, as described in the previous section.
  &lt;/p&gt;
  &lt;p&gt;
   To clarify this process, I have highlighted the distillation portion in the diagram below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!xUjE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xUjE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png 424w, https://substackcdn.com/image/fetch/$s_!xUjE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png 848w, https://substackcdn.com/image/fetch/$s_!xUjE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png 1272w, https://substackcdn.com/image/fetch/$s_!xUjE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1174,&quot;width&quot;:1444,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:283812,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1174&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!xUjE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xUjE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png 424w, https://substackcdn.com/image/fetch/$s_!xUjE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png 848w, https://substackcdn.com/image/fetch/$s_!xUjE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png 1272w, https://substackcdn.com/image/fetch/$s_!xUjE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7db7c46b-fe67-49f4-9f65-b0e7b7e5ac08_1444x1174.png 1456w&quot; width=&quot;1444&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     The development process of DeepSeek-R1-Distill models.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Why did they develop these distilled models? In my opinion, there are two key reasons:
  &lt;/p&gt;
  &lt;p&gt;
   1. Smaller models are more efficient. This means they are cheaper to run, but they also can run on lower-end hardware, which makes these especially interesting for many researchers and tinkerers like me.
  &lt;/p&gt;
  &lt;p&gt;
   2. A case study in pure SFT. These distilled models serve as an interesting benchmark, showing how far pure supervised fine-tuning (SFT) can take a model without reinforcement learning.
  &lt;/p&gt;
  &lt;p&gt;
   The table below compares the performance of these distilled models against other popular models, as well as DeepSeek-R1-Zero and DeepSeek-R1.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!XwZe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!XwZe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 424w, https://substackcdn.com/image/fetch/$s_!XwZe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 848w, https://substackcdn.com/image/fetch/$s_!XwZe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 1272w, https://substackcdn.com/image/fetch/$s_!XwZe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ebc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:604,&quot;width&quot;:1168,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;604&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!XwZe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!XwZe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 424w, https://substackcdn.com/image/fetch/$s_!XwZe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 848w, https://substackcdn.com/image/fetch/$s_!XwZe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 1272w, https://substackcdn.com/image/fetch/$s_!XwZe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febc749fb-6a79-483f-bcda-b219f284bc09_1168x604.png 1456w&quot; width=&quot;1168&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Benchmark comparison of distilled versus non-distilled models. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   As we can see, the distilled models are noticeably weaker than DeepSeek-R1, but they are surprisingly strong relative to DeepSeek-R1-Zero, despite being orders of magnitude smaller. It&#x27;s also interesting to note how well these models perform compared to o1 mini (I suspect o1-mini itself might be a similarly distilled version of o1).
  &lt;/p&gt;
  &lt;p&gt;
   Before wrapping up this section with a conclusion, there’s one more interesting comparison worth mentioning. The DeepSeek team tested whether the emergent reasoning behavior seen in DeepSeek-R1-Zero could also appear in smaller models. To investigate this, they applied the same pure RL approach from DeepSeek-R1-Zero directly to Qwen-32B.
  &lt;/p&gt;
  &lt;p&gt;
   The results of this experiment are summarized in the table below, where QwQ-32B-Preview serves as a reference reasoning model based on Qwen 2.5 32B developed by the Qwen team (I think the training details were never disclosed). This comparison provides some additional insights into whether pure RL alone can induce reasoning capabilities in models much smaller than DeepSeek-R1-Zero.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!5_5L!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!5_5L!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 424w, https://substackcdn.com/image/fetch/$s_!5_5L!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 848w, https://substackcdn.com/image/fetch/$s_!5_5L!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 1272w, https://substackcdn.com/image/fetch/$s_!5_5L!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:408,&quot;width&quot;:1448,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;408&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!5_5L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!5_5L!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 424w, https://substackcdn.com/image/fetch/$s_!5_5L!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 848w, https://substackcdn.com/image/fetch/$s_!5_5L!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 1272w, https://substackcdn.com/image/fetch/$s_!5_5L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F05514c9f-eb04-496b-bd98-bb4710c65b14_1448x408.png 1456w&quot; width=&quot;1448&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Benchmark comparison distillation and RL on a smaller 32B model. Annotated figure from the DeepSeek-R1 technical report (https://arxiv.org/abs/2501.12948).
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Interestingly, the results suggest that distillation is far more effective than pure RL for smaller models. This aligns with the idea that RL alone may not be sufficient to induce strong reasoning abilities in models of this scale, whereas SFT on high-quality reasoning data can be a more effective strategy when working with small models.
  &lt;/p&gt;
  &lt;p&gt;
   For completeness, it would have been useful to see additional comparisons in the table:
  &lt;/p&gt;
  &lt;p&gt;
   1. Qwen-32B trained with SFT + RL, similar to how DeepSeek-R1 was developed. This would help determine how much improvement can be made, compared to pure RL and pure SFT, when RL is combined with SFT.
  &lt;/p&gt;
  &lt;p&gt;
   2. DeepSeek-V3 trained with pure SFT, similar to how the distilled models were created. This would allow for a direct comparison to see how effective RL + SFT is over pure SFT.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Conclusion
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   In this section, we explored four different strategies for building and improving reasoning models:
  &lt;/p&gt;
  &lt;p&gt;
   1. Inference-time scaling requires no additional training but increases inference costs, making large-scale deployment more expensive as the number or users or query volume grows. Still, it remains a no-brainer for improving the performance of already strong models. I strongly suspect that o1 leverages inference-time scaling, which helps explain why it is more expensive on a per-token basis compared to DeepSeek-R1.
  &lt;/p&gt;
  &lt;p&gt;
   2. Pure RL is interesting for research purposes because it provides insights into reasoning as an emergent behavior. However, in practical model development, RL + SFT is the preferred approach as it leads to stronger reasoning models. I strongly suspect that o1 was trained using RL + SFT as well. More precisely, I believe o1 starts from a weaker, smaller base model than DeepSeek-R1 but compensates with RL + SFT and inference-time scaling.
  &lt;/p&gt;
  &lt;p&gt;
   3. As mentioned above, RL + SFT is the key approach for building high-performance reasoning models. DeepSeek-R1 is a nice blueprint showing how this can be done.
  &lt;/p&gt;
  &lt;p&gt;
   4. Distillation is an attractive approach, especially for creating smaller, more efficient models. However, the limitation is that distillation does not drive innovation or produce the next generation of reasoning models. For instance, distillation always depends on an existing, stronger model to generate the supervised fine-tuning (SFT) data.
  &lt;/p&gt;
  &lt;p&gt;
   One interesting aspect I expect to see next is to combine RL + SFT (approach 3) with inference-time scaling (approach 1). This is likely what OpenAI o1 is doing, except it&#x27;s probably based on a weaker base model than DeepSeek-R1, which explains why DeepSeek-R1 performs so well while remaining relatively cheap at inference time.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Thoughts about DeepSeek R1
  &lt;/h1&gt;
  &lt;p&gt;
   In recent weeks, many people have asked for my thoughts on the DeepSeek-R1 models. In short, I think they are an awesome achievement. As a research engineer, I particularly appreciate the detailed technical report, which provides insights into their methodology that I can learn from.
  &lt;/p&gt;
  &lt;p&gt;
   One of the most fascinating takeaways is how reasoning emerged as a behavior from pure RL. And it&#x27;s impressive that DeepSeek has open-sourced their models under a permissive open-source MIT license, which has even fewer restrictions than Meta&#x27;s Llama models.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    How does it compare to o1?
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Is DeepSeek-R1 better than o1? I’d say it’s roughly in the same ballpark. However, what stands out is that DeepSeek-R1 is more efficient at inference time. This suggests that DeepSeek likely invested more heavily in the training process, while OpenAI may have relied more on inference-time scaling for o1.
  &lt;/p&gt;
  &lt;p&gt;
   That said, it&#x27;s difficult to compare o1 and DeepSeek-R1 directly because OpenAI has not disclosed much about o1. For instance, we don’t know:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Is o1 also a Mixture of Experts (MoE)?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     How large is o1?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Could o1 just be a slightly refined version of GPT-4o with minimal RL + SFT and only extensive inference-time scaling?
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Without knowing these details, a direct comparison remains an apples-to-oranges comparison.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    The cost of training DeepSeek-R1
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Another point of discussion has been the cost of developing DeepSeek-R1. Some have mentioned a ~$6 million training cost, but they likely conflated DeepSeek-V3 (the base model released in December last year) and DeepSeek-R1.
  &lt;/p&gt;
  &lt;p&gt;
   The $6 million estimate is based on an assumed $2 per GPU hour and the number of GPU hours required for the final training run of DeepSeek-V3, which was originally discussed back in December 2024.
  &lt;/p&gt;
  &lt;p&gt;
   However, the DeepSeek team has never disclosed the exact GPU hours or development cost for R1, so any cost estimates remain pure speculation.
  &lt;/p&gt;
  &lt;p&gt;
   Either way, ultimately, DeepSeek-R1 is a major milestone in open-weight reasoning models, and its efficiency at inference time makes it an interesting alternative to OpenAI’s o1.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Developing reasoning models on a limited budget
  &lt;/h1&gt;
  &lt;p&gt;
   Developing a DeepSeek-R1-level reasoning model likely requires hundreds of thousands to millions of dollars, even when starting with an open-weight base model like DeepSeek-V3. This can feel discouraging for researchers or engineers working with limited budgets.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    The good news: Distillation can go a long way
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Fortunately, model distillation offers a more cost-effective alternative. The DeepSeek team demonstrated this with their R1-distilled models, which achieve surprisingly strong reasoning performance despite being significantly smaller than DeepSeek-R1. However, even this approach isn’t entirely cheap. Their distillation process used 800K SFT samples, which requires substantial compute.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Interestingly, just a few days before DeepSeek-R1 was released, I came across
   &lt;/span&gt;
   &lt;a href=&quot;https://novasky-ai.github.io/posts/sky-t1/&quot; rel=&quot;&quot;&gt;
    an article about Sky-T1
   &lt;/a&gt;
   &lt;span&gt;
    , a fascinating project where a small team trained an open-weight 32B model using only 17K SFT samples. The total cost? Just $450, which is less than the registration fee for most AI conferences.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This example highlights that while large-scale training remains expensive, smaller, targeted fine-tuning efforts can still yield impressive results at a fraction of the cost.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Y8HI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Y8HI!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 424w, https://substackcdn.com/image/fetch/$s_!Y8HI!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 848w, https://substackcdn.com/image/fetch/$s_!Y8HI!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 1272w, https://substackcdn.com/image/fetch/$s_!Y8HI!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:570,&quot;width&quot;:1364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;570&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Y8HI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Y8HI!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 424w, https://substackcdn.com/image/fetch/$s_!Y8HI!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 848w, https://substackcdn.com/image/fetch/$s_!Y8HI!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 1272w, https://substackcdn.com/image/fetch/$s_!Y8HI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8865a313-2326-4f07-a6dc-72cc94cb2ebe_1364x570.png 1456w&quot; width=&quot;1364&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Figure from the &quot;Sky-T1: Train your own O1 preview model within $450&quot; article, https://novasky-ai.github.io/posts/sky-t1/
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   According to their benchmarks, Sky-T1 performs roughly on par with o1, which is impressive given its low training cost.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Pure RL on a budget: TinyZero
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    While Sky-T1 focused on model distillation, I also came across some interesting work in the &quot;pure RL&quot; space. One notable example is
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/Jiayi-Pan/TinyZero/&quot; rel=&quot;&quot;&gt;
    TinyZero
   &lt;/a&gt;
   &lt;span&gt;
    , a 3B parameter model that replicates the DeepSeek-R1-Zero approach (side note: it costs less than $30 to train).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Surprisingly, even at just 3B parameters, TinyZero exhibits some emergent self-verification abilities, which supports the idea that reasoning can emerge through pure RL, even in small models.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/Jiayi-Pan/TinyZero/&quot; rel=&quot;&quot;&gt;
    TinyZero repository
   &lt;/a&gt;
   &lt;span&gt;
    mentions that a research report is still work in progress, and I’ll definitely be keeping an eye out for further details.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Ykdn!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Ykdn!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 424w, https://substackcdn.com/image/fetch/$s_!Ykdn!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 848w, https://substackcdn.com/image/fetch/$s_!Ykdn!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 1272w, https://substackcdn.com/image/fetch/$s_!Ykdn!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:869,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;869&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Ykdn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Ykdn!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 424w, https://substackcdn.com/image/fetch/$s_!Ykdn!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 848w, https://substackcdn.com/image/fetch/$s_!Ykdn!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 1272w, https://substackcdn.com/image/fetch/$s_!Ykdn!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6111f4b4-cfb9-494c-8390-ec251702914b_1600x955.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     A figure from the TinyZero repository (https://github.com/Jiayi-Pan/TinyZero) showing that the model is capable of self-verification. (It would have been interesting to see the response of the base model in comparison.)
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The two projects mentioned above demonstrate that interesting work on reasoning models is possible even with limited budgets. While both approaches replicate methods from DeepSeek-R1, one focusing on pure RL (TinyZero) and the other on pure SFT (Sky-T1), it would be fascinating to explore how these ideas can be extended further.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Beyond Traditional SFT: Journey Learning
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    One particularly interesting approach I came across last year is described in the paper
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2410.18982&quot; rel=&quot;&quot;&gt;
   &lt;/a&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2410.18982&quot; rel=&quot;&quot;&gt;
     O1 Replication Journey: A Strategic Progress Report – Part 1
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    . Despite its title, the paper does not actually replicate o1. Instead, it introduces an different way to improve the distillation (pure SFT) process.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The key idea in the paper is &quot;journey learning&quot; as an alternative to &quot;shortcut learning.&quot;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Shortcut learning refers to the traditional approach in instruction fine-tuning, where models are trained using only correct solution paths.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Journey learning, on the other hand, also includes incorrect solution paths, allowing the model to learn from mistakes.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   This approach is kind of related to the self-verification abilities observed in TinyZero’s pure RL training, but it focuses on improving the model entirely through SFT. By exposing the model to incorrect reasoning paths and their corrections, journey learning may also reinforce self-correction abilities, potentially making reasoning models more reliable this way.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!TxCO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!TxCO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 424w, https://substackcdn.com/image/fetch/$s_!TxCO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 848w, https://substackcdn.com/image/fetch/$s_!TxCO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 1272w, https://substackcdn.com/image/fetch/$s_!TxCO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:707,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;707&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!TxCO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!TxCO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 424w, https://substackcdn.com/image/fetch/$s_!TxCO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 848w, https://substackcdn.com/image/fetch/$s_!TxCO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 1272w, https://substackcdn.com/image/fetch/$s_!TxCO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7a0bfcd0-6d93-4c91-a0d6-28178839b7cf_1492x724.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Journey learning, as opposed to traditional shortcut learning, includes wrong solutions paths in the SFT data. Annotated figure from the O1 Replication Journey: A Strategic Progress Report – Part 1 (https://arxiv.org/abs/2410.18982)
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This could be an exciting direction for future work, particularly for low-budget reasoning model development, where RL-based approaches may be computationally impractical.
  &lt;/p&gt;
  &lt;p&gt;
   Anyways, a lot of interesting work is currently happening on the reasoning model front, and I&#x27;m sure we will see a lot more exciting work in the upcoming months!
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . (I am confident that you&#x27;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Build a Large Language Model (From Scratch) now
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      available on Amazon
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Your support means a great deal! Thank you!
   &lt;/strong&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Noteworthy AI Research Papers of 2024 (Part Two) </title>
<link>https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-2</link>
<pubDate>Wed, 15 Jan 2025 04:11:49 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   I hope your 2025 is off to a great start! To kick off the year, I&#x27;ve finally been able to complete the draft and second part of this AI Research Highlights of 2024 article. It covers a variety of relevant topics, from mixture-of-experts models to new LLM scaling laws for precision.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that this article is Part Two in this series, focusing on the second half of 2024 from July through December. You can find
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1&quot; rel=&quot;&quot;&gt;
    Part One, covering January to June here.
   &lt;/a&gt;
   &lt;span&gt;
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The selection criteria are admittedly subjective, based on what stood out to me this year. I&#x27;ve also aimed for some variety, so it&#x27;s not all just about LLM model releases.
  &lt;/p&gt;
  &lt;p&gt;
   I hope you are having a great 2025, and happy reading!
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   7. July: The Llama 3 Herd of Models
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    Readers are probably already well familiar with Meta AI&#x27;s Llama 3 models and paper, but since these are such important and widely-used models, I want to dedicate the July section to
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2407.21783&quot; rel=&quot;&quot;&gt;
    The Llama 3 Herd of Models
   &lt;/a&gt;
   &lt;span&gt;
    (July 2024) paper by Grattafiori and colleagues.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    What&#x27;s notable about the Llama 3 model family is the increased sophistication of the pre-training and post-training pipelines compared to its Llama 2 predecessor. Note that this is not only true for Llama 3 but other LLMs like
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2408.00118&quot; rel=&quot;&quot;&gt;
    Gemma 2
   &lt;/a&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2407.10671&quot; rel=&quot;&quot;&gt;
    Qwen 2
   &lt;/a&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2407.21075&quot; rel=&quot;&quot;&gt;
    Apple&#x27;s Foundation Models
   &lt;/a&gt;
   &lt;span&gt;
    , and others, as I described a few months ago in my
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training&quot; rel=&quot;&quot;&gt;
    New LLM Pre-training and Post-training Paradigms
   &lt;/a&gt;
   &lt;span&gt;
    article.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    7.1 Llama 3 architecture summary
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Llama 3 was first released in 8 billion and 70 billion parameter sizes, but the team kept iterating on the model, releasing 3.1, 3.2, and 3.3 versions of Llama. The sizes are summarized below:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Llama 3
   &lt;/strong&gt;
   &lt;span&gt;
    (April 2024)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     8B parameters
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     70B parameters
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Llama 3.1
   &lt;/strong&gt;
   &lt;span&gt;
    (July 2024, discussed in the paper)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     8B parameters
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     70B parameters
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     405B parameters
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Llama 3.2
   &lt;/strong&gt;
   &lt;span&gt;
    (September 2024)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     1B parameters
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     3B parameters
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     11B parameters (vision-enabled)
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     90B parameters (vision-enabled)
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    L
   &lt;/span&gt;
   &lt;strong&gt;
    lama 3.3
   &lt;/strong&gt;
   &lt;span&gt;
    (December 2024)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     70B parameters
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Overall, the Llama 3 architecture closely resembles that of Llama 2. The key differences lie in its larger vocabulary and the introduction of grouped-query attention for the smaller model variant. A summary of the differences is shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Od-H!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Od-H!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 424w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 848w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 1272w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b7794088-da83-4121-90db-f84daf266970_1600x888.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:808,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;808&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Od-H!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Od-H!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 424w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 848w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 1272w, https://substackcdn.com/image/fetch/$s_!Od-H!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7794088-da83-4121-90db-f84daf266970_1600x888.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;span&gt;
       Llama 2 vs 3 comparison from the
      &lt;/span&gt;
      &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama&quot; rel=&quot;&quot;&gt;
       bonus material of my Build a Large Language from Scratch book
      &lt;/a&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you&#x27;re curious about architectural details, a great way to learn is by implementing the model from scratch and loading pretrained weights as a sanity check.
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama&quot; rel=&quot;&quot;&gt;
    I have a GitHub repository with a from-scratch implementation
   &lt;/a&gt;
   &lt;span&gt;
    that converts GPT-2 to Llama 2, Llama 3, Llama 3.1, and Llama 3.2.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!z5sR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!z5sR!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png 424w, https://substackcdn.com/image/fetch/$s_!z5sR!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png 848w, https://substackcdn.com/image/fetch/$s_!z5sR!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png 1272w, https://substackcdn.com/image/fetch/$s_!z5sR!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:816,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;816&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!z5sR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!z5sR!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png 424w, https://substackcdn.com/image/fetch/$s_!z5sR!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png 848w, https://substackcdn.com/image/fetch/$s_!z5sR!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png 1272w, https://substackcdn.com/image/fetch/$s_!z5sR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F019ba256-f4e9-49b6-af78-9be76629c1b3_1600x897.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      GPT-2 to Llama 2, Llama 3, Llama 3.1, and Llama 3.2 conversion
     &lt;/span&gt;
     &lt;em&gt;
      &lt;span&gt;
       from the
      &lt;/span&gt;
      &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/07_gpt_to_llama&quot; rel=&quot;&quot;&gt;
       bonus material of my Build a Large Language from Scratch book
      &lt;/a&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    7.3 Llama 3 training
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Another noteworthy update over Llama 2 is that Llama 3 has now been trained on 15 trillion tokens.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!T2DE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!T2DE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png 424w, https://substackcdn.com/image/fetch/$s_!T2DE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png 848w, https://substackcdn.com/image/fetch/$s_!T2DE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png 1272w, https://substackcdn.com/image/fetch/$s_!T2DE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:757,&quot;width&quot;:1456,&quot;resizeWidth&quot;:681,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;354.0638736263736&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!T2DE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!T2DE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png 424w, https://substackcdn.com/image/fetch/$s_!T2DE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png 848w, https://substackcdn.com/image/fetch/$s_!T2DE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png 1272w, https://substackcdn.com/image/fetch/$s_!T2DE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88588e3b-8d4e-403d-a9a3-75ca22546dd8_1600x832.png 1456w&quot; width=&quot;681&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Comparison of the training set sizes of various models.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The pre-training process is now multi-staged. The paper primarily focuses on Llama 3.1, and for the sake of brevity, I have summarized its pre-training techniques in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!nfh6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nfh6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png 424w, https://substackcdn.com/image/fetch/$s_!nfh6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png 848w, https://substackcdn.com/image/fetch/$s_!nfh6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png 1272w, https://substackcdn.com/image/fetch/$s_!nfh6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:682,&quot;width&quot;:1354,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;682&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!nfh6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nfh6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png 424w, https://substackcdn.com/image/fetch/$s_!nfh6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png 848w, https://substackcdn.com/image/fetch/$s_!nfh6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png 1272w, https://substackcdn.com/image/fetch/$s_!nfh6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2500d56c-9565-4134-8f20-2e9590c43025_1354x682.png 1456w&quot; width=&quot;1354&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Summary of techniques used in pre-training Llama 3.1.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In post-training, a notable change from Llama 2 is the switch from RLHF-PPO to DPO. These methods are also summarized in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!ORr_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ORr_!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png 424w, https://substackcdn.com/image/fetch/$s_!ORr_!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png 848w, https://substackcdn.com/image/fetch/$s_!ORr_!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png 1272w, https://substackcdn.com/image/fetch/$s_!ORr_!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:668,&quot;width&quot;:1374,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;668&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!ORr_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ORr_!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png 424w, https://substackcdn.com/image/fetch/$s_!ORr_!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png 848w, https://substackcdn.com/image/fetch/$s_!ORr_!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png 1272w, https://substackcdn.com/image/fetch/$s_!ORr_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F1af88b42-330d-4505-a21b-2af5da777a65_1374x668.png 1456w&quot; width=&quot;1374&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Summary of techniques used in pre-training Llama 3.1.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    For the interest of brevity, since there are 5 more papers to be covered in this article, I will defer the additional details and comparisons to other models to one of my previous articles.
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training&quot; rel=&quot;&quot;&gt;
    New LLM Pre-training and Post-training Paradigms
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    7.4 Multimodal Llamas
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Note that Llama 3.2 models were also released with multimodal support. However, I haven&#x27;t observed widespread use of these models in practice, and they aren&#x27;t widely discussed. We&#x27;ll revisit multimodal techniques in the September section later in this article.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    7.5 Llama 3 impact and usage
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   While it&#x27;s been over half a year since Llama 3 was released, Llama models continue to be among the most widely recognized and used open-weight LLMs (based on my personal perception, as I don’t have a specific source to cite). These models are relatively easy to understand and use. The reason for their popularity is likely the Llama brand recognition coupled with robust performance across a variety of general tasks, and making it easy to finetune them.
  &lt;/p&gt;
  &lt;p&gt;
   Meta AI has also maintained momentum by iterating on the Llama 3 model, releasing versions 3.1, 3.2, and now 3.3, which span a variety of sizes to cater to diverse use cases, from on-device scenarios (1B) to high-performance applications (400B).
  &lt;/p&gt;
  &lt;p&gt;
   Although the field now includes many competitive open-source and open-weight LLMs like Olmo 2, Qwen 2.5, Gemma 2, and Phi-4, and many others, I believe Llama will remain the go-to model for most users, much like ChatGPT has retained its popularity despite competition from options like Anthropic Claude, Google Gemini, DeepSeek, and others.
  &lt;/p&gt;
  &lt;p&gt;
   Personally, I’m excited for Llama 4, which I hope will be released sometime in 2025.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   8. August: Improving LLMs by scaling inference-time compute
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    My pick for this month is
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2408.03314&quot; rel=&quot;&quot;&gt;
    Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters
   &lt;/a&gt;
   &lt;span&gt;
    (August 2024) because it is a very well-written and detailed paper that offers some interesting insights into improving LLM responses during inference time (i.e., deployment).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    8.1 Improve outputs by using more test-time computation
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The main premise of this paper is to investigate if and how increased test-time computation can be used to improve LLM outputs. As a rough analogy, suppose that humans, on hard tasks, can generate better responses if they are given more time to think. Analogously, LLMs may be able to produce better outputs given more time/resources to generate their responses. In more technical terms, the researchers try to find out how much better models can perform than they are trained to do if additional compute is used during inference.
  &lt;/p&gt;
  &lt;p&gt;
   In addition, the researchers also looked into whether, given a fixed compute budget, spending more compute on test time can improve the results over spending that compute for further pre-training a model. But more on that later.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    8.2 Optimizing test-time computation techniques
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The paper describes techniques for increasing and improving and test-time compute in great detail, and if you are serious about deploying LLMs in practice (e.g., the aforementioned Llama models), I highly recommend giving this paper a full read.
  &lt;/p&gt;
  &lt;p&gt;
   In short, the 2 main methods to scale test-time compute are
  &lt;/p&gt;
  &lt;p&gt;
   1. Generating multiple solutions and using a process-based verifier reward model (it has to be separately trained) to select the best response
  &lt;/p&gt;
  &lt;p&gt;
   2. Updating the model&#x27;s response distribution adaptively, which essentially means revising the responses during inference generation (this also requires a separate model).
  &lt;/p&gt;
  &lt;p&gt;
   To provide a simple example for category 1: One naive way to improve test time compute is to use best-of-N sampling. This means that we let the LLM generate multiple answers in parallel and then pick the best one based on a verifier reward model. Best of N is also just one example. Multiple search algorithms fall into this category: beam-search, lookahead-search, and best-of-N, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!AO8D!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!AO8D!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png 424w, https://substackcdn.com/image/fetch/$s_!AO8D!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png 848w, https://substackcdn.com/image/fetch/$s_!AO8D!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png 1272w, https://substackcdn.com/image/fetch/$s_!AO8D!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:836,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;836&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!AO8D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!AO8D!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png 424w, https://substackcdn.com/image/fetch/$s_!AO8D!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png 848w, https://substackcdn.com/image/fetch/$s_!AO8D!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png 1272w, https://substackcdn.com/image/fetch/$s_!AO8D!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0537fd6d-81d4-4b6a-b0ee-0eb6f83caed3_1600x919.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Different search-based methods rely on a process-reward-based model to select the best answer. Annotated figure from the LLM Test-Time Compute paper, https://arxiv.org/abs/2408.03314
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Another approach, which falls into category 2, is sequentially revising the model&#x27;s response, as illustrated in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!H8Yw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!H8Yw!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png 424w, https://substackcdn.com/image/fetch/$s_!H8Yw!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png 848w, https://substackcdn.com/image/fetch/$s_!H8Yw!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png 1272w, https://substackcdn.com/image/fetch/$s_!H8Yw!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1016,&quot;width&quot;:950,&quot;resizeWidth&quot;:48,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;51.334736842105265&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!H8Yw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!H8Yw!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png 424w, https://substackcdn.com/image/fetch/$s_!H8Yw!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png 848w, https://substackcdn.com/image/fetch/$s_!H8Yw!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png 1272w, https://substackcdn.com/image/fetch/$s_!H8Yw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcc963076-6e5a-43ed-b4ee-e33e91c2e000_950x1016.png 1456w&quot; width=&quot;48&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Sequential revision approaches. Annotated figure from the LLM Test-Time Compute paper, https://arxiv.org/abs/2408.03314
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Which approach works better? Unfortunately, there is no one-size-fits-all answer. It depends on the base LLM and the specific problem or query. For example, revision-based approaches perform better on harder questions, and they can actually harm performance on easy questions.
  &lt;/p&gt;
  &lt;p&gt;
   In the paper, they developed an &quot;optimal&quot; strategy based on a model that assesses the query&#x27;s difficulty level and then chooses the right strategy appropriately.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    8.3 Test-time computation versus pretraining a larger model
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   An interesting question to answer is, given a fixed compute budget, what gives the bigger bang for the buck: using a larger model or using an increased inference-time budget?
  &lt;/p&gt;
  &lt;p&gt;
   Here, suppose the price you pay for a query is the same because running a large model in inference is more costly than a small one.
  &lt;/p&gt;
  &lt;p&gt;
   They found that for challenging questions, larger models outperform smaller models that get additional inference compute via the inference scaling strategies discussed earlier.
  &lt;/p&gt;
  &lt;p&gt;
   However, for easy and medium questions, inference time compute can be used to match the performance of 14x larger models at the same compute budget!
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    8.4 Future relevance of test-time compute scaling
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   When using open-weight models like Llama 3 and others, we often let them generate responses as-is. However, as this paper highlights, response quality can be significantly enhanced by allocating more inference compute. (If you are deploying models, this is definitely THE paper to read.)
  &lt;/p&gt;
  &lt;p&gt;
   Of course, increasing the inference-compute budget for large, expensive models makes them even costlier to operate. Yet, when applied selectively based on the difficulty of the queries, it can provide a valuable boost in quality and accuracy for certain responses, which is something most users would undoubtedly appreciate. (It’s safe to assume that OpenAI, Anthropic, and Google already leverage such techniques behind the scenes.)
  &lt;/p&gt;
  &lt;p&gt;
   Another compelling use case is enhancing the performance of smaller, on-device LLMs. I think this will remain a hot topic in the months and years ahead as we&#x27;ve also seen with the big announcements and investments in Apple Intelligence and Microsoft’s Copilot PCs.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   9. September: Comparing multimodal LLM paradigms
  &lt;/h1&gt;
  &lt;p&gt;
   Multimodal LLMs were one of the major things I thought would make big leaps in 2024. And yes, we got some more open-weight LLMs this year!
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!jXH1!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!jXH1!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png 424w, https://substackcdn.com/image/fetch/$s_!jXH1!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png 848w, https://substackcdn.com/image/fetch/$s_!jXH1!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png 1272w, https://substackcdn.com/image/fetch/$s_!jXH1!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:865,&quot;width&quot;:1456,&quot;resizeWidth&quot;:579,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;343.9800824175824&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!jXH1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!jXH1!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png 424w, https://substackcdn.com/image/fetch/$s_!jXH1!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png 848w, https://substackcdn.com/image/fetch/$s_!jXH1!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png 1272w, https://substackcdn.com/image/fetch/$s_!jXH1!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff445a8b5-7b5f-49a4-bfce-20c19b77898d_1600x951.png 1456w&quot; width=&quot;579&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An illustration of a multimodal LLM that can accept different input modalities (audio, text, images, and videos) and returns text as the output modality.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    One paper that particularly stood out to me was NVIDIA&#x27;s
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2409.11402&quot; rel=&quot;&quot;&gt;
    NVLM: Open Frontier-Class Multimodal LLMs
   &lt;/a&gt;
   &lt;span&gt;
    (September 2024) by Dai and colleagues, because it nicely compares the two leading multimodal paradigms.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    9.1 Multimodal LLM paradigms
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   There are two main approaches to building multimodal LLMs:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Method A: Unified Embedding Decoder Architecture approach;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Method B: Cross-modality Attention Architecture approach.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!rOIy!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!rOIy!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png 424w, https://substackcdn.com/image/fetch/$s_!rOIy!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png 848w, https://substackcdn.com/image/fetch/$s_!rOIy!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png 1272w, https://substackcdn.com/image/fetch/$s_!rOIy!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:856,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;856&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!rOIy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!rOIy!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png 424w, https://substackcdn.com/image/fetch/$s_!rOIy!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png 848w, https://substackcdn.com/image/fetch/$s_!rOIy!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png 1272w, https://substackcdn.com/image/fetch/$s_!rOIy!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F4ebc87a3-5694-4968-a56d-a9fc0cae52bd_1600x941.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      The two main approaches to developing multimodal LLM architectures.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    As illustrated in the figure above, the
   &lt;/span&gt;
   &lt;strong&gt;
    Unified Embedding-Decoder Architecture
   &lt;/strong&gt;
   &lt;span&gt;
    (Method A) relies on a single decoder model, resembling an unmodified LLM architecture such as GPT-2 or Llama 3.2. This method converts images into tokens that share the same embedding size as text tokens, enabling the LLM to process concatenated text and image input tokens.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In contrast, the
   &lt;/span&gt;
   &lt;strong&gt;
    Cross-Modality Attention Architecture
   &lt;/strong&gt;
   &lt;span&gt;
    (Method B) incorporates a cross-attention mechanism to integrate image and text embeddings within the attention layer directly.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you are interested in additional details, I dedicated a whole article to multimodal LLMs earlier this year that goes over these two methods step by step:
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-multimodal-llms&quot; rel=&quot;&quot;&gt;
    Understanding Multimodal LLMs -- An introduction to the main techniques and latest models
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    9.2 Nvidia&#x27;s hybrid approach
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    Given all the multimodal developments this year, to me, NVIDIA&#x27;s paper
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2409.11402&quot; rel=&quot;&quot;&gt;
    NVLM: Open Frontier-Class Multimodal LLMs
   &lt;/a&gt;
   &lt;span&gt;
    stands out for its comprehensive apples-to-apples comparison of these multimodal approaches. Rather than focusing on a single method, they directly compared:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Method A: The Unified Embedding Decoder Architecture (&quot;decoder-only architecture,&quot; NVLM-D),
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Method B: The Cross-Modality Attention Architecture (&quot;cross-attention-based architecture,&quot; NVLM-X),
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     A hybrid approach (NVLM-H).
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!3QeO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3QeO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png 424w, https://substackcdn.com/image/fetch/$s_!3QeO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png 848w, https://substackcdn.com/image/fetch/$s_!3QeO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png 1272w, https://substackcdn.com/image/fetch/$s_!3QeO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:846,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;846&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!3QeO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3QeO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png 424w, https://substackcdn.com/image/fetch/$s_!3QeO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png 848w, https://substackcdn.com/image/fetch/$s_!3QeO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png 1272w, https://substackcdn.com/image/fetch/$s_!3QeO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ef4b49a-bb6b-4c40-9786-e99fc319b405_1600x930.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Overview of the three multimodal approaches. (Annotated figure from the NVLM: Open Frontier-Class Multimodal LLMs paper: https://arxiv.org/abs/2409.11402)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   As summarized in the figure above, NVLM-D aligns with Method A, and NVLM-X corresponds to Method B, as discussed earlier. The hybrid model (NVLM-H) combines the strengths of both approaches: it first accepts an image thumbnail as input, followed by a dynamic number of patches processed through cross-attention to capture finer high-resolution details.
  &lt;/p&gt;
  &lt;p&gt;
   In summary, the key findings are as follows:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     NVLM-X: Offers superior computational efficiency for high-resolution images.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     NVLM-D: Delivers higher accuracy for OCR-related tasks.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      NVLM-H: Combines the strengths of both approaches for optimal performance.
     &lt;/span&gt;
     &lt;br/&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    9.3 Multimodal LLMs in 2025
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    Multimodal LLMs are an interesting one. I think they are the next logical development up from regular text-based LLMs. Most LLM service providers like (OpenAI, Google, and Anthropic) support multimodal inputs like images. Personally, I need multimodal capabilities maybe 1% of the time (usually, it&#x27;s something like: &quot;extract the table in markdown format&quot; or something like that). I do expect the default of open-weight LLMs to be purely text-based because it adds less complexity. At the same time I do think we will see more options and widespread use of open-weight LLMs as the tooling and APIs evolve.
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   10. October: Replicating OpenAI o1&#x27;s reasoning capabilities
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    My pick for October is the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2410.18982&quot; rel=&quot;&quot;&gt;
    O1 Replication Journey: A Strategic Progress Report -- Part 1
   &lt;/a&gt;
   &lt;span&gt;
    . (October 2024) by Quin and colleagues.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   OpenAI ChatGPT&#x27;s o1 (and now o3) have gained significant popularity, as they seem to represent a paradigm shift in improving LLMs&#x27; performance on reasoning tasks.
  &lt;/p&gt;
  &lt;p&gt;
   The exact details of OpenAI&#x27;s o1 remain undisclosed, and several papers have attempted to describe or replicate it. So, why did I choose this one? Its unusual structure and broader philosophical arguments about the state of academic research resonated with me. In other words, there was something distinctive about it that stood out and made it an interesting choice.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    10.1 Shortcut learning vs journey learning
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   One of the key points of this paper is the researchers&#x27; hypothesis that O1 employs a process called journey learning as opposed to shortcut learning, as illustrated in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!3yqP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3yqP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png 424w, https://substackcdn.com/image/fetch/$s_!3yqP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png 848w, https://substackcdn.com/image/fetch/$s_!3yqP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png 1272w, https://substackcdn.com/image/fetch/$s_!3yqP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1078,&quot;width&quot;:1436,&quot;resizeWidth&quot;:524,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;393.3649025069638&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!3yqP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3yqP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png 424w, https://substackcdn.com/image/fetch/$s_!3yqP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png 848w, https://substackcdn.com/image/fetch/$s_!3yqP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png 1272w, https://substackcdn.com/image/fetch/$s_!3yqP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe76d574c-b3b9-4a69-ba8c-9e8ba5c9f658_1436x1078.png 1456w&quot; width=&quot;524&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Traditionally, LLMs are trained on the correct solution path (shortcut learning); in journey learning, the supervised finetuning encompasses the whole trial-and-error correction process. Annotated figure from the O1 Replication Report, https://arxiv.org/abs/2410.18982
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   It&#x27;s worth noting that the journey learning approach is somewhat similar to the tree-based or beam-search methods with revisions, as discussed earlier in the &quot;8. August: Improving LLMs by Scaling Inference-Time Compute&quot; section of this article.
  &lt;/p&gt;
  &lt;p&gt;
   The subtle difference, however, is that the researchers create journey learning training examples for model finetuning, rather than simply applying this technique during inference. (It&#x27;s worth noting that I couldn&#x27;t find any information on the techniques they used to augment the inference process.)
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    10.2 Constructing long thoughts
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The researchers constructed a reasoning tree to derive an extended thought process from it, emphasizing trial and error. This approach diverges from traditional methods that prioritize finding a direct path to the correct answer with valid intermediate steps. In their framework, each node in the reasoning tree was annotated with a rating provided by a reward model, indicating whether the step was correct or incorrect, along with reasoning to justify this evaluation.
  &lt;/p&gt;
  &lt;p&gt;
   Next, they trained a deepseek-math-7b-base model via supervised finetuning and DPO. Here, they trained two models.
  &lt;/p&gt;
  &lt;p&gt;
   1. First they used the traditional shortcut training paradigm where only the correct intermediate steps were provided.
  &lt;/p&gt;
  &lt;p&gt;
   2. Second, they trained the model with their proposed journey learning approach that included the thought process three with correct and incorrect answers, backtracking, and so forth.
  &lt;/p&gt;
  &lt;p&gt;
   (Sidenote: They only used 327 examples in each case!)
  &lt;/p&gt;
  &lt;p&gt;
   As shown in the figure below, the journey learning process outperformed shortcut learning by quite a wide margin on the MATH500 benchmark dataset.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!iztP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!iztP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png 424w, https://substackcdn.com/image/fetch/$s_!iztP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png 848w, https://substackcdn.com/image/fetch/$s_!iztP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png 1272w, https://substackcdn.com/image/fetch/$s_!iztP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1126,&quot;width&quot;:1456,&quot;resizeWidth&quot;:545,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;421.47664835164835&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!iztP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!iztP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png 424w, https://substackcdn.com/image/fetch/$s_!iztP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png 848w, https://substackcdn.com/image/fetch/$s_!iztP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png 1272w, https://substackcdn.com/image/fetch/$s_!iztP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0fc97155-9781-4854-ad3a-318b5cb513a3_1490x1152.png 1456w&quot; width=&quot;545&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      LLMs trained with shortcut and journey learning. Annotated figure from the O1 Replication Report, https://arxiv.org/abs/2410.18982
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    10.3 Distillation -- the quick fix?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    One month later, the team released another report:
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2411.16489&quot; rel=&quot;&quot;&gt;
    O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?
   &lt;/a&gt;
   &lt;span&gt;
    (November 2024) by Huang and colleagues.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Here, they used a distillation approach, meaning they used careful prompting to extract the thought processes from o1 to train a model to reach the same performance. Since this is a long article, I won&#x27;t go over the details, but I wanted to share an interesting figure from that paper that summarizes the cost trade-offs of collecting long-thought data.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!X24t!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!X24t!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png 424w, https://substackcdn.com/image/fetch/$s_!X24t!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png 848w, https://substackcdn.com/image/fetch/$s_!X24t!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png 1272w, https://substackcdn.com/image/fetch/$s_!X24t!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:693,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;693&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!X24t!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!X24t!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png 424w, https://substackcdn.com/image/fetch/$s_!X24t!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png 848w, https://substackcdn.com/image/fetch/$s_!X24t!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png 1272w, https://substackcdn.com/image/fetch/$s_!X24t!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff878e905-f725-45e5-a79e-e2360aff7f64_1496x712.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   They got really good performance with this distillation approach, performing on-part with o1-preview and o1-mini. However, along with these experiments, the researchers also shared some interesting and important thoughts about the state of research in light of this approach, which I will summarize in the next section.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    10.4 The state of AI research
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   One big focus of the part 2 report was the &quot;Bitter Lesson of Simple Distillation&quot;. Sure, distillation works well in practice, but it isn&#x27;t what drives progress. In the best case, using distillation, you are matching the performance of of an existing upstream model (but you are not setting a new performance record.) Below are three quotes from the paper that might serve as a warning call about the current status quo:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &quot;This shift from “how it works” to &#x27;what works&#x27; represents a fundamental change in research mentality that could have far-reaching consequences for the field’s future innovation capacity.&quot;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &quot;This erosion of first-principles thinking is particularly concerning as it undermines the very foundation of scientific innovation.&quot;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &quot;Pressure to produce quick results may overshadow the value of deeper technical investigations, while students may be discouraged from pursuing more challenging, fundamental research directions.&quot;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   My personal take is that I still think there are tons of great and important ideas coming out of academic labs (today also often in partnership with industry), and they can be really practical and impactful. (A couple of my favorites that come to mind are LoRA and DPO.) The catch is that a lot of promising ideas never get tested at scale because universities usually don&#x27;t have the massive resources needed for that.
  &lt;/p&gt;
  &lt;p&gt;
   I&#x27;m not sure what the perfect solution is, and I do realize that companies can&#x27;t just give away their trade secrets. But it would be really helpful if, whenever companies do end up using ideas from academic papers, they&#x27;d openly acknowledge it. That kind of recognition goes a long way in motivating and rewarding researchers who make their work freely available. Also, it helps move the field forward by finding out what actually works in practice.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    10.5 The future of LLMs in the light of o1 (and o3)
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Does the O1 Replication Journey paper replicate the exact mechanism behind o1? Probably not. But it is still a valuable read full of ideas that can help achieve better results. I believe that “long-thought” models like o1 and o3 will continue to play a key role in LLM research. They are more expensive to run, but they are basically the gold standard or the upper limit for performance on reasoning tasks.
  &lt;/p&gt;
  &lt;p&gt;
   But because of their higher cost, o1-type models are not always the best option for every situation. For simpler tasks like grammar fixes or translations, we likely do not need a reasoning-heavy model. It all comes down to balancing cost and utility. We pick the right LLM for the job based on budget, latency, and other factors.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   11. November: LLM scaling laws for precision
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    I was originally tempted to pick Allen AI&#x27;s
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2411.15124&quot; rel=&quot;&quot;&gt;
    Tulu 3: Pushing Frontiers in Open Language Model Post-Training
   &lt;/a&gt;
   &lt;span&gt;
    paper because they included a detailed description of their Llama post-training methods and recipe, including ablation studies of DPO vs PPO, and a new preference alignment method called reinforcement learning with verifiable feedbacks, where they use verifiable queries where one can easily generate a ground truth answer (such as math and code questions) instead of a reward model.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    But after some internal debate, I ultimately decided to go with the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2411.04330&quot; rel=&quot;&quot;&gt;
    Scaling Laws for Precision
   &lt;/a&gt;
   &lt;span&gt;
    paper (November 2024) by Kumar and colleagues, as it provides a much-needed update for the Chinchilla scaling laws from the 2022
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot; rel=&quot;&quot;&gt;
    Training Compute-Optimal Large Language Models
   &lt;/a&gt;
   &lt;span&gt;
    paper that is used to determine compute-optimal LLM parameter counts and dataset sizes for pretraining.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In short, the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2411.04330&quot; rel=&quot;&quot;&gt;
    Precision Scaling Laws
   &lt;/a&gt;
   &lt;span&gt;
    paper (November 2024) extends Chinchilla&#x27;s scaling laws to account for training and inference in low-precision settings (16-bit and below), which have become very popular in recent years. For instance, this paper unifies various low-precision and quantization-related observations into a single functional form that predicts the added loss from both low-precision training and post-training quantization.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    11.1 Chinchilla scaling laws refresher
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The original Chinchilla scaling laws from the 2022
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot; rel=&quot;&quot;&gt;
    Training Compute-Optimal Large Language Models
   &lt;/a&gt;
   &lt;span&gt;
    paper model how LLM parameter counts (
   &lt;/span&gt;
   &lt;em&gt;
    N
   &lt;/em&gt;
   &lt;span&gt;
    ) and dataset sizes (
   &lt;/span&gt;
   &lt;em&gt;
    D
   &lt;/em&gt;
   &lt;span&gt;
    ) jointly affect the validation loss of an LLM and are used as guidelines for deciding upon the LLM and training dataset sizes.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    As a rule of thumb, the best tradeoff between dataset size
   &lt;/span&gt;
   &lt;em&gt;
    D
   &lt;/em&gt;
   &lt;span&gt;
    and the number of parameters N (when you have a fixed compute budget) is approximately
   &lt;/span&gt;
   &lt;em&gt;
    D/N
   &lt;/em&gt;
   &lt;span&gt;
    ≈ 20.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This data-parameter ratio is often referred to as &quot;Chinchilla-optimal&quot; because it yields lower validation loss than other ratios at the same total training cost.
  &lt;/p&gt;
  &lt;p&gt;
   Note that there are many modern exceptions, though; for example, the Llama 3 team trained on 15 trillion tokens, as discussed earlier, and for the 8B version, that&#x27;d be 15,000,000,000,000 ÷ 8,000,000,000 = 1,875, for example.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In my opinion, what&#x27;s more important than the exact data-parameter ratio is the takeaway that model and dataset sizes have to be scaled proportionally.
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    11.2 Low-precision training
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Before discussing (or rather summarizing) the low-precision scaling laws further, let me start with a very short primer on different numeric precision formats for LLM (or deep neural network) weights in general.
  &lt;/p&gt;
  &lt;p&gt;
   To the best of my knowledge, these were the precision formats used for training GPT 2 &amp; 3 and Llama 2 &amp; 3 models for comparison:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!grNH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!grNH!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png 424w, https://substackcdn.com/image/fetch/$s_!grNH!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png 848w, https://substackcdn.com/image/fetch/$s_!grNH!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png 1272w, https://substackcdn.com/image/fetch/$s_!grNH!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:824,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:109732,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;824&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!grNH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!grNH!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png 424w, https://substackcdn.com/image/fetch/$s_!grNH!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png 848w, https://substackcdn.com/image/fetch/$s_!grNH!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png 1272w, https://substackcdn.com/image/fetch/$s_!grNH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0240c9f4-42c6-48f5-ae19-dff4cab66d5d_1498x848.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Float32 was the standard 32-bit floating-point format widely used for training deep neural networks, as it offers a good balance between range and precision. Everything below float32 is nowadays considered low-precision (although the definition of &quot;low&quot; is kind of a moving goalpost similar to the &quot;large&quot; in large language models).
  &lt;/p&gt;
  &lt;p&gt;
   Float16, or half-precision, uses just 16 bits, saving memory and speeding up computation but providing a narrower dynamic range.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!g3Z5!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!g3Z5!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png 424w, https://substackcdn.com/image/fetch/$s_!g3Z5!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png 848w, https://substackcdn.com/image/fetch/$s_!g3Z5!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png 1272w, https://substackcdn.com/image/fetch/$s_!g3Z5!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:745,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;745&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!g3Z5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!g3Z5!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png 424w, https://substackcdn.com/image/fetch/$s_!g3Z5!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png 848w, https://substackcdn.com/image/fetch/$s_!g3Z5!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png 1272w, https://substackcdn.com/image/fetch/$s_!g3Z5!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F18b90caf-2e5c-4881-9614-1d2e8bc38d7a_1470x752.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Comparison between 32-bit and 16-bit floating point precision
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Bfloat16 (brain float 16) is also a 16-bit format but trades off some of float16’s precision for a larger exponent, allowing it to represent very large and very small numbers more effectively. As a result, bfloat16 can help avoid numeric overflow or underflow in deep learning applications, although its lower precision can still lead to rounding errors
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!j8VB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!j8VB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png 424w, https://substackcdn.com/image/fetch/$s_!j8VB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png 848w, https://substackcdn.com/image/fetch/$s_!j8VB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png 1272w, https://substackcdn.com/image/fetch/$s_!j8VB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:748,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;748&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!j8VB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!j8VB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png 424w, https://substackcdn.com/image/fetch/$s_!j8VB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png 848w, https://substackcdn.com/image/fetch/$s_!j8VB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png 1272w, https://substackcdn.com/image/fetch/$s_!j8VB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd49ae533-2056-4b47-9902-fb91bf7fe757_1526x784.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Comparison between regular 16-bit floating point and the popular 16-bit brain floating point precision
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you want to learn more about the different precision formats and their impact on LLM model behavior, you might like the lengthier intro in my previous
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/the-missing-bits-llama-2-weights&quot; rel=&quot;&quot;&gt;
    The Missing Bits: Llama 2 Weights Have Changed
   &lt;/a&gt;
   &lt;span&gt;
    article.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Also note that I am only showing 32- and 16-bit formats, whereas there&#x27;s currently a race to even lower precisions for training, e.g., the 8-bit format that was mentioned (as experimental) in the Llama 3 paper. (The
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3&quot; rel=&quot;&quot;&gt;
    DeepSeek-v3 model
   &lt;/a&gt;
   &lt;span&gt;
    that was released on Dec 26 was entirely pretrained in 8-bit floating point precision.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    11.3 Precision scaling laws takeaways
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   It&#x27;s a long and interesting paper that I recommend reading in full. However, to get to the main point, the researchers extend the original Chinchilla scaling laws by adding a &quot;precision&quot; factor P. Concretely, they reinterpret the model parameter count N as an &quot;effective parameter count&quot; that shrinks as the precision decreases. (For the mathematical formulas, defer to the paper.)
  &lt;/p&gt;
  &lt;p&gt;
   Plus, they added an extra term to capture how post-training quantization degrades model performance. (I realize that I didn&#x27;t write an intro to quantization, but due to the excessive length of this article already, I may have to defer this to another time.)
  &lt;/p&gt;
  &lt;p&gt;
   The figure below is a nice illustration that more pretraining data is not always better and can actually be harmful if models are quantized after training with very small precision (int3), which I found super interesting.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!I8pP!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!I8pP!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png 424w, https://substackcdn.com/image/fetch/$s_!I8pP!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png 848w, https://substackcdn.com/image/fetch/$s_!I8pP!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png 1272w, https://substackcdn.com/image/fetch/$s_!I8pP!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:848,&quot;width&quot;:1208,&quot;resizeWidth&quot;:651,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;456.9933774834437&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!I8pP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!I8pP!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png 424w, https://substackcdn.com/image/fetch/$s_!I8pP!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png 848w, https://substackcdn.com/image/fetch/$s_!I8pP!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png 1272w, https://substackcdn.com/image/fetch/$s_!I8pP!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe72da1a4-61fa-4405-8f04-0d95903d0dec_1208x848.png 1456w&quot; width=&quot;651&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      The effect of more training data on the validation loss for various post-quantization formats
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   So, as a takeaway from the figure above, one might say that models trained on more and more data (like Llama 3) become harder to quantize to lower precision formats after training due to being &quot;overtrained&quot; on too much data.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    11.4 Model scaling laws in 2025
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Besides providing a much-needed update to the Chinchilla scaling laws, the research on Precision Scaling Laws provides an interesting perspective on a critical challenge for 2025: as models like LLaMA-3 are trained on larger datasets, they may become harder to quantize to low precision formats like INT3 without performance loss.
  &lt;/p&gt;
  &lt;p&gt;
   This finding underscores the need to rethink the &quot;more data is better&quot; mindset, balancing dataset size with the practical constraints of efficient inference. It&#x27;s also an important insight for driving hardware optimization.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    One of the aspects that I think is often neglected in these scaling laws studies is the dataset&#x27;s quality. I think the pretraining data&#x27;s nature can have a significant impact. (More on that in the Phi-4 discussion below.)
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   12. December: Phi-4 and Learning from Synthetic Data
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    Several interesting models were released in the latter half of 2024, including the impressive
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3&quot; rel=&quot;&quot;&gt;
    DeepSeek-V3
   &lt;/a&gt;
   &lt;span&gt;
    on Christmas day. While it might not be the biggest model release, ultimately, I decided to go with Microsoft&#x27;s
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2412.08905&quot; rel=&quot;&quot;&gt;
    Phi-4 Technical Report
   &lt;/a&gt;
   &lt;span&gt;
    because it offers interesting insights into the use of synthetic data.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    12.1 Phi-4 performance
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2412.08905&quot; rel=&quot;&quot;&gt;
    Phi-4 Technical Report
   &lt;/a&gt;
   &lt;span&gt;
    (December 2024) by Abdin and colleagues describes the training of Microsoft&#x27;s latest 14-billion-parameter open-weight LLM. What makes Phi-4 particularly interesting is that it was trained primarily on synthetic data generated by GPT-4o. According to the benchmarks, it outperforms other LLMs of a similar size, including its predecessor, Phi-3, which was trained predominantly on non-synthetic data.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!CJnC!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!CJnC!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png 424w, https://substackcdn.com/image/fetch/$s_!CJnC!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png 848w, https://substackcdn.com/image/fetch/$s_!CJnC!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png 1272w, https://substackcdn.com/image/fetch/$s_!CJnC!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1039,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1039&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!CJnC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!CJnC!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png 424w, https://substackcdn.com/image/fetch/$s_!CJnC!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png 848w, https://substackcdn.com/image/fetch/$s_!CJnC!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png 1272w, https://substackcdn.com/image/fetch/$s_!CJnC!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86f231ce-50df-4052-8b97-7f9c13dbc154_1600x1142.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Performance of phi-4 compared to other models of similar and different sizes (annotated table from the phi-4 paper, https://arxiv.org/abs/2412.08905)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   I’m not entirely sure why the model performs worse on SimpleQA, as shown in the table above. But one possible explanation is that SimpleQA is a relatively new benchmark, released on October 30, 2024. Since it was developed by OpenAI as part of their evaluation suite, it might not have been included in the training data for GPT-4o or incorporated into the web-crawled datasets. Furthermore, because GPT-4o was used to generate the synthetic data for this evaluation, none of the models would have encountered SimpleQA during training. However, phi-4 might be overfitting to other benchmarks, which could explain its comparatively lower performance on this unseen SimpleQA dataset. Anyways, that&#x27;s just my hypothesis.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    12.2 Synthetic data learnings
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Let&#x27;s look at the dataset composition before summarizing some of the ablation studies presented in this paper.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!sqLf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!sqLf!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png 424w, https://substackcdn.com/image/fetch/$s_!sqLf!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png 848w, https://substackcdn.com/image/fetch/$s_!sqLf!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png 1272w, https://substackcdn.com/image/fetch/$s_!sqLf!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:594,&quot;width&quot;:1434,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;594&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!sqLf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!sqLf!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png 424w, https://substackcdn.com/image/fetch/$s_!sqLf!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png 848w, https://substackcdn.com/image/fetch/$s_!sqLf!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png 1272w, https://substackcdn.com/image/fetch/$s_!sqLf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F467b5f2d-088c-48c6-b33d-54dead7a7b28_1434x594.png 1456w&quot; width=&quot;1434&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Dataset mix for training phi-4 (annotated table from the phi-4 paper, https://arxiv.org/abs/2412.08905).
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The researchers observed that while synthetic data is generally beneficial, models trained exclusively on synthetic data performed poorly on knowledge-based benchmarks. To me, this raises the question: does synthetic data lack sufficient knowledge-specific information, or does it include a higher proportion of factual errors, such as those caused by hallucinations?
  &lt;/p&gt;
  &lt;p&gt;
   At the same time, the researchers found that increasing the number of training epochs on synthetic data boosted the performance more than just adding more web data, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!3bRk!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3bRk!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png 424w, https://substackcdn.com/image/fetch/$s_!3bRk!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png 848w, https://substackcdn.com/image/fetch/$s_!3bRk!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png 1272w, https://substackcdn.com/image/fetch/$s_!3bRk!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:595,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;595&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!3bRk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3bRk!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png 424w, https://substackcdn.com/image/fetch/$s_!3bRk!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png 848w, https://substackcdn.com/image/fetch/$s_!3bRk!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png 1272w, https://substackcdn.com/image/fetch/$s_!3bRk!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F55e20e2b-a020-48cf-94e9-1d097483d5e0_1600x654.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Model performance comparison for different synthetic/web dataset ratios.
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;em&gt;
      (Annotated  figure from the phi-4 paper, https://arxiv.org/abs/2412.08905).
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In summary, an excessive proportion of synthetic data in the mix negatively impacts knowledge-based performance. However, within a more balanced synthetic-to-web data mix, increasing the number of iterations (epochs) over the synthetic dataset is beneficial.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    12.3 Future importance of synthetic data
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The phi-4 technical report offers interesting insights into the use of synthetic data, namely that it can be highly beneficial for model pre-training. Especially since scaling laws are said to be plateauing concerning both model and dataset sizes (although the Llama 3 paper noted that they haven&#x27;t seen a convergence at the 15T token level yet), researchers and engineers are looking for alternative ways to keep pushing the envelope.
  &lt;/p&gt;
  &lt;p&gt;
   Of course, the refinement and addition of pre- and especially post-training techniques will likely remain one of the big needle movers. Still, I think that the use of synthetic data will be regarded as an effective way to either create a) pretrained base models with less data or b) create even better base models (think 15 trillion tokens from the Llama 3 dataset plus 40% synthetic data tokens added to it).
  &lt;/p&gt;
  &lt;p&gt;
   I see the use of high-quality data as analogous to transfer learning. Instead of pre-training a model on raw, unstructured internet data and refining it during post-training, leveraging (some) synthetic data generated by a high-quality model (such as GPT-4o, which has already undergone extensive refinement) may serve as a kind of jumpstart. In other words, the use of high-quality training data might enable the model to learn more effectively from the outset.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Conclusion &amp; Outlook
  &lt;/h1&gt;
  &lt;p&gt;
   I hope you found these research summaries useful! As always, this article ended up being longer than I originally intended. But let me close out with a relatively short and snappy section on my predictions (or expectations) for 2025.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Multimodal LLMs
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Last year, I predicted LLMs would become increasingly multimodal. Now, all major proprietary LLM providers offer multimodal (or at least image) support. So, the transformation is now fully underway, and we will also see more open-source efforts toward this.
  &lt;/p&gt;
  &lt;p&gt;
   Based on what I&#x27;ve seen and read, there&#x27;s definitely been a sharp increase in multimodal papers. Maybe followed by my open-source finetuning methods and resources; although I&#x27;d argue for many use cases, text-only suffices and will continue to suffice, and the main focus will be on developing better reasoning models (like o1 and the upcoming o3).
  &lt;/p&gt;
  &lt;p&gt;
   &lt;br/&gt;
   &lt;strong&gt;
    Computational efficiency
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Pretraining and using LLMs is relatively expensive. So, I expect that we are going to see more clever tricks to improve computational efficiency of LLMs in the foreseeable future. For reference, training the recent
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf&quot; rel=&quot;&quot;&gt;
    DeepSeek-v3 model
   &lt;/a&gt;
   &lt;span&gt;
    would cost $5 million dollars assuming the GPU rental sticker prices (and this doesn&#x27;t include hyperparameter tuning, failed runs, and personnel cost).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!npRQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!npRQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png 424w, https://substackcdn.com/image/fetch/$s_!npRQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png 848w, https://substackcdn.com/image/fetch/$s_!npRQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png 1272w, https://substackcdn.com/image/fetch/$s_!npRQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:615,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;615&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!npRQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!npRQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png 424w, https://substackcdn.com/image/fetch/$s_!npRQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png 848w, https://substackcdn.com/image/fetch/$s_!npRQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png 1272w, https://substackcdn.com/image/fetch/$s_!npRQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb640d0f0-5ec4-442d-a766-6c2e9477122f_1600x676.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Back-off-the-envelope calculation from the DeepSeek-v3 report, https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    By the way, according to the official Meta AI
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md&quot; rel=&quot;&quot;&gt;
    Llama 3 model card
   &lt;/a&gt;
   &lt;span&gt;
    , Llama 3 405B used even ~10x more compute (30.84 million GPU hours vs 2.66 million GPU hours).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Popular examples of techniques to make LLMs efficient (although not all apply during training) include a mixture of experts (as discussed in my part 1 article), grouped-query attention as found in Llama models, and many others. Another interesting one is the use of multihead latent attention, as found in DeepSeek models, to make KV-caching in multihead attention more efficient.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Another interesting recent route is targeting the model input. For instance, the recently proposed
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2412.09871&quot; rel=&quot;&quot;&gt;
    Byte Latent Transformer
   &lt;/a&gt;
   &lt;span&gt;
    improves efficiency by dynamically encoding bytes into entropy-based patches, optimizing compute for scalability and faster inference without tokenization.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    State space models
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   You may have noticed that I didn&#x27;t cover state space models this year. That’s because my current focus is primarily on transformer-based LLMs. While I find state space models super interesting, they still seem quite experimental at this stage. Besides, transformers continue to demonstrate exceptional performance across a wide range of tasks, making it not very tempting to consider alternatives.
  &lt;/p&gt;
  &lt;p&gt;
   However, that doesn&#x27;t mean there hasn&#x27;t been any progress on the state space model front. I&#x27;ve seen a bunch of interesting papers in this area. And one interesting trend I noticed is that they are now all more or less hybrid models integrating self-attention from transformer models. For example,
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.12570&quot; rel=&quot;&quot;&gt;
      Jamba-1.5: Hybrid Transformer-Mamba Models at Scale
     &lt;/a&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.15237&quot; rel=&quot;&quot;&gt;
      The Mamba in the Llama: Distilling and Accelerating Hybrid Models
     &lt;/a&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      and
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot; rel=&quot;&quot;&gt;
      Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/p&gt;
    &lt;p&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   In that sense, they are also getting more computationally expensive. With efficiency tweaks to transformer-based LLMs and adding attention to state space models, they will probably meet somewhere in the middle if the current trends continue.  It&#x27;s definitely an interesting field of research to watch though.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    LLM progress through scaling
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Towards the end of the year, there was also some discussion of LLM scaling &quot;being over&quot; as there is no more internet data. This discussion came from a NeurIPS talk by Ilya Sutskever (one of OpenAI&#x27;s co-founders and co-author on the GPT papers), but unfortunately, I couldn&#x27;t attend the conference this year, so I am not familiar with the details.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In any case, it&#x27;s an interesting point because the internet grows exponentially fast. I could
   &lt;/span&gt;
   &lt;a href=&quot;https://edgedelta.com/company/blog/how-much-data-is-created-per-day&quot; rel=&quot;&quot;&gt;
    find resources saying that it grows
   &lt;/a&gt;
   &lt;span&gt;
    &quot;15.87 terabytes of data daily.&quot; Sure, the challenge is that not all of the data is text or useful for LLM training. However, as we have seen with Phi-4, there are still a lot of opportunities in data curation and refinement that can help make some leaps from training data alone.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   I agree with the diminishing returns of scaling via data, though. I expect that the gains will be smaller as we are probably heading towards plateauing. It&#x27;s not a bad thing, though, as it brings other improvement opportunities.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    One notable area where I expect a lot of future gains to come from is post-training. We&#x27;ve already seen a taste of these developments in this area with recent LLM releases, as I wrote about last summer in my
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training&quot; rel=&quot;&quot;&gt;
    New LLM Pre-training and Post-training Paradigms
   &lt;/a&gt;
   &lt;span&gt;
    article.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    What I am looking forward to in 2025
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   I really enjoyed tinkering and (re)implementing the various Llama models (3, 3.1, and 3.2) this year. I am really looking forward to the Llama 4 release, which hopefully also comes in small and convenient sizes that I can experiment with on my laptop or affordable cloud GPUs.
  &lt;/p&gt;
  &lt;p&gt;
   Moreover, it&#x27;s also the year where I want to experiment more with special-purpose model finetuning rather than generating general chatbots (it&#x27;s already pretty crowded in this space). We&#x27;ve seen a bit of that with various code and math models (the recent Qwen 2.5 Coder and Qwen 2.5 Math come to mind, which I unfortunately haven&#x27;t had a chance to cover in this report yet).
  &lt;/p&gt;
  &lt;p&gt;
   In any case, I could keep on going with this wish list and plans, as 2025 will be another interesting and fast-moving year! It&#x27;s definitely not going to be boring, that&#x27;s for sure!
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . (I am confident that you&#x27;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Build a Large Language Model (From Scratch) now
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      available on Amazon
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Your support means a great deal! Thank you!
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Noteworthy AI Research Papers of 2024 (Part One) </title>
<link>https://magazine.sebastianraschka.com/p/ai-research-papers-2024-part-1</link>
<pubDate>Tue, 31 Dec 2024 04:21:42 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   To kick off the year, I&#x27;ve finally been able to complete the draft of this AI Research Highlights of 2024 article. It covers a variety of topics, from mixture-of-experts models to new LLM scaling laws for precision.
  &lt;/p&gt;
  &lt;p&gt;
   Reflecting on all the major research highlights of 2024 would probably require writing an entire book. It&#x27;s been an extraordinarily productive year, even for such a fast-moving field. To keep things reasonably concise, I decided to focus exclusively on LLM research this year. But even then, how does one choose a subset of papers from such an eventful year? The simplest approach I could think of was to highlight one paper per month: January through December 2024.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, in this article, I&#x27;ll share research papers that I personally found fascinating, impactful, or, ideally, both. However, note that this article is just
   &lt;/span&gt;
   &lt;em&gt;
    Part One
   &lt;/em&gt;
   &lt;span&gt;
    , focusing on the first half of 2024 from January through June.
   &lt;/span&gt;
   &lt;em&gt;
    Part 2
   &lt;/em&gt;
   &lt;span&gt;
    of this series, covering July to December, will be shared later in January.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The selection criteria are admittedly subjective, based on what stood out to me this year. I&#x27;ve also aimed for some variety, so it&#x27;s not all just about LLM model releases.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you&#x27;re looking for a broader list of AI research papers, feel free to check out my earlier article (
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list&quot; rel=&quot;&quot;&gt;
    LLM Research Papers: The 2024 List
   &lt;/a&gt;
   &lt;span&gt;
    ).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     For those who read my
    &lt;/span&gt;
    &lt;a href=&quot;https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list&quot; rel=&quot;&quot;&gt;
     previous article
    &lt;/a&gt;
    &lt;span&gt;
     , I’m happy to share that I’m already feeling a bit better and slowly but steadily recovering! I also want to express my heartfelt thanks for all the kind wishes and support. It truly meant the world to me and helped me through some tough days!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Happy new year and happy reading!
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   1. January: Mixtral&#x27;s Mixture of Experts Approach
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    Only a few days into January 2024, the Mistral AI team shared the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2401.04088&quot; rel=&quot;&quot;&gt;
    Mixtral of Experts
   &lt;/a&gt;
   &lt;span&gt;
    paper (8 Jan 2024), which described Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) model.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The paper and model were both very influential at the time, as Mixtral 8x7B was (one of) the first open-weight MoE LLMs with an impressive performance: it outperformed Llama 2 70B and GPT-3.5 across various benchmarks.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1.1 Understanding MoE models
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   An MoE, or Mixture of Experts, is an ensemble model that combines several smaller &quot;expert&quot; subnetworks inside the GPT-like decoder architecture. Each subnetwork is said to be responsible for handling different types of tasks or, more concretely, tokens. The idea here is that by using multiple smaller subnetworks instead of one large network, MoEs aim to allocate computational resources more efficiently.
  &lt;/p&gt;
  &lt;p&gt;
   In particular, in Mixtral 8x7B, is to replace each feed-forward module in a transformer architecture with 8 expert layers, as illustrated in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!xyDu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xyDu!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png 424w, https://substackcdn.com/image/fetch/$s_!xyDu!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png 848w, https://substackcdn.com/image/fetch/$s_!xyDu!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png 1272w, https://substackcdn.com/image/fetch/$s_!xyDu!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1108,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1108&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!xyDu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xyDu!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png 424w, https://substackcdn.com/image/fetch/$s_!xyDu!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png 848w, https://substackcdn.com/image/fetch/$s_!xyDu!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png 1272w, https://substackcdn.com/image/fetch/$s_!xyDu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb6a8621c-2193-4d5f-b140-e8b669ccbc75_1490x1134.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated transformer architecture from Attention Is All You Need, https://arxiv.org/abs/1706.03762
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &quot;Sparse&quot; in the context of a &quot;Sparse Mixture of Experts&quot; refers to the fact that at any given time, only a subset of the expert layers (typically 1 or 2 out of the 8 in Mixtral 8x7B) are actively used for processing a token.
  &lt;/p&gt;
  &lt;p&gt;
   As illustrated in the figure above, the subnetworks replace the feed-forward module in the LLM. A feed-forward module is essentially a multilayer perceptron. In PyTorch-like pseudocode, it essentially looks like this:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;class FeedForward(torch.nn.Module):   
    def __init__(self, embed_dim, coef):        
        super().__init__()
        self.layers = nn.Sequential(
            torch.nn.Linear(embed_dim, coef*embed_dim),
            torch.nn.ReLU(),
            torch.nn.Linear(coef*n_embed, embed_dim),
            torch.nn.Dropout(dropout)
        )    

    def forward(self, x):
       return self.layers(x)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    In addition, there is also a
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Router
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    module (also known as a
   &lt;/span&gt;
   &lt;em&gt;
    gating network
   &lt;/em&gt;
   &lt;span&gt;
    ) that redirects each of the token embeddings to the 8 expert feed-forward modules, where only a subset of these experts are active at a time.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Since there are 11 more papers to cover in this article, I want to keep this description of the Mixtral model brief. However, you can find additional details in my previous article,
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/i/141130005/mixtral-of-experts&quot; rel=&quot;&quot;&gt;
    Model Merging, Mixtures of Experts, and Towards Smaller LLMs
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    1.2 The relevance of MoE models today
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   At the beginning of the year, I would have thought that open-weight MoE models would be more popular and widely used than they are today. While they are not irrelevant, many state-of-the-art models still rely on dense (traditional) LLMs rather than MoEs though, e.g., Llama 3, Qwen 2.5, Gemma 2, etc. However, it is, of course, impossible to say what proprietary architectures like GPT-4, Gemini, and Claude are based on; they might as well be using MoE under the hood.
  &lt;/p&gt;
  &lt;p&gt;
   In any case, MoE architectures are still relevant, especially as they offer a way to scale large language models efficiently by activating only a subset of the model&#x27;s parameters for each input, thus reducing computation costs without sacrificing model capacity.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    By the way, after writing this article, there was a nice surprise release of
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf&quot; rel=&quot;&quot;&gt;
    the very well-performing DeepSeek-V3 model in December
   &lt;/a&gt;
   &lt;span&gt;
    , which uses a MoE architecture. So, yes, MoEs continue to be very relevant!
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   2. February: Weight-decomposed LoRA
  &lt;/h1&gt;
  &lt;p&gt;
   If you are finetuning open-weight LLMs, chances are high that you have been using low-rank adaptation (LoRA), a method for parameter-efficient LLM finetuning, at some point.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you are new to LoRA, I have written a previous article on
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms&quot; rel=&quot;&quot;&gt;
    Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)
   &lt;/a&gt;
   &lt;span&gt;
    that you might helpful, and I have a from-scratch code implementation in Appendix D of my
   &lt;/span&gt;
   &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
    Build A Large Language Model (From Scratch)
   &lt;/a&gt;
   &lt;span&gt;
    book.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Since LoRA is such a popular and widely used method, and since I had so much fun implementing and playing with a newer variant, my pick for February is
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2402.09353&quot; rel=&quot;&quot;&gt;
    DoRA: Weight-Decomposed Low-Rank Adaptation
   &lt;/a&gt;
   &lt;span&gt;
    (February 2024) by Liu and colleagues.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.2 LoRA Recap
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Before introducing DoRA, here’s a quick LoRA refresher:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Full finetuning updates each large weight matrix
   &lt;/span&gt;
   &lt;em&gt;
    W
   &lt;/em&gt;
   &lt;span&gt;
    in an LLM by computing a large weight update matrix
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    . LoRA approximates
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    as the product of two smaller matrices
   &lt;/span&gt;
   &lt;em&gt;
    A
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    B
   &lt;/em&gt;
   &lt;span&gt;
    . So, Instead of
   &lt;/span&gt;
   &lt;em&gt;
    W + ΔW
   &lt;/em&gt;
   &lt;span&gt;
    , we have
   &lt;/span&gt;
   &lt;em&gt;
    W + A.B
   &lt;/em&gt;
   &lt;span&gt;
    . This greatly reduces computational and memory overhead.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The figure below illustrates these formulas for full finetuning (left) and LoRA (right) side by side.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!ap7E!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ap7E!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png 424w, https://substackcdn.com/image/fetch/$s_!ap7E!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png 848w, https://substackcdn.com/image/fetch/$s_!ap7E!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png 1272w, https://substackcdn.com/image/fetch/$s_!ap7E!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:639,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;639&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!ap7E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ap7E!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png 424w, https://substackcdn.com/image/fetch/$s_!ap7E!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png 848w, https://substackcdn.com/image/fetch/$s_!ap7E!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png 1272w, https://substackcdn.com/image/fetch/$s_!ap7E!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F014307aa-3e4d-47d1-a99f-37892d943c97_1600x702.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An illustration of regular finetuning (left) and LoRA finetuning (right).
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.2 From LoRA to DoRA
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    In
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2402.09353&quot; rel=&quot;&quot;&gt;
    DoRA: Weight-Decomposed Low-Rank Adaptation
   &lt;/a&gt;
   &lt;span&gt;
    (February 2024), Liu and colleagues.extend LoRA by first decomposing a pretrained weight matrix into two parts: a magnitude vector m and a directional matrix
   &lt;/span&gt;
   &lt;em&gt;
    V
   &lt;/em&gt;
   &lt;span&gt;
    . This decomposition is rooted in the idea that any vector can be represented by its length (magnitude) and direction (orientation), and here we apply it to each column vector of a weight matrix. Once we have m and
   &lt;/span&gt;
   &lt;em&gt;
    V
   &lt;/em&gt;
   &lt;span&gt;
    , DoRA applies LoRA-style low-rank updates only to the directional matrix
   &lt;/span&gt;
   &lt;em&gt;
    V
   &lt;/em&gt;
   &lt;span&gt;
    , while allowing the magnitude vector m to be trained separately.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!smvV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!smvV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png 424w, https://substackcdn.com/image/fetch/$s_!smvV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png 848w, https://substackcdn.com/image/fetch/$s_!smvV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png 1272w, https://substackcdn.com/image/fetch/$s_!smvV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1060,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1060&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!smvV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!smvV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png 424w, https://substackcdn.com/image/fetch/$s_!smvV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png 848w, https://substackcdn.com/image/fetch/$s_!smvV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png 1272w, https://substackcdn.com/image/fetch/$s_!smvV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe39fff89-8c1b-4e06-80c9-f2ca375af019_1600x1165.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated illustration from the DoRA paper (https://arxiv.org/abs/2402.09353)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   This two-step approach gives DoRA more flexibility than standard LoRA. Rather than uniformly scaling both magnitude and direction as LoRA tends to do, DoRA can make subtle directional adjustments without necessarily increasing the magnitude. The result is improved performance and robustness, as DoRA can outperform LoRA even when using fewer parameters and is less sensitive to the choice of rank.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Again, I am keeping this section brief since there are 10 more to go, but if you are interested in additional details, I dedicated a whole article to this method earlier this year:
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch&quot; rel=&quot;&quot;&gt;
    Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.3 The future of LoRA and LoRA-like methods
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    DoRA is a small, logical improvement over the original LoRA method. While it hasn’t been widely adopted yet, it adds minimal complexity and is worth considering the next time you finetune an LLM. In general, I expect LoRA and similar methods to remain popular. For example, Apple recently mentioned in their
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2407.21075&quot; rel=&quot;&quot;&gt;
    Apple Intelligence Foundation Language Models
   &lt;/a&gt;
   &lt;span&gt;
    paper that they use LoRA for on-device task specialization of LLMs.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   3. March: Tips for Continually Pretraining LLMs
  &lt;/h1&gt;
  &lt;p&gt;
   As far as I can tell, instruction-finetuning is the most popular form of finetuning by LLM practitioners. The goal here is to get openly available LLMs to better follow instructions or specialize these LLMs on subsets or new instructions.
  &lt;/p&gt;
  &lt;p&gt;
   However, when it comes to taking in new knowledge, continued pretraining (sometimes also referred to continually pretraining) is the way to go.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In this section, I want to briefly summarize the refreshingly straightforward
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2403.08763&quot; rel=&quot;&quot;&gt;
    Simple and Scalable Strategies to Continually Pre-train Large Language Models
   &lt;/a&gt;
   &lt;span&gt;
    (March 2024) paper by Ibrahim and colleagues.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3.1 Simple techniques work
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    This 24-page
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2403.08763&quot; rel=&quot;&quot;&gt;
    Continually Pre-train Large Language Models
   &lt;/a&gt;
   &lt;span&gt;
    paper reports a large number of experiments and comes with countless figures, which is very thorough for today&#x27;s standards.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   What were the main tips for applying continued pretraining successfully?
  &lt;/p&gt;
  &lt;p&gt;
   1. Simple re-warming and re-decaying the learning rate.
  &lt;/p&gt;
  &lt;p&gt;
   2. Adding a small portion (e.g., 5%) of the original pretraining data to the new dataset to prevent catastrophic forgetting. Note that smaller fractions like 0.5% and 1% were also effective.
  &lt;/p&gt;
  &lt;p&gt;
   To be a bit more concrete regarding point 1, re-warming and re-decaying, this means we employ the exact same learning rate schedule that was used during the initial pretraining stage of an LLM as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!6vK_!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!6vK_!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png 424w, https://substackcdn.com/image/fetch/$s_!6vK_!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png 848w, https://substackcdn.com/image/fetch/$s_!6vK_!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png 1272w, https://substackcdn.com/image/fetch/$s_!6vK_!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/eaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:504,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;504&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!6vK_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!6vK_!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png 424w, https://substackcdn.com/image/fetch/$s_!6vK_!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png 848w, https://substackcdn.com/image/fetch/$s_!6vK_!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png 1272w, https://substackcdn.com/image/fetch/$s_!6vK_!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Feaf69f6f-effb-4f77-9ffb-4eb7bbdbdf8c_1600x554.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      A schedule for continued pretraining. Figure based on Build a Large Language Model From Scratch, https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   As far as I know, the re-warming and re-decaying, as well as adding original pretraining data to the new data, is more or less common knowledge. However, I really appreciate that the researchers took the time to formally test this method in this very detailed 24-page report.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you are interested in additional details, I discussed this paper more thoroughly in my previous
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms&quot; rel=&quot;&quot;&gt;
    Tips for LLM Pretraining and Evaluating Reward Models article
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    3.2 Will these simple techniques continue to work?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    I have no reason to believe that these methods will not continue to work for future LLMs. However, it is important to note that pretraining pipelines have become more sophisticated in recent months, consisting of multiple stages, including short- and long-context pretraining. (I’ve written more about it in
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training&quot; rel=&quot;&quot;&gt;
    New LLM Pre-training and Post-training Paradigms
   &lt;/a&gt;
   &lt;span&gt;
    ).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   So, for optimal results, the recipes suggested in this paper may need to be tweaked under certain circumstances.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   4. April: DPO or PPO for LLM alignment, or both?
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    April is a tough choice. For instance,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2404.19756&quot; rel=&quot;&quot;&gt;
    Kolmogorov-Arnold Networks
   &lt;/a&gt;
   &lt;span&gt;
    made a big wave that month. But as far as I can tell, the excitement fizzled out pretty quickly. This is likely because their theoretical guarantees are difficult to implement practically, they lack competitive results or benchmarks, and other architectures are much more scalable.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, instead, my pick for April goes to a more practical paper:
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2404.10719&quot; rel=&quot;&quot;&gt;
    Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
   &lt;/a&gt;
   &lt;span&gt;
    (April 2024) by Xu and colleagues.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.1 RLHF-PPO and DPO: What Are They?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Before summarizing the paper itself, here&#x27;s an overview of Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), both popular methods in aligning LLMs via Reinforcement Learning with Human Feedback (RLHF). RLHF is the method of choice to align LLMs with human preferences, improving the quality but also the safety of their responses.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!xaW9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xaW9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xaW9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xaW9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xaW9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:144,&quot;width&quot;:1024,&quot;resizeWidth&quot;:450,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;63.28125&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!xaW9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xaW9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg 424w, https://substackcdn.com/image/fetch/$s_!xaW9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg 848w, https://substackcdn.com/image/fetch/$s_!xaW9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!xaW9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F96c470e8-0e63-4553-bfb9-4f82b1a821fd_1024x144.jpeg 1456w&quot; width=&quot;450&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      The typical (simplified) LLM training lifecycle.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Traditionally, RLHF-PPO has been a crucial step in training LLMs for models and platforms like InstructGPT and ChatGPT. However, DPO started gaining traction last year due to its simplicity and effectiveness. In contrast to RLHF-PPO, DPO does not require a separate reward model. Instead, it directly updates the LLM using a classification-like objective. Many LLMs now utilize DPO, although comprehensive comparisons with PPO are lacking.
  &lt;/p&gt;
  &lt;p&gt;
   Below are two resources on RLHF and DPO I developed and shared earlier this year:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives&quot; rel=&quot;&quot;&gt;
      LLM Training: RLHF and Its Alternatives
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb&quot; rel=&quot;&quot;&gt;
      Direct Preference Optimization (DPO) for LLM Alignment (From Scratch)
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.2 PPO Typically Outperforms DPO
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2404.10719&quot; rel=&quot;&quot;&gt;
    Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
   &lt;/a&gt;
   &lt;span&gt;
    is a well-written paper with numerous experiments and results. The key conclusions are that PPO tends to outperform DPO, and that DPO is inferior when dealing with out-of-distribution data.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Here, out-of-distribution data means the language model was previously trained on instruction data (via supervised finetuning) that differs from the preference data used for DPO. For instance, a model might be trained on the general Alpaca dataset before undergoing DPO finetuning on a different preference-labeled dataset. (However, one way to improve DPO on such out-of-distribution data is to first conduct a supervised instruction-finetuning step using the preference dataset, and then perform DPO finetuning.)
  &lt;/p&gt;
  &lt;p&gt;
   The main findings are summarized in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!xFjM!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xFjM!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png 424w, https://substackcdn.com/image/fetch/$s_!xFjM!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png 848w, https://substackcdn.com/image/fetch/$s_!xFjM!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png 1272w, https://substackcdn.com/image/fetch/$s_!xFjM!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:835,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;835&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!xFjM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xFjM!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png 424w, https://substackcdn.com/image/fetch/$s_!xFjM!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png 848w, https://substackcdn.com/image/fetch/$s_!xFjM!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png 1272w, https://substackcdn.com/image/fetch/$s_!xFjM!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5ade88a3-4ca5-43f4-92af-4bdfce84dc42_1600x918.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Annotated table from the Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study (https://arxiv.org/abs/2404.10719) paper.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.3 How are PPO and DPO used today?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   PPO might have a slight edge when it comes to the raw modeling performance of the resulting LLM. However, DPO is much easier to implement and computationally more efficient to apply (you don&#x27;t have to train and use a separate reward model, after all). Hence, to the best of my knowledge, DPO is also much more widely used in practice than RLHF-PPO.
  &lt;/p&gt;
  &lt;p&gt;
   One interesting example is Meta AI&#x27;s Llama models. While Llama 2 was trained with RLHF-PPO, the newer Llama 3 models used DPO.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Interestingly, recent models even use both PPO and DPO nowadays. Recent examples include
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2407.21075&quot; rel=&quot;&quot;&gt;
    Apple&#x27;s Foundation Models
   &lt;/a&gt;
   &lt;span&gt;
    and Allen AI&#x27;s
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2411.15124&quot; rel=&quot;&quot;&gt;
    Tulu 3
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   5. May: LoRA learns less and forgets less
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    I found another LoRA paper this year particularly interesting (this is the last LoRA paper in this 12-paper selection, I promise!). I wouldn&#x27;t call it groundbreaking, but I really like it since it formalizes some of the common knowledge around finetuning LLMs with (and without) LoRA:
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2405.09673&quot; rel=&quot;&quot;&gt;
    LoRA Learns Less and Forgets Less
   &lt;/a&gt;
   &lt;span&gt;
    (May 2024) by Biderman and colleagues.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2405.09673&quot; rel=&quot;&quot;&gt;
    LoRA Learns Less and Forgets Less
   &lt;/a&gt;
   &lt;span&gt;
    is an empirical study comparing low-rank adaptation (LoRA) to full finetuning on large language models (LLMs), focusing on two domains (programming and mathematics) and two tasks (instruction finetuning and continued pretraining). Check out the February section above if you&#x27;d like a refresher on LoRA before proceeding.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    5.1 LoRA learns less
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2405.09673&quot; rel=&quot;&quot;&gt;
    LoRA Learns Less and Forgets Less
   &lt;/a&gt;
   &lt;span&gt;
    study shows LoRA learns noticeably less than full finetuning, especially in tasks like coding, where new knowledge needs to be acquired. The gap is smaller when only instruction finetuning is performed. This suggests that pretraining on new data (learning new knowledge) benefit more from full finetuning than converting a pretrained model into an instruction follower.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!-sha!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-sha!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png 424w, https://substackcdn.com/image/fetch/$s_!-sha!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png 848w, https://substackcdn.com/image/fetch/$s_!-sha!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png 1272w, https://substackcdn.com/image/fetch/$s_!-sha!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:724,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;724&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!-sha!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-sha!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png 424w, https://substackcdn.com/image/fetch/$s_!-sha!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png 848w, https://substackcdn.com/image/fetch/$s_!-sha!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png 1272w, https://substackcdn.com/image/fetch/$s_!-sha!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F68aeccb5-f78d-4fad-9ec5-2fc2f46b8340_1600x796.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;span&gt;
       Full finetuning vs LoRA. The performance is measured on HumanEval, which is a dataset consisting of 164 coding challenges. Annotated figures from LoRA Learns Less and Forgets Less,
      &lt;/span&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2405.09673&quot; rel=&quot;&quot;&gt;
       https://arxiv.org/abs/2405.09673
      &lt;/a&gt;
      &lt;span&gt;
       .
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   There are some more nuances, though. For math tasks, for example, the difference between LoRA and full finetuning shrinks. This may be because math problems are more familiar to the LLM, and they likely encountered similar problems during pretraining. In contrast, coding involves a more distinct domain, requiring more new knowledge. Thus, the farther a new task is from the model’s pretraining data, the more beneficial full finetuning becomes in terms of learning capacity.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    5.2 LoRA forgets less
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   When examining how much previously acquired knowledge is lost, LoRA consistently forgets less. This is particularly clear when adapting to data far from the source domain (e.g., coding). With coding tasks, full finetuning leads to significant forgetting, while LoRA preserves more original capabilities. In math, where the model’s original knowledge was already closer to the new task, the difference is less pronounced.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!gtHU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!gtHU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png 424w, https://substackcdn.com/image/fetch/$s_!gtHU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png 848w, https://substackcdn.com/image/fetch/$s_!gtHU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png 1272w, https://substackcdn.com/image/fetch/$s_!gtHU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:602,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;602&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!gtHU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!gtHU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png 424w, https://substackcdn.com/image/fetch/$s_!gtHU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png 848w, https://substackcdn.com/image/fetch/$s_!gtHU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png 1272w, https://substackcdn.com/image/fetch/$s_!gtHU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F84c8af4d-e189-483c-a2f3-a7ecf3812584_1600x662.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;span&gt;
       Full finetuning vs LoRA on the original source tasks after training on programming data. Annotated figures from LoRA Learns Less and Forgets Less,
      &lt;/span&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2405.09673&quot; rel=&quot;&quot;&gt;
       https://arxiv.org/abs/2405.09673
      &lt;/a&gt;
      &lt;span&gt;
       .
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    5.3 The LoRA trade-off
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Overall, there is a trade-off: full finetuning is better for absorbing new knowledge from more distant domains but leads to more forgetting of previously learned tasks. LoRA, by changing fewer parameters, learns less new information but retains more of the original capabilities.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    5.4 Future ​approaches to finetuning LLMs
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The study primarily compares LoRA to full finetuning. In practice, LoRA has gained popularity because it is far more resource-efficient than full finetuning. In many cases, full finetuning is simply not feasible due to hardware constraints. Moreover, if you only need to address specialized applications, LoRA alone may be sufficient. Since LoRA adapters can be stored separately from the base LLM, it&#x27;s easy to preserve the original capabilities while adding new ones. Additionally, it&#x27;s possible to combine both methods by using full finetuning for knowledge updates and LoRA for subsequent specialization.
  &lt;/p&gt;
  &lt;p&gt;
   In short, I think both methods will continue to be very relevant in the upcoming year(s). It&#x27;s more about using the right approach for the task at hand.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   6. June: The 15 Trillion Token FineWeb Dataset
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2406.17557&quot; rel=&quot;&quot;&gt;
    The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale
   &lt;/a&gt;
   &lt;span&gt;
    (June 2024) paper by Penedo and colleagues describes the creation of a 15 trillion token dataset for LLMs and making it publicly available, including
   &lt;/span&gt;
   &lt;a href=&quot;https://huggingface.co/datasets/HuggingFaceFW/fineweb&quot; rel=&quot;&quot;&gt;
    a link to download the dataset
   &lt;/a&gt;
   &lt;span&gt;
    and a code repository (
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/huggingface/datatrove/blob/main/examples/fineweb.py&quot; rel=&quot;&quot;&gt;
    datatrove/examples/fineweb.py
   &lt;/a&gt;
   &lt;span&gt;
    ) to reproduce the dataset preparation steps.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    6.1 Comparison to other datasets
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Since several other large datasets for LLM pretraining are available, what&#x27;s so special about this one? Other datasets are comparatively small: RefinedWeb (500B tokens), C4 (172B tokens), the Common Crawl-based part of Dolma 1.6 (3T tokens) and 1.7 (1.2T tokens), The Pile (340B tokens), SlimPajama (627B tokens), the deduplicated variant of RedPajama (20T tokens), English CommonCrawl section of Matrix (1.3T tokens), English CC-100 (70B tokens), Colossal-OSCAR (850B tokens).
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For example, ~360 billion tokens are only suited for small LLMs (for instance, 1.7 B, according to the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot; rel=&quot;&quot;&gt;
    Chinchilla scaling laws
   &lt;/a&gt;
   &lt;span&gt;
    ). On the other hand, the 15 trillion tokens in the FineWeb dataset should be optimal for models up to 500 billion parameters according to the Chinchilla scaling laws. (Note that
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/togethercomputer/RedPajama-Data&quot; rel=&quot;&quot;&gt;
    RedPajama
   &lt;/a&gt;
   &lt;span&gt;
    contains 20 trillion tokens, but the researchers found that models trained on RedPajama result in poorer quality than FineWeb due to the different filtering rules applied.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!j-61!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!j-61!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png 424w, https://substackcdn.com/image/fetch/$s_!j-61!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png 848w, https://substackcdn.com/image/fetch/$s_!j-61!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png 1272w, https://substackcdn.com/image/fetch/$s_!j-61!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:751,&quot;width&quot;:1456,&quot;resizeWidth&quot;:682,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;351.77335164835165&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!j-61!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!j-61!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png 424w, https://substackcdn.com/image/fetch/$s_!j-61!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png 848w, https://substackcdn.com/image/fetch/$s_!j-61!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png 1272w, https://substackcdn.com/image/fetch/$s_!j-61!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F988590ff-fab6-4b18-af4d-2945bc529c34_1600x825.png 1456w&quot; width=&quot;682&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of the dataset sizes used to pretrain LLMs over the years. Note that this is simply a general reference and is not directly related to the FineWeb paper or the Chinchilla scaling laws paper.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    In short, the FineWeb dataset (English-only) makes it theoretically possible for researchers and practitioners to train large-scale LLMs. (Side note: The
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2407.21783&quot; rel=&quot;&quot;&gt;
    Llama 3 models
   &lt;/a&gt;
   &lt;span&gt;
    with 8B, 70B, and 405B sizes were trained on 15 trillion tokens as well, but Meta AI&#x27;s training dataset is not publicly available.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    6.2 Principled dataset development
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    In addition, the paper contains principled ablation studies and insights into how the filtering rules were developed and applied to arrive at the FineWeb dataset (starting from the
   &lt;/span&gt;
   &lt;a href=&quot;https://commoncrawl.org/&quot; rel=&quot;&quot;&gt;
    CommonCrawl
   &lt;/a&gt;
   &lt;span&gt;
    web corpus). In short, for each filtering rule they tried, they took a 360 billion token random sample from the original and the filtered data and then trained a small 1.71 billion parameter Llama-like model to see whether the filtering rule is beneficial or not based on the models&#x27; performances on standard benchmarks such as HellaSwag, ARC, MMLU, and others.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!JYkT!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JYkT!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png 424w, https://substackcdn.com/image/fetch/$s_!JYkT!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png 848w, https://substackcdn.com/image/fetch/$s_!JYkT!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png 1272w, https://substackcdn.com/image/fetch/$s_!JYkT!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1059,&quot;width&quot;:1456,&quot;resizeWidth&quot;:618,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;449.49313186813185&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!JYkT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JYkT!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png 424w, https://substackcdn.com/image/fetch/$s_!JYkT!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png 848w, https://substackcdn.com/image/fetch/$s_!JYkT!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png 1272w, https://substackcdn.com/image/fetch/$s_!JYkT!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fba7ca02d-dcb3-44a2-89fe-50578eb670b8_1600x1164.png 1456w&quot; width=&quot;618&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    6.3 The relevance of FineWeb today
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Overall, while pretraining multi-billion parameter LLMs may still be beyond the reach of most research labs and companies, this dataset is a substantial step toward democratizing the study and development of LLMs. In summary, this paper represents a commendable effort and introduces a valuable public resource for advancing pretraining in LLMs.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   July to December
  &lt;/h1&gt;
  &lt;p&gt;
   I hope you found the research summaries useful! Since I am still recovering from my injury, and since it would have been an excessively long article anyway, I decided to split this year&#x27;s review article into two parts.
  &lt;/p&gt;
  &lt;p&gt;
   The second (July to December) part is actually even more exciting (for me personally), as I am discussing the more recent papers on scaling laws, reproducing O1, and the role of synthetic data in LLM training. In addition, I will also share my thoughts for 2025 and what I expect to be on the horizon. Stay tuned!
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . (I am confident that you&#x27;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Build a Large Language Model (From Scratch) now
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      available on Amazon
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Your support means a great deal! Thank you!
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> LLM Research Papers: The 2024 List </title>
<link>https://magazine.sebastianraschka.com/p/llm-research-papers-the-2024-list</link>
<pubDate>Sun, 08 Dec 2024 12:11:04 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   It’s been a very eventful and exciting year in AI research. This is especially true if you are interested in LLMs.
  &lt;/p&gt;
  &lt;p&gt;
   I had big plans for this December edition and was planning to publish a new article with a discussion of all my research highlights from 2024. I still plan to do so, but due to an accident and serious injury, I am currently unable to work at a computer and finish the draft. But I hope to recover in the upcoming weeks and be back on my feet soon.
  &lt;/p&gt;
  &lt;p&gt;
   In the meantime, I want to share my running bookmark list of many fascinating (mostly LLM-related) papers I stumbled upon in 2024. It’s just a list, but maybe it will come in handy for those who are interested in finding some gems to read for the holidays.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    And if you are interested in more code-heavy reading and tinkering,
   &lt;/span&gt;
   &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
    My Build A Large Language Model (From Scratch)
   &lt;/a&gt;
   &lt;span&gt;
    book is out on Amazon as of last month.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In addition, I added a lot of bonus materials to the
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot; rel=&quot;&quot;&gt;
    GitHub repository
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!WXSG!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!WXSG!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg 424w, https://substackcdn.com/image/fetch/$s_!WXSG!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg 848w, https://substackcdn.com/image/fetch/$s_!WXSG!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!WXSG!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1861,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:371770,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;1861&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!WXSG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!WXSG!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg 424w, https://substackcdn.com/image/fetch/$s_!WXSG!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg 848w, https://substackcdn.com/image/fetch/$s_!WXSG!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!WXSG!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9390f1a2-1dd5-4471-9ed7-80359f95639a_1616x2066.jpeg 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot; rel=&quot;&quot;&gt;
      Bonus materials in the GitHub repository
     &lt;/a&gt;
     &lt;span&gt;
      (stars highlight my personal favorites)
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Thanks for your understanding and support, and I hope to make a full recovery soon and be back with the Research Highlights 2024 article in a few weeks!
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    January 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Astraios: Parameter-Efficient Instruction Tuning Code Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.00788&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.00788
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      A Comprehensive Study of Knowledge Editing for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.01286&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.01286
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.01325&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.01325
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.01335&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.01335
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      LLaMA Beyond English: An Empirical Study on Language Capability Transfer
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.01055&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.01055
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO and Toxicity
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.01967&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.01967
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      LLaMA Pro: Progressive LLaMA with Block Expansion
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.02415&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.02415
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      LLM Augmented LLMs: Expanding Capabilities through Composition
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.02412&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.02412
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Blending Is All You Need: Cheaper, Better Alternative to Trillion-Parameters LLM
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.02994&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.02994
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      DeepSeek LLM: Scaling Open-Source Language Models with Longtermism
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.02954&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.02954
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Denoising Vision Transformers
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.02957&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.02957
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Soaring from 4K to 400K: Extending LLM’s Context with Activation Beacon
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.03462&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.03462
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Mixtral of Experts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.04088&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.04088
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.04081&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.04081
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      A Minimaximalist Approach to Reinforcement Learning from Human Feedback
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.04056&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.04056
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.04679&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.04679
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.05566&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.05566
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Transformers are Multi-State RNNs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.06104&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.06104
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      A Closer Look at AUROC and AUPRC under Class Imbalance
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.06091&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.06091
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.06692&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.06692
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Tuning Language Models by Proxy
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.08565&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.08565
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Scalable Pre-training of Large Autoregressive Image Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.08541&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.08541
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Code Generation with AlphaCodium: From Prompt Engineering to Flow Engineering
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.08500&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.08500
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.08406&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.08406
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      ReFT: Reasoning with Reinforced Fine-Tuning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.08967&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.08967
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      DiffusionGPT: LLM-Driven Text-to-Image Generation System
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.10061&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.10061
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Self-Rewarding Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.10020&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.10020
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      VMamba: Visual State Space Model
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.10166&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.10166
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Knowledge Fusion of Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.10491&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.10491
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      SpatialVLM: Endowing Vision-Language Models with Spatial Reasoning Capabilities
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.12168&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.12168
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      WARM: On the Benefits of Weight Averaged Reward Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.12187&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.12187
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.12070&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.12070
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      MambaByte: Token-free Selective State Space Model
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.13660&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.13660
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      SpacTor-T5: Pre-training T5 Models with Span Corruption and Replaced Token Detection
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.13160&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.13160
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Rethinking Patch Dependence for Masked Autoencoders
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.14391&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.14391
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Pix2gestalt: Amodal Segmentation by Synthesizing Wholes
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.14398&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.14398
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.14405&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.14405
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.15077&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.15077
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      MoE-LLaVA: Mixture of Experts for Large Vision-Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.15947&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.15947
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.16380&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.16380
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Jan,
     &lt;/span&gt;
     &lt;em&gt;
      KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2401.18079&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2401.18079
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    February 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Efficient Exploration for LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.00396&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.00396
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      OLMo: Accelerating the Science of Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.00838&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.00838
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.00841&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.00841
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Repeat After Me: Transformers are Better than State Space Models at Copying
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.01032&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.01032
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      LiPO: Listwise Preference Optimization through Learning-to-Rank
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.01878&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.01878
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      FindingEmo: An Image Dataset for Emotion Recognition in the Wild
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.01355&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.01355
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      More Agents Is All You Need
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.05120&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.05120
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.03300&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.03300
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      MobileVLM V2: Faster and Stronger Baseline for Vision Language Model
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.03766&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.03766
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      A Phase Transition Between Positional and Semantic Learning in a Solvable Model of Dot-Product Attention
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.03902&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.03902
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Laws for Downstream Task Performance of Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.04177&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.04177
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      MOMENT: A Family of Open Time-series Foundation Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.03885&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.03885
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.03749&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.03749
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Self-Discover: Large Language Models Self-Compose Reasoning Structures
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.03620&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.03620
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Grandmaster-Level Chess Without Search
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.04494&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.04494
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Direct Language Model Alignment from Online AI Feedback
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.04792&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.04792
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Buffer Overflow in Mixture of Experts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.05526&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.05526
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      The Boundary of Neural Network Trainability is Fractal
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.06184&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.06184
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      ODIN: Disentangled Reward Mitigates Hacking in RLHF
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.07319&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.07319
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Policy Improvement using Language Feedback Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.07876&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.07876
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Laws for Fine-Grained Mixture of Experts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.07871&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.07871
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Aya Model: An Instruction Finetuned Open-Access Multilingual Language Model
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.07610&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.07610
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Step-On-Feet Tuning: Scaling Self-Alignment of LLMs via Bootstrapping
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.07610&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.07610
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Suppressing Pink Elephants with Direct Principle Feedback
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.07896&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.07896
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      World Model on Million-Length Video And Language With RingAttention
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.08268&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.08268
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Mixtures of Experts Unlock Parameter Scaling for Deep RL
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.08609&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.08609
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      DoRA: Weight-Decomposed Low-Rank Adaptation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.09353&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.09353
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Transformers Can Achieve Length Generalization But Not Robustly
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.09371&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.09371
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      BASE TTS: Lessons From Building a Billion-Parameter Text-to-Speech Model on 100K Hours of Data
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.08093&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.08093
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Recovering the Pre-Fine-Tuning Weights of Generative Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.10208&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.10208
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Generative Representational Instruction Tuning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.09906&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.09906
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      FinTral: A Family of GPT-4 Level Multimodal Financial Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.10986&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.10986
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      OneBit: Towards Extremely Low-bit Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.11295&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.11295
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.11550&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.11550
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Reformatted Alignment
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.12219&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.12219
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.12226&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.12226
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Towards Cross-Tokenizer Distillation: the Universal Logit Distillation Loss for LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.12030&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.12030
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      LoRA+: Efficient Low Rank Adaptation of Large Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.12354&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.12354
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Neural Network Diffusion
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.13144&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.13144
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.13616&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.13616
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.13753&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.13753
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Large Language Models for Data Annotation: A Survey
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.13446&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.13446
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      TinyLLaVA: A Framework of Small-scale Large Multimodal Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.14289&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.14289
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.14740&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.14740
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Genie: Generative Interactive Environments
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.15391&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.15391
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      CARTE: Pretraining and Transfer for Tabular Learning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.16785&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.16785
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.17764&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.17764
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Sora Generates Videos with Stunning Geometrical Consistency
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.17403&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.17403
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      When Scaling Meets LLM Finetuning: The Effect of Data, Model and Finetuning Method
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.17193&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.17193
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Feb,
     &lt;/span&gt;
     &lt;em&gt;
      Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2402.19427&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2402.19427
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    March 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Learning and Leveraging World Models in Visual Representation Learning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.00504&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.00504
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Improving LLM Code Generation with Grammar Augmentation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.01632&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.01632
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      The Hidden Attention of Mamba Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.01590&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.01590
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Training-Free Pretrained Model Merging
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.01753&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.01753
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Vision-RWKV: Efficient and Scalable Visual Perception with RWKV-Like Architectures
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.02308&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.02308
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      The WMDP Benchmark: Measuring and Reducing Malicious Use With Unlearning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03218&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03218
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Evolution Transformer: In-Context Evolutionary Optimization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.02985&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.02985
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Enhancing Vision-Language Pre-training with Rich Supervisions
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03346&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03346
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Rectified Flow Transformers for High-Resolution Image Synthesis
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03206&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03206
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Design2Code: How Far Are We From Automating Front-End Engineering?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03163&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03163
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      ShortGPT: Layers in Large Language Models are More Redundant Than You Expect
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03853&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03853
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Backtracing: Retrieving the Cause of the Query
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03956&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03956
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Learning to Decode Collaboratively with Multiple Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03870&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03870
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      SaulLM-7B: A pioneering Large Language Model for Law
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03883&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03883
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03864&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03864
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      3D Diffusion Policy
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03954&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03954
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      MedMamba: Vision Mamba for Medical Image Classification
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03849&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03849
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03507&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03507
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Stop Regressing: Training Value Functions via Classification for Scalable Deep RL
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.03950&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.03950
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      How Far Are We from Intelligent Visual Deductive Reasoning?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.04732&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.04732
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Common 7B Language Models Already Possess Strong Math Capabilities
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.04706&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.04706
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.05530&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.05530
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Is Cosine-Similarity of Embeddings Really About Similarity?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.05440&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.05440
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      LLM4Decompile: Decompiling Binary Code with Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.05286&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.05286
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Algorithmic Progress in Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.05812&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.05812
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Stealing Part of a Production Language Model
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.06634&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.06634
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Chronos: Learning the Language of Time Series
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.07815&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.07815
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Simple and Scalable Strategies to Continually Pre-train Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.08763&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.08763
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Language Models Scale Reliably With Over-Training and on Downstream Tasks
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.08540&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.08540
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      BurstAttention: An Efficient Distributed Attention Framework for Extremely Long Sequences
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.09347&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.09347
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      LocalMamba: Visual State Space Model with Windowed Selective Scan
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.09338&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.09338
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      GiT: Towards Generalist Vision Transformer through Universal Language Interface
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.09394&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.09394
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.09611&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.09611
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      RAFT: Adapting Language Model to Domain Specific RAG
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.10131&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.10131
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      TnT-LLM: Text Mining at Scale with Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.12173&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.12173
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.15447&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.15447
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      PERL: Parameter Efficient Reinforcement Learning from Human Feedback
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.10704&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.10704
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      RewardBench: Evaluating Reward Models for Language Modeling
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.13787&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.13787
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.13372&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.13372
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      RakutenAI-7B: Extending Large Language Models for Japanese
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.15484&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.15484
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      SiMBA: Simplified Mamba-Based Architecture for Vision and Multivariate Time Series
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.15360&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.15360
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Can Large Language Models Explore In-Context?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.15371&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.15371
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.15042&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.15042
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      LLM Agent Operating System
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.16971&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.16971
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      The Unreasonable Ineffectiveness of the Deeper Layers
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.17887&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.17887
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      BioMedLM: A 2.7B Parameter Language Model Trained On Biomedical Text
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.18421&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.18421
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      ViTAR: Vision Transformer with Any Resolution
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.18361&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.18361
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Long-form Factuality in Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.18802&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.18802
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Mini-Gemini: Mining the Potential of Multi-modality Vision Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.18814&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.18814
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.17919&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.17919
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Mechanistic Design and Scaling of Hybrid Architectures
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.17844&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.17844
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      MagicLens: Self-Supervised Image Retrieval with Open-Ended Instructions
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.19651&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.19651
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Mar,
     &lt;/span&gt;
     &lt;em&gt;
      Model Stock: All We Need Is Just a Few Fine-Tuned Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2403.19522&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2403.19522
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    April 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Do Language Models Plan Ahead for Future Tokens?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.00859&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.00859
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Bigger is not Always Better: Scaling Properties of Latent Diffusion Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.01367&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.01367
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      The Fine Line: Navigating Large Language Model Pretraining with Down-streaming Capability Analysis
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.01204&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.01204
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Diffusion-RWKV: Scaling RWKV-Like Architectures for Diffusion Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.04478&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.04478
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Mixture-of-Depths: Dynamically Allocating Compute in Transformer-Based Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.02258&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.02258
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Long-context LLMs Struggle with Long In-context Learning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.02060&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.02060
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Emergent Abilities in Reduced-Scale Generative Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.02204&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.02204
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.02151&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.02151
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      On the Scalability of Diffusion-based Text-to-Image Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.02883&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.02883
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.02827&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.02827
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.02747&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.02747
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Direct Nash Optimization: Teaching Language Models to Self-Improve with General Preferences
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.02151&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.02151
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Training LLMs over Neurally Compressed Text
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.03626&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.03626
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      CantTalkAboutThis: Aligning Language Models to Stay on Topic in Dialogues
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.03820&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.03820
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      ReFT: Representation Finetuning for Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.03592&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.03592
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Verifiable by Design: Aligning Language Models to Quote from Pre-Training Data
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.03862&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.03862
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Sigma: Siamese Mamba Network for Multi-Modal Semantic Segmentation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.04256&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.04256
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      AutoCodeRover: Autonomous Program Improvement
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.05427&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.05427
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.05892&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.05892
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      CodecLM: Aligning Language Models with Tailored Synthetic Data
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.05875&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.05875
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.06395&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.06395
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.06209&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.06209
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      LLM2Vec: Large Language Models Are Secretly Powerful Text Encoders
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.05961&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.05961
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Adapting LLaMA Decoder to Vision Transformer
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.06773&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.06773
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.07143&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.07143
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      LLoCO: Learning Long Contexts Offline
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.07979&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.07979
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      JetMoE: Reaching Llama2 Performance with 0.1M Dollars
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.07413&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.07413
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Best Practices and Lessons Learned on Synthetic Data for Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.07503&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.07503
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Rho-1: Not All Tokens Are What You Need
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.07965&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.07965
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Pre-training Small Base LMs with Fewer Tokens
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.08634&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.08634
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Dataset Reset Policy Optimization for RLHF
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.08495&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.08495
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      LLM In-Context Recall is Prompt Dependent
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.08865&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.08865
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      State Space Model for New-Generation Network Alternative to Transformers: A Survey
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.09516&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.09516
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Chinchilla Scaling: A Replication Attempt
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.10102&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.10102
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Learn Your Reference Model for Real Good Alignment
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.09656&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.09656
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.10719&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.10719
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling (Down) CLIP: A Comprehensive Analysis of Data, Architecture, and Training Strategies
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.08197&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.08197
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      How Faithful Are RAG Models? Quantifying the Tug-of-War Between RAG and LLMs&#x27; Internal Prior
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.10198&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.10198
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      A Survey on Retrieval-Augmented Text Generation for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.10981&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.10981
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      When LLMs are Unfit Use FastFit: Fast and Effective Text Classification with Many Classes
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.12365&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.12365
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.12253&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.12253
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      OpenBezoar: Small, Cost-Effective and Open Models Trained on Mixes of Instruction Data
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.12195&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.12195
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      The Instruction Hierarchy: Training LLMs to Prioritize Privileged Instructions
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.13208&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.13208
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      How Good Are Low-bit Quantized LLaMA3 Models? An Empirical Study
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.14047&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.14047
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.14219&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.14219
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      OpenELM: An Efficient Language Model Family with Open-source Training and Inference Framework
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.14619&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.14619
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      A Survey on Self-Evolution of Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.14662&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.14662
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Multi-Head Mixture-of-Experts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.15045&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.15045
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      NExT: Teaching Large Language Models to Reason about Code Execution
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.14662&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.14662
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Graph Machine Learning in the Era of Large Language Models (LLMs)
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.14928&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.14928
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Retrieval Head Mechanistically Explains Long-Context Factuality
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.15574&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.15574
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Layer Skip: Enabling Early Exit Inference and Self-Speculative Decoding
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.16710&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.16710
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Make Your LLM Fully Utilize the Context
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.16811&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.16811
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      LoRA Land: 310 Fine-tuned LLMs that Rival GPT-4, A Technical Report
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.00732&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.00732
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      Better &amp; Faster Large Language Models via Multi-token Prediction
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.19737&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.19737
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.19543&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.19543
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      A Primer on the Inner Workings of Transformer-based Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.00208&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.00208
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      When to Retrieve: Teaching LLMs to Utilize Information Retrieval Effectively
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.19705&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.19705
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Apr,
     &lt;/span&gt;
     &lt;em&gt;
      KAN: Kolmogorov–Arnold Networks
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2404.19756&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2404.19756
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    May 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 May,
     &lt;/span&gt;
     &lt;em&gt;
      Is Bigger Edit Batch Size Always Better? An Empirical Study on Model Editing with Llama-3
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.00664&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.00664
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 May,
     &lt;/span&gt;
     &lt;em&gt;
      Self-Play Preference Optimization for Language Model Alignment
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.00675&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.00675
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 May,
     &lt;/span&gt;
     &lt;em&gt;
      A Careful Examination of Large Language Model Performance on Grade School Arithmetic
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.00332&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.00332
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 May,
     &lt;/span&gt;
     &lt;em&gt;
      Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.01535&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.01535
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 May,
     &lt;/span&gt;
     &lt;em&gt;
      What Matters When Building Vision-Language Models?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.02246&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.02246
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 May,
     &lt;/span&gt;
     &lt;em&gt;
      Is Flash Attention Stable?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.02803&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.02803
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 May,
     &lt;/span&gt;
     &lt;em&gt;
      vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.04437&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.04437
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 May,
     &lt;/span&gt;
     &lt;em&gt;
      xLSTM: Extended Long Short-Term Memory
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.04517&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.04517
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 May,
     &lt;/span&gt;
     &lt;em&gt;
      You Only Cache Once: Decoder-Decoder Architectures for Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.05254&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.05254
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 May,
     &lt;/span&gt;
     &lt;em&gt;
      DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.04434&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.04434
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 May,
     &lt;/span&gt;
     &lt;em&gt;
      Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.05417&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.05417
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 May,
     &lt;/span&gt;
     &lt;em&gt;
      Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.05904&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.05904
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 May,
     &lt;/span&gt;
     &lt;em&gt;
      Value Augmented Sampling for Language Model Alignment and Personalization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.06639&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.06639
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 May,
     &lt;/span&gt;
     &lt;em&gt;
      PHUDGE: Phi-3 as Scalable Judge
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.08029&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.08029
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 May,
     &lt;/span&gt;
     &lt;em&gt;
      RLHF Workflow: From Reward Modeling to Online RLHF
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.07863&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.07863
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 May,
     &lt;/span&gt;
     &lt;em&gt;
      LoRA Learns Less and Forgets Less
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.09673&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.09673
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 May,
     &lt;/span&gt;
     &lt;em&gt;
      Xmodel-VLM: A Simple Baseline for Multimodal Vision Language Model
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.09215&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.09215
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 May,
     &lt;/span&gt;
     &lt;em&gt;
      Chameleon: Mixed-Modal Early-Fusion Foundation Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.09818&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.09818
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 May,
     &lt;/span&gt;
     &lt;em&gt;
      Towards Modular LLMs by Building and Reusing a Library of LoRAs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.11157&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.11157
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 May,
     &lt;/span&gt;
     &lt;em&gt;
      SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.11582&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.11582
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 May,
     &lt;/span&gt;
     &lt;em&gt;
      MoRA: High-Rank Updating for Parameter-Efficient Fine-Tuning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.12130&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.12130
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 May,
     &lt;/span&gt;
     &lt;em&gt;
      Attention as an RNN
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.13956&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.13956
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 May,
     &lt;/span&gt;
     &lt;em&gt;
      Dense Connector for MLLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.13800&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.13800
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 May,
     &lt;/span&gt;
     &lt;em&gt;
      AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.14129&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.14129
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 May,
     &lt;/span&gt;
     &lt;em&gt;
      SimPO: Simple Preference Optimization with a Reference-Free Reward
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.14734&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.14734
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 May,
     &lt;/span&gt;
     &lt;em&gt;
      Instruction Tuning With Loss Over Instructions
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.14394&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.14394
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 May,
     &lt;/span&gt;
     &lt;em&gt;
      The Road Less Scheduled
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.15682&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.15682
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 May,
     &lt;/span&gt;
     &lt;em&gt;
      Stacking Your Transformers: A Closer Look at Model Growth for Efficient LLM Pre-Training
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.15319&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.15319
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 May,
     &lt;/span&gt;
     &lt;em&gt;
      gzip Predicts Data-dependent Scaling Laws
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.16684&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.16684
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 May,
     &lt;/span&gt;
     &lt;em&gt;
      Trans-LoRA: Towards Data-free Transferable Parameter Efficient Finetuning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.17258&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.17258
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 May,
     &lt;/span&gt;
     &lt;em&gt;
      VeLoRA: Memory Efficient Training using Rank-1 Sub-Token Projections
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.17991&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.17991
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 May,
     &lt;/span&gt;
     &lt;em&gt;
      LLaMA-NAS: Efficient Neural Architecture Search for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.18377&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.18377
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 May,
     &lt;/span&gt;
     &lt;em&gt;
      Contextual Position Encoding: Learning to Count What&#x27;s Important
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2405.18719&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2405.18719
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    June 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Show, Don&#x27;t Tell: Aligning Language Models with Demonstrated Feedback
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.00888&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.00888
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Skywork-MoE: A Deep Dive into Training Techniques for Mixture-of-Experts Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.06563&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.06563
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      OLoRA: Orthonormal Low-Rank Adaptation of Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.01775&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.01775
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      The Geometry of Categorical and Hierarchical Concepts in Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.01506&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.01506
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Towards Scalable Automated Alignment of LLMs: A Survey
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.01252&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.01252
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Scalable MatMul-free Language Modeling
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.02528&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.02528
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Block Transformer: Global-to-Local Language Modeling for Fast Inference
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.02657&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.02657
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04271&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04271
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      The Prompt Report: A Systematic Survey of Prompting Techniques
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.06608&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.06608
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Transformers Need Glasses! Information Over-Squashing in Language Tasks
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04267&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04267
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Are We Done with MMLU?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04127&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04127
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Step-aware Preference Optimization: Aligning Preference with Denoising Performance at Each Step
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04314&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04314
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Boosting Large-scale Parallel Training Efficiency with C4: A Communication-Driven Approach
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04594&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04594
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      CRAG -- Comprehensive RAG Benchmark
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04744&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04744
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      WildBench: Benchmarking LLMs with Challenging Tasks from Real Users in the Wild
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04770&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04770
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Mixture-of-Agents Enhances Large Language Model Capabilities
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04692&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04692
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      BERTs are Generative In-Context Learners
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.04823&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.04823
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.05132&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.05132
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Creativity Has Left the Chat: The Price of Debiasing Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.05587&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.05587
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.06525&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.06525
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Margin-aware Preference Optimization for Aligning Diffusion Models Without Reference
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.06424&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.06424
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Husky: A Unified, Open-Source Language Agent for Multi-Step Reasoning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.06469&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.06469
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Turbo Sparse: Achieving LLM SOTA Performance with Minimal Activated Parameters
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.05955&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.05955
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.06326&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.06326
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      An Image is Worth 32 Tokens for Reconstruction and Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.07550&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.07550
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      TextGrad: Automatic &quot;Differentiation&quot; via Text
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.07496&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.07496
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Simple and Effective Masked Diffusion Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.07524&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.07524
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Never Miss A Beat: An Efficient Recipe for Context Window Extension of Large Language Models with Consistent &quot;Middle&quot; Enhancement
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.07138&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.07138
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.07522&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.07522
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.08464&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.08464
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      What If We Recaption Billions of Web Images with LLaMA-3?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.08478&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.08478
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Large Language Model Unlearning via Embedding-Corrupted Prompts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.07933&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.07933
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Large Language Models Must Be Taught to Know What They Don&#x27;t Know
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.08391&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.08391
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      An Empirical Study of Mamba-based Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.07887&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.07887
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Discovering Preference Optimization Algorithms with and for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.08414&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.08414
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Transformers Meet Neural Algorithmic Reasoners
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.09308&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.09308
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.09297&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.09297
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.09415&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.09415
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      FouRA: Fourier Low Rank Adaptation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.08798&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.08798
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Bootstrapping Language Models with DPO Implicit Rewards
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.09760&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.09760
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Be like a Goldfish, Don&#x27;t Memorize! Mitigating Memorization in Generative LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.10209&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.10209
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      14 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.10216&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.10216
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      THEANINE: Revisiting Memory Management in Long-term Conversations with Timeline-augmented Response Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.10996&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.10996
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Task Me Anything
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11775&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11775
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      How Do Large Language Models Acquire Factual Knowledge During Pretraining?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11813&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11813
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      mDPO: Conditional Preference Optimization for Multimodal Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11839&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11839
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Nemotron-4 340B Technical Report
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11704&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11704
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      DataComp-LM: In Search of the Next Generation of Training Sets for Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11794&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11794
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Tokenization Falling Short: The Curse of Tokenization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11687&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11687
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11931&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11931
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Unveiling Encoder-Free Vision-Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11832&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11832
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11817&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11817
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      HARE: HumAn pRiors, a key to small language model Efficiency
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11410&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11410
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Measuring memorization in RLHF for code completion
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.11715&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.11715
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Self-MoE: Towards Compositional Large Language Models with Self-Specialized Experts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.12034&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.12034
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      From RAGs to Rich Parameters: Probing How Language Models Utilize External Knowledge Over Parametric Information for Factual Queries
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.12824&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.12824
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.12624&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.12624
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.13121&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.13121
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Instruction Pre-Training: Language Models are Supervised Multitask Learners
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.14491&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.14491
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Can LLMs Learn by Teaching? A Preliminary Study
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.14629&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.14629
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      A Tale of Trust and Accuracy: Base vs. Instruct LLMs in RAG Systems
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.14972&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.14972
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      LongRAG: Enhancing Retrieval-Augmented Generation with Long-context LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.15319&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.15319
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.14909&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.14909
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Efficient Continual Pre-training by Mitigating the Stability Gap
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.14833&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.14833
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.16747&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.16747
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      WARP: On the Benefits of Weight Averaged Rewarded Policies
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.16768&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.16768
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Adam-mini: Use Fewer Learning Rates To Gain More
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.16793&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.16793
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.17557&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.17557
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      LongIns: A Challenging Long-context Instruction-based Exam for LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.17588&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.17588
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Following Length Constraints in Instructions
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.17744&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.17744
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      A Closer Look into Mixture-of-Experts in Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.18219&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.18219
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      RouteLLM: Learning to Route LLMs with Preference Data
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.18665&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.18665
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.18629&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.18629
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Dataset Size Recovery from LoRA Weights
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.19395&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.19395
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      From Artificial Needles to Real Haystacks: Improving Retrieval Capabilities in LLMs by Finetuning on Synthetic Data
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.19292&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.19292
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Changing Answer Order Can Decrease MMLU Accuracy
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.19470&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.19470
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Direct Preference Knowledge Distillation for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.19774&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.19774
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      LLM Critics Help Catch LLM Bugs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.00215&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.00215
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Jun,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Synthetic Data Creation with 1,000,000,000 Personas
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2406.20094&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2406.20094
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Jul 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      LLM See, LLM Do: Guiding Data Generation to Target Non-Differentiable Objectives
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.01490&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.01490
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Searching for Best Practices in Retrieval-Augmented Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.01219&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.01219
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.01906&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.01906
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Diffusion Forcing: Next-token Prediction Meets Full-Sequence Diffusion
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.01392&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.01392
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Eliminating Position Bias of Language Models: A Mechanistic Approach
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.01100&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.01100
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      JMInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.02490&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.02490
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      TokenPacker: Efficient Visual Projector for Multimodal LLM
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.02392&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.02392
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Reasoning in Large Language Models: A Geometric Perspective
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.02678&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.02678
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.02485&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.02485
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      AgentInstruct: Toward Generative Teaching with Agentic Flows
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.03502&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.03502
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      HEMM: Holistic Evaluation of Multimodal Foundation Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.03418&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.03418
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Mixture of A Million Experts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.04153&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.04153
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Learning to (Learn at Test Time): RNNs with Expressive Hidden States
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.04620&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.04620
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Vision Language Models Are Blind
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.06581&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.06581
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Self-Recognition in Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.06946&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.06946
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Inference Performance Optimization for Large Language Models on CPUs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.07304&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.07304
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Gradient Boosting Reinforcement Learning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.08250&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.08250
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.08608&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.08608
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      SpreadsheetLLM: Encoding Spreadsheets for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.09025&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.09025
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      New Desiderata for Direct Preference Optimization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.09072&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.09072
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Context Embeddings for Efficient Answer Generation in RAG
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.09252&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.09252
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Qwen2 Technical Report
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.10671&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.10671
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      The Good, The Bad, and The Greedy: Evaluation of LLMs Should Not Ignore Non-Determinism
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.10457&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.10457
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      From GaLore to WeLore: How Low-Rank Weights Non-uniformly Emerge from Low-Rank Gradients
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.11239&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.11239
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.12077&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.12077
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Diffusion Transformers to 16 Billion Parameters
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.11633&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.11633
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.11963&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.11963
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Patch-Level Training for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.12665&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.12665
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.12772&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.12772
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.12994&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.12994
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Spectra: A Comprehensive Study of Ternary, Quantized, and FP16 Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.12327&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.12327
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Attention Overflow: Language Model Input Blur during Long-Context Missing Items Recommendation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.13481&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.13481
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Weak-to-Strong Reasoning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.13647&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.13647
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Understanding Reference Policies in Direct Preference Optimization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.13709&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.13709
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Laws with Vocabulary: Larger Models Deserve Larger Vocabularies
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.13623&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.13623
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      BOND: Aligning LLMs with Best-of-N Distillation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.14622&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.14622
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Compact Language Models via Pruning and Knowledge Distillation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.14679&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.14679
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.14057&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.14057
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Mini-Sequence Transformer: Optimizing Intermediate Memory for Long Sequences Training
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.15892&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.15892
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      DDK: Distilling Domain Knowledge for Efficient Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.16154&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.16154
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Generation Constraint Scaling Can Mitigate Hallucination
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.16908&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.16908
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Retrieval Augmented Generation or Long-Context LLMs? A Comprehensive Study and Hybrid Approach
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.16833&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.16833
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Course-Correction: Safety Alignment Using Synthetic Preferences
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.16637&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.16637
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.16607&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.16607
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.19594&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.19594
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Improving Retrieval Augmented Language Model with Self-Reasoning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.19813&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.19813
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Apple Intelligence Foundation Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.21075&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.21075
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      ThinK: Thinner Key Cache by Query-Driven Pruning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.21018&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.21018
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      The Llama 3 Herd of Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2407.21783&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2407.21783
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Jul,
     &lt;/span&gt;
     &lt;em&gt;
      Gemma 2: Improving Open Language Models at a Practical Size
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.00118&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.00118
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    August 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Aug, S
     &lt;/span&gt;
     &lt;em&gt;
      AM 2: Segment Anything in Images and Videos,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.00714&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.00714
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      POA: Pre-training Once for Models of All Sizes,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.01031&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.01031
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.01262&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.01262
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      A Survey of Mamba,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.01129&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.01129
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      MiniCPM-V: A GPT-4V Level MLLM on Your Phone,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.01800&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.01800
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.02545&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.02545
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Self-Taught Evaluators,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.02666&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.02666
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.02600&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.02600
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Self-Taught Evaluators,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.02666&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.02666
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      EXAONE 3.0 7.8B Instruction Tuned Language Model,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.03541&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.03541
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your Language Model Thrives on Quality Data,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.03506&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.03506
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Conversational Prompt Engineering,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.04560&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.04560
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.04303&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.04303
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.06292&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.06292
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Hermes 3 Technical Report,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.12570&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.12570
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Customizing Language Models with Instance-wise LoRA for Sequential Recommendation,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.10159&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.10159
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Aug
     &lt;/span&gt;
     &lt;em&gt;
      , Enhancing Robustness in Large Language Models: Prompting for Mitigating the Impact of Irrelevant Information,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.10615&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.10615
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      To Code, or Not To Code? Exploring Impact of Code in Pre-training,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.10914&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.10914
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Aug ,
     &lt;/span&gt;
     &lt;em&gt;
      LLM Pruning and Distillation in Practice: The Minitron Approach,
     &lt;/em&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.11796&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.11796
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Jamba-1.5: Hybrid Transformer-Mamba Models at Scale,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.12570&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.12570
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Controllable Text Generation for Large Language Models: A Survey,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.12599&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.12599
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Multi-Layer Transformers Gradient Can be Approximated in Almost Linear Time,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.13233&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.13233
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      A Practitioner&#x27;s Guide to Continual Multimodal Pretraining,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.14471&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.14471
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Building and better understanding vision-language models: insights and future directions,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.12637&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.12637
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.14572&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.14572
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      The Mamba in the Llama: Distilling and Accelerating Hybrid Models,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.15237&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.15237
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      ReMamba: Equip Mamba with Effective Long-Sequence Modeling,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.15496&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.15496
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      Smaller, Weaker, Yet Better: Training LLM Reasoners via Compute-Optimal Sampling,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2408.16737&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2408.16737
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Aug,
     &lt;/span&gt;
     &lt;em&gt;
      LongRecipe: Recipe for Efficient Long Context Generalization in Large Languge Models,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.00509&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.00509
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    September 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      OLMoE: Open Mixture-of-Experts Language Models,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.02060&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.02060
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Sep 2024,
     &lt;/span&gt;
     &lt;em&gt;
      In Defense of RAG in the Era of Long-Context Language Models,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.01666&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.01666
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      Attention Heads of Large Language Models: A Survey,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.03752&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.03752
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      LongCite: Enabling LLMs to Generate Fine-grained Citations in Long-context QA
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.02897&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.02897
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      How Do Your Code LLMs Perform? Empowering Code Instruction Tuning with High-Quality Data,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.03810&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.03810
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Sep, T
     &lt;/span&gt;
     &lt;em&gt;
      heory, Analysis, and Best Practices for Sigmoid Self-Attention,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.04431&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.04431
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      LLaMA-Omni: Seamless Speech Interaction with Large Language Models,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.06666&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.06666
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      What is the Role of Small Models in the LLM Era: A Survey,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.06857&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.06857
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      Policy Filtration in RLHF to Fine-Tune LLM for Code Generation,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.06957&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.06957
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.10516&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.10516
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      Qwen2.5-Math Technical Report: Toward Mathematical Expert Model via Self-Improvement
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.12122&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.12122
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      Qwen2.5-Coder Technical Report
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.12186&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.12186
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      Instruction Following without Instruction Tuning,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.14254&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.14254
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Sep, I
     &lt;/span&gt;
     &lt;em&gt;
      s Preference Alignment Always the Best Option to Enhance LLM-Based Translation? An Empirical Analysis,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.20059&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.20059
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Sep,
     &lt;/span&gt;
     &lt;em&gt;
      The Perfect Blend: Redefining RLHF with Mixture of Judges,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2409.20370&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2409.20370
     &lt;/a&gt;
     &lt;span&gt;
      (New paper by Meta on how they did RLHF for Llama 3)
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    October 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Addition is All You Need for Energy-efficient Language Models,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.00907&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.00907
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Oct
     &lt;/span&gt;
     &lt;em&gt;
      Quantifying Generalization Complexity for Large Language Models,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.01769&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.01769
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      When a language model is optimized for reasoning, does it still show embers of autoregression? An analysis of OpenAI o1
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.01792&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.01792
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Oct, W
     &lt;/span&gt;
     &lt;em&gt;
      ere RNNs All We Needed?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.01201&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.01201
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Selective Attention Improves Transformer
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.02703&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.02703
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.02707&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.02707
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      LLaVA-Critic: Learning to Evaluate Multimodal Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.02712&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.02712
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Differential Transformer
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.05258&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.05258
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.05229&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.05229
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      ARIA: An Open Multimodal Native Mixture-of-Experts Model
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.05993&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.05993
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      O1 Replication Journey: A Strategic Progress Report -- Part 1
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.18982&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.18982
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Long-Context LLMs Meet RAG: Overcoming Challenges for Long Inputs in RAG,
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.05983&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.05983
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      From Generalist to Specialist: Adapting Vision Language Models via Task-Specific Visual Instruction Tuning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.06456&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.06456
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      10 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      KV Prediction for Improved Time to First Token
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.08391&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.08391
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Baichuan-Omni Technical Report
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.08565&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.08565
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.10139&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.10139
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      LOKI: A Comprehensive Synthetic Data Detection Benchmark using Large Multimodal Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.09732&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.09732
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      AFlow: Automating Agentic Workflow Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.10762&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.10762
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Toward General Instruction-Following Alignment for Retrieval-Augmented Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.09584&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.09584
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Pre-training Distillation for Large Language Models: A Design Space Exploration
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.16215&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.16215
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      MIA-DPO: Multi-Image Augmented Direct Preference Optimization For Large Vision-Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.17637&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.17637
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Scalable Ranked Preference Optimization for Text-to-Image Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.18013&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.18013
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Diffusion Language Models via Adaptation from Autoregressive Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.17891&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.17891
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Hybrid Preferences: Learning to Route Instances for Human vs. AI Feedback
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.19133&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.19133
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Counting Ability of Large Language Models and Impact of Tokenization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.19730&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.19730
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      A Survey of Small Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.20011&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.20011
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Accelerating Direct Preference Optimization with Prefix Sharing
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.20305&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.20305
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Mind Your Step (by Step): Chain-of-Thought can Reduce Performance on Tasks where Thinking Makes Humans Worse
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.21333&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.21333
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      LongReward: Improving Long-context Large Language Models with AI Feedback
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.21252&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.21252
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      28 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.21465&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.21465
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.21943&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.21943
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      30 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      CORAL: Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.23090&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.23090
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      What Happened in LLMs Layers when Trained for Fast vs. Slow Thinking: A Gradient Perspective
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.23743&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.23743
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      GPT or BERT: why not both?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.24159&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.24159
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Oct,
     &lt;/span&gt;
     &lt;em&gt;
      Language Models can Self-Lengthen to Generate Long Texts
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2410.23933&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2410.23933
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    November 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Adding Error Bars to Evals: A Statistical Approach to Language Model Evaluations
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.00640&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.00640
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Nov 2024,
     &lt;/span&gt;
     &lt;em&gt;
      Adapting While Learning: Grounding LLMs for Scientific Problems with Intelligent Tool Usage Adaptation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.00412&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.00412
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      1 Nov 2024,
     &lt;/span&gt;
     &lt;em&gt;
      Multi-expert Prompting Improves Reliability, Safety, and Usefulness of Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.00492&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.00492
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Nov, S
     &lt;/span&gt;
     &lt;em&gt;
      ample-Efficient Alignment for LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.01493&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.01493
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Nov 2024,
     &lt;/span&gt;
     &lt;em&gt;
      A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.03350&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.03350
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      &quot;Give Me BF16 or Give Me Death&quot;? Accuracy-Performance Trade-Offs in LLM Quantization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.02355&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.02355
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Parameter-Efficient Fine-Tuning of Large Language Models for Unit Test Generation: An Empirical Study
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.02462&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.02462
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      HtmlRAG: HTML is Better Than Plain Text for Modeling Retrieved Knowledge in RAG Systems
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.02959&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.02959
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM Data Contamination
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.03823&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.03823
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Language Models are Hidden Reasoners: Unlocking Latent Reasoning Capabilities via Self-Rewarding
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.04282&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.04282
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Number Cookbook: Number Understanding of Language Models and How to Improve It
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.03766&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.03766
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Mixture-of-Transformers: A Sparse and Scalable Architecture for Multi-Modal Foundation Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.04996&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.04996
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      BitNet a4.8: 4-bit Activations for 1-bit LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.04965&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.04965
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Laws for Precision
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.04330&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.04330
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Energy Efficient Protein Language Models: Leveraging Small Language Models with LoRA for Controllable Protein Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.05966&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.05966
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Balancing Pipeline Parallelism with Vocabulary Parallelism
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.05288&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.05288
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Toward Optimal Search and Retrieval for RAG
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.07396&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.07396
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Large Language Models Can Self-Improve in Long-context Reasoning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.08147&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.08147
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Stronger Models are NOT Stronger Teachers for Instruction Tuning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.07133&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.07133
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Direct Preference Optimization Using Sparse Feature-Level Constraints
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.07618&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.07618
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Cut Your Losses in Large-Vocabulary Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.09009&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.09009
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      15 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Does Prompt Formatting Have Any Impact on LLM Performance?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.10541&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.10541
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      SymDPO: Boosting In-Context Learning of Large Multimodal Models with Symbol Demonstration Direct Preference Optimization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.11909&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.11909
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      SageAttention2 Technical Report: Accurate 4 Bit Attention for Plug-and-play Inference Acceleration
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.10958&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.10958
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Bi-Mamba: Towards Accurate 1-Bit State Space Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.11843&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.11843
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Nov, RedPajama: an Open Dataset for Training Large Language Models,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.12372&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.12372
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Hymba: A Hybrid-head Architecture for Small Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.13676&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.13676
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Loss-to-Loss Prediction: Scaling Laws for All Datasets
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.12925&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.12925
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.13476&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.13476
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Multimodal Autoregressive Pre-training of Large Vision Encoders
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.14402&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.14402
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      21 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Natural Language Reinforcement Learning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.14251&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.14251
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Large Multi-modal Models Can Interpret Features in Large Multi-modal Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.14982&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.14982
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      22 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      TÜLU 3: Pushing Frontiers in Open Language Model Post-Training
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.15124&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.15124
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      23 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.15296&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.15296
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      LLMs Do Not Think Step-by-step In Implicit Reasoning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.15862&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.15862
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      25 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      O1 Replication Journey -- Part 2: Surpassing O1-preview through Simple Distillation, Big Progress or Bitter Lesson?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.16489&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.16489
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      26 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Star Attention: Efficient LLM Inference over Long Sequences
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.17116&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.17116
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.17691&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.17691
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      27 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.17686&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.17686
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Reverse Thinking Makes LLMs Stronger Reasoners
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.19865&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.19865
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      29 Nov,
     &lt;/span&gt;
     &lt;em&gt;
      Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM&#x27;s Reasoning Capability
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2411.19943&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2411.19943
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    December 2024
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Switti: Designing Scale-Wise Transformers for Text-to-Image Synthesis
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.01819&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.01819
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      X-Prompt: Towards Universal In-Context Image Generation in Auto-Regressive Vision Language Foundation Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.01824&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.01824
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      2 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Free Process Rewards without Process Labels
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.01981&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.01981
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Scaling Image Tokenizers with Grouped Spherical Quantization
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.02632&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.02632
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      3 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      RARE: Retrieval-Augmented Reasoning Enhancement for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.02830&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.02830
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Perception Tokens Enhance Visual Reasoning in Multimodal Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.03548&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.03548
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Evaluating Language Models as Synthetic Data Generators
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.03679&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.03679
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Best-of-N Jailbreaking
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.03556&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.03556
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      4 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      PaliGemma 2: A Family of Versatile VLMs for Transfer
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.03555&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.03555
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      VisionZip: Longer is Better but Not Necessary in Vision Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.04467&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.04467
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      5 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Evaluating and Aligning CodeLLMs on Human Preference
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.05210&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.05210
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.05237&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.05237
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      6 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.05271&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.05271
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      7 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      LLMs-as-Judges: A Comprehensive Survey on LLM-based Evaluation Methods
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.05579&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.05579
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      8 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Does RLHF Scale? Exploring the Impacts From Data, Model, and Method
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.06000&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.06000
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Unraveling the Complexity of Memory in RL Agents: An Approach for Classification and Evaluation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.06531&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.06531
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Training Large Language Models to Reason in a Continuous Latent Space
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.06769&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.06769
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      9 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      AutoReason: Automatic Few-Shot Reasoning Decomposition
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.06975&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.06975
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      11 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Large Concept Models: Language Modeling in a Sentence Representation Space
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.08821&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.08821
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      12 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Phi-4 Technical Report
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.08905&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.08905
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Byte Latent Transformer: Patches Scale Better Than Tokens
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.09871&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.09871
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      SCBench: A KV Cache-Centric Analysis of Long-Context Methods
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.10319&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.10319
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Cultural Evolution of Cooperation among LLM Agents
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.10270&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.10270
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      13 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.10302&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.10302
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      No More Adam: Learning Rate Scaling at Initialization is All You Need
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.11768&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.11768
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Precise Length Control in Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.11937&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.11937
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      The Open Source Advantage in Large Language Models (LLMs)
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.12004&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.12004
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      16 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method &amp; Challenges
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.11936&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.11936
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      17 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Are Your LLMs Capable of Stable Reasoning?
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.13147&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.13147
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      LLM Post-Training Recipes, Improving Reasoning in LLMs
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.14135&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.14135
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Hansel: Output Length Controlling Framework for Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.14033&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.14033
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Mind Your Theory: Theory of Mind Goes Deeper Than Reasoning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.13631&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.13631
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Alignment Faking in Large Language Models
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.14093&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.14093
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      18 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      SCOPE: Optimizing Key-Value Cache Compression in Long-Context Generation
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.13649&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.13649
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      19 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-Context Multitasks
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.15204&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.15204
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      20 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Offline Reinforcement Learning for LLM Multi-Step Reasoning
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.16145&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.16145
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      24 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Mulberry: Empowering MLLM with O1-like Reasoning and Reflection via Collective Monte Carlo Tree Search
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2412.18319&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2412.18319
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      31 Dec,
     &lt;/span&gt;
     &lt;em&gt;
      Titans: Learning to Memorize at Test Time
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2501.00663v1&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2501.00663
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . (I am confident that you&#x27;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;a href=&quot;https://amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
       Build a Large Language Model (From Scratch)
      &lt;/a&gt;
      &lt;span&gt;
       now available on Amazon
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Alternatively, I also recently enabled the paid subscription option on Substack to support this magazine directly.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Understanding Multimodal LLMs </title>
<link>https://magazine.sebastianraschka.com/p/understanding-multimodal-llms</link>
<pubDate>Sun, 03 Nov 2024 12:44:00 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   It was a wild two months. There have once again been many developments in AI research, with two Nobel Prizes awarded to AI and several interesting research papers published.
  &lt;/p&gt;
  &lt;p&gt;
   Among others, Meta AI released their latest Llama 3.2 models, which include open-weight versions for the 1B and 3B large language models and two multimodal models.
  &lt;/p&gt;
  &lt;p&gt;
   In this article, I aim to explain how multimodal LLMs function. Additionally, I will review and summarize roughly a dozen other recent multimodal papers and models published in recent weeks (including Llama 3.2) to compare their approaches.
  &lt;/p&gt;
  &lt;p&gt;
   (To see a table of contents menu, click on the stack of lines on the left-hand side.)
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Pq2z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Pq2z!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png 424w, https://substackcdn.com/image/fetch/$s_!Pq2z!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png 848w, https://substackcdn.com/image/fetch/$s_!Pq2z!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png 1272w, https://substackcdn.com/image/fetch/$s_!Pq2z!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:859,&quot;width&quot;:1456,&quot;resizeWidth&quot;:527,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;310.91552197802196&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Pq2z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Pq2z!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png 424w, https://substackcdn.com/image/fetch/$s_!Pq2z!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png 848w, https://substackcdn.com/image/fetch/$s_!Pq2z!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png 1272w, https://substackcdn.com/image/fetch/$s_!Pq2z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d76dab1-362f-45b6-9b12-a12ac131edc5_1600x944.png 1456w&quot; width=&quot;527&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An illustration of a multimodal LLM that can accept different input modalities (audio, text, images, and videos) and returns text as the output modality.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    But before we begin, I also have some exciting news to share on the personal front! My book,
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &quot;Build A Large Language Model (From Scratch)&quot;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    &lt;span&gt;
     , is now finally
    &lt;/span&gt;
    &lt;a href=&quot;https://amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     available on Amazon
    &lt;/a&gt;
    &lt;span&gt;
     !
    &lt;/span&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;a href=&quot;https://amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
       Build a Large Language Model (From Scratch)
      &lt;/a&gt;
      &lt;span&gt;
       now available on Amazon
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Writing this book was a tremendous effort, and I’m incredibly grateful for all the support and motivating feedback over the past two years—especially in these last couple of months, as so many kind readers have shared their feedback. Thank you all, and as an author, there is nothing more motivating than to hear that the book makes a difference in your careers!
  &lt;/p&gt;
  &lt;p&gt;
   For those who have finished the book and are eager for more, stay tuned! I’ll be adding some bonus content to the GitHub repository in the coming months.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    &lt;span&gt;
     P.S. If you have read the book, I&#x27;d really appreciate it if you could leave a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167/&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     ; it truly helps us authors!
    &lt;/span&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   1. Use cases of multimodal LLMs
  &lt;/h1&gt;
  &lt;p&gt;
   What are multimodal LLMs? As hinted at in the introduction, multimodal LLMs are large language models capable of processing multiple types of inputs, where each &quot;modality&quot; refers to a specific type of data—such as text (like in traditional LLMs), sound, images, videos, and more. For simplicity, we will primarily focus on the image modality alongside text inputs.
  &lt;/p&gt;
  &lt;p&gt;
   A classic and intuitive application of multimodal LLMs is image captioning: you provide an input image, and the model generates a description of the image, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!8kaL!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!8kaL!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png 424w, https://substackcdn.com/image/fetch/$s_!8kaL!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png 848w, https://substackcdn.com/image/fetch/$s_!8kaL!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png 1272w, https://substackcdn.com/image/fetch/$s_!8kaL!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1186,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1186&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!8kaL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!8kaL!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png 424w, https://substackcdn.com/image/fetch/$s_!8kaL!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png 848w, https://substackcdn.com/image/fetch/$s_!8kaL!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png 1272w, https://substackcdn.com/image/fetch/$s_!8kaL!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F93884822-79f1-498d-a33a-8a367ba57134_1500x1222.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;span&gt;
       Example use of a multimodal LLM explaining
      &lt;/span&gt;
      &lt;a href=&quot;https://x.com/PainSci/status/1309570607458086914&quot; rel=&quot;&quot;&gt;
       a meme
      &lt;/a&gt;
      &lt;span&gt;
       .
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Of course, there are many other use cases. For example, one of my favorites is extracting information from a PDF table and converting it into LaTeX or Markdown.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   2. Common approaches to building multimodal LLMs
  &lt;/h1&gt;
  &lt;p&gt;
   There are two main approaches to building multimodal LLMs:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Method A: Unified Embedding Decoder Architecture approach;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Method B: Cross-modality Attention Architecture approach.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   (By the way, I don’t believe official terms for these techniques exist yet, but let me know if you’ve come across any. For instance, briefer descriptions may be &quot;decoder-only&quot; and &quot;cross-attention-based&quot; approaches.)
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!8miE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!8miE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png 424w, https://substackcdn.com/image/fetch/$s_!8miE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png 848w, https://substackcdn.com/image/fetch/$s_!8miE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png 1272w, https://substackcdn.com/image/fetch/$s_!8miE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:854,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;854&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!8miE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!8miE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png 424w, https://substackcdn.com/image/fetch/$s_!8miE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png 848w, https://substackcdn.com/image/fetch/$s_!8miE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png 1272w, https://substackcdn.com/image/fetch/$s_!8miE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F53956ae8-9cd8-474e-8c10-ef6bddb88164_1600x938.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      The two main approaches to developing multimodal LLM architectures.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    As shown in the figure above, the
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Unified Embedding-Decoder Architecture
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    utilizes a single decoder model, much like an unmodified LLM architecture such as GPT-2 or Llama 3.2. In this approach, images are converted into tokens with the same embedding size as the original text tokens, allowing the LLM to process both text and image input tokens together after concatenation.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Cross-Modality Attention Architecture
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    employs a cross-attention mechanism to integrate image and text embeddings directly within the attention layer.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In the following sections, we will explore how these methods work on a conceptual level. Then, we will look at recent research papers on multimodal LLMs to see how they are applied in practice.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.1 Method A: Unified Embedding Decoder Architecture
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Let’s begin with the unified embedding decoder architecture, illustrated again in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Ws6n!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Ws6n!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png 424w, https://substackcdn.com/image/fetch/$s_!Ws6n!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png 848w, https://substackcdn.com/image/fetch/$s_!Ws6n!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png 1272w, https://substackcdn.com/image/fetch/$s_!Ws6n!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1400,&quot;width&quot;:1166,&quot;resizeWidth&quot;:539,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;647.1698113207547&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Ws6n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Ws6n!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png 424w, https://substackcdn.com/image/fetch/$s_!Ws6n!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png 848w, https://substackcdn.com/image/fetch/$s_!Ws6n!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png 1272w, https://substackcdn.com/image/fetch/$s_!Ws6n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F91955021-7da5-4bc4-840e-87d080152b18_1166x1400.png 1456w&quot; width=&quot;539&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of the unified embedding decoder architecture, which is an unmodified decoder-style LLM (like GPT-2, Phi-3, Gemma, or Llama 3.2) that receives inputs consisting of image token and text token embeddings.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In the unified embedding-decoder architecture, an image is converted into embedding vectors, similar to how input text is converted into embeddings in a standard text-only LLM.
  &lt;/p&gt;
  &lt;p&gt;
   For a typical text-only LLM that processes text, the text input is usually tokenized (e.g., using Byte-Pair Encoding) and then passed through an embedding layer, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!dOba!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!dOba!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png 424w, https://substackcdn.com/image/fetch/$s_!dOba!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png 848w, https://substackcdn.com/image/fetch/$s_!dOba!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png 1272w, https://substackcdn.com/image/fetch/$s_!dOba!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:858,&quot;width&quot;:986,&quot;resizeWidth&quot;:513,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;446.4036511156187&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!dOba!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!dOba!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png 424w, https://substackcdn.com/image/fetch/$s_!dOba!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png 848w, https://substackcdn.com/image/fetch/$s_!dOba!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png 1272w, https://substackcdn.com/image/fetch/$s_!dOba!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc97009dd-cee6-455f-87fe-64c33a868e9f_986x858.png 1456w&quot; width=&quot;513&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of the standard process for tokenizing text and converting it into token embedding vectors, which are subsequently passed to an LLM during training and inference.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.1.1 Understanding Image encoders
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   Analogous to the tokenization and embedding of text, image embeddings are generated using an image encoder module (instead of a tokenizer), as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!PlBh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!PlBh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png 424w, https://substackcdn.com/image/fetch/$s_!PlBh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png 848w, https://substackcdn.com/image/fetch/$s_!PlBh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png 1272w, https://substackcdn.com/image/fetch/$s_!PlBh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:750,&quot;width&quot;:790,&quot;resizeWidth&quot;:341,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;323.73417721518985&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!PlBh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!PlBh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png 424w, https://substackcdn.com/image/fetch/$s_!PlBh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png 848w, https://substackcdn.com/image/fetch/$s_!PlBh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png 1272w, https://substackcdn.com/image/fetch/$s_!PlBh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F15e9cc2f-95de-4723-9de5-9f2af7573aaa_790x750.png 1456w&quot; width=&quot;341&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of the process for encoding an image into image patch embeddings.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   What happens inside the image encoder shown above? To process an image, we first divide it into smaller patches, much like breaking words into subwords during tokenization. These patches are then encoded by a pretrained vision transformer (ViT), as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!_DNf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_DNf!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png 424w, https://substackcdn.com/image/fetch/$s_!_DNf!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png 848w, https://substackcdn.com/image/fetch/$s_!_DNf!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png 1272w, https://substackcdn.com/image/fetch/$s_!_DNf!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1033,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1033&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!_DNf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!_DNf!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png 424w, https://substackcdn.com/image/fetch/$s_!_DNf!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png 848w, https://substackcdn.com/image/fetch/$s_!_DNf!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png 1272w, https://substackcdn.com/image/fetch/$s_!_DNf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffef5f8cb-c76c-4c97-9771-7fdb87d7d8cd_1600x1135.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;span&gt;
       Illustration of a classic vision transformer (ViT) setup, similar to the model proposed in
      &lt;/span&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot; rel=&quot;&quot;&gt;
       An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
      &lt;/a&gt;
      &lt;span&gt;
       (2020).
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Note that ViTs are often used for classification tasks, so I included the classification head in the figure above. However, in this case, we only need the image encoder part.
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.1.2 The role of the linear projection module
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   The &quot;linear projection&quot; shown in the previous figure consists of a single linear layer (i.e., a fully connected layer). The purpose of this layer is to project the image patches, which are flattened into a vector, into an embedding size compatible with the transformer encoder. This linear projection is illustrated in the figure below. An image patch, flattened into a 256-dimensional vector, is up-projected to a 768-dimensional vector.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!i9i4!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!i9i4!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png 424w, https://substackcdn.com/image/fetch/$s_!i9i4!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png 848w, https://substackcdn.com/image/fetch/$s_!i9i4!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png 1272w, https://substackcdn.com/image/fetch/$s_!i9i4!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:620,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;620&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!i9i4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!i9i4!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png 424w, https://substackcdn.com/image/fetch/$s_!i9i4!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png 848w, https://substackcdn.com/image/fetch/$s_!i9i4!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png 1272w, https://substackcdn.com/image/fetch/$s_!i9i4!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fee32d720-92d7-48c2-b39d-adf61a870075_1600x681.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of a linear projection layer that projects flattened image patches from a 256-dimensional into a 768-dimensional embedding space.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   For those who prefer seeing a code example, In PyTorch code, we could implement the linear projection for the image patches as follows:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;import torch


class PatchProjectionLayer(torch.nn.Module):

    def __init__(self, patch_size, num_channels, embedding_dim):
        super().__init__()
        self.patch_size = patch_size
        self.num_channels = num_channels
        self.embedding_dim = embedding_dim
        self.projection = torch.nn.Linear(
            patch_size * patch_size * num_channels, embedding_dim
        )

    def forward(self, x):

        batch_size, num_patches, channels, height, width = x.size()
        x = x.view(batch_size, num_patches, -1)  # Flatten each patch
        x = self.projection(x)  # Project each flattened patch
        return x


# Example Usage:
batch_size = 1
num_patches = 9  # Total patches per image
patch_size = 16  # 16x16 pixels per patch
num_channels = 3  # RGB image
embedding_dim = 768  # Size of the embedding vector

projection_layer = PatchProjectionLayer(patch_size, num_channels, embedding_dim)

patches = torch.rand(
    batch_size, num_patches, num_channels, patch_size, patch_size
)

projected_embeddings = projection_layer(patches)
print(projected_embeddings.shape)

# This prints
# torch.Size([1, 9, 768])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you have read my
   &lt;/span&gt;
   &lt;a href=&quot;https://www.amazon.com/Machine-Learning-AI-Essential-Questions/dp/1718503768/&quot; rel=&quot;&quot;&gt;
    Machine Learning Q and AI
   &lt;/a&gt;
   &lt;span&gt;
    book by chance, you may know there are ways to replace linear layers with convolution operations that can be implemented to be mathematically equivalent. Here, this can be especially handy as we can combine the creation of patches and projection into two lines of code:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;layer = torch.nn.Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))

image = torch.rand(batch_size, 3, 48, 48)
projected_patches = layer(image)

print(projected_patches.flatten(-2).transpose(-1, -2).shape)
# This prints
# torch.Size([1, 9, 768])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h3 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.1.3 Image vs text tokenization
   &lt;/strong&gt;
  &lt;/h3&gt;
  &lt;p&gt;
   Now that we briefly discussed the purpose of the image encoder (and the linear projection that is part of the encoder), let&#x27;s return to the text tokenization analogy from earlier and look at text and image tokenization and embedding side by side, as depicted in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!zjmg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!zjmg!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png 424w, https://substackcdn.com/image/fetch/$s_!zjmg!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png 848w, https://substackcdn.com/image/fetch/$s_!zjmg!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png 1272w, https://substackcdn.com/image/fetch/$s_!zjmg!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1154,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1154&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!zjmg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!zjmg!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png 424w, https://substackcdn.com/image/fetch/$s_!zjmg!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png 848w, https://substackcdn.com/image/fetch/$s_!zjmg!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png 1272w, https://substackcdn.com/image/fetch/$s_!zjmg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F0d56ea06-d202-4eb7-9e01-9aac492ee309_1522x1206.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Image tokenization and embedding (left) and text tokenization and embedding (right) side by side.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    As you can see in the figure above, I included an additional
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     projector
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    module that follows the image encoder. This
   &lt;/span&gt;
   &lt;em&gt;
    projector
   &lt;/em&gt;
   &lt;span&gt;
    is usually just another
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     linear projection
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    layer that is similar to the one explained earlier. The purpose is to project the image encoder outputs into a dimension that matches the dimensions of the embedded text tokens, as illustrated in the figure below. (As we will see later, the projector is sometimes also called adapter, adaptor, or connector.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!TaTW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!TaTW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png 424w, https://substackcdn.com/image/fetch/$s_!TaTW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png 848w, https://substackcdn.com/image/fetch/$s_!TaTW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png 1272w, https://substackcdn.com/image/fetch/$s_!TaTW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1173,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1173&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!TaTW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!TaTW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png 424w, https://substackcdn.com/image/fetch/$s_!TaTW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png 848w, https://substackcdn.com/image/fetch/$s_!TaTW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png 1272w, https://substackcdn.com/image/fetch/$s_!TaTW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d0be64c-da90-4193-86db-804f6a8a0abb_1542x1242.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Another side-by-side comparison between image tokenization and text tokenization, where the role of the projector is to match the text token embedding dimensions.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Now that the image patch embeddings have the same embedding dimension as the text token embeddings, we can simply concatenate them as input to the LLM, as shown in the figure at the beginning of this section. Below is the same figure again for easier reference.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!FTft!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FTft!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png 424w, https://substackcdn.com/image/fetch/$s_!FTft!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png 848w, https://substackcdn.com/image/fetch/$s_!FTft!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png 1272w, https://substackcdn.com/image/fetch/$s_!FTft!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a219f185-211b-4569-9398-2e080e2c5619_1166x1400.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1400,&quot;width&quot;:1166,&quot;resizeWidth&quot;:471,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;565.5231560891938&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!FTft!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FTft!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png 424w, https://substackcdn.com/image/fetch/$s_!FTft!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png 848w, https://substackcdn.com/image/fetch/$s_!FTft!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png 1272w, https://substackcdn.com/image/fetch/$s_!FTft!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa219f185-211b-4569-9398-2e080e2c5619_1166x1400.png 1456w&quot; width=&quot;471&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      After projecting the image patch tokens into the same dimension as the text token embeddings, we can simply concatenate them as input to a standard LLM.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    By the way, the image encoder we discussed in this section is usually a pretrained vision transformer. A popular choice is
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/openai/CLIP&quot; rel=&quot;&quot;&gt;
    CLIP
   &lt;/a&gt;
   &lt;span&gt;
    or
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/mlfoundations/open_clip&quot; rel=&quot;&quot;&gt;
    OpenCLIP
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    However, there are also versions of Method A that operate directly on patches, such as
   &lt;/span&gt;
   &lt;a href=&quot;https://www.adept.ai/blog/fuyu-8b&quot; rel=&quot;&quot;&gt;
    Fuyu
   &lt;/a&gt;
   &lt;span&gt;
    , which is shown in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!LB1L!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!LB1L!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png 424w, https://substackcdn.com/image/fetch/$s_!LB1L!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png 848w, https://substackcdn.com/image/fetch/$s_!LB1L!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png 1272w, https://substackcdn.com/image/fetch/$s_!LB1L!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:587,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;587&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!LB1L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!LB1L!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png 424w, https://substackcdn.com/image/fetch/$s_!LB1L!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png 848w, https://substackcdn.com/image/fetch/$s_!LB1L!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png 1272w, https://substackcdn.com/image/fetch/$s_!LB1L!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F28269d0d-b806-4ae7-bf96-b282affd7e93_1600x645.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;span&gt;
       Annotated figure of the Fuyu multimodal LLM that operates directly on the image patches without image encoder. (Annotated figure from
      &lt;/span&gt;
      &lt;a href=&quot;https://www.adept.ai/blog/fuyu-8b&quot; rel=&quot;&quot;&gt;
       https://www.adept.ai/blog/fuyu-8b
      &lt;/a&gt;
      &lt;span&gt;
       .)
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    As illustrated in the figure above, Fuyu passes the input patches directly into a linear projection (or embedding layer) to learn its own image patch embeddings rather than relying on an additional pretrained image encoder like other models and methods do. This greatly simplifies the architecture and training setup.
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    2.2 Method B: Cross-Modality Attention Architecture
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Now that we have discussed the unified embedding decoder architecture approach to building multimodal LLMs and understand the basic concept behind image encoding, let&#x27;s talk about an alternative way of implementing multimodal LLMs via cross-attention, as summarized in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!7Xvv!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!7Xvv!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png 424w, https://substackcdn.com/image/fetch/$s_!7Xvv!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png 848w, https://substackcdn.com/image/fetch/$s_!7Xvv!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png 1272w, https://substackcdn.com/image/fetch/$s_!7Xvv!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1338,&quot;width&quot;:1296,&quot;resizeWidth&quot;:525,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;542.0138888888889&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!7Xvv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!7Xvv!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png 424w, https://substackcdn.com/image/fetch/$s_!7Xvv!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png 848w, https://substackcdn.com/image/fetch/$s_!7Xvv!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png 1272w, https://substackcdn.com/image/fetch/$s_!7Xvv!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd9c06055-b959-45d1-87b2-1f4e90ceaf2d_1296x1338.png 1456w&quot; width=&quot;525&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An illustration of the Cross-Modality Attention Architecture approach to building multimodal LLMs.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In the Cross-Modality Attention Architecture method depicted in the figure above, we still use the same image encoder setup we discussed previously. However, instead of encoding the patches as input to the LLM, we connect the input patches in the multi-head attention layer via a cross-attention mechanism.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The idea is related and goes back to the original transformer architecture from the 2017
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
    Attention Is All You Need
   &lt;/a&gt;
   &lt;span&gt;
    paper, highlighted in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!JYyE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JYyE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png 424w, https://substackcdn.com/image/fetch/$s_!JYyE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png 848w, https://substackcdn.com/image/fetch/$s_!JYyE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png 1272w, https://substackcdn.com/image/fetch/$s_!JYyE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1582,&quot;width&quot;:1370,&quot;resizeWidth&quot;:451,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;520.7897810218979&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!JYyE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JYyE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png 424w, https://substackcdn.com/image/fetch/$s_!JYyE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png 848w, https://substackcdn.com/image/fetch/$s_!JYyE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png 1272w, https://substackcdn.com/image/fetch/$s_!JYyE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5d028b95-7965-43e0-b8fc-350609a69377_1370x1582.png 1456w&quot; width=&quot;451&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      High-level illustration of the cross-attention mechanism used in the original transformer architecture. (Annotated figure from the &quot;Attention Is All You Need&quot; paper: https://arxiv.org/abs/1706.03762.)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that the original &quot;Attention Is All You Need&quot; transformer depicted in the figure above was originally developed for language translation. So, it consists of a text
   &lt;/span&gt;
   &lt;strong&gt;
    en
   &lt;/strong&gt;
   &lt;span&gt;
    coder (left part of the figure) that takes the sentence to be translated and generates the translation via a text
   &lt;/span&gt;
   &lt;strong&gt;
    de
   &lt;/strong&gt;
   &lt;span&gt;
    coder (right part of the figure). In the context of multimodal LLM, the encoder is an image encoder instead of a text encoder, but the same idea applies.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   How does cross-attention work? Let&#x27;s have a look at a conceptual drawing of what happens inside the regular self-attention mechanism.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!HqoQ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!HqoQ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png 424w, https://substackcdn.com/image/fetch/$s_!HqoQ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png 848w, https://substackcdn.com/image/fetch/$s_!HqoQ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png 1272w, https://substackcdn.com/image/fetch/$s_!HqoQ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1194,&quot;width&quot;:1440,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1194&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!HqoQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!HqoQ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png 424w, https://substackcdn.com/image/fetch/$s_!HqoQ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png 848w, https://substackcdn.com/image/fetch/$s_!HqoQ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png 1272w, https://substackcdn.com/image/fetch/$s_!HqoQ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff763532b-1eed-4f7d-ae2c-7783d4f4fc46_1440x1194.png 1456w&quot; width=&quot;1440&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Outline of the regular self-attention mechanism. (This flow depicts one of the heads in a regular multi-head attention module.)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the figure above, x is the input, and
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     W
    &lt;/span&gt;
    &lt;sub&gt;
     q
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    is a weight matrix used to generate the queries (
   &lt;/span&gt;
   &lt;em&gt;
    Q
   &lt;/em&gt;
   &lt;span&gt;
    ). Similarly,
   &lt;/span&gt;
   &lt;em&gt;
    K
   &lt;/em&gt;
   &lt;span&gt;
    stands for keys, and
   &lt;/span&gt;
   &lt;em&gt;
    V
   &lt;/em&gt;
   &lt;span&gt;
    stands for values. A represents the attention scores matrix, and
   &lt;/span&gt;
   &lt;em&gt;
    Z
   &lt;/em&gt;
   &lt;span&gt;
    are the inputs (x) transformed into the output context vectors. (If this seems confusing, you may find a comprehensive introduction in Chapter 3 of my
   &lt;/span&gt;
   &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167/&quot; rel=&quot;&quot;&gt;
    Build a Large Language Model from Scratch book
   &lt;/a&gt;
   &lt;span&gt;
    helpful; alternatively, you may also find my article,
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention&quot; rel=&quot;&quot;&gt;
    Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs
   &lt;/a&gt;
   &lt;span&gt;
    helpful here.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In cross-attention, in contrast to self-attention, we have two different input sources, as illustrated in the following figure.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!3PZD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3PZD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png 424w, https://substackcdn.com/image/fetch/$s_!3PZD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png 848w, https://substackcdn.com/image/fetch/$s_!3PZD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png 1272w, https://substackcdn.com/image/fetch/$s_!3PZD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/fe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1081,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1081&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!3PZD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3PZD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png 424w, https://substackcdn.com/image/fetch/$s_!3PZD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png 848w, https://substackcdn.com/image/fetch/$s_!3PZD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png 1272w, https://substackcdn.com/image/fetch/$s_!3PZD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ffe4cc6f4-ca9a-431b-b572-95a1fda373a7_1508x1120.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;span&gt;
       Illustration of cross attention, where there can be two different inputs x
      &lt;/span&gt;
      &lt;sub&gt;
       1
      &lt;/sub&gt;
      &lt;span&gt;
       and x
      &lt;/span&gt;
      &lt;sub&gt;
       2
      &lt;/sub&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As illustrated in the previous two figures, in self-attention, we work with the same input sequence. In cross-attention, we mix or combine two different input sequences.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the case of the original transformer architecture in the
   &lt;/span&gt;
   &lt;em&gt;
    Attention Is All You Need
   &lt;/em&gt;
   &lt;span&gt;
    paper, the two inputs
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     x
    &lt;/span&gt;
    &lt;sub&gt;
     1
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     x
    &lt;/span&gt;
    &lt;sub&gt;
     2
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    correspond to the sequence returned by the encoder module on the left (
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     x
    &lt;/span&gt;
    &lt;sub&gt;
     2
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ) and the input sequence being processed by the decoder part on the right (
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     x
    &lt;/span&gt;
    &lt;sub&gt;
     1
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ). In the context of a multimodal LLM,
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     x
    &lt;/span&gt;
    &lt;sub&gt;
     2
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    is the output of an image encoder. (Note that the queries usually come from the decoder, and the keys and values typically come from the encoder.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that in cross-attention, the two input sequences
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     x
    &lt;/span&gt;
    &lt;sub&gt;
     1
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     x
    &lt;/span&gt;
    &lt;sub&gt;
     2
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    can have different numbers of elements. However, their embedding dimensions must match. If we set
   &lt;/span&gt;
   &lt;em&gt;
    &lt;span&gt;
     x
    &lt;/span&gt;
    &lt;sub&gt;
     1
    &lt;/sub&gt;
    &lt;span&gt;
     = x
    &lt;/span&gt;
    &lt;sub&gt;
     2
    &lt;/sub&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , this is equivalent to self-attention.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   3. Unified decoder and cross-attention model training
  &lt;/h1&gt;
  &lt;p&gt;
   Now that we have talked a bit about the two major multimodal design choices, let&#x27;s briefly talk about how we deal with the three major components during model training, which are summarized in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!e2P-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e2P-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png 424w, https://substackcdn.com/image/fetch/$s_!e2P-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png 848w, https://substackcdn.com/image/fetch/$s_!e2P-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png 1272w, https://substackcdn.com/image/fetch/$s_!e2P-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:701,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;701&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!e2P-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!e2P-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png 424w, https://substackcdn.com/image/fetch/$s_!e2P-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png 848w, https://substackcdn.com/image/fetch/$s_!e2P-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png 1272w, https://substackcdn.com/image/fetch/$s_!e2P-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F24a12032-d32e-41f6-b390-4e321e1ea29f_1600x770.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An overview of the different components in a multimodal LLM. The components numbered 1-3 can be frozen or unfrozen during the multimodal training process.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Similar to the development of traditional text-only LLMs, the training of multimodal LLMs also involves two phases: pretraining and instruction finetuning. However, unlike starting from scratch, multimodal LLM training typically begins with a pretrained, instruction-finetuned text-only LLM as the base model.
  &lt;/p&gt;
  &lt;p&gt;
   For the image encoder, CLIP is commonly used and often remains unchanged during the entire training process, though there are exceptions, as we will explore later. Keeping the LLM part frozen during the pretraining phase is also usual, focusing only on training the projector—a linear layer or a small multi-layer perceptron. Given the projector&#x27;s limited learning capacity, usually comprising just one or two layers, the LLM is often unfrozen during multimodal instruction finetuning (stage 2) to allow for more comprehensive updates. However, note that in the cross-attention-based models (Method B), the cross-attention layers are unfrozen throughout the entire training process.
  &lt;/p&gt;
  &lt;p&gt;
   After introducing the two primary approaches (Method A: Unified Embedding Decoder Architecture and Method B: Cross-modality Attention Architecture), you might be wondering which is more effective. The answer depends on specific trade-offs.
  &lt;/p&gt;
  &lt;p&gt;
   The Unified Embedding Decoder Architecture (Method A) is typically easier to implement since it doesn&#x27;t require any modifications to the LLM architecture itself.
  &lt;/p&gt;
  &lt;p&gt;
   The Cross-modality Attention Architecture (Method B) is often considered more computationally efficient because it doesn&#x27;t overload the input context with additional image tokens, introducing them later in the cross-attention layers instead. Additionally, this approach maintains the text-only performance of the original LLM if the LLM parameters are kept frozen during training.
  &lt;/p&gt;
  &lt;p&gt;
   We will revisit the discussion on modeling performance and response quality in a later section, where we will discuss NVIDIA&#x27;s NVLM paper.
  &lt;/p&gt;
  &lt;p&gt;
   This marks the end of what turned out to be a rather extensive introduction to multimodal LLMs. As I write this, I realize that the discussion has become lengthier than initially planned, which probably makes this a good place to conclude the article.
  &lt;/p&gt;
  &lt;p&gt;
   However, to provide a practical perspective, it would be nice to examine a few recent research papers that implement these approaches. So, we will explore these papers in the remaining sections of this article.
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   4. Recent multimodal models and methods
  &lt;/h1&gt;
  &lt;p&gt;
   For the remainder of this article, I will review recent literature concerning multimodal LLMs, focusing specifically on works published in the last few weeks to maintain a reasonable scope.
  &lt;/p&gt;
  &lt;p&gt;
   Thus, this is not a historical overview or comprehensive review of multimodal LLMs but rather a brief look at the latest developments. I will also try to keep these summaries short and without too much fluff as there are 10 of them.
  &lt;/p&gt;
  &lt;p&gt;
   The conclusion section at the end of this has an overview that compares the methods used in these papers.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.1 The Llama 3 Herd of Models
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2407.21783&quot; rel=&quot;&quot;&gt;
     The Llama 3 Herd of Models
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper (July 31, 2024) by Meta AI came out earlier this summer, which feels like ages ago in LLM terms. However, given that they only described but did not release their multimodal models until much later, I think it&#x27;s fair to include Llama 3 in this list. (Llama 3.2 models were officially announced and made available on September 25.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The multimodal Llama 3.2 models, which come in an 11-billion and 90-billion parameter version, are image-text models that use the previously described cross-attention-based approach, which is illustrated in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!fTYU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!fTYU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png 424w, https://substackcdn.com/image/fetch/$s_!fTYU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png 848w, https://substackcdn.com/image/fetch/$s_!fTYU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png 1272w, https://substackcdn.com/image/fetch/$s_!fTYU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:758,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;758&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!fTYU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!fTYU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png 424w, https://substackcdn.com/image/fetch/$s_!fTYU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png 848w, https://substackcdn.com/image/fetch/$s_!fTYU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png 1272w, https://substackcdn.com/image/fetch/$s_!fTYU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7c8578fa-70f2-474f-9e98-87621f2dce96_1600x833.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of the multimodal LLM approach used by Llama 3.2. (Annotated figure from the Llama 3 paper: https://arxiv.org/abs/2407.21783.The video and speech parts are visually occluded to focus the attention on the image part.)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Note that while the figure also depicts video and speech as possible modalities, the models that were released as of this writing focus only on image and text.
  &lt;/p&gt;
  &lt;p&gt;
   Llama 3.2 uses the cross-attention-based approach. However, it differs a bit from what I wrote about earlier, namely that in multimodal LLM development, we usually freeze the image encoder and only update the LLM parameters during pretraining.
  &lt;/p&gt;
  &lt;p&gt;
   Here, the researchers almost take the opposite approach: they update the image encoder but do not update the language model&#x27;s parameters. They write that this is intentional and done to preserve the text-only capabilities so that the 11B and 90B multimodal models can be used as drop-in replacements for the Llama 3.1 8B and 70B text-only model on text tasks.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The training itself is done in multiple iterations, starting with the Llama 3.1 text models. After adding the image encoder and projection (here called &quot;adapter&quot;) layers, they pretrain the model on image-text data. Then, similar to the Llama 3 model text-only training (I wrote about it in
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/i/147749119/llama-overview&quot; rel=&quot;&quot;&gt;
    an earlier article
   &lt;/a&gt;
   &lt;span&gt;
    ), they follow up with instruction and preference finetuning.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Instead of adopting a pretrained model such as CLIP as an image encoder, the researchers used a vision transformer that they pretrained from scratch. Specifically, they adopted the  ViT-H/14 variant (630 million parameters) of the classic vision transformer architecture (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2010.11929&quot; rel=&quot;&quot;&gt;
    Dosovitskiy et al., 2020
   &lt;/a&gt;
   &lt;span&gt;
    ). They then pretrained the ViT on a dataset of 2.5 billion image-text pairs over five epochs; this was done before connecting the image encoder to the LLM. (The image encoder takes 224×224 resolution images and divides them into a 14×14 grid of patches, with each patch sized at 16×16 pixels.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As the cross-attention layers add a substantial amount of parameters, they are only added in every fourth transformer block. (For the 8B model, this adds 3B parameters, and for the 70B model, this adds 20 billion parameters.)
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.2 Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://www.arxiv.org/abs/2409.17146&quot; rel=&quot;&quot;&gt;
     The Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper (September 25, 2024) is notable because it promises to open source not only the model weights but also the dataset and source code similar to the language-only OLMo LLM. (This is great for LLM research as it allows us to take a look at the exact training procedure and code and also lets us run ablation studies and reproduce results on the same dataset.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   If you are wondering why there are two names in the paper title, Molmo refers to the model (Multimodal Open Language Model), and PixMo (Pixels for Molmo) is the dataset.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!9P0w!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!9P0w!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png 424w, https://substackcdn.com/image/fetch/$s_!9P0w!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png 848w, https://substackcdn.com/image/fetch/$s_!9P0w!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png 1272w, https://substackcdn.com/image/fetch/$s_!9P0w!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:704,&quot;width&quot;:1104,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;704&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!9P0w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!9P0w!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png 424w, https://substackcdn.com/image/fetch/$s_!9P0w!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png 848w, https://substackcdn.com/image/fetch/$s_!9P0w!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png 1272w, https://substackcdn.com/image/fetch/$s_!9P0w!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F73337002-8feb-4f1b-a109-1407096e32c5_1104x704.png 1456w&quot; width=&quot;1104&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of the Molmo decoder-only approach (Method A). Annotated figure adapted from the Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models paper: https://www.arxiv.org/abs/2409.17146.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;br/&gt;
   &lt;span&gt;
    As illustrated in the figure above, the image encoder employs an off-the-shelf vision transformer, specifically CLIP. The term &quot;connector&quot; here refers to a &quot;projector&quot; that aligns image features with the language model.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Molmo streamlines the training process by avoiding multiple pretraining stages, choosing instead a simple pipeline that updates all parameters in a unified approach—including those of the base LLM, the connector, and the image encoder.
  &lt;/p&gt;
  &lt;p&gt;
   The Molmo team offers several options for the base LLM:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     OLMo-7B-1024 (a fully open model backbone),
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     OLMoE-1B-7B (a mixture-of-experts architecture; the most efficient model),
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Qwen2 7B (an open-weight model that performs better than OLMo-7B-1024),
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Qwen2 72B (an open-weight model and the best-performing model)
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.3 NVLM: Open Frontier-Class Multimodal LLMs
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    NVIDIA&#x27;s
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2409.11402&quot; rel=&quot;&quot;&gt;
     NVLM: Open Frontier-Class Multimodal LLMs
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper (September 17, 2024) is particularly interesting because, rather than focusing on a single approach, it explores both methods:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Method A, the Unified Embedding Decoder Architecture (&quot;decoder-only architecture,&quot; NVLM-D), and
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Method B, the Cross-Modality Attention Architecture (&quot;cross-attention-based architecture,&quot; NVLM-X).
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Additionally, they develop a hybrid approach (NVLM-H) and provide an apples-to-apples comparison of all three methods.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!6n6Y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!6n6Y!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png 424w, https://substackcdn.com/image/fetch/$s_!6n6Y!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png 848w, https://substackcdn.com/image/fetch/$s_!6n6Y!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png 1272w, https://substackcdn.com/image/fetch/$s_!6n6Y!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:844,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;844&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!6n6Y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!6n6Y!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png 424w, https://substackcdn.com/image/fetch/$s_!6n6Y!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png 848w, https://substackcdn.com/image/fetch/$s_!6n6Y!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png 1272w, https://substackcdn.com/image/fetch/$s_!6n6Y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F45916952-b1ee-4972-a956-e45703e3fe36_1600x927.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Overview of the three multimodal approaches. (Annotated figure from the NVLM: Open Frontier-Class Multimodal LLMs paper: https://arxiv.org/abs/2409.11402)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   As summarized in the figure below, NVLM-D corresponds to Method A, and NVLM-X corresponds to Method B, as discussed earlier. The concept behind the hybrid model (NVLM-H) is to combine the strengths of both methods: an image thumbnail is provided as input, followed by a dynamic number of patches passed through cross-attention to capture finer high-resolution details.
  &lt;/p&gt;
  &lt;p&gt;
   In short, the research team find that:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     NVLM-X demonstrates superior computational efficiency for high-resolution images.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     NVLM-D achieves higher accuracy in OCR-related tasks.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     NVLM-H combines the advantages of both methods.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Similar to Molmo and other approaches, they begin with a text-only LLM rather than pretraining a multimodal model from scratch (as this generally performs better). Additionally, they use an instruction-tuned LLM instead of a base LLM. Specifically, the backbone LLM is Qwen2-72B-Instruct (to my knowledge, Molmo used the Qwen2-72B base model).
  &lt;/p&gt;
  &lt;p&gt;
   While training all LLM parameters in the NVLM-D approach, they found that for NVLM-X, it works well to freeze the original LLM parameters and train only the cross-attention layers during both pretraining and instruction finetuning.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For the image encoder, instead of using a typical CLIP model, they use
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2312.14238&quot; rel=&quot;&quot;&gt;
    InternViT-6B
   &lt;/a&gt;
   &lt;span&gt;
    , which remains frozen throughout all stages.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The projector is a multilayer perceptron rather than a single linear layer.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.4 Qwen2-VL: Enhancing Vision-Language Model’s Perception of the World at Any Resolution
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The previous two papers and models, Molmo and NVLM, were based on Qwen2-72B LLM. In this paper, the Qwen research team itself announces a multimodal LLM,
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2409.12191&quot; rel=&quot;&quot;&gt;
     Qwen2-VL: Enhancing Vision-Language Model&#x27;s Perception of the World at Any Resolution
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    (October 3rd, 2024).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   At the core of this work is their so-called &quot;Naive Dynamic Resolution&quot; mechanism (the term &quot;naive&quot; is intentional and not a typo for &quot;native,&quot; though &quot;native&quot; could also be fitting). This mechanism allows the model to handle images of varying resolutions without simple downsampling, enabling the input of images in their original resolution.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Zrt8!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Zrt8!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png 424w, https://substackcdn.com/image/fetch/$s_!Zrt8!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png 848w, https://substackcdn.com/image/fetch/$s_!Zrt8!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png 1272w, https://substackcdn.com/image/fetch/$s_!Zrt8!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2247e684-253a-462e-afb4-549411d5741a_1490x1068.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1044,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1044&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Zrt8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Zrt8!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png 424w, https://substackcdn.com/image/fetch/$s_!Zrt8!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png 848w, https://substackcdn.com/image/fetch/$s_!Zrt8!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png 1272w, https://substackcdn.com/image/fetch/$s_!Zrt8!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2247e684-253a-462e-afb4-549411d5741a_1490x1068.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An overview of the multimodal Qwen model, which can process input images with various different resolutions natively. (Annotated figure from the Qwen2-VL paper: https://arxiv.org/abs/2409.12191)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The native resolution input is implemented via a modified ViT by removing the original absolute position embeddings and introducing 2D-RoPE.
  &lt;/p&gt;
  &lt;p&gt;
   They used a classic vision encoder with 675M parameters and LLM backbones of varying sizes, as shown in the table below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!NdAJ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!NdAJ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png 424w, https://substackcdn.com/image/fetch/$s_!NdAJ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png 848w, https://substackcdn.com/image/fetch/$s_!NdAJ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png 1272w, https://substackcdn.com/image/fetch/$s_!NdAJ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:482,&quot;width&quot;:1396,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;482&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!NdAJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!NdAJ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png 424w, https://substackcdn.com/image/fetch/$s_!NdAJ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png 848w, https://substackcdn.com/image/fetch/$s_!NdAJ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png 1272w, https://substackcdn.com/image/fetch/$s_!NdAJ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2ce9ce4a-d7ec-476d-91cb-29b6f5440b3b_1396x482.png 1456w&quot; width=&quot;1396&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     The components of the different Qwen2-VL models. (Annotated figure from the Qwen2-VL paper: https://arxiv.org/abs/2409.12191)
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The training itself consists of 3 stages: (1) pretraining only the image encoder, (2) unfreezing all parameters (including LLM), and (3) freezing the image encoder and instruction-finetuning only the LLM.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.5 Pixtral 12B
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://mistral.ai/news/pixtral-12b/&quot; rel=&quot;&quot;&gt;
     Pixtral 12B
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    (September 17, 2024), which uses the Method A: Unified Embedding Decoder Architecture approach, is the first multimodal model from Mistral AI. Unfortunately, there is no technical paper or report available, but the Mistral team shared a few interesting tidbits in their
   &lt;/span&gt;
   &lt;a href=&quot;https://mistral.ai/news/pixtral-12b/&quot; rel=&quot;&quot;&gt;
    blog post
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Interestingly, they chose not to use a pretrained image encoder, instead training one with 400 million parameters from scratch. For the LLM backbone, they used the 12-billion-parameter
   &lt;/span&gt;
   &lt;a href=&quot;https://mistral.ai/news/mistral-nemo/&quot; rel=&quot;&quot;&gt;
    Mistral NeMo
   &lt;/a&gt;
   &lt;span&gt;
    model.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Similar to Qwen2-VL, Pixtral also supports variable image sizes natively, as illustrated in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!eW3C!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!eW3C!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png 424w, https://substackcdn.com/image/fetch/$s_!eW3C!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png 848w, https://substackcdn.com/image/fetch/$s_!eW3C!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png 1272w, https://substackcdn.com/image/fetch/$s_!eW3C!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:726,&quot;width&quot;:1144,&quot;resizeWidth&quot;:611,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;387.75&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!eW3C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!eW3C!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png 424w, https://substackcdn.com/image/fetch/$s_!eW3C!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png 848w, https://substackcdn.com/image/fetch/$s_!eW3C!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png 1272w, https://substackcdn.com/image/fetch/$s_!eW3C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F37bb0f12-4533-4f44-8907-1da868006ff3_1144x726.png 1456w&quot; width=&quot;611&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of how Pixtral processes images of different sizes. (Annotated figure from the Pixtral blog  post: https://mistral.ai/news/pixtral-12b/)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.6 MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM Fine-tuning
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2409.20566&quot; rel=&quot;&quot;&gt;
     MM1.5: Methods, Analysis &amp; Insights from Multimodal LLM Fine-tuning
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper (September 30, 2024) provides practical tips and introduces a mixture-of-experts multimodal model alongside a dense model similar to Molmo. The models span a wide size range, from 1 billion to 30 billion parameters.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The models described in this paper focuse on Method A, a Unified Embedding Transformer Architecture, which structures inputs effectively for multimodal learning.
  &lt;/p&gt;
  &lt;p&gt;
   In addition, the paper has a series of interesting ablation studies looking into data mixtures and the effects of using coordinate tokens.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!fMsE!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!fMsE!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png 424w, https://substackcdn.com/image/fetch/$s_!fMsE!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png 848w, https://substackcdn.com/image/fetch/$s_!fMsE!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png 1272w, https://substackcdn.com/image/fetch/$s_!fMsE!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1178,&quot;width&quot;:1402,&quot;resizeWidth&quot;:645,&quot;bytes&quot;:988570,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;541.9472182596292&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!fMsE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!fMsE!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png 424w, https://substackcdn.com/image/fetch/$s_!fMsE!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png 848w, https://substackcdn.com/image/fetch/$s_!fMsE!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png 1272w, https://substackcdn.com/image/fetch/$s_!fMsE!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F71b22b97-e901-4c5f-a9c2-67e32c867823_1402x1178.png 1456w&quot; width=&quot;645&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Illustration of the MM1.5 approach, which includes additional coordinate tokens to denote bounding boxes. (Annotated figure from the MM1.5 paper: https://arxiv.org/abs/2409.20566.)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.7 Aria: An Open Multimodal Native Mixture-of-Experts Model
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2410.05993&quot; rel=&quot;&quot;&gt;
     Aria: An Open Multimodal Native Mixture-of-Experts Model
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper (October 8, 2024) introduces another mixture-of-experts model approach, similar to one of the variants in the Molmo and MM1.5 lineups.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The Aria model has 24.9 billion parameters, with 3.5 billion parameters allocated per text token. The image encoder (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2303.15343&quot; rel=&quot;&quot;&gt;
    SigLIP
   &lt;/a&gt;
   &lt;span&gt;
    ) has 438-million-parameters.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This model is based on a cross-attention approach with the following overall training procedure:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Training the LLM backbone entirely from scratch.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Pretraining both the LLM backbone and the vision encoder.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.8 Baichuan-Omni
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2410.08565&quot; rel=&quot;&quot;&gt;
     Baichuan-Omni Technical Report
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    (October 11, 2024) introduces Baichuan-Omni, a 7-billion-parameter multimodal LLM based on Method A: the Unified Embedding Decoder Architecture approach, as shown in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!-IYi!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-IYi!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png 424w, https://substackcdn.com/image/fetch/$s_!-IYi!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png 848w, https://substackcdn.com/image/fetch/$s_!-IYi!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png 1272w, https://substackcdn.com/image/fetch/$s_!-IYi!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:918,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:730957,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;918&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!-IYi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-IYi!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png 424w, https://substackcdn.com/image/fetch/$s_!-IYi!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png 848w, https://substackcdn.com/image/fetch/$s_!-IYi!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png 1272w, https://substackcdn.com/image/fetch/$s_!-IYi!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F142c39bd-2d3f-4813-9363-5ecf616cb784_2102x1326.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An overview of the Baichuan-Omni model, which can handle various input modalities. (Annotated figure from the Baichuan-Omni paper: https://arxiv.org/abs/2410.08565)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The training process for Baichuan-Omni involves a three-stage approach:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;strong&gt;
      Projector training
     &lt;/strong&gt;
     &lt;span&gt;
      : Initially, only the projector is trained, while both the vision encoder and the language model (LLM) remain frozen.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;strong&gt;
      Vision encoder training
     &lt;/strong&gt;
     &lt;span&gt;
      : Next, the vision encoder is unfrozen and trained, with the LLM still frozen.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;strong&gt;
      Full model training
     &lt;/strong&gt;
     &lt;span&gt;
      : Finally, the LLM is unfrozen, allowing the entire model to be trained end-to-end.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   &lt;span&gt;
    The model utilizes the SigLIP vision encoder and incorporates the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2204.07156&quot; rel=&quot;&quot;&gt;
    AnyRes
   &lt;/a&gt;
   &lt;span&gt;
    module to handle high-resolution images through down-sampling techniques.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While the report does not explicitly specify the LLM backbone, it is likely based on the Baichuan 7B LLM, given the model&#x27;s parameter size and the naming convention.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.9 Emu3: Next-Token Prediction is All You Need
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;em&gt;
    Emu3: Next-Token Prediction is All You Need
   &lt;/em&gt;
   &lt;span&gt;
    paper (September 27, 2024) presents a compelling alternative to diffusion models for image generation, which is solely based on a transformer-based decoder architecture. Although it&#x27;s not a multimodal LLM in the classic sense (i.e., models focused on image understanding rather than generation), Emu3 is super interesting as it demonstrates that it&#x27;s possible to use transformer decoders for image generation, which is a task typically dominated by diffusion methods. (However, note that there have been other similar approaches before, such as
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2406.06525&quot; rel=&quot;&quot;&gt;
    Autoregressive Model Beats Diffusion: Llama for Scalable Image Generation
   &lt;/a&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!IWU7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!IWU7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png 424w, https://substackcdn.com/image/fetch/$s_!IWU7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png 848w, https://substackcdn.com/image/fetch/$s_!IWU7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png 1272w, https://substackcdn.com/image/fetch/$s_!IWU7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:904,&quot;width&quot;:1056,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;904&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!IWU7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!IWU7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png 424w, https://substackcdn.com/image/fetch/$s_!IWU7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png 848w, https://substackcdn.com/image/fetch/$s_!IWU7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png 1272w, https://substackcdn.com/image/fetch/$s_!IWU7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F775db9c7-662f-4314-a5c4-c3f5efe0238d_1056x904.png 1456w&quot; width=&quot;1056&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Emu3 is primarily an LLM for image generation as an alternative to diffusion models. (Annotated figure from the Emu3 paper: https://arxiv.org/abs/2409.18869)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    The researchers trained Emu3 from scratch and then used
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch07/04_preference-tuning-with-dpo/dpo-from-scratch.ipynb&quot; rel=&quot;&quot;&gt;
    Direct Preference Optimization
   &lt;/a&gt;
   &lt;span&gt;
    (DPO) to align the model with human preferences.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The architecture includes a vision tokenizer inspired by
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2209.09002&quot; rel=&quot;&quot;&gt;
    SBER-MoVQGAN
   &lt;/a&gt;
   &lt;span&gt;
    . The core LLM architecture is based on Llama 2, yet it is trained entirely from scratch.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    4.10 Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    We previously focused on multimodal LLMs for image understanding and just saw one example for image generation with Emu 3 above. Now, the
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2410.13848&quot; rel=&quot;&quot;&gt;
     Janus: Decoupling Visual Encoding for Unified Multimodal Understanding and Generation
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper (October 17, 2024) introduces a framework that unifies multimodal understanding and generation tasks within a single LLM backbone.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   A key feature of Janus is the decoupling of visual encoding pathways to address the distinct requirements of understanding and generation tasks. The researchers argue that image understanding tasks require high-dimensional semantic representations, while generation tasks require detailed local information and global consistency in images. By separating these pathways, Janus effectively manages these differing needs.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The model employs the SigLIP vision encoder, similar to that used in Baichuan-Omni, for processing visual inputs. For image generation, it utilizes a
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2406.06525&quot; rel=&quot;&quot;&gt;
    Vector Quantized (VQ)
   &lt;/a&gt;
   &lt;span&gt;
    tokenizer to handle the generation process. The base LLM in Janus is the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2401.02954&quot; rel=&quot;&quot;&gt;
    DeepSeek-LLM
   &lt;/a&gt;
   &lt;span&gt;
    with 1.3 billion parameters.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!9UFg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!9UFg!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png 424w, https://substackcdn.com/image/fetch/$s_!9UFg!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png 848w, https://substackcdn.com/image/fetch/$s_!9UFg!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png 1272w, https://substackcdn.com/image/fetch/$s_!9UFg!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89d62626-4386-4e73-8992-158550752ce2_1434x692.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:692,&quot;width&quot;:1434,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;692&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!9UFg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!9UFg!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png 424w, https://substackcdn.com/image/fetch/$s_!9UFg!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png 848w, https://substackcdn.com/image/fetch/$s_!9UFg!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png 1272w, https://substackcdn.com/image/fetch/$s_!9UFg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89d62626-4386-4e73-8992-158550752ce2_1434x692.png 1456w&quot; width=&quot;1434&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      An overview of the unified decoder-only framework used in Janus. (Annotated figure from the Janus paper: https://arxiv.org/abs/2410.13848.)
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The training process for the model in this image follows three stages, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Da5n!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Da5n!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png 424w, https://substackcdn.com/image/fetch/$s_!Da5n!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png 848w, https://substackcdn.com/image/fetch/$s_!Da5n!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png 1272w, https://substackcdn.com/image/fetch/$s_!Da5n!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2fb4f079-0771-4d21-8805-fded73134983_1536x648.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:614,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:218868,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;614&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Da5n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Da5n!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png 424w, https://substackcdn.com/image/fetch/$s_!Da5n!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png 848w, https://substackcdn.com/image/fetch/$s_!Da5n!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png 1272w, https://substackcdn.com/image/fetch/$s_!Da5n!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2fb4f079-0771-4d21-8805-fded73134983_1536x648.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Illustration of the 3-stage training process of the Janus model. (Annotated figure from the Janus paper: https://arxiv.org/abs/2410.13848)
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   In Stage I, only the projector layers and image output layer are trained while the LLM, understanding, and generation encoders remain frozen. In Stage II, the LLM backbone and text output layer are unfrozen, allowing for unified pretraining across understanding and generation tasks. Finally, in Stage III, the entire model, including the SigLIP image encoder, is unfrozen for supervised fine-tuning, enabling the model to fully integrate and refine its multimodal capabilities.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Conclusion
  &lt;/h1&gt;
  &lt;p&gt;
   As you may have noticed, I almost entirely skipped both the modeling and the computational performance comparisons. First, comparing the performance of LLMs and multimodal LLMs on public benchmarks is challenging due to prevalent data contamination, meaning that the test data may have been included in the training data.
  &lt;/p&gt;
  &lt;p&gt;
   Additionally, the architectural components vary so much that making an apples-to-apples comparison is difficult. So, big kudos to the NVIDIA team for developing NVLM in different flavors, which allowed for a comparison between the decoder-only and cross-attention approaches at least.
  &lt;/p&gt;
  &lt;p&gt;
   In any case, the main takeaway from this article is that multimodal LLMs can be built successfully in many different ways. Below is a figure that summarizes the different components of the models covered in this article.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!R_9Y!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!R_9Y!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png 424w, https://substackcdn.com/image/fetch/$s_!R_9Y!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png 848w, https://substackcdn.com/image/fetch/$s_!R_9Y!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png 1272w, https://substackcdn.com/image/fetch/$s_!R_9Y!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:773,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:520878,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;773&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!R_9Y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!R_9Y!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png 424w, https://substackcdn.com/image/fetch/$s_!R_9Y!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png 848w, https://substackcdn.com/image/fetch/$s_!R_9Y!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png 1272w, https://substackcdn.com/image/fetch/$s_!R_9Y!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb043e6d7-78e5-4628-987a-b333d3a58829_2224x1180.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     An overview of the different models covered in this article along with their subcomponents and training approaches.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   I hope you found reading this article educational and now have a better understanding of how multimodal LLMs work!
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . (I am confident that you&#x27;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;a href=&quot;https://amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
       Build a Large Language Model (From Scratch)
      &lt;/a&gt;
      &lt;span&gt;
       now available on Amazon
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Alternatively, I also recently enabled the paid subscription option on Substack to support this magazine directly.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Your support means a great deal! Thank you!
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Building A GPT-Style LLM Classifier From Scratch </title>
<link>https://magazine.sebastianraschka.com/p/building-a-gpt-style-llm-classifier</link>
<pubDate>Sat, 21 Sep 2024 12:07:44 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   In this article, I want to show you how to transform pretrained large language models (LLMs) into strong text classifiers.
  &lt;/p&gt;
  &lt;p&gt;
   But why focus on classification? First, finetuning a pretrained model for classification offers a gentle yet effective introduction to model finetuning. Second, many real-world and business challenges revolve around text classification: spam detection, sentiment analysis, customer feedback categorization, topic labeling, and more.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!mmsV!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!mmsV!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg 424w, https://substackcdn.com/image/fetch/$s_!mmsV!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg 848w, https://substackcdn.com/image/fetch/$s_!mmsV!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!mmsV!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:781,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;781&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!mmsV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!mmsV!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg 424w, https://substackcdn.com/image/fetch/$s_!mmsV!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg 848w, https://substackcdn.com/image/fetch/$s_!mmsV!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!mmsV!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc52e5b0c-836c-4ae4-9b09-02e364004195_1600x858.jpeg 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Turning a GPT model into a text classifier
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    What You’ll Learn in This Article
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   To celebrate the book’s release, I’m sharing an excerpt from one of the chapters that walks you through how to finetune a pretrained LLM as a spam classifier.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Important Note
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The chapter on classification finetuning is 35 pages long—too long for a single article. So, in this post, I’ll focus on a ~10-page subset that introduces the context and core concepts behind classification finetuning.
  &lt;/p&gt;
  &lt;p&gt;
   Additionally, I’ll share insights from some extra experiments that aren’t included in the book and address common questions readers might have. (Please note that the excerpt below is based on my personal draft before Manning’s professional text editing and final figure design.)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The full code for this excerpt can be found
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch/blob/main/ch06/01_main-chapter-code/ch06.ipynb&quot; rel=&quot;&quot;&gt;
    here on GitHub
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In addition, I&#x27;ll also answer 7 questions you might have regarding training LLM classifiers:
  &lt;/p&gt;
  &lt;p&gt;
   1) Do we need to train all layers?
  &lt;/p&gt;
  &lt;p&gt;
   2) Why finetuning the last token, not the first token?
  &lt;/p&gt;
  &lt;p&gt;
   3) How does BERT compare to GPT performance-wise?
  &lt;/p&gt;
  &lt;p&gt;
   4) Should we disable the causal mask?
  &lt;/p&gt;
  &lt;p&gt;
   5) What impact does increasing the model size have?
  &lt;/p&gt;
  &lt;p&gt;
   6) What improvements can we expect from LoRA?
  &lt;/p&gt;
  &lt;p&gt;
   7) Padding or no padding?
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Happy reading!
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   Different categories of finetuning
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    The most common ways to finetune language models are
   &lt;/span&gt;
   &lt;em&gt;
    instruction finetuning
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    classification finetuning
   &lt;/em&gt;
   &lt;span&gt;
    . Instruction finetuning involves training a language model on a set of tasks using specific instructions to improve its ability to understand and execute tasks described in natural language prompts, as illustrated in Figure 1 below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!b-8-!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!b-8-!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png 424w, https://substackcdn.com/image/fetch/$s_!b-8-!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png 848w, https://substackcdn.com/image/fetch/$s_!b-8-!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png 1272w, https://substackcdn.com/image/fetch/$s_!b-8-!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d3384270-ff15-41ba-b698-5490ab809d20_1568x624.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:579,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;579&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!b-8-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!b-8-!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png 424w, https://substackcdn.com/image/fetch/$s_!b-8-!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png 848w, https://substackcdn.com/image/fetch/$s_!b-8-!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png 1272w, https://substackcdn.com/image/fetch/$s_!b-8-!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd3384270-ff15-41ba-b698-5490ab809d20_1568x624.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 1: Illustration of two different instruction finetuning scenarios. At the top, the model is tasked with determining whether a given text is spam. At the bottom, the model is given an instruction on how to translate an English sentence into German.
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The next chapter will discuss instruction finetuning, as illustrated in Figure 1 above. Meanwhile, this chapter is centered on classification finetuning, a concept you might already be acquainted with if you have a background in machine learning.
  &lt;/p&gt;
  &lt;p&gt;
   In classification finetuning, the model is trained to recognize a specific set of class labels, such as &quot;spam&quot; and &quot;not spam.&quot; Examples of classification tasks extend beyond large language models and email filtering; they include identifying different species of plants from images, categorizing news articles into topics like sports, politics, or technology, and distinguishing between benign and malignant tumors in medical imaging.
  &lt;/p&gt;
  &lt;p&gt;
   The key point is that a classification-finetuned model is restricted to predicting classes it has encountered during its training—for instance, it can determine whether something is &quot;spam&quot; or &quot;not spam,&quot; as illustrated in Figure 2 below, but it can&#x27;t say anything else about the input text.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!0RXf!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!0RXf!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png 424w, https://substackcdn.com/image/fetch/$s_!0RXf!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png 848w, https://substackcdn.com/image/fetch/$s_!0RXf!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png 1272w, https://substackcdn.com/image/fetch/$s_!0RXf!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:532,&quot;width&quot;:994,&quot;resizeWidth&quot;:516,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;276.16901408450707&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!0RXf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!0RXf!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png 424w, https://substackcdn.com/image/fetch/$s_!0RXf!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png 848w, https://substackcdn.com/image/fetch/$s_!0RXf!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png 1272w, https://substackcdn.com/image/fetch/$s_!0RXf!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F80e640cf-da89-4479-ae8f-b39e6255e2de_994x532.png 1456w&quot; width=&quot;516&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      Figure 2: Illustration of a text classification scenario using an LLM. A model finetuned for spam classification does not require additional instructions alongside the input. However, in contrast to an instruction-finetuned model, it can only respond with &quot;spam&quot; and &quot;not spam.&quot;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    In contrast to the classification-finetuned model depicted in Figure 2, an instruction-finetuned model typically has the capability to undertake a broader range of tasks. We can view a classification-finetuned model as highly specialized, and generally, it is easier to develop a specialized model than a generalist model that works well across various tasks.
   &lt;/span&gt;
   &lt;strong&gt;
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    &lt;em&gt;
     &lt;strong&gt;
      Choosing the right approach
     &lt;/strong&gt;
    &lt;/em&gt;
   &lt;/p&gt;
   &lt;p&gt;
    &lt;em&gt;
     Instruction finetuning improves a model&#x27;s ability to understand and generate responses based on specific user instructions. Instruction finetuning is best suited for models that need to handle a variety of tasks based on complex user instructions, improving flexibility and interaction quality. Classification finetuning, on the other hand, is ideal for projects requiring precise categorization of data into predefined classes, such as sentiment analysis or spam detection.
    &lt;/em&gt;
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   While instruction finetuning is more versatile, it demands larger datasets and greater computational resources to develop models proficient in various tasks. In contrast, classification finetuning requires less data and compute power, but its use is confined to the specific classes on which the model has been trained.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Building LLMs from the Ground Up: A 3-hour Coding Workshop </title>
<link>https://magazine.sebastianraschka.com/p/building-llms-from-the-ground-up</link>
<pubDate>Sat, 31 Aug 2024 03:39:35 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   If you’d like to spend a few hours this weekend to dive into Large Language Models (LLMs) and understand how they work, I&#x27;ve prepared a 3-hour coding workshop presentation on implementing, training, and using LLMs.
  &lt;/p&gt;
  &lt;div class=&quot;youtube-wrap&quot; data-attrs=&#x27;{&quot;videoId&quot;:&quot;quh7z1q7-uc&quot;,&quot;startTime&quot;:null,&quot;endTime&quot;:null}&#x27; data-component-name=&quot;Youtube2ToDOM&quot; id=&quot;youtube2-quh7z1q7-uc&quot;&gt;
   &lt;div class=&quot;youtube-inner&quot;&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
   Below, you&#x27;ll find a table of contents to get an idea of what this video covers (the video itself has clickable chapter marks, allowing you to jump directly to topics of interest):
  &lt;/p&gt;
  &lt;p&gt;
   0:00 – Workshop overview
  &lt;/p&gt;
  &lt;p&gt;
   2:17 – Part 1: Intro to LLMs
  &lt;/p&gt;
  &lt;p&gt;
   9:14 – Workshop materials
  &lt;/p&gt;
  &lt;p&gt;
   10:48 – Part 2: Understanding LLM input data
  &lt;/p&gt;
  &lt;p&gt;
   23:25 – A simple tokenizer class
  &lt;/p&gt;
  &lt;p&gt;
   41:03 – Part 3: Coding an LLM architecture
  &lt;/p&gt;
  &lt;p&gt;
   45:01 – GPT-2 and Llama 2
  &lt;/p&gt;
  &lt;p&gt;
   1:07:11 – Part 4: Pretraining
  &lt;/p&gt;
  &lt;p&gt;
   1:29:37 – Part 5.1: Loading pretrained weights
  &lt;/p&gt;
  &lt;p&gt;
   1:45:12 – Part 5.2: Pretrained weights via LitGPT
  &lt;/p&gt;
  &lt;p&gt;
   1:53:09 – Part 6.1: Instruction finetuning
  &lt;/p&gt;
  &lt;p&gt;
   2:08:21 – Part 6.2: Instruction finetuning via LitGPT
  &lt;/p&gt;
  &lt;p&gt;
   02:26:45 – Part 6.3: Benchmark evaluation
  &lt;/p&gt;
  &lt;p&gt;
   02:36:55 – Part 6.4: Evaluating conversational performance
  &lt;/p&gt;
  &lt;p&gt;
   02:42:40 – Conclusion
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   It&#x27;s a slight departure from my usual text-based content, but the last time I did this a few months ago, it was so well-received that I thought it might be nice to do another one!
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Happy viewing!
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   References
  &lt;/h2&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://mng.bz/M96o&quot; rel=&quot;&quot;&gt;
      Build an LLM from Scratch book
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLMs-from-scratch&quot; rel=&quot;&quot;&gt;
      Build an LLM from Scratch GitHub repository
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/rasbt/LLM-workshop-2024&quot; rel=&quot;&quot;&gt;
      GitHub repository with workshop code
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://lightning.ai/lightning-ai/studios/llms-from-the-ground-up-workshop&quot; rel=&quot;&quot;&gt;
      Lightning Studio for this workshop
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/Lightning-AI/litgpt&quot; rel=&quot;&quot;&gt;
      LitGPT GitHub repository
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . (I am confident that you&#x27;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Build a Large Language Model (From Scratch) now
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      available on Amazon
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Alternatively, I also recently enabled the paid subscription option on Substack to support this magazine directly.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Understanding and Coding Self-Attention, Multi-Head Attention, Causal-Attention, and Cross-Attention in LLMs </title>
<link>https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention</link>
<pubDate>Sun, 14 Jan 2024 03:55:06 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   This article will teach you about self-attention mechanisms used in transformer architectures and large language models (LLMs) such as GPT-4 and Llama. Self-attention and related mechanisms are core components of LLMs, making them a useful topic to understand when working with these models.
  &lt;/p&gt;
  &lt;p&gt;
   However, rather than just discussing the self-attention mechanism, we will code it in Python and PyTorch from the ground up. In my opinion, coding algorithms, models, and techniques from scratch is an excellent way to learn!
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    As a side note, this article is a modernized and extended version of &quot;
   &lt;/span&gt;
   &lt;a href=&quot;https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html&quot; rel=&quot;&quot;&gt;
    Understanding and Coding the Self-Attention Mechanism of Large Language Models From Scratch
   &lt;/a&gt;
   &lt;span&gt;
    ,&quot; which I published on my old blog almost exactly a year ago. Since I really enjoy writing (and reading) &#x27;from scratch&#x27; articles, I wanted to modernize this article for
   &lt;/span&gt;
   &lt;em&gt;
    Ahead of AI
   &lt;/em&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Additionally, this article motivated me to write the book
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;http://mng.bz/amjo&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (from Scratch)
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , which is currently in progress. Below is a mental model that summarizes the book and illustrates how the self-attention mechanism fits into the bigger picture.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!zkNz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!zkNz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png 424w, https://substackcdn.com/image/fetch/$s_!zkNz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png 848w, https://substackcdn.com/image/fetch/$s_!zkNz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png 1272w, https://substackcdn.com/image/fetch/$s_!zkNz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:440,&quot;width&quot;:1342,&quot;resizeWidth&quot;:562,&quot;bytes&quot;:129127,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:false,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;184.2622950819672&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!zkNz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!zkNz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png 424w, https://substackcdn.com/image/fetch/$s_!zkNz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png 848w, https://substackcdn.com/image/fetch/$s_!zkNz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png 1272w, https://substackcdn.com/image/fetch/$s_!zkNz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F924a2d16-e8c2-4acb-84ba-bf95f2df1b31_1342x440.png 1456w&quot; width=&quot;562&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      An overview of the topics covered in the
     &lt;/span&gt;
     &lt;em&gt;
      &lt;a href=&quot;http://mng.bz/amjo&quot; rel=&quot;&quot;&gt;
       Build a Large Language Model (from Scratch)
      &lt;/a&gt;
      &lt;span&gt;
       book
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   To keep the length of this article somewhat reasonable, I&#x27;ll assume you already know about LLMs and you also know about attention mechanisms on a basic level. The goal and focus of this article is to understand how attention mechanisms work via a Python &amp; PyTorch code walkthrough.
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Introducing Self-Attention
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    Since its introduction via the original transformer paper (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
    Attention Is All You Need
   &lt;/a&gt;
   &lt;span&gt;
    ), self-attention has become a cornerstone of many state-of-the-art deep learning models, particularly in the field of Natural Language Processing (NLP). Since self-attention is now everywhere, it&#x27;s important to understand how it works.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!BtVj!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!BtVj!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png 424w, https://substackcdn.com/image/fetch/$s_!BtVj!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png 848w, https://substackcdn.com/image/fetch/$s_!BtVj!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png 1272w, https://substackcdn.com/image/fetch/$s_!BtVj!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1150,&quot;width&quot;:1304,&quot;resizeWidth&quot;:426,&quot;bytes&quot;:395300,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;375.69018404907973&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!BtVj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!BtVj!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png 424w, https://substackcdn.com/image/fetch/$s_!BtVj!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png 848w, https://substackcdn.com/image/fetch/$s_!BtVj!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png 1272w, https://substackcdn.com/image/fetch/$s_!BtVj!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F97567e7b-f8b9-4dea-a678-162378609a75_1304x1150.png 1456w&quot; width=&quot;426&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      The original transformer architecture from
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1706.03762
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    The concept of &quot;attention&quot; in deep learning
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot; rel=&quot;&quot;&gt;
    has its roots in the effort to improve Recurrent Neural Networks (RNNs)
   &lt;/a&gt;
   &lt;span&gt;
    for handling longer sequences or sentences. For instance, consider translating a sentence from one language to another. Translating a sentence word-by-word is usually not an option because it ignores the complex grammatical structures and idiomatic expressions unique to each language, leading to inaccurate or nonsensical translations.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!uRaO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!uRaO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 424w, https://substackcdn.com/image/fetch/$s_!uRaO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 848w, https://substackcdn.com/image/fetch/$s_!uRaO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 1272w, https://substackcdn.com/image/fetch/$s_!uRaO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:614,&quot;width&quot;:1180,&quot;resizeWidth&quot;:432,&quot;bytes&quot;:114671,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;224.7864406779661&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!uRaO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!uRaO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 424w, https://substackcdn.com/image/fetch/$s_!uRaO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 848w, https://substackcdn.com/image/fetch/$s_!uRaO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 1272w, https://substackcdn.com/image/fetch/$s_!uRaO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2d632b81-5bd6-432a-a456-37f20788be20_1180x614.png 1456w&quot; width=&quot;432&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     An incorrect word-by-word translation (top) compared to a correct translation (bottom)
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    To overcome this issue, attention mechanisms were introduced to give access to all sequence elements at each time step. The key is to be selective and determine which words are most important in a specific context.
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
    In 2017, the transformer architecture
   &lt;/a&gt;
   &lt;span&gt;
    introduced a standalone self-attention mechanism, eliminating the need for RNNs altogether.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   (For brevity, and to keep the article focused on the technical self-attention details, I am keeping this background motivation section brief so that we can focus on the code implementation.)
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!sSRX!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!sSRX!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 424w, https://substackcdn.com/image/fetch/$s_!sSRX!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 848w, https://substackcdn.com/image/fetch/$s_!sSRX!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 1272w, https://substackcdn.com/image/fetch/$s_!sSRX!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1971,&quot;width&quot;:1456,&quot;resizeWidth&quot;:530,&quot;bytes&quot;:2180804,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;717.4656593406594&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!sSRX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!sSRX!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 424w, https://substackcdn.com/image/fetch/$s_!sSRX!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 848w, https://substackcdn.com/image/fetch/$s_!sSRX!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 1272w, https://substackcdn.com/image/fetch/$s_!sSRX!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F49828e20-cd55-47bb-84a3-59effec1be79_3140x4251.png 1456w&quot; width=&quot;530&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      A visualization from the “Attention is All You Need” paper (
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1706.03762
     &lt;/a&gt;
     &lt;span&gt;
      ) showing how much the word “making” depends or focuses on other words in the input via attention weights (the color intensity is proportional the attention weight value).
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   We can think of self-attention as a mechanism that enhances the information content of an input embedding by including information about the input&#x27;s context. In other words, the self-attention mechanism enables the model to weigh the importance of different elements in an input sequence and dynamically adjust their influence on the output. This is especially important for language processing tasks, where the meaning of a word can change based on its context within a sentence or document.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that there are many variants of self-attention. A particular focus has been on making self-attention more efficient. However, most papers still implement the original scaled-dot product attention mechanism introduced in the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
    Attention Is All You Need paper
   &lt;/a&gt;
   &lt;span&gt;
    since self-attention is rarely a computational bottleneck for most companies training large-scale transformers.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, in this article, we focus on the original scaled-dot product attention mechanism (referred to as self-attention), which remains the most popular and most widely used attention mechanism in practice. However, if you are interested in other types of attention mechanisms, check out the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2009.06732&quot; rel=&quot;&quot;&gt;
    2020
   &lt;/a&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2009.06732&quot; rel=&quot;&quot;&gt;
     Efficient Transformers: A Survey
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2302.01107&quot; rel=&quot;&quot;&gt;
    2023
   &lt;/a&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2302.01107&quot; rel=&quot;&quot;&gt;
     A Survey on Efficient Training of Transformers
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    review, and the recent
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot; rel=&quot;&quot;&gt;
    FlashAttention
   &lt;/a&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2307.08691&quot; rel=&quot;&quot;&gt;
    FlashAttention-v2
   &lt;/a&gt;
   &lt;span&gt;
    papers.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Embedding an Input Sentence
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    Before we begin, let&#x27;s consider an input sentence
   &lt;/span&gt;
   &lt;em&gt;
    &quot;Life is short, eat dessert first&quot;
   &lt;/em&gt;
   &lt;span&gt;
    that we want to put through the self-attention mechanism. Similar to other types of modeling approaches for processing text (e.g., using recurrent neural networks or convolutional neural networks), we create a sentence embedding first.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For simplicity, here our dictionary
   &lt;/span&gt;
   &lt;code&gt;
    dc
   &lt;/code&gt;
   &lt;span&gt;
    is restricted to the words that occur in the input sentence. In a real-world application, we would consider all words in the training dataset (typical vocabulary sizes range between 30k to 50k entries).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;sentence = &#x27;Life is short, eat dessert first&#x27;
​
dc = {s:i for i,s 
      in enumerate(sorted(sentence.replace(&#x27;,&#x27;, &#x27;&#x27;).split()))}

print(dc)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;{&#x27;Life&#x27;: 0, &#x27;dessert&#x27;: 1, &#x27;eat&#x27;: 2, &#x27;first&#x27;: 3, &#x27;is&#x27;: 4, &#x27;short&#x27;: 5}&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Next, we use this dictionary to assign an integer index to each word:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;import torch
​
sentence_int = torch.tensor(
    [dc[s] for s in sentence.replace(&#x27;,&#x27;, &#x27;&#x27;).split()]
)
print(sentence_int)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([0, 4, 5, 2, 1, 3])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Now, using the integer-vector representation of the input sentence, we can use an embedding layer to encode the inputs into a real-vector embedding. Here, we will use a tiny 3-dimensional embedding such that each input word is represented by a 3-dimensional vector.
  &lt;/p&gt;
  &lt;p&gt;
   Note that embedding sizes typically range from hundreds to thousands of dimensions. For instance, Llama 2 utilizes embedding sizes of 4,096. The reason we use 3-dimensional embeddings here is purely for illustration purposes. This allows us to examine the individual vectors without filling the entire page with numbers.
  &lt;/p&gt;
  &lt;p&gt;
   Since the sentence consists of 6 words, this will result in a 6×3-dimensional embedding:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;vocab_size = 50_000
​
torch.manual_seed(123)
embed = torch.nn.Embedding(vocab_size, 3)
embedded_sentence = embed(sentence_int).detach()
​
print(embedded_sentence)
print(embedded_sentence.shape)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[ 0.3374, -0.1778, -0.3035],
        [ 0.1794,  1.8951,  0.4954],
        [ 0.2692, -0.0770, -1.0205],
        [-0.2196, -0.3792,  0.7671],
        [-0.5880,  0.3486,  0.6603],
        [-1.1925,  0.6984, -1.4097]])
torch.Size([6, 3])&lt;/code&gt;&lt;/pre&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Defining the Weight Matrices
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Now, let&#x27;s discuss the widely utilized self-attention mechanism known as the scaled dot-product attention, which is an integral part of the transformer architecture.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Self-attention utilizes three weight matrices, referred to as
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      W
     &lt;/span&gt;
     &lt;sub&gt;
      q
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      W
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , and
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      W
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , which are adjusted as model parameters during training. These matrices serve to project the inputs into
   &lt;/span&gt;
   &lt;em&gt;
    query
   &lt;/em&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;em&gt;
    key
   &lt;/em&gt;
   &lt;span&gt;
    , and
   &lt;/span&gt;
   &lt;em&gt;
    value
   &lt;/em&gt;
   &lt;span&gt;
    components of the sequence, respectively.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The respective query, key and value sequences are obtained via matrix multiplication between the weight matrices
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     W
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    and the embedded inputs
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     x
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    :
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      Query sequence:
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;span&gt;
        q
       &lt;/span&gt;
       &lt;sup&gt;
        (i)
       &lt;/sup&gt;
      &lt;/strong&gt;
      &lt;span&gt;
      &lt;/span&gt;
     &lt;/em&gt;
     &lt;span&gt;
      =
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;span&gt;
        x
       &lt;/span&gt;
       &lt;sup&gt;
        (i)
       &lt;/sup&gt;
       &lt;span&gt;
        W
       &lt;/span&gt;
       &lt;sub&gt;
        q
       &lt;/sub&gt;
      &lt;/strong&gt;
     &lt;/em&gt;
     &lt;span&gt;
      for
     &lt;/span&gt;
     &lt;em&gt;
      i
     &lt;/em&gt;
     &lt;span&gt;
      in sequence
     &lt;/span&gt;
     &lt;em&gt;
      1 … T
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      Key sequence:
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;span&gt;
        k
       &lt;/span&gt;
       &lt;sup&gt;
        (i)
       &lt;/sup&gt;
      &lt;/strong&gt;
      &lt;span&gt;
      &lt;/span&gt;
     &lt;/em&gt;
     &lt;span&gt;
      =
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;span&gt;
        x
       &lt;/span&gt;
       &lt;sup&gt;
        (i)
       &lt;/sup&gt;
       &lt;span&gt;
        W
       &lt;/span&gt;
       &lt;sub&gt;
        k
       &lt;/sub&gt;
      &lt;/strong&gt;
     &lt;/em&gt;
     &lt;span&gt;
      for
     &lt;/span&gt;
     &lt;em&gt;
      i
     &lt;/em&gt;
     &lt;span&gt;
      in sequence
     &lt;/span&gt;
     &lt;em&gt;
      1 … T
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      Value sequence:
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;span&gt;
        v
       &lt;/span&gt;
       &lt;sup&gt;
        (i)
       &lt;/sup&gt;
      &lt;/strong&gt;
      &lt;span&gt;
      &lt;/span&gt;
     &lt;/em&gt;
     &lt;span&gt;
      =
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       &lt;span&gt;
        x
       &lt;/span&gt;
       &lt;sup&gt;
        (i)
       &lt;/sup&gt;
       &lt;span&gt;
        W
       &lt;/span&gt;
       &lt;sub&gt;
        v
       &lt;/sub&gt;
      &lt;/strong&gt;
     &lt;/em&gt;
     &lt;span&gt;
      for
     &lt;/span&gt;
     &lt;em&gt;
      i
     &lt;/em&gt;
     &lt;span&gt;
      in sequence
     &lt;/span&gt;
     &lt;em&gt;
      1 … T
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;span&gt;
    The index
   &lt;/span&gt;
   &lt;em&gt;
    i
   &lt;/em&gt;
   &lt;span&gt;
    refers to the token index position in the input sequence, which has length
   &lt;/span&gt;
   &lt;em&gt;
    T
   &lt;/em&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!3woZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3woZ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 424w, https://substackcdn.com/image/fetch/$s_!3woZ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 848w, https://substackcdn.com/image/fetch/$s_!3woZ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 1272w, https://substackcdn.com/image/fetch/$s_!3woZ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:786,&quot;width&quot;:366,&quot;resizeWidth&quot;:174,&quot;bytes&quot;:58530,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;373.672131147541&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!3woZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!3woZ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 424w, https://substackcdn.com/image/fetch/$s_!3woZ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 848w, https://substackcdn.com/image/fetch/$s_!3woZ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 1272w, https://substackcdn.com/image/fetch/$s_!3woZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fecef3e00-4c0e-4c7a-9a9f-42ac4e4ada69_366x786.png 1456w&quot; width=&quot;174&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Computing the query, key, and value vectors via the input x and weights W.
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Here, both
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      q
     &lt;/span&gt;
     &lt;sup&gt;
      (i)
     &lt;/sup&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      k
     &lt;/span&gt;
     &lt;sup&gt;
      (i)
     &lt;/sup&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    are vectors of dimension
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    . The projection matrices
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      W
     &lt;/span&gt;
     &lt;sub&gt;
      q
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      W
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    have a shape of
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     d
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ×
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , while
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      W
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    has the shape
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     d
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ×
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (It&#x27;s important to note that
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     d
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    represents the size of each word vector,
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     x
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Since we are computing the dot-product between the query and key vectors, these two vectors have to contain the same number of elements (
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      q
     &lt;/sub&gt;
     &lt;span&gt;
      = d
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ). In many LLMs, we use the same size for the value vectors such that
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      q
     &lt;/sub&gt;
     &lt;span&gt;
      = d
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
     &lt;span&gt;
      = d
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    . However, the number of elements in the value vector
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      v
     &lt;/span&gt;
     &lt;sup&gt;
      (i)
     &lt;/sup&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , which determines the size of the resulting context vector, can be arbitrary.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, for the following code walkthrough, we will set
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      q
     &lt;/sub&gt;
     &lt;span&gt;
      = d
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
     &lt;span&gt;
      = 2
     &lt;/span&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    and use
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
     &lt;span&gt;
      = 4
     &lt;/span&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , initializing the projection matrices as follows:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;torch.manual_seed(123)
​
d = embedded_sentence.shape[1]
​
d_q, d_k, d_v = 2, 2, 4
​
W_query = torch.nn.Parameter(torch.rand(d, d_q))
W_key = torch.nn.Parameter(torch.rand(d, d_k))
W_value = torch.nn.Parameter(torch.rand(d, d_v))&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    (Similar to the word embedding vectors earlier, the dimensions
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      q
     &lt;/sub&gt;
     &lt;span&gt;
      , d
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
     &lt;span&gt;
      , d
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    are usually much larger, but we use small numbers here for illustration purposes.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Computing the Unnormalized Attention Weights
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Now, let&#x27;s suppose we are interested in computing the attention vector for the second input element -- the second input element acts as the query here:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!qmcs!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qmcs!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 424w, https://substackcdn.com/image/fetch/$s_!qmcs!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 848w, https://substackcdn.com/image/fetch/$s_!qmcs!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 1272w, https://substackcdn.com/image/fetch/$s_!qmcs!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:962,&quot;width&quot;:588,&quot;resizeWidth&quot;:192,&quot;bytes&quot;:124665,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;314.1224489795918&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!qmcs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qmcs!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 424w, https://substackcdn.com/image/fetch/$s_!qmcs!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 848w, https://substackcdn.com/image/fetch/$s_!qmcs!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 1272w, https://substackcdn.com/image/fetch/$s_!qmcs!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff9774001-ea9d-48bf-9857-3c911b0a279d_588x962.png 1456w&quot; width=&quot;192&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      For the following sections below, we focus on the second input,
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       x
      &lt;/strong&gt;
     &lt;/em&gt;
     &lt;span&gt;
      (2)
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In code, this looks like as follows:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;x_2 = embedded_sentence[1]
query_2 = x_2 @ W_query
key_2 = x_2 @ W_key
value_2 = x_2 @ W_value
​
print(query_2.shape)
print(key_2.shape)
print(value_2.shape)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;torch.Size([2])
torch.Size([2])
torch.Size([4])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   We can then generalize this to compute the remaining key, and value elements for all inputs as well, since we will need them in the next step when we compute the unnormalized attention weights later:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;keys = embedded_sentence @ W_key
values = embedded_sentence @ W_value
​
print(&quot;keys.shape:&quot;, keys.shape)
print(&quot;values.shape:&quot;, values.shape)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;keys.shape: torch.Size([6, 2])
values.shape: torch.Size([6, 4])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Now that we have all the required keys and values, we can proceed to the next step and compute the unnormalized attention weights
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     ω
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    (omega), which are illustrated in the figure below:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!7mUN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!7mUN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 424w, https://substackcdn.com/image/fetch/$s_!7mUN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 848w, https://substackcdn.com/image/fetch/$s_!7mUN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 1272w, https://substackcdn.com/image/fetch/$s_!7mUN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/baf9e308-223b-429e-8527-a7b868003e8c_814x912.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:912,&quot;width&quot;:814,&quot;resizeWidth&quot;:296,&quot;bytes&quot;:156710,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;331.6363636363636&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!7mUN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!7mUN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 424w, https://substackcdn.com/image/fetch/$s_!7mUN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 848w, https://substackcdn.com/image/fetch/$s_!7mUN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 1272w, https://substackcdn.com/image/fetch/$s_!7mUN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbaf9e308-223b-429e-8527-a7b868003e8c_814x912.png 1456w&quot; width=&quot;296&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Computing the unnormalized attention weights
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       ω
      &lt;/strong&gt;
     &lt;/em&gt;
     &lt;span&gt;
      (omega)
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    As illustrated in the figure above, we compute
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      ω
     &lt;/span&gt;
     &lt;sub&gt;
      i,j
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    as the dot product between the query and key sequences,
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      ω
     &lt;/span&gt;
     &lt;sub&gt;
      i,j
     &lt;/sub&gt;
     &lt;span&gt;
     &lt;/span&gt;
    &lt;/strong&gt;
    &lt;span&gt;
     =
    &lt;/span&gt;
   &lt;/em&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      q
     &lt;/span&gt;
     &lt;sup&gt;
      (i)
     &lt;/sup&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      k
     &lt;/span&gt;
     &lt;sup&gt;
      (j)
     &lt;/sup&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   For example, we can compute the unnormalized attention weight for the query and 5th input element (corresponding to index position 4) as follows:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;omega_24 = query_2.dot(keys[4])
print(omega_24)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    (Note that
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     ω
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    is the symbol for the Greek letter &quot;omega&quot;, hence the code variable with the same name above.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor(1.2903)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Since we will need those unnormalized attention weights
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     ω
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    to compute the actual attention weights later, let&#x27;s compute the
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     ω
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    values for all input tokens as illustrated in the previous figure:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;omega_2 = query_2 @ keys.T
print(omega_2)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374])&lt;/code&gt;&lt;/pre&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Computing the Attention Weights
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    The subsequent step in self-attention is to normalize the unnormalized attention weights,
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     ω
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , to obtain the normalized attention weights,
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     α
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    (alpha), by applying the softmax function. Additionally, 1/√{
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    } is used to scale
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     ω
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    before normalizing it through the softmax function, as shown below:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!rgXp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!rgXp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png 424w, https://substackcdn.com/image/fetch/$s_!rgXp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png 848w, https://substackcdn.com/image/fetch/$s_!rgXp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png 1272w, https://substackcdn.com/image/fetch/$s_!rgXp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:798,&quot;width&quot;:1400,&quot;resizeWidth&quot;:578,&quot;bytes&quot;:151169,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;329.46&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!rgXp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!rgXp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png 424w, https://substackcdn.com/image/fetch/$s_!rgXp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png 848w, https://substackcdn.com/image/fetch/$s_!rgXp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png 1272w, https://substackcdn.com/image/fetch/$s_!rgXp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F42da287a-18e8-45c7-860c-46e8a3a534fc_1400x798.png 1456w&quot; width=&quot;578&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Computing the normalized attention weights
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       α
      &lt;/strong&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    The scaling by
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      k
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ensures that the Euclidean length of the weight vectors will be approximately in the same magnitude. This helps prevent the attention weights from becoming too small or too large, which could lead to numerical instability or affect the model&#x27;s ability to converge during training.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In code, we can implement the computation of the attention weights as follows:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;import torch.nn.functional as F
​
attention_weights_2 = F.softmax(omega_2 / d_k**0.5, dim=0)
print(attention_weights_2)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Finally, the last step is to compute the context vector
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      z
     &lt;/span&gt;
     &lt;sup&gt;
      (2)
     &lt;/sup&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , which is an attention-weighted version of our original query input
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      x
     &lt;/span&gt;
     &lt;sup&gt;
      (2)
     &lt;/sup&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , including all the other input elements as its context via the attention weights:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!GpWw!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!GpWw!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 424w, https://substackcdn.com/image/fetch/$s_!GpWw!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 848w, https://substackcdn.com/image/fetch/$s_!GpWw!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 1272w, https://substackcdn.com/image/fetch/$s_!GpWw!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:796,&quot;width&quot;:1456,&quot;resizeWidth&quot;:664,&quot;bytes&quot;:290098,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;363.010989010989&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!GpWw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!GpWw!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 424w, https://substackcdn.com/image/fetch/$s_!GpWw!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 848w, https://substackcdn.com/image/fetch/$s_!GpWw!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 1272w, https://substackcdn.com/image/fetch/$s_!GpWw!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F6e1dcdeb-e096-4ff9-bdf9-3338e4efa4b4_1916x1048.png 1456w&quot; width=&quot;664&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      The attention weights are specific to a certain input element. Here, we chose input element
     &lt;/span&gt;
     &lt;em&gt;
      &lt;strong&gt;
       x
      &lt;/strong&gt;
      &lt;span&gt;
       (2).
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In code, this looks like as follows:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;context_vector_2 = attention_weights_2 @ values
​
print(context_vector_2.shape)
print(context_vector_2)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;torch.Size([4])
tensor([0.5313, 1.3607, 0.7891, 1.3110])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that this output vector has more dimensions (
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
     &lt;span&gt;
      = 4
     &lt;/span&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ) than the original input vector (
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     d
    &lt;/strong&gt;
    &lt;span&gt;
    &lt;/span&gt;
    &lt;strong&gt;
     = 3
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    ) since we specified
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
    &lt;/strong&gt;
    &lt;span&gt;
    &lt;/span&gt;
    &lt;strong&gt;
     &amp;gt; d
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    earlier; however, the embedding size choice
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     &lt;span&gt;
      d
     &lt;/span&gt;
     &lt;sub&gt;
      v
     &lt;/sub&gt;
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    is arbitrary.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Self-Attention
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    Now, to wrap up the code implementation of the self-attention mechanism in the previous sections above, we can summarize the previous code in a compact
   &lt;/span&gt;
   &lt;code&gt;
    SelfAttention
   &lt;/code&gt;
   &lt;span&gt;
    class:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;import torch.nn as nn
​
class SelfAttention(nn.Module):
​
    def __init__(self, d_in, d_out_kq, d_out_v):
        super().__init__()
        self.d_out_kq = d_out_kq
        self.W_query = nn.Parameter(torch.rand(d_in, d_out_kq))
        self.W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))
        self.W_value = nn.Parameter(torch.rand(d_in, d_out_v))
​
    def forward(self, x):
        keys = x @ self.W_key
        queries = x @ self.W_query
        values = x @ self.W_value
        
        attn_scores = queries @ keys.T  # unnormalized attention weights    
        attn_weights = torch.softmax(
            attn_scores / self.d_out_kq**0.5, dim=-1
        )
        
        context_vec = attn_weights @ values
        return context_vec&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Following PyTorch conventions, the
   &lt;/span&gt;
   &lt;code&gt;
    SelfAttention
   &lt;/code&gt;
   &lt;span&gt;
    class above initializes the self-attention parameters in the
   &lt;/span&gt;
   &lt;code&gt;
    __init__
   &lt;/code&gt;
   &lt;span&gt;
    method and computes attention weights and context vectors for all inputs via the
   &lt;/span&gt;
   &lt;code&gt;
    forward
   &lt;/code&gt;
   &lt;span&gt;
    method. We can use this class as follows:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;torch.manual_seed(123)
​
# reduce d_out_v from 4 to 1, because we have 4 heads
d_in, d_out_kq, d_out_v = 3, 2, 4
​
sa = SelfAttention(d_in, d_out_kq, d_out_v)
print(sa(embedded_sentence))&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[-0.1564,  0.1028, -0.0763, -0.0764],
        [ 0.5313,  1.3607,  0.7891,  1.3110],
        [-0.3542, -0.1234, -0.2627, -0.3706],
        [ 0.0071,  0.3345,  0.0969,  0.1998],
        [ 0.1008,  0.4780,  0.2021,  0.3674],
        [-0.5296, -0.2799, -0.4107, -0.6006]], grad_fn=&amp;lt;MmBackward0&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you look at the second row, you can see that it matches the values in
   &lt;/span&gt;
   &lt;code&gt;
    context_vector_2
   &lt;/code&gt;
   &lt;span&gt;
    from the previous section exactly:
   &lt;/span&gt;
   &lt;code&gt;
    tensor([0.5313, 1.3607, 0.7891, 1.3110])
   &lt;/code&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Multi-Head Attention
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the very first figure, at the top of this article (also shown again for convenience below), we saw that transformers use a module called
   &lt;/span&gt;
   &lt;em&gt;
    multi-head attention
   &lt;/em&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!vVWg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vVWg!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png 424w, https://substackcdn.com/image/fetch/$s_!vVWg!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png 848w, https://substackcdn.com/image/fetch/$s_!vVWg!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png 1272w, https://substackcdn.com/image/fetch/$s_!vVWg!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1152,&quot;width&quot;:918,&quot;resizeWidth&quot;:384,&quot;bytes&quot;:309249,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;481.88235294117646&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!vVWg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vVWg!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png 424w, https://substackcdn.com/image/fetch/$s_!vVWg!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png 848w, https://substackcdn.com/image/fetch/$s_!vVWg!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png 1272w, https://substackcdn.com/image/fetch/$s_!vVWg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F57269a7a-8ecc-4ed6-b6d7-79a9982dd776_918x1152.png 1456w&quot; width=&quot;384&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      The multi-head attention modules in the original transformer architecture from
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1706.03762
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   How does this &quot;multi-head&quot; attention module relate to the self-attention mechanism (scaled-dot product attention) we walked through above?
  &lt;/p&gt;
  &lt;p&gt;
   In scaled dot-product attention, the input sequence was transformed using three matrices representing the query, key, and value. These three matrices can be considered as a single attention head in the context of multi-head attention. The figure below summarizes this single attention head we covered and implemented previously:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!sfR2!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!sfR2!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 424w, https://substackcdn.com/image/fetch/$s_!sfR2!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 848w, https://substackcdn.com/image/fetch/$s_!sfR2!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 1272w, https://substackcdn.com/image/fetch/$s_!sfR2!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:885,&quot;width&quot;:1456,&quot;resizeWidth&quot;:500,&quot;bytes&quot;:178737,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;303.91483516483515&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!sfR2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!sfR2!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 424w, https://substackcdn.com/image/fetch/$s_!sfR2!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 848w, https://substackcdn.com/image/fetch/$s_!sfR2!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 1272w, https://substackcdn.com/image/fetch/$s_!sfR2!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7651e279-01ed-4316-91dc-d6be8ef4e947_1836x1116.png 1456w&quot; width=&quot;500&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Summarizing the self-attention mechanism implemented previously
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As its name implies, multi-head attention involves multiple such heads, each consisting of query, key, and value matrices. This concept is similar to the use of multiple kernels in convolutional neural networks, producing feature maps with multiple output channels.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!57T6!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!57T6!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 424w, https://substackcdn.com/image/fetch/$s_!57T6!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 848w, https://substackcdn.com/image/fetch/$s_!57T6!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 1272w, https://substackcdn.com/image/fetch/$s_!57T6!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:951,&quot;width&quot;:1456,&quot;resizeWidth&quot;:526,&quot;bytes&quot;:199096,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;343.5618131868132&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!57T6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!57T6!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 424w, https://substackcdn.com/image/fetch/$s_!57T6!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 848w, https://substackcdn.com/image/fetch/$s_!57T6!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 1272w, https://substackcdn.com/image/fetch/$s_!57T6!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff406d55e-990a-4e3b-be82-d966eb74a3e7_1766x1154.png 1456w&quot; width=&quot;526&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Multi-head attention: self-attention with multiple heads
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    To illustrate this in code, we can write a
   &lt;/span&gt;
   &lt;code&gt;
    MultiHeadAttentionWrapper
   &lt;/code&gt;
   &lt;span&gt;
    class for our previous
   &lt;/span&gt;
   &lt;code&gt;
    SelfAttention
   &lt;/code&gt;
   &lt;span&gt;
    class:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;class MultiHeadAttentionWrapper(nn.Module):
​
    def __init__(self, d_in, d_out_kq, d_out_v, num_heads):
        super().__init__()
        self.heads = nn.ModuleList(
            [SelfAttention(d_in, d_out_kq, d_out_v) 
             for _ in range(num_heads)]
        )
​
    def forward(self, x):
        return torch.cat([head(x) for head in self.heads], dim=-1)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;code&gt;
    d_*
   &lt;/code&gt;
   &lt;span&gt;
    parameters are the same as before in the
   &lt;/span&gt;
   &lt;code&gt;
    SelfAttention
   &lt;/code&gt;
   &lt;span&gt;
    class -- the only new input parameter here is the number of attention heads:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;code&gt;
      d_in
     &lt;/code&gt;
     &lt;span&gt;
      : Dimension of the input feature vector.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;code&gt;
      d_out_kq
     &lt;/code&gt;
     &lt;span&gt;
      : Dimension for both query and key outputs.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;code&gt;
      d_out_v
     &lt;/code&gt;
     &lt;span&gt;
      : Dimension for value outputs.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;code&gt;
      num_heads
     &lt;/code&gt;
     &lt;span&gt;
      : Number of attention heads.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;span&gt;
    We initialize the
   &lt;/span&gt;
   &lt;code&gt;
    SelfAttention
   &lt;/code&gt;
   &lt;span&gt;
    class
   &lt;/span&gt;
   &lt;code&gt;
    num_heads
   &lt;/code&gt;
   &lt;span&gt;
    times using these input parameters. And we use a PyTorch
   &lt;/span&gt;
   &lt;code&gt;
    nn.ModuleList
   &lt;/code&gt;
   &lt;span&gt;
    to store these multiple
   &lt;/span&gt;
   &lt;code&gt;
    SelfAttention
   &lt;/code&gt;
   &lt;span&gt;
    instances.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Then, the
   &lt;/span&gt;
   &lt;code&gt;
    forward
   &lt;/code&gt;
   &lt;span&gt;
    pass involves applying each
   &lt;/span&gt;
   &lt;code&gt;
    SelfAttention
   &lt;/code&gt;
   &lt;span&gt;
    head (stored in
   &lt;/span&gt;
   &lt;code&gt;
    self.heads
   &lt;/code&gt;
   &lt;span&gt;
    ) to the input
   &lt;/span&gt;
   &lt;code&gt;
    x
   &lt;/code&gt;
   &lt;span&gt;
    independently. The results from each head are then concatenated along the last dimension (
   &lt;/span&gt;
   &lt;code&gt;
    dim=-1
   &lt;/code&gt;
   &lt;span&gt;
    ). Let&#x27;s see it in action below!
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   First, let&#x27;s suppose we have a single Self-Attention head with output dimension 1 to keep it simple for illustration purposes:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;torch.manual_seed(123)
​
d_in, d_out_kq, d_out_v = 3, 2, 1
​
sa = SelfAttention(d_in, d_out_kq, d_out_v)
print(sa(embedded_sentence))&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[-0.0185],
        [ 0.4003],
        [-0.1103],
        [ 0.0668],
        [ 0.1180],
        [-0.1827]], grad_fn=&amp;lt;MmBackward0&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Now, let&#x27;s extend this to 4 attention heads:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;torch.manual_seed(123)
​
block_size = embedded_sentence.shape[1]
mha = MultiHeadAttentionWrapper(
    d_in, d_out_kq, d_out_v, num_heads=4
)
​
context_vecs = mha(embedded_sentence)
​
print(context_vecs)
print(&quot;context_vecs.shape:&quot;, context_vecs.shape)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[-0.0185,  0.0170,  0.1999, -0.0860],
        [ 0.4003,  1.7137,  1.3981,  1.0497],
        [-0.1103, -0.1609,  0.0079, -0.2416],
        [ 0.0668,  0.3534,  0.2322,  0.1008],
        [ 0.1180,  0.6949,  0.3157,  0.2807],
        [-0.1827, -0.2060, -0.2393, -0.3167]], grad_fn=&amp;lt;CatBackward0&amp;gt;)
context_vecs.shape: torch.Size([6, 4])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Based on the output above, you can see that the single self-attention head created earlier now represents the first column in the output tensor above.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Notice that the multi-head attention result is a 6×4-dimensional tensor: We have 6 input tokens and 4 self-attention heads, where each self-attention head returns a 1-dimensional output. Previously, in the Self-Attention section, we also produced a 6×4-dimensional tensor. That&#x27;s because we set the output dimension to 4 instead of 1. In practice, why do we even need multiple attention heads if we can regulate the output embedding size in the
   &lt;/span&gt;
   &lt;code&gt;
    SelfAttention
   &lt;/code&gt;
   &lt;span&gt;
    class itself?
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The distinction between increasing the output dimension of a single self-attention head and using multiple attention heads lies in how the model processes and learns from the data. While both approaches increase the capacity of the model to represent different features or aspects of the data, they do so in fundamentally different ways.
  &lt;/p&gt;
  &lt;p&gt;
   For instance, each attention head in multi-head attention can potentially learn to focus on different parts of the input sequence, capturing various aspects or relationships within the data. This diversity in representation is key to the success of multi-head attention.
  &lt;/p&gt;
  &lt;p&gt;
   Multi-head attention can also be more efficient, especially in terms of parallel computation. Each head can be processed independently, making it well-suited for modern hardware accelerators like GPUs or TPUs that excel at parallel processing.
  &lt;/p&gt;
  &lt;p&gt;
   In short, the use of multiple attention heads is not just about increasing the model&#x27;s capacity but about enhancing its ability to learn a diverse set of features and relationships within the data. For example, the 7B Llama 2 model uses 32 attention heads.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Causal Self-Attention
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   In this section, we are adapting the previously discussed self-attention mechanism into a causal self-attention mechanism, specifically for GPT-like (decoder-style) LLMs that are used to generate text. This causal self-attention mechanism  is also often referred to as “masked self-attention”. In the original transformer architecture, it corresponds to the “masked multi-head attention” module — for simplicity, we will look at a single attention head in this section, but the same concept generalizes to multiple heads.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!vQYr!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vQYr!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png 424w, https://substackcdn.com/image/fetch/$s_!vQYr!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png 848w, https://substackcdn.com/image/fetch/$s_!vQYr!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png 1272w, https://substackcdn.com/image/fetch/$s_!vQYr!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1148,&quot;width&quot;:1026,&quot;resizeWidth&quot;:372,&quot;bytes&quot;:367740,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;416.233918128655&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!vQYr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vQYr!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png 424w, https://substackcdn.com/image/fetch/$s_!vQYr!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png 848w, https://substackcdn.com/image/fetch/$s_!vQYr!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png 1272w, https://substackcdn.com/image/fetch/$s_!vQYr!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc51bfe11-c2cf-4ce5-95d4-4f8a57eac997_1026x1148.png 1456w&quot; width=&quot;372&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      The causal self-attention module in the original transformer architecture (via “Attention Is All You Need”,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1706.03762
     &lt;/a&gt;
     &lt;span&gt;
      )
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Causal self-attention ensures that the  outputs for a certain position in a sequence is based only on the known outputs at previous positions and not on future positions. In simpler terms, it ensures that the prediction for each next word should only depend on the preceding words. To achieve this in GPT-like LLMs, for each token processed, we mask out the future tokens, which come after the current token in the input text.
  &lt;/p&gt;
  &lt;p&gt;
   The application of a causal mask to the attention weights for hiding future input tokens in the inputs is illustrated in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!W-5R!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!W-5R!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 424w, https://substackcdn.com/image/fetch/$s_!W-5R!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 848w, https://substackcdn.com/image/fetch/$s_!W-5R!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 1272w, https://substackcdn.com/image/fetch/$s_!W-5R!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:932,&quot;width&quot;:1456,&quot;resizeWidth&quot;:518,&quot;bytes&quot;:611309,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;331.5769230769231&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!W-5R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!W-5R!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 424w, https://substackcdn.com/image/fetch/$s_!W-5R!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 848w, https://substackcdn.com/image/fetch/$s_!W-5R!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 1272w, https://substackcdn.com/image/fetch/$s_!W-5R!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb7dd5ec8-4912-4e0d-8265-b335c08d2958_1760x1126.png 1456w&quot; width=&quot;518&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    To illustrate and implement causal self-attention, let&#x27;s work with the unweighted attention scores and attention weights from the previous section. First, we quickly recap the computation of the attention scores from the previous
   &lt;/span&gt;
   &lt;em&gt;
    Self-Attention
   &lt;/em&gt;
   &lt;span&gt;
    section:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;torch.manual_seed(123)
​
d_in, d_out_kq, d_out_v = 3, 2, 4
​
W_query = nn.Parameter(torch.rand(d_in, d_out_kq))
W_key   = nn.Parameter(torch.rand(d_in, d_out_kq))
W_value = nn.Parameter(torch.rand(d_in, d_out_v))
​
x = embedded_sentence
​
keys = x @ W_key
queries = x @ W_query
values = x @ W_value
​
# attn_scores are the &quot;omegas&quot;, 
# the unnormalized attention weights
attn_scores = queries @ keys.T 
​
print(attn_scores)
print(attn_scores.shape)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[ 0.0613, -0.3491,  0.1443, -0.0437, -0.1303,  0.1076],
        [-0.6004,  3.4707, -1.5023,  0.4991,  1.2903, -1.3374],
        [ 0.2432, -1.3934,  0.5869, -0.1851, -0.5191,  0.4730],
        [-0.0794,  0.4487, -0.1807,  0.0518,  0.1677, -0.1197],
        [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216, -0.2787],
        [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],
       grad_fn=&amp;lt;MmBackward0&amp;gt;)
torch.Size([6, 6])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Similar to the
   &lt;/span&gt;
   &lt;em&gt;
    Self-Attention
   &lt;/em&gt;
   &lt;span&gt;
    section before, the output above is a 6×6 tensor containing these pairwise unnormalized attention weights (also called attention scores) for the 6 input tokens.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Previously, we then computed the scaled dot-product attention via the softmax function as follows:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;attn_weights = torch.softmax(attn_scores / d_out_kq**0.5, dim=1)
print(attn_weights)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[0.1772, 0.1326, 0.1879, 0.1645, 0.1547, 0.1831],
        [0.0386, 0.6870, 0.0204, 0.0840, 0.1470, 0.0229],
        [0.1965, 0.0618, 0.2506, 0.1452, 0.1146, 0.2312],
        [0.1505, 0.2187, 0.1401, 0.1651, 0.1793, 0.1463],
        [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.1231],
        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],
       grad_fn=&amp;lt;SoftmaxBackward0&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    The 6×6 output above represents the attention weights, which we also computed in the
   &lt;/span&gt;
   &lt;em&gt;
    Self-Attention
   &lt;/em&gt;
   &lt;span&gt;
    section before.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Now, in GPT-like LLMs, we train the model to read and generate one token (or word) at a time, from left to right. If we have a training text sample like &quot;Life is short eat desert first&quot; we have the following setup, where the context vectors for the word to the right side of the arrow should only incorporate itself and the previous words:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &quot;Life&quot; → &quot;is&quot;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &quot;Life is&quot; → &quot;short&quot;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &quot;Life is short&quot; → &quot;eat&quot;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &quot;Life is short eat&quot; → &quot;desert&quot;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &quot;Life is short eat desert&quot; → &quot;first&quot;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   The simplest way to achieve this setup above is to mask out all future tokens by applying a mask to the attention weight matrix above the diagonal, as illustrated in the figure below. This way, “future” words will not be included when creating the context vectors, which are created as a attention-weighted sum over the inputs.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!rOs7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!rOs7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 424w, https://substackcdn.com/image/fetch/$s_!rOs7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 848w, https://substackcdn.com/image/fetch/$s_!rOs7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 1272w, https://substackcdn.com/image/fetch/$s_!rOs7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/e1317a05-3542-4158-94bf-085109a5793a_1220x702.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:702,&quot;width&quot;:1220,&quot;resizeWidth&quot;:516,&quot;bytes&quot;:379180,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;296.91147540983604&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!rOs7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!rOs7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 424w, https://substackcdn.com/image/fetch/$s_!rOs7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 848w, https://substackcdn.com/image/fetch/$s_!rOs7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 1272w, https://substackcdn.com/image/fetch/$s_!rOs7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fe1317a05-3542-4158-94bf-085109a5793a_1220x702.png 1456w&quot; width=&quot;516&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     Attention weights above the diagonal should be masked out
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In code, we can achieve this via PyTorch&#x27;s
   &lt;/span&gt;
   &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.tril.html#&quot; rel=&quot;&quot;&gt;
    tril
   &lt;/a&gt;
   &lt;span&gt;
    function, which we first use to create a mask of 1&#x27;s and 0&#x27;s:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;block_size = attn_scores.shape[0]
mask_simple = torch.tril(torch.ones(block_size, block_size))
print(mask_simple)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[1., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1.]])&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Next, we multiply the attention weights with this mask to zero out all the attention weights above the diagonal:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;masked_simple = attn_weights*mask_simple
print(masked_simple)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[0.1772, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0386, 0.6870, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1965, 0.0618, 0.2506, 0.0000, 0.0000, 0.0000],
        [0.1505, 0.2187, 0.1401, 0.1651, 0.0000, 0.0000],
        [0.1347, 0.2758, 0.1162, 0.1621, 0.1881, 0.0000],
        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],
       grad_fn=&amp;lt;MulBackward0&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   While the above is one way to mask out future words, notice that the attention weights in each row don&#x27;t sum to one anymore. To mitigate that, we can normalize the rows such that they sum up to 1 again, which is a standard convention for attention weights:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;row_sums = masked_simple.sum(dim=1, keepdim=True)
masked_simple_norm = masked_simple / row_sums
print(masked_simple_norm)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],
        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],
        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],
        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],
       grad_fn=&amp;lt;DivBackward0&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   As we can see, the attention weights in each row now sum up to 1.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Normalizing attention weights in neural networks, such as in transformer models, is advantageous over unnormalized weights for two main reasons. First, normalized attention weights that sum to 1 resemble a probability distribution. This makes it easier to interpret the model&#x27;s attention to various parts of the input in terms of proportions. Second, by constraining the attention weights to sum to 1, this normalization helps control the scale of the weights and gradients to improve the training dynamics.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    More efficient masking without renormalization
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In the causal self-attention procedure we coded above, we first compute the attention scores, then compute the attention weights, mask out attention weights above the diagonal, and lastly renormalize the attention weights. This is summarized in the figure below:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!pRz7!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!pRz7!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 424w, https://substackcdn.com/image/fetch/$s_!pRz7!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 848w, https://substackcdn.com/image/fetch/$s_!pRz7!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 1272w, https://substackcdn.com/image/fetch/$s_!pRz7!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:272,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:74590,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;272&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!pRz7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!pRz7!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 424w, https://substackcdn.com/image/fetch/$s_!pRz7!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 848w, https://substackcdn.com/image/fetch/$s_!pRz7!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 1272w, https://substackcdn.com/image/fetch/$s_!pRz7!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7fcce180-cda2-4094-bb7c-32ea12e60c9a_1468x274.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     The previously implemented causal self-attention procedure
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Alternatively, there is a more efficient way to achieve the same results. In this approach, we take the attention scores and replace the values above the diagonal with negative infinity before the values are input into the softmax function to compute the attention weights. This is summarized in the figure below:
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!vh8x!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vh8x!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 424w, https://substackcdn.com/image/fetch/$s_!vh8x!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 848w, https://substackcdn.com/image/fetch/$s_!vh8x!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 1272w, https://substackcdn.com/image/fetch/$s_!vh8x!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/a94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:270,&quot;width&quot;:1054,&quot;resizeWidth&quot;:538,&quot;bytes&quot;:57470,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;137.8178368121442&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!vh8x!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!vh8x!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 424w, https://substackcdn.com/image/fetch/$s_!vh8x!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 848w, https://substackcdn.com/image/fetch/$s_!vh8x!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 1272w, https://substackcdn.com/image/fetch/$s_!vh8x!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fa94e36c7-ec67-43d6-96c0-7f9781f402b5_1054x270.png 1456w&quot; width=&quot;538&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     An alternative, more efficient approach to implementing causal self-attention
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   We can code up this procedure in PyTorch as follows, starting with masking the attention scores above the diagonal:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)
masked = attn_scores.masked_fill(mask.bool(), -torch.inf)
print(masked)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    The code above first creates a
   &lt;/span&gt;
   &lt;code&gt;
    mask
   &lt;/code&gt;
   &lt;span&gt;
    with 0s below the diagonal, and 1s above the diagonal. Here,
   &lt;/span&gt;
   &lt;code&gt;
    torch.triu
   &lt;/code&gt;
   &lt;span&gt;
    (
   &lt;/span&gt;
   &lt;strong&gt;
    u
   &lt;/strong&gt;
   &lt;span&gt;
    pper
   &lt;/span&gt;
   &lt;strong&gt;
    tri
   &lt;/strong&gt;
   &lt;span&gt;
    angle) retains the elements on and above the main diagonal of a matrix, zeroing out the elements below it, thus preserving the upper triangular portion. In contrast,
   &lt;/span&gt;
   &lt;code&gt;
    torch.tril
   &lt;/code&gt;
   &lt;span&gt;
    (
   &lt;/span&gt;
   &lt;strong&gt;
    l
   &lt;/strong&gt;
   &lt;span&gt;
    ower
   &lt;/span&gt;
   &lt;strong&gt;
    t
   &lt;/strong&gt;
   &lt;span&gt;
    riangle) keeps the elements on and below the main diagonal.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;code&gt;
    masked_fill
   &lt;/code&gt;
   &lt;span&gt;
    method then replaces all the elements above the diagonal via positive mask values (1s) with
   &lt;/span&gt;
   &lt;code&gt;
    -torch.inf
   &lt;/code&gt;
   &lt;span&gt;
    , with the results being shown below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[ 0.0613,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.6004,  3.4707,    -inf,    -inf,    -inf,    -inf],
        [ 0.2432, -1.3934,  0.5869,    -inf,    -inf,    -inf],
        [-0.0794,  0.4487, -0.1807,  0.0518,    -inf,    -inf],
        [-0.1510,  0.8626, -0.3597,  0.1112,  0.3216,    -inf],
        [ 0.4344, -2.5037,  1.0740, -0.3509, -0.9315,  0.9265]],
       grad_fn=&amp;lt;MaskedFillBackward0&amp;gt;)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Then, all we have to do is to apply the softmax function as usual to obtain the normalized and masked attention weights:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    In:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;attn_weights = torch.softmax(masked / d_out_kq**0.5, dim=1)
print(attn_weights)&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Out:
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0532, 0.9468, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3862, 0.1214, 0.4924, 0.0000, 0.0000, 0.0000],
        [0.2232, 0.3242, 0.2078, 0.2449, 0.0000, 0.0000],
        [0.1536, 0.3145, 0.1325, 0.1849, 0.2145, 0.0000],
        [0.1973, 0.0247, 0.3102, 0.1132, 0.0751, 0.2794]],
       grad_fn=&amp;lt;SoftmaxBackward0&amp;gt;)  &lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    Why does this work? The softmax function, applied in the last step, converts the input values into a probability distribution. When
   &lt;/span&gt;
   &lt;code&gt;
    -inf
   &lt;/code&gt;
   &lt;span&gt;
    is present in the inputs, softmax effectively treats them as zero probability. This is because
   &lt;/span&gt;
   &lt;code&gt;
    e^(-inf)
   &lt;/code&gt;
   &lt;span&gt;
    approaches 0, and thus these positions contribute nothing to the output probabilities.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Conclusion
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   In this article, we explored the inner workings of self-attention through a step-by-step coding approach. Using this as a foundation, we then looked into multi-head attention, a fundamental component of large language transformers.
  &lt;/p&gt;
  &lt;p&gt;
   We then also coded cross-attention, a variant of self-attention that is particularly effective when applied between two distinct sequences. And lastly, we coded causal self-attention, a concept crucial for generating coherent and contextually appropriate sequences in decoder-style LLMs such as GPT and Llama.
  &lt;/p&gt;
  &lt;p&gt;
   By coding these complex mechanisms from scratch, you hopefully gained a good understanding of the inner workings of the self-attention mechanism used in transformers and LLMs.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (Note that the code presented in this article is intended for illustrative purposes. If you plan to implement self-attention for training LLMs, I recommend considering optimized implementations like
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2307.08691&quot; rel=&quot;&quot;&gt;
    Flash Attention
   &lt;/a&gt;
   &lt;span&gt;
    , which reduce memory footprint and computational load.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Bonus Topic: Cross-Attention
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the code walkthrough above, the sections on Self-Attention and Causal-Attention, we set
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     d_q = d_k = 2
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     d_v = 4
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    . In other words, we used the same dimensions for query and key sequences. While the value matrix
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     W_v
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    is often chosen to have the same dimension as the query and key matrices (such as in PyTorch&#x27;s
   &lt;/span&gt;
   &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html&quot; rel=&quot;&quot;&gt;
    MultiHeadAttention
   &lt;/a&gt;
   &lt;span&gt;
    class), we can select an arbitrary number size for the value dimensions…
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation) </title>
<link>https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms</link>
<pubDate>Sun, 19 Nov 2023 04:11:26 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot; rel=&quot;&quot;&gt;
    Low-rank adaptation
   &lt;/a&gt;
   &lt;span&gt;
    (LoRA) is among the most widely used and effective techniques for efficiently training custom LLMs. For those interested in open-source LLMs, it&#x27;s an essential technique worth familiarizing oneself with.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://lightning.ai/pages/community/lora-insights/&quot; rel=&quot;&quot;&gt;
    Last month, I shared an article with several LoRA experiments
   &lt;/a&gt;
   &lt;span&gt;
    , based on the open-source
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/Lightning-AI/lit-gpt&quot; rel=&quot;&quot;&gt;
    Lit-GPT repository
   &lt;/a&gt;
   &lt;span&gt;
    that I co-maintain with my colleagues at Lightning AI. This Ahead of AI article aims to discuss the primary lessons I derived from my experiments. Additionally, I&#x27;ll address some of the frequently asked questions related to the topic.  If you are interested in finetuning custom LLMs, I hope these insights will save you some time in &quot;the long run&quot; (no pun intended).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In brief, the main takeaways I am discussing in this article are the following:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Despite the inherent randomness of LLM training (or when training models on GPUs in general), the outcomes remain remarkably consistent across multiple runs.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     QLoRA presents a trade-off that might be worthwhile if you&#x27;re constrained by GPU memory. It offers 33% memory savings at the cost of a 39% increase in runtime.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     When finetuning LLMs, the choice of optimizer shouldn&#x27;t be a major concern. While SGD on its own is suboptimal, there&#x27;s minimal variation in outcomes whether you employ AdamW, SGD with a scheduler, or AdamW with a scheduler.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     While Adam is often labeled a memory-intensive optimizer due to its introduction of two new parameters for every model parameter, this doesn&#x27;t significantly affect the peak memory demands of the LLM. This is because the majority of the memory is allocated for large matrix multiplications rather than retaining extra parameters.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     For static datasets, iterating multiple times, as done in multi-epoch training, might not be beneficial. It often deteriorates the results, probably due to overfitting.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     If you&#x27;re incorporating LoRA, ensure it&#x27;s applied across all layers, not just to the Key and Value matrices, to maximize model performance.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Adjusting the LoRA rank is essential, and so is selecting an apt alpha value. A good heuristic is setting alpha at twice the rank&#x27;s value.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     7 billion parameter models can be finetuned efficiently within a few hours on a single GPU possessing 14 GB of RAM. With a static dataset, optimizing an LLM to excel across all benchmark tasks is unattainable. Addressing this requires diverse data sources, or perhaps LoRA might not be the ideal tool.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
   In addition, I will answer ten common questions around LoRA:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q1: How Important is the Dataset?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q2: Does LoRA Work for Domain Adaptation?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q3: How Do You Select the Best Rank?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q4: Does LoRA Need to Be Enabled for All Layers?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q5: How To Avoid Overfitting?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q6: What about Other Optimizers?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q7: What Other Factors Influence Memory Usage?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q8: How Does it Compare to Full Finetuning and RLHF?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q9: Can LoRA Weights be Combined?
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Q10: What about Layer-wise Optimal Rank Adaptation?
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   (In the previous issue of AI, I mentioned that I wanted to write a more general introduction with a from-scratch code implementation of LoRA sometime if there&#x27;s interest. According to your feedback, there&#x27;s a lot of interest, and I plan to share another article on LoRA in the future. For now, this article is focused on the broader ideas and takeaways from working with LoRA—a top-down view.)
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   A Brief Introduction to LoRA
  &lt;/h1&gt;
  &lt;p&gt;
   Large language models are large, and it can be expensive to update all model weights during training due to GPU memory limitations.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For example, suppose we have an LLM with 7B parameters represented in a weight matrix
   &lt;/span&gt;
   &lt;em&gt;
    W
   &lt;/em&gt;
   &lt;span&gt;
    . (In reality, the model parameters are, of course, distributed across different matrices in many layers, but for simplicity, we refer to a single weight matrix here). During backpropagation, we learn a
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    matrix, which contains information on how much we want to update the original weights to minimize the loss function during training.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The weight update is then as follows:
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    W
   &lt;/em&gt;
   &lt;sub&gt;
    updated
   &lt;/sub&gt;
   &lt;span&gt;
    =
   &lt;/span&gt;
   &lt;em&gt;
    W
   &lt;/em&gt;
   &lt;span&gt;
    +
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    If the weight matrix
   &lt;/span&gt;
   &lt;em&gt;
    W
   &lt;/em&gt;
   &lt;span&gt;
    contains 7B parameters, then the weight update matrix
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    also contains 7B parameters, and computing the matrix
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    can be very compute and memory intensive.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The LoRA method proposed by
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot; rel=&quot;&quot;&gt;
    Hu
   &lt;/a&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot; rel=&quot;&quot;&gt;
     et al.
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    replaces to decompose the weight changes,
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    , into a lower-rank representation. To be precise, it does not require to explicitly compute
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    . Instead, LoRA learns the decomposed representation of
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    directly during training which is where the savings are coming from, as shown in the figure below.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!i3QH!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!i3QH!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 424w, https://substackcdn.com/image/fetch/$s_!i3QH!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 848w, https://substackcdn.com/image/fetch/$s_!i3QH!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1272w, https://substackcdn.com/image/fetch/$s_!i3QH!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:612,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;612&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!i3QH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!i3QH!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 424w, https://substackcdn.com/image/fetch/$s_!i3QH!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 848w, https://substackcdn.com/image/fetch/$s_!i3QH!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1272w, https://substackcdn.com/image/fetch/$s_!i3QH!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5dfbd169-eb7e-41e1-a050-556ccd6fb679_1600x672.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    As illustrated above, the decomposition of
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    means that we represent the large matrix
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    with two smaller LoRA matrices,
   &lt;/span&gt;
   &lt;em&gt;
    A
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    B
   &lt;/em&gt;
   &lt;span&gt;
    . If
   &lt;/span&gt;
   &lt;em&gt;
    A
   &lt;/em&gt;
   &lt;span&gt;
    has the same number of rows as
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    B
   &lt;/em&gt;
   &lt;span&gt;
    has the same number of columns as
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    , we can write the decomposition as
   &lt;/span&gt;
   &lt;em&gt;
    ΔW = AB
   &lt;/em&gt;
   &lt;span&gt;
    . (
   &lt;/span&gt;
   &lt;em&gt;
    AB
   &lt;/em&gt;
   &lt;span&gt;
    is the matrix multiplication result between matrices
   &lt;/span&gt;
   &lt;em&gt;
    A
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    B
   &lt;/em&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    How much memory does this save? It depends on the rank
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    , which is a hyperparameter. For example, if
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    has 10,000 rows and 20,000 columns, it stores 200,000,000 parameters. If we choose
   &lt;/span&gt;
   &lt;em&gt;
    A
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    B
   &lt;/em&gt;
   &lt;span&gt;
    with
   &lt;/span&gt;
   &lt;em&gt;
    r=8
   &lt;/em&gt;
   &lt;span&gt;
    , then
   &lt;/span&gt;
   &lt;em&gt;
    A
   &lt;/em&gt;
   &lt;span&gt;
    has 10,000 rows and 8 columns, and
   &lt;/span&gt;
   &lt;em&gt;
    B
   &lt;/em&gt;
   &lt;span&gt;
    has 8 rows and 20,000 columns, that&#x27;s 10,000×8 + 8×20,000 = 240,000 parameters, which is about 830× less than 200,000,000.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Of course,
   &lt;/span&gt;
   &lt;em&gt;
    A
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    B
   &lt;/em&gt;
   &lt;span&gt;
    can&#x27;t capture all the information that
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    could capture, but this is by design. When using LoRA, we hypothesize that the model requires
   &lt;/span&gt;
   &lt;em&gt;
    W
   &lt;/em&gt;
   &lt;span&gt;
    to be a large matrix with full rank to capture all the knowledge in the pretraining dataset. However, when we finetune an LLM, we don&#x27;t need to update all the weights and capture the core information for the adaptation in a smaller number of weights than
   &lt;/span&gt;
   &lt;em&gt;
    ΔW
   &lt;/em&gt;
   &lt;span&gt;
    would; hence, we have the low-rank updates via
   &lt;/span&gt;
   &lt;em&gt;
    AB
   &lt;/em&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   1. LoRA Consistency
  &lt;/h1&gt;
  &lt;p&gt;
   Running multiple experiments with LoRA, I found that the benchmark results are surprisingly consistent across the different runs despite the inherent randomness of LLM training or when training models on GPUs in general. This is a good basis for additional comparison studies.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!U2YD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!U2YD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 424w, https://substackcdn.com/image/fetch/$s_!U2YD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 848w, https://substackcdn.com/image/fetch/$s_!U2YD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1272w, https://substackcdn.com/image/fetch/$s_!U2YD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:278,&quot;width&quot;:1244,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;278&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!U2YD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!U2YD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 424w, https://substackcdn.com/image/fetch/$s_!U2YD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 848w, https://substackcdn.com/image/fetch/$s_!U2YD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1272w, https://substackcdn.com/image/fetch/$s_!U2YD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9fcb4b0c-0dec-4f19-8311-73e80de73a62_1244x278.png 1456w&quot; width=&quot;1244&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    (Note that the results were obtained with default settings using a small
   &lt;/span&gt;
   &lt;em&gt;
    r=8
   &lt;/em&gt;
   &lt;span&gt;
    . The experimental details can be found in my other article
   &lt;/span&gt;
   &lt;a href=&quot;https://lightning.ai/pages/community/lora-insights/&quot; rel=&quot;&quot;&gt;
    here
   &lt;/a&gt;
   &lt;span&gt;
    .)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   2. QLoRA Compute-Memory Trade-offs
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2305.14314&quot; rel=&quot;&quot;&gt;
    QLoRA by Dettmers
   &lt;/a&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2305.14314&quot; rel=&quot;&quot;&gt;
     et al.
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    , short for quantized LoRA, is a technique that further reduces memory usage during finetuning. During backpropagation, QLoRA quantizes the pretrained weights to 4-bit precision and uses paged optimizers to handle memory spikes.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Indeed, I found that one can save 33% of GPU memory when using QLoRA. However, this comes at a 39% increased training runtime caused by the additional quantization and dequantization of the pretrained model weights in QLoRA.
  &lt;/p&gt;
  &lt;p&gt;
   Default LoRA with 16-bit brain floating point precision:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Training time: 1.85 h
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Memory used: 21.33 GB
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;span&gt;
    QLoRA with 4-bit
   &lt;/span&gt;
   &lt;em&gt;
    Normal Floats:
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Training time: 2.79 h
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Memory used: 14.18 GB
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   Moreover, I found that the modeling performance was barely affected, which makes QLoRA a feasible alternative to regular LoRA training to work around the common GPU memory bottleneck.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!7-ZU!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!7-ZU!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 424w, https://substackcdn.com/image/fetch/$s_!7-ZU!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 848w, https://substackcdn.com/image/fetch/$s_!7-ZU!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1272w, https://substackcdn.com/image/fetch/$s_!7-ZU!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:216,&quot;width&quot;:1240,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;216&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!7-ZU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!7-ZU!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 424w, https://substackcdn.com/image/fetch/$s_!7-ZU!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 848w, https://substackcdn.com/image/fetch/$s_!7-ZU!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1272w, https://substackcdn.com/image/fetch/$s_!7-ZU!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fddf4b7b6-9174-4c45-823f-4976b2b1a013_1240x216.png 1456w&quot; width=&quot;1240&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   3. Learning Rate Schedulers
  &lt;/h1&gt;
  &lt;p&gt;
   Learning rate schedulers lower the learning rate throughout the training to optimize convergence and avoid overshooting the loss minima.
  &lt;/p&gt;
  &lt;p&gt;
   Cosine annealing is a learning rate scheduler that adjusts the learning rate following a cosine curve. It starts with a high learning rate, which then decreases smoothly, approaching zero in a cosine-like manner. A commonly used variant is the half-cycle variant, where only a half-cosine cycle is completed over the course of training, as shown in the figure below.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!T-Eg!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!T-Eg!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 424w, https://substackcdn.com/image/fetch/$s_!T-Eg!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 848w, https://substackcdn.com/image/fetch/$s_!T-Eg!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1272w, https://substackcdn.com/image/fetch/$s_!T-Eg!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ebde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1092,&quot;width&quot;:1456,&quot;resizeWidth&quot;:441,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;330.75&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!T-Eg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!T-Eg!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 424w, https://substackcdn.com/image/fetch/$s_!T-Eg!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 848w, https://substackcdn.com/image/fetch/$s_!T-Eg!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1272w, https://substackcdn.com/image/fetch/$s_!T-Eg!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Febde4c59-276d-4d77-8950-a1f475182c69_1600x1200.png 1456w&quot; width=&quot;441&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   As part of my experiments, I added a cosine annealing scheduler to the LoRA finetuning scripts and observed that it improved the SGD performance noticeably. However, it has less impact on Adam and AdamW optimizers and makes almost no difference.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!qcxe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qcxe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 424w, https://substackcdn.com/image/fetch/$s_!qcxe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 848w, https://substackcdn.com/image/fetch/$s_!qcxe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1272w, https://substackcdn.com/image/fetch/$s_!qcxe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:286,&quot;width&quot;:1368,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;286&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!qcxe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qcxe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 424w, https://substackcdn.com/image/fetch/$s_!qcxe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 848w, https://substackcdn.com/image/fetch/$s_!qcxe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1272w, https://substackcdn.com/image/fetch/$s_!qcxe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F420f28ae-2601-4226-98ca-4ca0f9cccc86_1368x286.png 1456w&quot; width=&quot;1368&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The potential advantages of SGD over Adam are discussed in the next section.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   4. Adam vs SGD
  &lt;/h1&gt;
  &lt;p&gt;
   Adam and AdamW optimizers remain popular choices in deep learning even though they are very memory-intensive when we are working with large models. The reason is that Adam optimizers maintain two moving averages for each model parameter: the first moment (mean) of the gradients and the second moment (uncentered variance) of the gradients. In other words, Adam optimizers store two additional values for each single model parameter in memory. If we are working with a 7B parameter model, that&#x27;s an extra 14B parameters to track during training.
  &lt;/p&gt;
  &lt;p&gt;
   SGD optimizers don&#x27;t need to track any additional parameters during training, so a question is: what advantage does swapping Adam by SGD have on the peak memory requirements when training LLMs?
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In my experiments, training a 7B parameter Llama 2 model trained with AdamW and LoRA defaults (
   &lt;/span&gt;
   &lt;em&gt;
    r=8
   &lt;/em&gt;
   &lt;span&gt;
    ) required 14.18 GB of GPU memory. Training the same model with SGD instead required 14.15 GB of GPU memory. In other words, the savings (0.03 GB) were minimal.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Why are the memory savings so small? That&#x27;s because with LoRA, we only have a small number of trainable parameters. For instance, if
   &lt;/span&gt;
   &lt;em&gt;
    r=8
   &lt;/em&gt;
   &lt;span&gt;
    , we have 4,194,304 trainable LoRA parameters out of all 6,738,415,616 parameters in a 7B Llama 2 model.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   If we just look at the bare numbers, 4,194,304 trainable parameters still sound like a lot, but if we do the math, we only have 4,194,304 × 2 × 16 bit = 134.22 megabits = 16.78 megabytes. (We observed a 0.03 Gb = 30 Mb difference since there is an additional overhead in storing and copying optimizer states.) The 2 represents the number of extra parameters that Adam stores, and the 16-bit refers to the default precision for the model weights.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!4mmF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!4mmF!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 424w, https://substackcdn.com/image/fetch/$s_!4mmF!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 848w, https://substackcdn.com/image/fetch/$s_!4mmF!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1272w, https://substackcdn.com/image/fetch/$s_!4mmF!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:738,&quot;width&quot;:1042,&quot;resizeWidth&quot;:409,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;289.6756238003839&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!4mmF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!4mmF!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 424w, https://substackcdn.com/image/fetch/$s_!4mmF!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 848w, https://substackcdn.com/image/fetch/$s_!4mmF!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1272w, https://substackcdn.com/image/fetch/$s_!4mmF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb9f35ed5-6eba-45fc-a1dd-8f32e8abe6ef_1042x738.png 1456w&quot; width=&quot;409&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    However, if we increase the LoRA
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    to 256, something I&#x27;ve done in later experiments, the difference between Adam and SGD optimizers becomes more noticeable:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     17.86 GB with AdamW
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     14.46 GB with SGD
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;span&gt;
    As a takeaway, swapping Adam optimizers with SGD may not be worthwhile when LoRA&#x27;s
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    is small. However, it may be worthwhile when we are increasing
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   5. Multiple Training Epochs
  &lt;/h1&gt;
  &lt;p&gt;
   In conventional deep learning, we often iterate over a training set multiple times -- an iteration over the training set is called an epoch. It&#x27;s common to run hundreds of training epochs when training convolutional neural networks, for example. Is multi-epoch training useful for instruction finetuning as well?
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    When I increased the number of iterations for the
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/tatsu-lab/stanford_alpaca&quot; rel=&quot;&quot;&gt;
    50k-example Alpaca
   &lt;/a&gt;
   &lt;span&gt;
    instruction finetuning dataset by a factor of two (analogous to 2 training epochs), I noticed a decline in model performance.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!bZNB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!bZNB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 424w, https://substackcdn.com/image/fetch/$s_!bZNB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 848w, https://substackcdn.com/image/fetch/$s_!bZNB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1272w, https://substackcdn.com/image/fetch/$s_!bZNB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:186,&quot;width&quot;:1358,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;186&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!bZNB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!bZNB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 424w, https://substackcdn.com/image/fetch/$s_!bZNB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 848w, https://substackcdn.com/image/fetch/$s_!bZNB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1272w, https://substackcdn.com/image/fetch/$s_!bZNB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F78d8aef4-99cf-422b-8cba-3413740446f6_1358x186.png 1456w&quot; width=&quot;1358&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The takeaway is that multi-epoch training might not benefit instruction finetuning since it can deteriorate the results. I observed the same with the 1k-example LIMA dataset. This performance decline is likely due to increased overfitting, which warrants additional investigation.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   6. Enable LoRA for More Layers
  &lt;/h1&gt;
  &lt;p&gt;
   The tables above showed experiments where LoRA was only enabled for select weight matrices, i.e., the Key and Value weight matrices in each transformer layer. In addition, we can also enable LoRA for the Query weight matrices, the projection layers, the other linear layers between the multihead attention blocks, and the linear output layer.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!FBfD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FBfD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 424w, https://substackcdn.com/image/fetch/$s_!FBfD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 848w, https://substackcdn.com/image/fetch/$s_!FBfD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1272w, https://substackcdn.com/image/fetch/$s_!FBfD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:892,&quot;width&quot;:1072,&quot;resizeWidth&quot;:309,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;257.11567164179104&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!FBfD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!FBfD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 424w, https://substackcdn.com/image/fetch/$s_!FBfD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 848w, https://substackcdn.com/image/fetch/$s_!FBfD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1272w, https://substackcdn.com/image/fetch/$s_!FBfD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7242d57e-dea7-4b6d-91c3-5174ceb9c514_1072x892.png 1456w&quot; width=&quot;309&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   If we enable LoRA for all these additional layers, we increase the number of trainable parameters by a factor of 5, from 4,194,304 to 20,277,248, for a 7B Llama 2 model. This also comes with a larger memory requirement (16.62 GB instead of 14.18 GB) but can increase the modeling performance noticeably.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!JswZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JswZ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 424w, https://substackcdn.com/image/fetch/$s_!JswZ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 848w, https://substackcdn.com/image/fetch/$s_!JswZ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1272w, https://substackcdn.com/image/fetch/$s_!JswZ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:168,&quot;width&quot;:1364,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:62148,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;168&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!JswZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JswZ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 424w, https://substackcdn.com/image/fetch/$s_!JswZ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 848w, https://substackcdn.com/image/fetch/$s_!JswZ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1272w, https://substackcdn.com/image/fetch/$s_!JswZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff34e72a3-fb4e-4167-aabb-2904adf29122_1364x168.png 1456w&quot; width=&quot;1364&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   However, a limitation of my experiment is that I only explored two settings: (1) LoRA for only the query and value weight matrices enabled, and (2) LoRA for all layers enabled. It might be worthwhile exploring the other combinations in future experiments. For example, it would be interesting to know whether activating LoRA for the projection layer is actually beneficial.
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   7. Balancing LoRA Hyperparameters: R and Alpha
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    As the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot; rel=&quot;&quot;&gt;
    original LoRA paper
   &lt;/a&gt;
   &lt;span&gt;
    outlines, LoRA introduces an additional scaling coefficient for applying the LoRA weights to the pretrained weights during the forward pass. The scaling involves the rank parameter r, which we discussed earlier, as well as another hyperparameter α (alpha) that is applied as follows:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;scaling = alpha / r
weight += (lora_B @ lora_A) * scaling &lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   As we can see in the code formula above, the larger the influence of the LoRA weights.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Previous experiments used
   &lt;/span&gt;
   &lt;em&gt;
    r=8
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    alpha=16
   &lt;/em&gt;
   &lt;span&gt;
    , which resulted in a 2-fold scaling. Choosing alpha as two times
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    is a common rule of thumb when using LoRA for LLMs, but I was curious if this still holds for larger
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    values. In other words, “alpha = 2×rank” really seems to be a sweet spot. However, in this specific combination of model and dataset, where
   &lt;/span&gt;
   &lt;em&gt;
    r=256
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    alpha=128
   &lt;/em&gt;
   &lt;span&gt;
    (a 0.5-fold scaling) performance is even better.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!qKUB!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qKUB!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png 424w, https://substackcdn.com/image/fetch/$s_!qKUB!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png 848w, https://substackcdn.com/image/fetch/$s_!qKUB!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png 1272w, https://substackcdn.com/image/fetch/$s_!qKUB!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:548,&quot;width&quot;:1370,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:187728,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;548&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!qKUB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!qKUB!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png 424w, https://substackcdn.com/image/fetch/$s_!qKUB!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png 848w, https://substackcdn.com/image/fetch/$s_!qKUB!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png 1272w, https://substackcdn.com/image/fetch/$s_!qKUB!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8b11ebe1-781b-4e05-b05d-93dd8c3b00cc_1370x548.png 1456w&quot; width=&quot;1370&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    (I experimented with
   &lt;/span&gt;
   &lt;em&gt;
    r=32
   &lt;/em&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;em&gt;
    r=64
   &lt;/em&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;em&gt;
    r=128
   &lt;/em&gt;
   &lt;span&gt;
    , and
   &lt;/span&gt;
   &lt;em&gt;
    r=512
   &lt;/em&gt;
   &lt;span&gt;
    but omitted the results for clarity as
   &lt;/span&gt;
   &lt;em&gt;
    r=256
   &lt;/em&gt;
   &lt;span&gt;
    resulted in the best performance.)
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Choosing alpha as two times as large as
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    may often result in the best outcomes, but it may also not hurt to experiment with different ratios.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   8. Training 7B Parameter Models on a Single GPU
  &lt;/h1&gt;
  &lt;p&gt;
   &lt;span&gt;
    One of the main takeaways is that LoRA allows us to finetune 7B parameter LLMs on a single GPU. In this particular case, using QLoRA with the best setting (
   &lt;/span&gt;
   &lt;em&gt;
    r=256
   &lt;/em&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;em&gt;
    alpha=512
   &lt;/em&gt;
   &lt;span&gt;
    ) requires 17.86 GB with AdamW and takes about 3 hours (on an A100) for 50k training examples (here, the Alpaca dataset).
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Rm03!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Rm03!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 424w, https://substackcdn.com/image/fetch/$s_!Rm03!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 848w, https://substackcdn.com/image/fetch/$s_!Rm03!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1272w, https://substackcdn.com/image/fetch/$s_!Rm03!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d309a2b3-6087-4094-a280-bc60feca149a_1370x174.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:174,&quot;width&quot;:1370,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;174&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Rm03!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Rm03!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 424w, https://substackcdn.com/image/fetch/$s_!Rm03!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 848w, https://substackcdn.com/image/fetch/$s_!Rm03!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1272w, https://substackcdn.com/image/fetch/$s_!Rm03!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd309a2b3-6087-4094-a280-bc60feca149a_1370x174.png 1456w&quot; width=&quot;1370&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    In the remaining sections of this article, I am answering additional questions you might have.
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h1 class=&quot;header-anchor-post&quot;&gt;
   &lt;span&gt;
    Answers to Common Questions
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/h1&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q1: How Important is the Dataset?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   The dataset can be critical. I used the Alpaca dataset, which contains 50k training examples, for my experiments. I chose this dataset because it&#x27;s quite popular, and experimenting with different datasets was out of scope due to the already extensive length of the article.
  &lt;/p&gt;
  &lt;p&gt;
   However, it&#x27;s worth noting that Alpaca is a synthetic dataset that was generated by querying an old version of ChatGPT and is probably not the best by today&#x27;s standards.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Data quality can be very important. For example, in June, I discussed the LIMA dataset (
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/ahead-of-ai-9-llm-tuning-and-dataset&quot; rel=&quot;&quot;&gt;
    Ahead of AI #9: LLM Tuning &amp; Dataset Perspectives
   &lt;/a&gt;
   &lt;span&gt;
    ), a curated dataset consisting of only 1k examples.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    According to the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2305.11206&quot; rel=&quot;&quot;&gt;
    LIMA: Less Is More for Alignment
   &lt;/a&gt;
   &lt;span&gt;
    paper, a 65B Llama model finetuned on LIMA noticeably outperforms a 65B Llama model finetuned on Alpaca.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!JXzN!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JXzN!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 424w, https://substackcdn.com/image/fetch/$s_!JXzN!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 848w, https://substackcdn.com/image/fetch/$s_!JXzN!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1272w, https://substackcdn.com/image/fetch/$s_!JXzN!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/d7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:540,&quot;width&quot;:1456,&quot;resizeWidth&quot;:675,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;250.3434065934066&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!JXzN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JXzN!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 424w, https://substackcdn.com/image/fetch/$s_!JXzN!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 848w, https://substackcdn.com/image/fetch/$s_!JXzN!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1272w, https://substackcdn.com/image/fetch/$s_!JXzN!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fd7d97014-0757-45bc-b1f9-b5cb69a69df8_1484x550.png 1456w&quot; width=&quot;675&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    Using the best configuration (
   &lt;/span&gt;
   &lt;em&gt;
    r=256,
   &lt;/em&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    alpha=512
   &lt;/em&gt;
   &lt;span&gt;
    ) on LIMA, I got similar, if not better, performance than the 50x larger Alpaca dataset.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!oV8z!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!oV8z!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 424w, https://substackcdn.com/image/fetch/$s_!oV8z!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 848w, https://substackcdn.com/image/fetch/$s_!oV8z!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1272w, https://substackcdn.com/image/fetch/$s_!oV8z!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/bc33e126-6803-4a18-837a-6525226d068b_1388x270.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:270,&quot;width&quot;:1388,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;270&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!oV8z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!oV8z!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 424w, https://substackcdn.com/image/fetch/$s_!oV8z!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 848w, https://substackcdn.com/image/fetch/$s_!oV8z!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1272w, https://substackcdn.com/image/fetch/$s_!oV8z!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fbc33e126-6803-4a18-837a-6525226d068b_1388x270.png 1456w&quot; width=&quot;1388&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q2: Does LoRA Work for Domain Adaptation?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Unfortunately, I don&#x27;t have a good answer to this question. As a rule of thumb, knowledge is usually absorbed from the pretraining dataset. Instruction finetuning is generally more about helping or guiding the LLM towards following instructions.
  &lt;/p&gt;
  &lt;p&gt;
   However, it&#x27;s worth noting that if memory is a concern, LoRA can also be used for further pretraining existing pretrained LLMs on domain-specific datasets.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Note that my experiments also included two arithmetic benchmarks (they are included in
   &lt;/span&gt;
   &lt;a href=&quot;https://lightning.ai/pages/community/lora-insights/&quot; rel=&quot;&quot;&gt;
    my other more technical write-up
   &lt;/a&gt;
   &lt;span&gt;
    ), on which LoRA-finetuned models performed significantly worse than the pretrained base models. My hypothesis is that the model unlearned arithmetic because the Alpaca dataset did not contain corresponding examples. Whether the model completely lost the knowledge or whether it&#x27;s because the model can&#x27;t handle the instructions anymore would require further investigation. However, a takeaway here is that it&#x27;s probably a good idea to include examples of each task you care about when finetuning LLMs.
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q3: How Do You Select the Best Rank?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    Unfortunately, I don&#x27;t have any good heuristic for selecting a good
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    and think that it&#x27;s a hyperparameter that needs to be explored for each LLM and each dataset. I suspect that choosing an
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    that is too large could result in more overfitting. On the other hand, a small r may not be able to capture diverse tasks in a dataset. In other words, I suspect that the more diverse the tasks in the dataset, the larger the
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    should be. For example, if I only want a model that carries out basic 2-digit arithmetic, then a tiny
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    might already be sufficient. However, this is only a hypothesis and would require additional investigation.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q4: Does LoRA Need to Be Enabled for All Layers?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   I only explored two settings: (1) LoRA for only the query and value weight matrices enabled, and (2) LoRA for all layers enabled. It might be worthwhile exploring the other combinations in future experiments. For example, it would be interesting to know whether activating LoRA for the projection layer is actually beneficial.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!4x88!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!4x88!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 424w, https://substackcdn.com/image/fetch/$s_!4x88!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 848w, https://substackcdn.com/image/fetch/$s_!4x88!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!4x88!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:869,&quot;width&quot;:1024,&quot;resizeWidth&quot;:271,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;229.9794921875&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!4x88!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!4x88!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 424w, https://substackcdn.com/image/fetch/$s_!4x88!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 848w, https://substackcdn.com/image/fetch/$s_!4x88!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!4x88!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5433397c-c384-45cb-ab59-69641fe67a6e_1024x869.jpeg 1456w&quot; width=&quot;271&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    For instance, if we consider the various settings (
   &lt;/span&gt;
   &lt;code&gt;
    lora_query
   &lt;/code&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;code&gt;
    lora_key
   &lt;/code&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;code&gt;
    lora_value
   &lt;/code&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;code&gt;
    lora_projection
   &lt;/code&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;code&gt;
    lora_mlp
   &lt;/code&gt;
   &lt;span&gt;
    , and
   &lt;/span&gt;
   &lt;code&gt;
    lora_head
   &lt;/code&gt;
   &lt;span&gt;
    ), that&#x27;s
   &lt;/span&gt;
   &lt;em&gt;
    2^6 = 64
   &lt;/em&gt;
   &lt;span&gt;
    combinations to explore. This exploration would be an interesting topic for future studies.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q5: How To Avoid Overfitting?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    Generally, a larger
   &lt;/span&gt;
   &lt;em&gt;
    r
   &lt;/em&gt;
   &lt;span&gt;
    can lead to more overfitting because it determines the number of trainable parameters. If a model suffers from overfitting, decreasing r or increasing the dataset size are the first candidates to explore. Moreover, you could try to increase the weight decay rate in AdamW or SGD optimizers, and you can consider increasing the dropout value for LoRA layers.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The LoRA dropout parameter that I haven&#x27;t explored in my experiments (I used a fixed dropout rate of 0.05), is an interesting topic for future investigations.
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q6: What about Other Optimizers?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    Other interesting optimizers for LLMs are worth exploring in the future. One such optimizer is
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2305.14342&quot; rel=&quot;&quot;&gt;
    Sophia: A Scalable Stochastic Second-order Optimizer for Language Model Pre-training
   &lt;/a&gt;
   &lt;span&gt;
    , which was published in May.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Sophia is a second-order optimization algorithm that promises to be particularly attractive for LLMs where Adam and AdamW are usually the dominant ones. Compared to Adam, Sophia is 2× faster, and models trained with Sophia can achieve better modeling performance, according to the paper. In a nutshell, Sophia normalizes the gradients by gradient curvature instead of gradient variance, as in Adam.
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q7: What Other Factors Influence Memory Usage?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Besides precision and quantization settings, the model size, the batch size, and the number of trainable LoRA parameters, the dataset can also influence memory usage.
  &lt;/p&gt;
  &lt;p&gt;
   Note that Llama 2 has a block size of 4048. For instance, if an LLM has a block size of 4048 tokens, it can process sequences of up to 4048 tokens at once. However, shorter training sequences can result in substantial memory savings due to the masking of future tokens.
  &lt;/p&gt;
  &lt;p&gt;
   For example, the Alpaca dataset is relatively small, with a maximum length of 1304 tokens.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Nv2M!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Nv2M!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Nv2M!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Nv2M!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Nv2M!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:960,&quot;width&quot;:1280,&quot;resizeWidth&quot;:539,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;404.25&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Nv2M!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Nv2M!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 424w, https://substackcdn.com/image/fetch/$s_!Nv2M!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 848w, https://substackcdn.com/image/fetch/$s_!Nv2M!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!Nv2M!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F16b239ab-e72e-4de4-badc-94cfa69042fa_1280x960.jpeg 1456w&quot; width=&quot;539&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;span&gt;
    When I experimented with other datasets that had lengths of up to 2048 tokens, I noticed that the memory usage went up from 17.86 GB to 26.96 GB.
   &lt;/span&gt;
   &lt;br/&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q8: How Does it Compare to Full Finetuning and RLHF?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    I did not run any RLHF experiments (for those who are curious, I covered RLHF
   &lt;/span&gt;
   &lt;a href=&quot;https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives&quot; rel=&quot;&quot;&gt;
    here
   &lt;/a&gt;
   &lt;span&gt;
    ), but I did consider full finetuning. Full finetuning required at least 2 GPUs and was completed in 3.5 h using 36.66 GB on each GPU. However, the benchmark results were not very good, likely due to overfitting or suboptimal hyperparameters.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!nrLe!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nrLe!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 424w, https://substackcdn.com/image/fetch/$s_!nrLe!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 848w, https://substackcdn.com/image/fetch/$s_!nrLe!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1272w, https://substackcdn.com/image/fetch/$s_!nrLe!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:218,&quot;width&quot;:1358,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;218&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!nrLe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!nrLe!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 424w, https://substackcdn.com/image/fetch/$s_!nrLe!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 848w, https://substackcdn.com/image/fetch/$s_!nrLe!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1272w, https://substackcdn.com/image/fetch/$s_!nrLe!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F9913f32a-fd3b-4b82-8337-3a9cb6805808_1358x218.png 1456w&quot; width=&quot;1358&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q9: Can LoRA Weights be Combined?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   Yes, it&#x27;s possible to combine multiple sets of LoRA weights. During training, we keep the LoRA weights separate from the pretrained weights and add them during each forward pass.
  &lt;/p&gt;
  &lt;p&gt;
   However, If you have a real-world application with many sets of LoRA weights, for example, one set for each application customer, it makes sense to store these weights separately to save disk space. However, it&#x27;s possible to merge the pretrained weights with the LoRA weights after training to create a single model. This way, we don&#x27;t have to apply the LoRA weights in each forward pass:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;weight += (lora_B @ lora_A) * scaling&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   Instead, we apply the weight update as shown above and save the merged (added) weights.
  &lt;/p&gt;
  &lt;p&gt;
   Similarly, we can keep adding multiple LoRA weight sets:
  &lt;/p&gt;
  &lt;pre&gt;&lt;code&gt;weight += (lora_B_set1 @ lora_A_set1) * scaling_set1
weight += (lora_B_set2 @ lora_A_set2) * scaling_set2
weight += (lora_B_set3 @ lora_A_set3) * scaling_set3
...&lt;/code&gt;&lt;/pre&gt;
  &lt;p&gt;
   &lt;span&gt;
    I have yet to do experiments to evaluate the performance of such an approach, but this is technically already possible via the
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/Lightning-AI/lit-gpt/blob/main/scripts/merge_lora.py&quot; rel=&quot;&quot;&gt;
    scripts/merge_lora.py
   &lt;/a&gt;
   &lt;span&gt;
    script provided in Lit-GPT.
   &lt;/span&gt;
   &lt;br/&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   &lt;strong&gt;
    Q10: What about Layer-wise Optimal Rank Adaptation?
   &lt;/strong&gt;
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    For simplicity, we usually train deep neural networks with the same learning rate for each layer, and the learning rate is a hyperparameter that we need to optimize. To take it further, we can also choose a different learning rate for each layer (
   &lt;/span&gt;
   &lt;a href=&quot;https://kozodoi.me/blog/20220329/discriminative-lr#:~:text=The%20implementation%20of%20layer%2Dwise,with%20the%20corresponding%20learning%20rates.&quot; rel=&quot;&quot;&gt;
    in PyTorch, this is not too complicated
   &lt;/a&gt;
   &lt;span&gt;
    ). However, it&#x27;s rarely done in practice because it adds additional overhead, and there are usually already so many knobs to tune when training deep neural networks.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Analogous to choosing different learning rates for different layers, we can also choose different LoRA ranks for different layers. I haven&#x27;t found any experiments on this, but a document that details this approach is
   &lt;/span&gt;
   &lt;a href=&quot;https://medium.com/@tom_21755/llm-optimization-layer-wise-optimal-rank-adaptation-lora-1444dfbc8e6a&quot; rel=&quot;&quot;&gt;
    Layer-wise Optimal Rank Adaptation
   &lt;/a&gt;
   &lt;span&gt;
    (also abbreviated LORA). In theory, this sounds like a good idea in practice. However, it also adds an extensive number of choices when optimizing hyperparameters.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . (I am confident that you&#x27;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:&quot;&quot;,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Build a Large Language Model (From Scratch) now
     &lt;/span&gt;
     &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
      available on Amazon
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Alternatively, I also recently enabled the paid subscription option on Substack to support this magazine directly.
  &lt;/p&gt;
  &lt;div class=&quot;subscription-widget-wrap&quot;&gt;
   &lt;div class=&quot;subscription-widget show-subscribe&quot;&gt;
    &lt;div class=&quot;preamble&quot;&gt;
     &lt;p&gt;
      Ahead of AI is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber.
     &lt;/p&gt;
    &lt;/div&gt;
    &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
    &lt;/div&gt;
   &lt;/div&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Understanding Large Language Models </title>
<link>https://magazine.sebastianraschka.com/p/understanding-large-language-models</link>
<pubDate>Sun, 16 Apr 2023 05:33:46 -0000</pubDate>
<description>
&lt;div class=&quot;available-content&quot;&gt;
 &lt;div class=&quot;body markup&quot; dir=&quot;auto&quot;&gt;
  &lt;p&gt;
   Large language models have taken the public attention by storm – no pun intended. In just half a decade large language models – transformers – have almost completely changed the field of natural language processing. Moreover, they have also begun to revolutionize fields such as computer vision and computational biology.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Since transformers have such a big impact on everyone’s research agenda, I wanted to flesh out a short reading list (an extended version of
   &lt;/span&gt;
   &lt;a href=&quot;https://www.linkedin.com/feed/update/urn:li:activity:7028449312300834816?commentUrn=urn%3Ali%3Acomment%3A%28activity%3A7028449312300834816%2C7028519126105030656%29&amp;dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287028519126105030656%2Curn%3Ali%3Aactivity%3A7028449312300834816%29&quot; rel=&quot;&quot;&gt;
    my comment yesterday
   &lt;/a&gt;
   &lt;span&gt;
    ) for machine learning researchers and practitioners getting started.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The following list below is meant to be read mostly chronologically, and I am entirely focusing on academic research papers. Of course, there are many additional resources out there that are useful. For example,
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      the
     &lt;/span&gt;
     &lt;a href=&quot;http://jalammar.github.io/illustrated-transformer/&quot; rel=&quot;&quot;&gt;
      Illustrated Transformer
     &lt;/a&gt;
     &lt;span&gt;
      by Jay Alammar;
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      a
     &lt;/span&gt;
     &lt;a href=&quot;https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/&quot; rel=&quot;&quot;&gt;
      more technical blog article
     &lt;/a&gt;
     &lt;span&gt;
      by Lilian Weng;
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://amatriain.net/blog/transformer-models-an-introduction-and-catalog-2d1e9039f376/&quot; rel=&quot;&quot;&gt;
      a catalog and family tree
     &lt;/a&gt;
     &lt;span&gt;
      of all major transformers to date by Xavier Amatriain;
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://github.com/karpathy/nanoGPT&quot; rel=&quot;&quot;&gt;
      a minimal code implementation
     &lt;/a&gt;
     &lt;span&gt;
      of a generative language model for educational purposes by Andrej Karpathy;
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;a href=&quot;https://sebastianraschka.com/blog/2021/dl-course.html#l19-self-attention-and-transformer-networks&quot; rel=&quot;&quot;&gt;
      a lecture series
     &lt;/a&gt;
     &lt;span&gt;
      and
     &lt;/span&gt;
     &lt;a href=&quot;https://github.com/rasbt/machine-learning-book/tree/main/ch16&quot; rel=&quot;&quot;&gt;
      book chapter
     &lt;/a&gt;
     &lt;span&gt;
      by yours truly.
     &lt;/span&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   Understanding the Main Architecture and Tasks
  &lt;/h2&gt;
  &lt;p&gt;
   If you are new to transformers / large language models, it makes the most sense to start at the beginning.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (1)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Neural Machine Translation by Jointly Learning to Align and Translate
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2014)
   &lt;/strong&gt;
   &lt;span&gt;
    by Bahdanau, Cho, and Bengio,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/1409.0473
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   I recommend beginning with the above paper if you have a few minutes to spare. It introduces an attention mechanism for recurrent neural networks (RNN) to improve long-range sequence modeling capabilities. This allows RNNs to translate longer sentences more accurately – the motivation behind developing the original transformer architecture later.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!Ss64!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Ss64!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png 424w, https://substackcdn.com/image/fetch/$s_!Ss64!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png 848w, https://substackcdn.com/image/fetch/$s_!Ss64!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png 1272w, https://substackcdn.com/image/fetch/$s_!Ss64!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/f907c364-b179-4112-a2b9-50c389d0377b_904x396.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:396,&quot;width&quot;:904,&quot;resizeWidth&quot;:595,&quot;bytes&quot;:93962,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:true,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; fetchpriority=&quot;high&quot; height=&quot;260.64159292035396&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!Ss64!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!Ss64!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png 424w, https://substackcdn.com/image/fetch/$s_!Ss64!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png 848w, https://substackcdn.com/image/fetch/$s_!Ss64!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png 1272w, https://substackcdn.com/image/fetch/$s_!Ss64!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Ff907c364-b179-4112-a2b9-50c389d0377b_904x396.png 1456w&quot; width=&quot;595&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1409.0473
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (2)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Attention Is All You Need
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2017)
   &lt;/strong&gt;
   &lt;span&gt;
    by Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/1706.03762
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The paper above introduces the original transformer architecture consisting of an encoder- and decoder part that will become relevant as separate modules later. Moreover, this paper introduces concepts such as the scaled dot product attention mechanism, multi-head attention blocks, and positional input encoding that remain the foundation of modern transformers.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!GCTD!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!GCTD!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png 424w, https://substackcdn.com/image/fetch/$s_!GCTD!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png 848w, https://substackcdn.com/image/fetch/$s_!GCTD!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png 1272w, https://substackcdn.com/image/fetch/$s_!GCTD!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/c820b36a-8e07-4eea-8975-d7e391006d52_824x690.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:690,&quot;width&quot;:824,&quot;resizeWidth&quot;:515,&quot;bytes&quot;:284876,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;431.25&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!GCTD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!GCTD!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png 424w, https://substackcdn.com/image/fetch/$s_!GCTD!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png 848w, https://substackcdn.com/image/fetch/$s_!GCTD!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png 1272w, https://substackcdn.com/image/fetch/$s_!GCTD!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc820b36a-8e07-4eea-8975-d7e391006d52_824x690.png 1456w&quot; width=&quot;515&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1706.03762
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (3)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     On Layer Normalization in the Transformer Architecture
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2020)
   &lt;/strong&gt;
   &lt;span&gt;
    by Xiong, Yang, He, K Zheng, S Zheng, Xing, Zhang, Lan, Wang, and Liu,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2002.04745&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2002.04745
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    While the original transformer figure above (from Attention Is All You Need,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/1706.03762
   &lt;/a&gt;
   &lt;span&gt;
    ) is a helpful summary of the original encoder-decoder architecture, the location of the LayerNorm in this figure remains a hotly debated subject.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For instance, the Attention Is All You Need transformer figure places the layer normalization between the residual blocks, which doesn&#x27;t match the
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/tensorflow/tensor2tensor/commit/f5c9b17e617ea9179b7d84d36b1e8162cb369f25&quot; rel=&quot;&quot;&gt;
    official (updated) code implementation
   &lt;/a&gt;
   &lt;span&gt;
    accompanying the original transformer paper. The variant shown in the Attention Is All You Need figure is known as Post-LN Transformer, and the updated code implementation defaults to the Pre-LN variant.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The
   &lt;/span&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2002.04745&quot; rel=&quot;&quot;&gt;
     Layer Normalization in the Transformer Architecture
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper suggests that Pre-LN works better, addressing gradient problems, as shown below. Many architectures adopted this in practice, but it can result in representation collapse.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    So, while there&#x27;s still an ongoing discussion regarding using Post-LN or Pre-LN, there&#x27;s also a new paper that proposes taking advantage of both worlds:
   &lt;/span&gt;
   &lt;em&gt;
    ResiDual: Transformer with Dual Residual Connections
   &lt;/em&gt;
   &lt;span&gt;
    (
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2304.14802&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2304.14802
   &lt;/a&gt;
   &lt;span&gt;
    ); whether it will turn out useful in practice remains to be seen.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!JOyz!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JOyz!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png 424w, https://substackcdn.com/image/fetch/$s_!JOyz!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png 848w, https://substackcdn.com/image/fetch/$s_!JOyz!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png 1272w, https://substackcdn.com/image/fetch/$s_!JOyz!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:682,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:460742,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;682&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!JOyz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!JOyz!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png 424w, https://substackcdn.com/image/fetch/$s_!JOyz!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png 848w, https://substackcdn.com/image/fetch/$s_!JOyz!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png 1272w, https://substackcdn.com/image/fetch/$s_!JOyz!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F5a0a1e39-de22-4a95-8a42-b4040cf33b54_2040x956.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Sources:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1706.03762
     &lt;/a&gt;
     &lt;span&gt;
      (left and center) and
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2002.04745&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2002.04745
     &lt;/a&gt;
     &lt;span&gt;
      (right)
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (4)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Neural Networks
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (1991)
   &lt;/strong&gt;
   &lt;span&gt;
    by Schmidhuber,
   &lt;/span&gt;
   &lt;a href=&quot;https://www.semanticscholar.org/paper/Learning-to-Control-Fast-Weight-Memories%3A-An-to-Schmidhuber/bc22e87a26d020215afe91c751e5bdaddd8e4922&quot; rel=&quot;&quot;&gt;
    https://www.semanticscholar.org/paper/Learning-to-Control-Fast-Weight-Memories%3A-An-to-Schmidhuber/bc22e87a26d020215afe91c751e5bdaddd8e4922
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This paper is recommended for those interested in historical tidbits and early approaches fundamentally similar to modern transformers.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    For instance, in 1991, which is about two-and-a-half decades before the original transformer paper above (&quot;Attention Is All You Need&quot;), Juergen Schmidhuber proposed an alternative to recurrent neural networks called
   &lt;/span&gt;
   &lt;em&gt;
    Fast Weight Programmers
   &lt;/em&gt;
   &lt;span&gt;
    (FWP). The FWP approach involves a feedforward neural network that slowly learns by gradient descent to program the changes of the fast weights of another neural network.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The analogy to modern transformers is explained
   &lt;/span&gt;
   &lt;a href=&quot;https://people.idsia.ch//~juergen/fast-weight-programmer-1991-transformer.html#sec2&quot; rel=&quot;&quot;&gt;
    in this blog post
   &lt;/a&gt;
   &lt;span&gt;
    as follows:
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;blockquote&gt;
   &lt;p&gt;
    &lt;em&gt;
     In today&#x27;s Transformer terminology, FROM and TO are called key and value, respectively. The INPUT to which the fast net is applied is called the query. Essentially, the query is processed by the fast weight matrix, which is a sum of outer products of keys and values (ignoring normalizations and projections). Since all operations of both networks are differentiable, we obtain end-to-end differentiable active control of fast weight changes through additive outer products or second order tensor products.[FWP0-3a] Hence the slow net can learn by gradient descent to rapidly modify the fast net during sequence processing. This is mathematically equivalent (apart from normalization) to what was later called Transformers with linearized self-attention (or linear Transformers).
    &lt;/em&gt;
   &lt;/p&gt;
  &lt;/blockquote&gt;
  &lt;p&gt;
   &lt;span&gt;
    As mentioned in the blog post excerpt above, this approach is now called &quot;linear Transformers&quot; or &quot;Transformers with linearized self-attention&quot; via the more recent papers
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2006.16236&quot; rel=&quot;&quot;&gt;
    Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention
   &lt;/a&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2009.14794&quot; rel=&quot;&quot;&gt;
    Rethinking Attention with Performers
   &lt;/a&gt;
   &lt;span&gt;
    that appeared on arXiv in 2020.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    In 2021, the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2102.11174&quot; rel=&quot;&quot;&gt;
    Linear Transformers Are Secretly Fast Weight Programmers
   &lt;/a&gt;
   &lt;span&gt;
    paper then explicitly showed the equivalence between linearized self-attention and the fast weight programmers from the 1990s.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!xKpc!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xKpc!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png 424w, https://substackcdn.com/image/fetch/$s_!xKpc!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png 848w, https://substackcdn.com/image/fetch/$s_!xKpc!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png 1272w, https://substackcdn.com/image/fetch/$s_!xKpc!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:894,&quot;width&quot;:884,&quot;resizeWidth&quot;:266,&quot;bytes&quot;:248638,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;269.00904977375563&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!xKpc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!xKpc!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png 424w, https://substackcdn.com/image/fetch/$s_!xKpc!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png 848w, https://substackcdn.com/image/fetch/$s_!xKpc!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png 1272w, https://substackcdn.com/image/fetch/$s_!xKpc!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F2217256f-c000-49bf-878e-7a3e062c7fb8_884x894.png 1456w&quot; width=&quot;266&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source: Annotated figure based on
     &lt;/span&gt;
     &lt;a href=&quot;https://people.idsia.ch//~juergen/fast-weight-programmer-1991-transformer.html#sec2&quot; rel=&quot;&quot;&gt;
      https://people.idsia.ch//~juergen/fast-weight-programmer-1991-transformer.html#sec2
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (5)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Universal Language Model Fine-tuning for Text Classification
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2018)
   &lt;/strong&gt;
   &lt;span&gt;
    by Howard and Ruder,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/1801.06146
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    This is another paper that&#x27;s very interesting from a historical perspective. While it was written one year after the original
   &lt;/span&gt;
   &lt;em&gt;
    Attention Is All You Need
   &lt;/em&gt;
   &lt;span&gt;
    transformer was released, it doesn&#x27;t involve transformers but instead focuses on recurrent neural networks. However, it&#x27;s still noteworthy since it effectively proposed pretraining language models and transfer learning for downstream tasks.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While transfer learning was already established in computer vision, it wasn&#x27;t yet prevalent in natural language processing (NLP). ULMFit was among the first papers to demonstrate that pretraining a language model and finetuning it on a specific task could yield state-of-the-art results in many NLP tasks.
  &lt;/p&gt;
  &lt;p&gt;
   The three-stage process for finetuning the language models suggested by ULMFit was as follows:
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     Train a language model on a large corpus of text.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Finetune this pretrained language model on task-specific data, allowing it to adapt to the specific style and vocabulary of the text.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Finetune a classifier on the task-specific data with gradual unfreezing of layers to avoid catastrophic forgetting.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   This recipe -- training a language model on a large corpus and then finetuning it on a downstream task -- is the central approach used in transformer-based models and foundation models like BERT, GPT-2/3/4, RoBERTa, and others.
  &lt;/p&gt;
  &lt;p&gt;
   However, the gradual unfreezing, a key part of ULMFiT, is usually not routinely done in practice when working with transformer architectures, where all layers are typically finetuned at once.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!G1sW!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!G1sW!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png 424w, https://substackcdn.com/image/fetch/$s_!G1sW!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png 848w, https://substackcdn.com/image/fetch/$s_!G1sW!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png 1272w, https://substackcdn.com/image/fetch/$s_!G1sW!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:702,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:497885,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;702&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!G1sW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!G1sW!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png 424w, https://substackcdn.com/image/fetch/$s_!G1sW!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png 848w, https://substackcdn.com/image/fetch/$s_!G1sW!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png 1272w, https://substackcdn.com/image/fetch/$s_!G1sW!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb2d217b3-4596-42bb-a070-c7c1ae9b90cb_1762x850.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1801.06146
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (6)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2018)
   &lt;/strong&gt;
   &lt;span&gt;
    by Devlin, Chang, Lee, and Toutanova,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/1810.04805
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Following the original transformer architecture, large language model research started to bifurcate in two directions: encoder-style transformers for predictive modeling tasks such as text classification and decoder-style transformers for generative modeling tasks such as translation, summarization, and other forms of text creation.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The BERT paper above introduces the original concept of masked-language modeling, and next-sentence prediction. It still is the most influential encoder-style architecture. If you are interested in this research branch, I recommend following up with
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1907.11692&quot; rel=&quot;&quot;&gt;
    RoBERTa
   &lt;/a&gt;
   &lt;span&gt;
    , which simplified the pretraining objectives by removing the next-sentence prediction tasks.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!f1ZR!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!f1ZR!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png 424w, https://substackcdn.com/image/fetch/$s_!f1ZR!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png 848w, https://substackcdn.com/image/fetch/$s_!f1ZR!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png 1272w, https://substackcdn.com/image/fetch/$s_!f1ZR!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/89243595-9867-4420-b753-802ca1226518_1368x830.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:830,&quot;width&quot;:1368,&quot;resizeWidth&quot;:543,&quot;bytes&quot;:291858,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;329.45175438596493&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!f1ZR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!f1ZR!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png 424w, https://substackcdn.com/image/fetch/$s_!f1ZR!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png 848w, https://substackcdn.com/image/fetch/$s_!f1ZR!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png 1272w, https://substackcdn.com/image/fetch/$s_!f1ZR!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F89243595-9867-4420-b753-802ca1226518_1368x830.png 1456w&quot; width=&quot;543&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1810.04805
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (7)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Improving Language Understanding by Generative Pre-Training
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2018) by Radford and Narasimhan
   &lt;/strong&gt;
   &lt;span&gt;
    ,
   &lt;/span&gt;
   &lt;a href=&quot;https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035&quot; rel=&quot;&quot;&gt;
    https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   The original GPT paper introduced the popular decoder-style architecture and pretraining via next-word prediction. Where BERT can be considered a bidirectional transformer due to its masked language model pretraining objective, GPT is a unidirectional, autoregressive model. While GPT embeddings can also be used for classification, the GPT approach is at the core of today’s most influential LLMs, such as chatGPT.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you are interested in this research branch, I recommend following up with the
   &lt;/span&gt;
   &lt;a href=&quot;https://www.semanticscholar.org/paper/Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu/9405cc0d6169988371b2755e573cc28650d14dfe&quot; rel=&quot;&quot;&gt;
    GPT-2
   &lt;/a&gt;
   &lt;span&gt;
    and
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot; rel=&quot;&quot;&gt;
    GPT-3
   &lt;/a&gt;
   &lt;span&gt;
    papers. These two papers illustrate that LLMs are capable of zero- and few-shot learning and highlight the emergent abilities of LLMs. GPT-3 is also still a popular baseline and base model for training current-generation LLMs such as ChatGPT – we will cover the InstructGPT approach that lead to ChatGPT later as a separate entry.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!--fa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!--fa!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png 424w, https://substackcdn.com/image/fetch/$s_!--fa!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png 848w, https://substackcdn.com/image/fetch/$s_!--fa!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png 1272w, https://substackcdn.com/image/fetch/$s_!--fa!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:658,&quot;width&quot;:1314,&quot;resizeWidth&quot;:565,&quot;bytes&quot;:329547,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;282.92998477929984&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!--fa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!--fa!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png 424w, https://substackcdn.com/image/fetch/$s_!--fa!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png 848w, https://substackcdn.com/image/fetch/$s_!--fa!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png 1272w, https://substackcdn.com/image/fetch/$s_!--fa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F573a9c49-2ff3-4a14-98b6-6b8a44333601_1314x658.png 1456w&quot; width=&quot;565&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035&quot; rel=&quot;&quot;&gt;
      https://www.semanticscholar.org/paper/Improving-Language-Understanding-by-Generative-Radford-Narasimhan/cd18800a0fe0b668a1cc19f2ec95b5003d0a5035
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (8)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2019)
   &lt;/strong&gt;
   &lt;span&gt;
    by Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy, Stoyanov, and Zettlemoyer,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/1910.13461
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   As mentioned earlier, BERT-type encoder-style LLMs are usually preferred for predictive modeling tasks, whereas GPT-type decoder-style LLMs are better at generating texts. To get the best of both worlds, the BART paper above combines both the encoder and decoder parts (not unlike the original transformer – the second paper in this list).
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!cnoO!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!cnoO!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png 424w, https://substackcdn.com/image/fetch/$s_!cnoO!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png 848w, https://substackcdn.com/image/fetch/$s_!cnoO!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png 1272w, https://substackcdn.com/image/fetch/$s_!cnoO!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:432,&quot;width&quot;:1004,&quot;resizeWidth&quot;:495,&quot;bytes&quot;:156473,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;212.98804780876495&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!cnoO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!cnoO!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png 424w, https://substackcdn.com/image/fetch/$s_!cnoO!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png 848w, https://substackcdn.com/image/fetch/$s_!cnoO!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png 1272w, https://substackcdn.com/image/fetch/$s_!cnoO!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F7797ec59-f683-4f30-8281-ed0503076b6b_1004x432.png 1456w&quot; width=&quot;495&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
      &lt;div&gt;
      &lt;/div&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/1910.13461&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/1910.13461
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (9)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2023)
   &lt;/strong&gt;
   &lt;span&gt;
    by Yang, Jin, Tang, Han, Feng, Jiang, Yin, and Hu,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2304.13712&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2304.13712
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This is not a research paper but probably the best general architecture survey to date illustrating how the different architectures evolved. However, next to discussing BERT-style masked language models (encoders) and GPT-style autoregressive language models (decoders), it also provides useful discussions and guidance regarding pretraining and finetuning data.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!w0MI!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!w0MI!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png 424w, https://substackcdn.com/image/fetch/$s_!w0MI!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png 848w, https://substackcdn.com/image/fetch/$s_!w0MI!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png 1272w, https://substackcdn.com/image/fetch/$s_!w0MI!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/b1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1125,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:582201,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1125&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!w0MI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!w0MI!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png 424w, https://substackcdn.com/image/fetch/$s_!w0MI!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png 848w, https://substackcdn.com/image/fetch/$s_!w0MI!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png 1272w, https://substackcdn.com/image/fetch/$s_!w0MI!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fb1f675dc-d643-405d-8111-eab067b7012f_1538x1188.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      The evolutionary tree of modern LLMs via
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2304.13712&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2304.13712
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   Scaling Laws and Improving Efficiency
  &lt;/h2&gt;
  &lt;p&gt;
   &lt;span&gt;
    If you want to learn more about the various techniques to improve the efficiency of transformers, I recommend the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2009.06732&quot; rel=&quot;&quot;&gt;
    2020
   &lt;/a&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2009.06732&quot; rel=&quot;&quot;&gt;
     Efficient Transformers: A Survey
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper followed by the
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2302.01107&quot; rel=&quot;&quot;&gt;
    2023
   &lt;/a&gt;
   &lt;em&gt;
    &lt;a href=&quot;https://arxiv.org/abs/2302.01107&quot; rel=&quot;&quot;&gt;
     A Survey on Efficient Training of Transformers
    &lt;/a&gt;
   &lt;/em&gt;
   &lt;span&gt;
    paper.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In addition, below are papers that I found particularly interesting and worth reading.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (10)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    (2022), by Dao, Fu, Ermon, Rudra, and Ré,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2205.14135
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   While most transformer papers don’t bother about replacing the original scaled dot product mechanism for implementing self-attention, FlashAttention is one mechanism I have seen most often referenced lately.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!92eZ!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!92eZ!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png 424w, https://substackcdn.com/image/fetch/$s_!92eZ!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png 848w, https://substackcdn.com/image/fetch/$s_!92eZ!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png 1272w, https://substackcdn.com/image/fetch/$s_!92eZ!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:614,&quot;width&quot;:1068,&quot;resizeWidth&quot;:553,&quot;bytes&quot;:134388,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;317.92322097378275&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!92eZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!92eZ!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png 424w, https://substackcdn.com/image/fetch/$s_!92eZ!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png 848w, https://substackcdn.com/image/fetch/$s_!92eZ!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png 1272w, https://substackcdn.com/image/fetch/$s_!92eZ!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F659f5e23-5641-406f-9a08-a2461ffff345_1068x614.png 1456w&quot; width=&quot;553&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2205.14135&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2205.14135
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (11)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Cramming: Training a Language Model on a Single GPU in One Day (2022)
    &lt;/strong&gt;
    &lt;span&gt;
    &lt;/span&gt;
   &lt;/em&gt;
   &lt;span&gt;
    by Geiping and Goldstein,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2212.14034&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2212.14034
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In this paper, the researchers trained a masked language model / encoder-style LLM (here: BERT) for 24h on a single GPU. For comparison, the original 2018 BERT paper trained it on 16 TPUs for four days. An interesting insight is that while smaller models have higher throughput, smaller models also learn less efficiently. Thus, larger models do not require more training time to reach a specific predictive performance threshold.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!ElDh!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ElDh!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png 424w, https://substackcdn.com/image/fetch/$s_!ElDh!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png 848w, https://substackcdn.com/image/fetch/$s_!ElDh!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png 1272w, https://substackcdn.com/image/fetch/$s_!ElDh!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:620,&quot;width&quot;:1310,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:230173,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;620&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!ElDh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!ElDh!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png 424w, https://substackcdn.com/image/fetch/$s_!ElDh!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png 848w, https://substackcdn.com/image/fetch/$s_!ElDh!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png 1272w, https://substackcdn.com/image/fetch/$s_!ElDh!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F8197f14a-dcb6-471e-8958-09032c75e73f_1310x620.png 1456w&quot; width=&quot;1310&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2212.14034&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2212.14034
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (12)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     LoRA: Low-Rank Adaptation of Large Language Models (2021)
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    by Hu, Shen, Wallis, Allen-Zhu, Li, L Wang, S Wang, and Chen,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2106.09685
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Modern large language models that are pretrained on large datasets show emergent abilities and perform well on various tasks, including language translation, summarization, coding, and Q&amp;A. However, if we want to improve the ability of transformers on domain-specific data and specialized tasks, it’s worthwhile to finetune transformers.
  &lt;/p&gt;
  &lt;p&gt;
   Low-rank adaptation (LoRA) is one of the most influential approaches for finetuning large language models in a parameter-efficient manner. While other methods for parameter-efficient finetuning exist (see the survey below), LoRA is particularly worth highlighting as it is both an elegant and very general method that can be applied to other types of models as well.
  &lt;/p&gt;
  &lt;p&gt;
   While the weights of a pretrained model have full rank on the pretrained tasks, the LoRA authors point out that pretrained large language models have a low “intrinsic dimension” when they are adapted to a new task. So, the main idea behind LoRA is to decompose the weight changes, ΔW, into a lower-rank representation, which is more parameter efficient.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!-mrt!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-mrt!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png 424w, https://substackcdn.com/image/fetch/$s_!-mrt!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png 848w, https://substackcdn.com/image/fetch/$s_!-mrt!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png 1272w, https://substackcdn.com/image/fetch/$s_!-mrt!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:459,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:437876,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;459&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!-mrt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!-mrt!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png 424w, https://substackcdn.com/image/fetch/$s_!-mrt!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png 848w, https://substackcdn.com/image/fetch/$s_!-mrt!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png 1272w, https://substackcdn.com/image/fetch/$s_!-mrt!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F542db321-11fe-4641-8f63-2fa7ac7a6536_2302x726.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      An illustration of LoRA alongside its performance from
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2106.09685&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2106.09685
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (13)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning (2022)
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;span&gt;
    by Lialin, Deshpande, and Rumshisky,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2303.15647&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2303.15647
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Modern large language models that are pretrained on large datasets show emergent abilities and perform well on various tasks, including language translation, summarization, coding, and Q&amp;A. However, if we want to improve the ability of transformers on domain-specific data and specialized tasks, it’s worthwhile to finetune transformers. This survey reviews more than 40 papers on parameter-efficient finetuning methods (including popular techniques such as prefix tuning, adapters, and low-rank adaptation) to make finetuning (very) computationally efficient.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!O4Fa!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!O4Fa!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png 424w, https://substackcdn.com/image/fetch/$s_!O4Fa!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png 848w, https://substackcdn.com/image/fetch/$s_!O4Fa!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png 1272w, https://substackcdn.com/image/fetch/$s_!O4Fa!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:747,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:953226,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;747&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!O4Fa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!O4Fa!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png 424w, https://substackcdn.com/image/fetch/$s_!O4Fa!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png 848w, https://substackcdn.com/image/fetch/$s_!O4Fa!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png 1272w, https://substackcdn.com/image/fetch/$s_!O4Fa!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F47c015d7-1345-48b4-8170-fd9bb636b92f_2102x1078.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2303.15647&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2303.15647
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (14)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2022)
   &lt;/strong&gt;
   &lt;span&gt;
    by Rae and colleagues (78 co-authors!),
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2112.11446&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2112.11446
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    Gopher
   &lt;/em&gt;
   &lt;span&gt;
    is a particularly nice paper including tons of analysis to understand LLM training. Here, the researchers trained a 280 billion parameter model with 80 layers on 300 billions tokens. This includes interesting architecture modifications such as using RMSNorm (Root Mean Square Normalization) instead of LayerNorm (Layer Normalization). Both LayerNorm and RMSNorm are preferred over BatchNorm since they don&#x27;t depend on the batch size and doesn&#x27;t require synchronization, which is an advantage in distributed settings with smaller batch sizes. However, RMSNorm is generally said to stabilize the training in deeper architectures.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Besides interesting tidbits such the ones above, the main focus of this paper is the analysis of task performance for different scales. The evaluation on 152 diverse tasks reveal that increasing model sizes benefits tasks like comprehension, fact-checking, and the identification of toxic language the most. However, tasks related to logical and mathematical reasoning benefit less from architecture scaling.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!BVt9!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!BVt9!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png 424w, https://substackcdn.com/image/fetch/$s_!BVt9!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png 848w, https://substackcdn.com/image/fetch/$s_!BVt9!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png 1272w, https://substackcdn.com/image/fetch/$s_!BVt9!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:1147,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:432101,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;1147&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!BVt9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!BVt9!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png 424w, https://substackcdn.com/image/fetch/$s_!BVt9!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png 848w, https://substackcdn.com/image/fetch/$s_!BVt9!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png 1272w, https://substackcdn.com/image/fetch/$s_!BVt9!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F86050fd8-33e1-483e-bdb8-daa1873fd018_1788x1408.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source: Figure from
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2112.11446&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2112.11446
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (15)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Training Compute-Optimal Large Language Models
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2022)
   &lt;/strong&gt;
   &lt;span&gt;
    by Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai, Rutherford, de Las Casas, Hendricks, Welbl, Clark, Hennigan, Noland, Millican, van den Driessche, Damoc, Guy, Osindero, Simonyan, Elsen, Rae, Vinyals, and Sifre,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2203.15556
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   This paper introduces the 70-billion parameter Chinchilla model that outperforms the popular 175-billion parameter GPT-3 model on generative modeling tasks. However, its main punchline is that contemporary large language models are “significantly undertrained.”
  &lt;/p&gt;
  &lt;p&gt;
   The paper defines the linear scaling law for large language model training. For example, while Chinchilla is only half the size of GPT-3, it outperformed GPT-3 because it was trained on 1.4 trillion (instead of just 300 billion) tokens. In other words, the number of training tokens is as vital as the model size.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!biVl!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!biVl!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png 424w, https://substackcdn.com/image/fetch/$s_!biVl!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png 848w, https://substackcdn.com/image/fetch/$s_!biVl!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png 1272w, https://substackcdn.com/image/fetch/$s_!biVl!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:744,&quot;width&quot;:1196,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:177317,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;744&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!biVl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!biVl!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png 424w, https://substackcdn.com/image/fetch/$s_!biVl!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png 848w, https://substackcdn.com/image/fetch/$s_!biVl!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png 1272w, https://substackcdn.com/image/fetch/$s_!biVl!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F760d60f7-c4e6-4a8b-a540-09c8d1ffef36_1196x744.png 1456w&quot; width=&quot;1196&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2203.15556&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2203.15556
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (16)
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Pythia
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    :
   &lt;/strong&gt;
   &lt;em&gt;
    &lt;strong&gt;
     A Suite for Analyzing Large Language Models Across Training and Scaling
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2023)
   &lt;/strong&gt;
   &lt;span&gt;
    by Biderman, Schoelkopf, Anthony, Bradley, O&#x27;Brien, Hallahan, Khan, Purohit, Prashanth, Raff, Skowron, Sutawika, and van der Wal,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2304.01373&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2304.01373
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Pythia is a suite of open-source LLMs (70M to 12B parameters) to study how LLMs evolve over the course of training.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    The architecture is similar to GPT-3 but includes some improvements, for example, Flash Attention (like
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2302.13971&quot; rel=&quot;&quot;&gt;
    LLaMA
   &lt;/a&gt;
   &lt;span&gt;
    ) and Rotary Positional Embeddings (like
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2204.02311&quot; rel=&quot;&quot;&gt;
    PaLM
   &lt;/a&gt;
   &lt;span&gt;
    ). Pythia was trained on
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2101.00027&quot; rel=&quot;&quot;&gt;
    The Pile dataset
   &lt;/a&gt;
   &lt;span&gt;
    (825 Gb) for 300 B tokens (~1 epoch on regular PILE, ~1.5 epochs on deduplicated PILE.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!L-Fu!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!L-Fu!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg 424w, https://substackcdn.com/image/fetch/$s_!L-Fu!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg 848w, https://substackcdn.com/image/fetch/$s_!L-Fu!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!L-Fu!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/cf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:509,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:234920,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/jpeg&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;509&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!L-Fu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!L-Fu!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg 424w, https://substackcdn.com/image/fetch/$s_!L-Fu!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg 848w, https://substackcdn.com/image/fetch/$s_!L-Fu!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg 1272w, https://substackcdn.com/image/fetch/$s_!L-Fu!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fcf0be27f-4d1b-4761-9019-b04735c3bd25_1854x648.jpeg 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      The Pythia model suite via
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2304.01373&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2304.01373
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   The main insights of the Pythia study are as follows:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Training on duplicated data (due to how LLMs are trained, this means training for more than one epoch) does not benefit or hurt performance.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Training order does not influence memorization. This is unfortunate because if the opposite were true, we could mitigate undesirable verbatim memorization issues by reordering the training data.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Pretrained term frequency influences task performance. For instance, few-shot accuracy tends to be higher for more frequent terms.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Doubling the batch size halves the training time but doesn&#x27;t hurt convergence.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   Alignment – Steering Large Language Models to Intended Goals and Interests
  &lt;/h2&gt;
  &lt;p&gt;
   In recent years, we have seen many relatively capable large language models that can generate realistic texts (for example, GPT-3 and Chinchilla, among others). It seems that we have reached a ceiling in terms of what we can achieve with the commonly used pretraining paradigms.
  &lt;/p&gt;
  &lt;p&gt;
   To make language models more helpful and reduce misinformation and harmful language, researchers designed additional training paradigms to fine-tune the pretrained base models.
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (17)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Training Language Models to Follow Instructions with Human Feedback
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2022)
   &lt;/strong&gt;
   &lt;span&gt;
    by Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2203.02155
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In this so-called InstructGPT paper, the researchers use a reinforcement learning mechanism with humans in the loop (RLHF). They start with a pretrained GPT-3 base model and fine-tune it further using supervised learning on prompt-response pairs generated by humans (Step 1). Next, they ask humans to rank model outputs to train a reward model (step 2). Finally, they use the reward model to update the pretrained and fine-tuned GPT-3 model using reinforcement learning via proximal policy optimization (step 3).
  &lt;/p&gt;
  &lt;p&gt;
   As a sidenote, this paper is also known as the paper describing the idea behind ChatGPT – according to the recent rumors, ChatGPT is a scaled-up version of InstructGPT that has been fine-tuned on a larger dataset.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!bc_C!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!bc_C!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png 424w, https://substackcdn.com/image/fetch/$s_!bc_C!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png 848w, https://substackcdn.com/image/fetch/$s_!bc_C!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png 1272w, https://substackcdn.com/image/fetch/$s_!bc_C!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:822,&quot;width&quot;:1294,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:202580,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;822&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!bc_C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!bc_C!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png 424w, https://substackcdn.com/image/fetch/$s_!bc_C!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png 848w, https://substackcdn.com/image/fetch/$s_!bc_C!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png 1272w, https://substackcdn.com/image/fetch/$s_!bc_C!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F79d09d26-5644-4830-a1de-a94c7149d50c_1294x822.png 1456w&quot; width=&quot;1294&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2203.02155&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2203.02155
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (18)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Constitutional AI: Harmlessness from AI Feedback
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2022
   &lt;/strong&gt;
   &lt;span&gt;
    ) by Yuntao, Saurav, Sandipan, Amanda, Jackson, Jones, Chen, Anna, Mirhoseini, McKinnon, Chen, Olsson, Olah, Hernandez, Drain, Ganguli, Li, Tran-Johnson, Perez, Kerr, Mueller, Ladish, Landau, Ndousse, Lukosuite, Lovitt, Sellitto, Elhage, Schiefer, Mercado, DasSarma, Lasenby, Larson, Ringer, Johnston, Kravec, El Showk, Fort, Lanham, Telleen-Lawton, Conerly, Henighan, Hume, Bowman, Hatfield-Dodds, Mann, Amodei, Joseph, McCandlish, Brown, Kaplan,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2212.08073
   &lt;/a&gt;
   &lt;span&gt;
    .
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In this paper, the researchers are taking the alignment idea one step further, proposing a training mechanism for creating a “harmless” AI system. Instead of direct human supervision, the researchers propose a self-training mechanism that is based on a list of rules (which are provided by a human). Similar to the InstructGPT paper mentioned above, the proposed method uses a reinforcement learning approach.
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!h1rb!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!h1rb!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png 424w, https://substackcdn.com/image/fetch/$s_!h1rb!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png 848w, https://substackcdn.com/image/fetch/$s_!h1rb!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png 1272w, https://substackcdn.com/image/fetch/$s_!h1rb!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/debca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:624,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:153621,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;624&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!h1rb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!h1rb!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png 424w, https://substackcdn.com/image/fetch/$s_!h1rb!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png 848w, https://substackcdn.com/image/fetch/$s_!h1rb!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png 1272w, https://substackcdn.com/image/fetch/$s_!h1rb!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fdebca6d8-b504-4645-ac80-8d9aaabccd3b_1466x628.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Source:
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2212.08073&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2212.08073
     &lt;/a&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;strong&gt;
    (19)
   &lt;/strong&gt;
   &lt;span&gt;
   &lt;/span&gt;
   &lt;em&gt;
    &lt;strong&gt;
     Self-Instruct: Aligning Language Model with Self Generated Instruction
    &lt;/strong&gt;
   &lt;/em&gt;
   &lt;strong&gt;
    (2022)
   &lt;/strong&gt;
   &lt;span&gt;
    by Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi,
   &lt;/span&gt;
   &lt;a href=&quot;https://arxiv.org/abs/2212.10560&quot; rel=&quot;&quot;&gt;
    https://arxiv.org/abs/2212.10560
   &lt;/a&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Instruction finetuning is how we get from GPT-3-like pretrained base models to more capable LLMs like ChatGPT. And open-source human-generated instruction datasets like
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/databrickslabs/dolly/tree/master/data&quot; rel=&quot;&quot;&gt;
    databricks-dolly-15k
   &lt;/a&gt;
   &lt;span&gt;
    can help make this possible. But how do we scale this? One way is bootstrapping an LLM off its own generations.
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Self-Instruct is one (almost annotation-free) way to align pretrained LLMs with instructions.
  &lt;/p&gt;
  &lt;p&gt;
   How does this work? In a nutshell, it&#x27;s a 4-step process:
  &lt;/p&gt;
  &lt;ol&gt;
   &lt;li&gt;
    &lt;p&gt;
     Seed task pool with a set of human-written instructions (175 in this case) and sample instructions.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Use a pretrained LLM (like GPT-3) to determine the task category.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Given the new instruction, let a pretrained LLM generate the response.
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     Collect, prune, and filter the responses before adding them to the task pool.
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ol&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!t9yF!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!t9yF!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png 424w, https://substackcdn.com/image/fetch/$s_!t9yF!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png 848w, https://substackcdn.com/image/fetch/$s_!t9yF!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png 1272w, https://substackcdn.com/image/fetch/$s_!t9yF!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:659,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:561643,&quot;alt&quot;:null,&quot;title&quot;:null,&quot;type&quot;:&quot;image/png&quot;,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;659&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!t9yF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!t9yF!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png 424w, https://substackcdn.com/image/fetch/$s_!t9yF!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png 848w, https://substackcdn.com/image/fetch/$s_!t9yF!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png 1272w, https://substackcdn.com/image/fetch/$s_!t9yF!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F88f7c2ca-5592-423d-b551-9794e325bafc_2218x1004.png 1456w&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;span&gt;
      Annotated version of the self-instruct method from
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2212.10560&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2212.10560
     &lt;/a&gt;
     &lt;span&gt;
      .
     &lt;/span&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;p&gt;
   In practice, this works relatively well based on the ROUGE scores.
  &lt;/p&gt;
  &lt;p&gt;
   For example, a Self-Instruct-finetuned LLM outperforms the GPT-3 base LLM (1) and can compete with an LLM pretrained on a large human-written instruction set (2). And self-instruct can also benefit LLMs that were already finetuned on human instructions (3).
  &lt;/p&gt;
  &lt;p&gt;
   But of course, the gold standard for evaluating LLMs is to ask human raters. Based on human evaluation, Self-Instruct outperforms base LLM, and LLMs trained on human instruction datasets in supervised fashion (SuperNI, T0 Trainer). But interestingly, Self-Instruct does not outperform methods trained via reinforcement learning with human feedback (RLHF)
  &lt;/p&gt;
  &lt;p&gt;
   &lt;span&gt;
    Which is more promising, human-generated instruction datasets or self-instruct-datasets? I vote for both. Why not start with a human-generated instruction dataset like the 15k instructions from
   &lt;/span&gt;
   &lt;a href=&quot;https://github.com/databrickslabs/dolly/tree/master/data&quot; rel=&quot;&quot;&gt;
    databricks-dolly-15k
   &lt;/a&gt;
   &lt;span&gt;
    and then scale this with self-instruct?
   &lt;/span&gt;
  &lt;/p&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   Reinforcement Learning with Human Feedback (RLHF)
  &lt;/h2&gt;
  &lt;p&gt;
   For additional explanations of Reinforcement Learning with Human Feedback (RLHF), plus papers on proximal policy optimization for implementing RLHF, please see my more detailed article below:
  &lt;/p&gt;
  &lt;div class=&quot;digestPostEmbed-flwiST&quot; data-component-name=&quot;DigestPostEmbed&quot;&gt;
  &lt;/div&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
  &lt;/h2&gt;
  &lt;p&gt;
  &lt;/p&gt;
  &lt;h2 class=&quot;header-anchor-post&quot;&gt;
   Conclusion and Further Reading
  &lt;/h2&gt;
  &lt;p&gt;
   I tried to keep the list above nice and concise, focusing on the top-10 papers (plus 3 bonus papers on RLHF) to understand the design, constraints, and evolution behind contemporary large language models.
  &lt;/p&gt;
  &lt;p&gt;
   For further reading, I suggest following the references in the papers mentioned above. Or, to give you some additional pointers, here are some additional resources (these lists are not comprehensive):
  &lt;/p&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Open-source alternatives to GPT
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      BLOOM: A 176B-Parameter Open-Access Multilingual Language Model
     &lt;/em&gt;
     &lt;span&gt;
      (2022),
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2211.05100&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2211.05100
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      OPT: Open Pre-trained Transformer Language Models
     &lt;/em&gt;
     &lt;span&gt;
      (2022),
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2205.01068&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2205.01068
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      UL2:
     &lt;/em&gt;
     &lt;span&gt;
     &lt;/span&gt;
     &lt;em&gt;
      Unifying Language Learning Paradigms
     &lt;/em&gt;
     &lt;span&gt;
      (2022)
     &lt;/span&gt;
     &lt;em&gt;
      &lt;span&gt;
       ,
      &lt;/span&gt;
      &lt;a href=&quot;https://arxiv.org/abs/2205.05131&quot; rel=&quot;&quot;&gt;
       https://arxiv.org/abs/2205.05131
      &lt;/a&gt;
     &lt;/em&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;strong&gt;
    ChatGPT alternatives
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      LaMDA: Language Models for Dialog Applications
     &lt;/em&gt;
     &lt;span&gt;
      (2022),
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2201.08239&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2201.08239
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      (Bloomz)
     &lt;/span&gt;
     &lt;em&gt;
      Crosslingual Generalization through Multitask Finetuning
     &lt;/em&gt;
     &lt;span&gt;
      (2022),
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2211.01786&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2211.01786
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;span&gt;
      (Sparrow)
     &lt;/span&gt;
     &lt;em&gt;
      Improving Alignment of Dialogue Agents via Targeted Human Judgements
     &lt;/em&gt;
     &lt;span&gt;
      (2022),
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2209.14375&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2209.14375
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      BlenderBot 3: A Deployed Conversational Agent that Continually Learns to Responsibly Engage
     &lt;/em&gt;
     &lt;span&gt;
      ,
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2208.03188&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2208.03188
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;p&gt;
   &lt;strong&gt;
    Large language models in computational biology
   &lt;/strong&gt;
  &lt;/p&gt;
  &lt;ul&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing
     &lt;/em&gt;
     &lt;span&gt;
      (2021),
     &lt;/span&gt;
     &lt;a href=&quot;https://arxiv.org/abs/2007.06225&quot; rel=&quot;&quot;&gt;
      https://arxiv.org/abs/2007.06225
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      Highly Accurate Protein Structure Prediction with AlphaFold
     &lt;/em&gt;
     &lt;span&gt;
      (2021),
     &lt;/span&gt;
     &lt;a href=&quot;https://www.nature.com/articles/s41586-021-03819-2&quot; rel=&quot;&quot;&gt;
      https://www.nature.com/articles/s41586-021-03819-2
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
   &lt;li&gt;
    &lt;p&gt;
     &lt;em&gt;
      Large Language Models Generate Functional Protein Sequences Across Diverse Families
     &lt;/em&gt;
     &lt;span&gt;
      (2023),
     &lt;/span&gt;
     &lt;a href=&quot;https://www.nature.com/articles/s41587-022-01618-2&quot; rel=&quot;&quot;&gt;
      https://www.nature.com/articles/s41587-022-01618-2
     &lt;/a&gt;
    &lt;/p&gt;
   &lt;/li&gt;
  &lt;/ul&gt;
  &lt;div&gt;
   &lt;hr/&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     This magazine is a personal passion project. For those who wish to support me, please consider purchasing a copy of my
    &lt;/span&gt;
    &lt;a href=&quot;https://amzn.to/4fqvn0D&quot; rel=&quot;&quot;&gt;
     Build a Large Language Model (From Scratch) book
    &lt;/a&gt;
    &lt;span&gt;
     . (I am confident that you&#x27;ll get lots out of this book as it explains how LLMs work in a level of detail that is not found anywhere else.)
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;div class=&quot;captioned-image-container&quot;&gt;
   &lt;figure&gt;
    &lt;a class=&quot;image-link image2 is-viewable-img&quot; data-component-name=&quot;Image2ToDOM&quot; href=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; rel=&quot;&quot; target=&quot;_blank&quot;&gt;
     &lt;div class=&quot;image2-inset&quot;&gt;
      &lt;picture&gt;
       &lt;source sizes=&quot;100vw&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_webp,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; type=&quot;image/webp&quot;&gt;
        &lt;img alt=&quot;&quot; class=&quot;sizing-normal&quot; data-attrs=&#x27;{&quot;src&quot;:&quot;https://substack-post-media.s3.amazonaws.com/public/images/ea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot;,&quot;srcNoWatermark&quot;:null,&quot;fullscreen&quot;:null,&quot;imageSize&quot;:null,&quot;height&quot;:819,&quot;width&quot;:1456,&quot;resizeWidth&quot;:null,&quot;bytes&quot;:null,&quot;alt&quot;:&quot;&quot;,&quot;title&quot;:null,&quot;type&quot;:null,&quot;href&quot;:null,&quot;belowTheFold&quot;:true,&quot;topImage&quot;:false,&quot;internalRedirect&quot;:null,&quot;isProcessing&quot;:false,&quot;align&quot;:null,&quot;offset&quot;:false}&#x27; height=&quot;819&quot; loading=&quot;lazy&quot; sizes=&quot;100vw&quot; src=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png&quot; srcset=&quot;https://substackcdn.com/image/fetch/$s_!woQp!,w_424,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 424w, https://substackcdn.com/image/fetch/$s_!woQp!,w_848,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 848w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1272,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1272w, https://substackcdn.com/image/fetch/$s_!woQp!,w_1456,c_limit,f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fea1152a0-18d9-4a8a-9398-c6b1ca67726a_1600x900.png 1456w&quot; title=&quot;&quot; width=&quot;1456&quot;/&gt;
       &lt;/source&gt;
      &lt;/picture&gt;
     &lt;/div&gt;
    &lt;/a&gt;
    &lt;figcaption class=&quot;image-caption&quot;&gt;
     &lt;em&gt;
      &lt;a href=&quot;https://amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
       Build a Large Language Model (From Scratch)
      &lt;/a&gt;
      &lt;span&gt;
       now available on Amazon
      &lt;/span&gt;
     &lt;/em&gt;
    &lt;/figcaption&gt;
   &lt;/figure&gt;
  &lt;/div&gt;
  &lt;p&gt;
   &lt;em&gt;
    &lt;span&gt;
     If you read the book and have a few minutes to spare, I&#x27;d really appreciate a
    &lt;/span&gt;
    &lt;a href=&quot;https://www.amazon.com/Build-Large-Language-Model-Scratch/dp/1633437167&quot; rel=&quot;&quot;&gt;
     brief review
    &lt;/a&gt;
    &lt;span&gt;
     . It helps us authors a lot!
    &lt;/span&gt;
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   Alternatively, I also recently enabled the paid subscription option on Substack to support this magazine directly.
  &lt;/p&gt;
  &lt;div class=&quot;subscribe-widget&quot; data-component-name=&quot;SubscribeWidget&quot;&gt;
  &lt;/div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
</channel>
</rss>
