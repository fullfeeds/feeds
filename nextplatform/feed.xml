<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" version="2.0">
<channel><title>Next Platform</title>
<lastBuildDate>Sun, 01 Mar 2026 23:19:56 -0000</lastBuildDate>
<item>
<title> Some More Game Theory, This Time On The AMD-Meta Platforms Deal </title>
<link>https://www.nextplatform.com/2026/02/24/some-more-game-theory-this-time-on-the-amd-meta-platforms-deal/#respond</link>
<pubDate>Tue, 24 Feb 2026 17:51:46 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/11/amd-fad-2025-lisa-su-1030x438.jpg&quot; title=&quot;amd-fad-2025-lisa-su&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  If may seem like you are having flashbacks, but you are not. The deal that AMD has just announced with Meta Platforms is extremely similar to
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/06/did-amd-use-chatgpt-to-come-up-with-its-openai-partnership-deal/&quot;&gt;
   the one it inked back in October with model builder OpenAI
  &lt;/a&gt;
  , right down to the 6 gigawatts of total datacenter capacity of compute, storage, and networking and to the same warrant for 160 million shares of AMD to sweeten the deal.
 &lt;/p&gt;
 &lt;p&gt;
  Eight more deals like this, and ten companies will own 100 percent of the stakes in AMD within five years. (No, this is not going to happen, and yes, that was a joke. We hope it is, anyway. . . .) But seriously, OpenAI and Meta Platforms can sell their shares after they are converted from the warrants (after reaching milestones in terms of technology installed and AMD share price driven by the success of those installations) – so OpenAI and Meta Platforms have more money to spend on GPUs!
 &lt;/p&gt;
 &lt;p&gt;
  Crazy new world we live in, ain’t it?
 &lt;/p&gt;
 &lt;p&gt;
  Anyway, one big difference between the Meta Platforms deal and the AMD deal is that we are pretty sure the former, which also has aspirations to be one of the top AI model builders in the world, will actually have the money to follow through on its hardware purchase commitments. People are still trying to figure out how OpenAI is going to come up with hundreds of billions of dollars in cash when it only makes tens of billions of dollars a year in revenues. We think it will beg, borrow, and stop just short of stealing and figure out some way to buy an enormous amount of datacenter capacity.
 &lt;/p&gt;
 &lt;p&gt;
  This deal AMD has inked with Meta Platforms is also similar in magnitude and nearly in scope to the one that
  &lt;a href=&quot;https://www.nextplatform.com/2026/02/18/some-game-theory-on-that-nvidia-meta-platforms-partnership/&quot;&gt;
   the social networker signed with AI hardware juggernaut Nvidia last week
  &lt;/a&gt;
  . Nvidia was not terribly specific about the money Meta Platforms was spending, but said that the company would be buying access to millions of “Blackwell” and “Rubin” GPU accelerators from Nvidia, but the fine print, as we pointed out and that many people missed, is that some of this would be for cloud capacity, not on premises gear. Our best guess was that the Nvidia deal was for 2 million to 3 million GPUs, and lots of “Vera” Arm server CPUs both inside the AI systems and outside of them, as well as NVSwitch scale up networks for the GPUs and SpectrumX networks for scale out clustering, this might represent somewhere between $110 billion and $167 billion in revenue for GB300 NVL72 equivalents alone. (The Vera CPU clusters would be above and beyond this, but probably not enough to move the needle.)
 &lt;/p&gt;
 &lt;p&gt;
  How much of this investment by Meta Platforms was already on the books at the clouds and neoclouds  over what we expect to be the next four or five years is unknown. Neither company was precise on the term of the deal, except to say that it was a “multiyear, multigenerational strategic partnership.”
 &lt;/p&gt;
 &lt;p&gt;
  AMD and Meta Platforms are being a little more precise, saying that the deal runs for five years, starting in the second half of 2026 when the first 1 gigawatt of systems based on a custom MI450 GPU accelerator and the “Helios” Open Rack Wide v3 rackscale system that was co-designed with Meta Platforms.
 &lt;/p&gt;
 &lt;p&gt;
  Depending on the accelerator used, 1 gigawatt of capacity is reckoned to be somewhere between 500,000 to 600,000 GPUs. Take the midpoint of that, and you are talking 3.3 million MI400 series equivalents across the 6 gigawatts installed over five years. At an average price of $35,000, that is $115.5 billion just for the GPUs, which averages $23.1 billion a year, which is consistent with the “double digit billions per gigawatt” that AMD chief executive officer Lisa Su cited in the conference call with Wall Street. Add in the cost of the rack plus networking (some from AMD, such as the Pensando DPUs) and storage, you are probably talking $35 billion per gigawatt for the iron, with the rest going into facilities, power, and cooling.
 &lt;/p&gt;
 &lt;p&gt;
  On the call, Su said that Meta Platforms was an early adopter of the “Antares” MI300X and MI350X GPUs from AMD, and without such a deal with the social networker she believed that the “Altair” MI450 series “would have done well” selling at Meta Platforms.
 &lt;/p&gt;
 &lt;p&gt;
  “But what we are looking to do is do something transformational,” Su explained. “And when you talk about gigawatt-scale deployment and six gigawatts over five years, that is transformational in terms of where we see our business. And in addition to that, they are at the forefront of what is happening with models. They are optimizing workloads for the for their future, and we are optimized alongside with them.”
 &lt;/p&gt;
 &lt;p&gt;
  Which brings up another unique part of this deal. Meta Platforms is getting semicustom MI450 series GPUs from AMD as part of the deal, and starting with the initial shipments of Helios racks in the second half of 2026. This is the first custom part of the MI400 series generation; Lawrence Livermore National Laboratory got a hybrid CPU-GPU part, the “Antares-A” MI300A with six GPU chiplets and two CPU chiplets, during the MI300 series generation. There could be other customers who get custom MI400 series parts, which is possible thanks to the chiplet design, provided they have significant enough volumes to warrant manufacturing a special part.
 &lt;/p&gt;
 &lt;p&gt;
  The exact nature of this custom MI450X part that Meta Platforms has commissioned is unclear, but Jean Hsu, AMD’s chief financial officer, said that this custom part did not require any additional tapeout during the MI400 cycle. We also know that the custom MI450X tuned for Meta Platform’s own inference workloads.
 &lt;/p&gt;
 &lt;p&gt;
  So it could have more or less HBM stacked memory than the standard parts or higher or lower clock speeds on the GPUs, depending on if Meta Platforms is optimizing for cost per performance per watt or just trying to drive the absolute best performance. If you wanted to get better balance on HBM capacity and bandwidth against the inherent compute in a socket, you might want to actually gear down the number of GPU chiplets, run them at slightly higher clocks, and ramp up the amount and speed of the HBM memory to get more capacity and bandwidth per unit of compute while also lowering the thermals on compute.
 &lt;/p&gt;
 &lt;p&gt;
  There are many different levers to pull – especially if AMD is separating out vector and tensor cores onto different tiles, as we hope it is. If this is the case, then Meta could dial up the tensor cores and dial down the vector cores, for instance, if that helped its inference workload and still stay in the MI450 socket.
 &lt;/p&gt;
 &lt;p&gt;
  Meta Platforms is also an early and big adopter of the impending “Venice” Zen 6 Epyc 9006 CPUs, and will be also scooping up lots of the future “Verrano” Zen 7 Epyc 9007 CPUs. These CPUs will be used in the Helios AI racks, of course, but will also be deployed to run more generic applications at Meta Platforms, supporting Facebook, Instagram, and other applications.
 &lt;/p&gt;
 &lt;p&gt;
  The first 1 gigawatt of capacity has been committed, and the remain five will be put under contract successively between now and 2030. That gives AMD 2 gigawatts of commitments from OpenAI and Meta Platforms alone, which means it can sign deals with its suppliers to fulfill those orders without much of a risk. Each new signing of the next batch of gigawatts – it averages 1.25 gigawatts a year from 2027 through 2030 – gives it confidence to do the manufacturing for each batch.
 &lt;/p&gt;
 &lt;p&gt;
  As we pointed out back in our OpenAI deal analysis, if you assume those warrants come available more or less linearly and that AMD can move its stock fairly linearly to $600 by 2030, then the value of that 160 million shares for both OpenAI and Meta Platforms would be somewhere around $69 billion by the end of the period of these contracts. (The details of this stock deal are in
  &lt;a href=&quot;https://www.sec.gov/ix?doc=/Archives/edgar/data/0000002488/000000248826000045/amd-20260223.htm&quot;&gt;
   an 8-K filed by AMD with the US Securities and Exchange Commission
  &lt;/a&gt;
  .) That is somewhere around 2 gigawatts of GPU system capacity right there if AMD is undercutting Nvidia slightly on AI compute engines and racks, which we believe will be the case. This amounts to a discount on the hardware paid for with stock funny money instead of real cash. Getting AMD shares to more than triple in five years might be a challenge, but it is certainly doable.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the neat bit: With this deal, AMD could have around 40 percent revenue share for AI accelerators at Meta Platforms, compared to around 50 percent for Nvidia and 10 percent for the company’s own MTIA devices and maybe some Google TPUs should that rumored deal come to pass. These accelerators together would represent a little more than half ($327 billion) of the $600 billion in datacenter investments that Meta Platforms chief executive officer Mark Zuckerberg has committed the company to spend out to the end of the decade. That’s some rough math on the back of a drink’s napkin, we realize, but so is the budgeting out four years from now.
 &lt;/p&gt;
 &lt;p&gt;
  We shall see what really happens. But one thing for sure is what Lisa Su said at the end of the call: “We are making a big bet on Meta, and Meta is making a big bet on AMD.”
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Some More Game Theory, This Time On The AMD-Meta Platforms Deal </title>
<link>https://www.nextplatform.com/2026/02/24/some-more-game-theory-this-time-on-the-amd-meta-platforms-deal/</link>
<pubDate>Tue, 24 Feb 2026 17:51:42 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/11/amd-fad-2025-lisa-su-1030x438.jpg&quot; title=&quot;amd-fad-2025-lisa-su&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  If may seem like you are having flashbacks, but you are not. The deal that AMD has just announced with Meta Platforms is extremely similar to
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/06/did-amd-use-chatgpt-to-come-up-with-its-openai-partnership-deal/&quot;&gt;
   the one it inked back in October with model builder OpenAI
  &lt;/a&gt;
  , right down to the 6 gigawatts of total datacenter capacity of compute, storage, and networking and to the same warrant for 160 million shares of AMD to sweeten the deal.
 &lt;/p&gt;
 &lt;p&gt;
  Eight more deals like this, and ten companies will own 100 percent of the stakes in AMD within five years. (No, this is not going to happen, and yes, that was a joke. We hope it is, anyway. . . .) But seriously, OpenAI and Meta Platforms can sell their shares after they are converted from the warrants (after reaching milestones in terms of technology installed and AMD share price driven by the success of those installations) – so OpenAI and Meta Platforms have more money to spend on GPUs!
 &lt;/p&gt;
 &lt;p&gt;
  Crazy new world we live in, ain’t it?
 &lt;/p&gt;
 &lt;p&gt;
  Anyway, one big difference between the Meta Platforms deal and the AMD deal is that we are pretty sure the former, which also has aspirations to be one of the top AI model builders in the world, will actually have the money to follow through on its hardware purchase commitments. People are still trying to figure out how OpenAI is going to come up with hundreds of billions of dollars in cash when it only makes tens of billions of dollars a year in revenues. We think it will beg, borrow, and stop just short of stealing and figure out some way to buy an enormous amount of datacenter capacity.
 &lt;/p&gt;
 &lt;p&gt;
  This deal AMD has inked with Meta Platforms is also similar in magnitude and nearly in scope to the one that
  &lt;a href=&quot;https://www.nextplatform.com/2026/02/18/some-game-theory-on-that-nvidia-meta-platforms-partnership/&quot;&gt;
   the social networker signed with AI hardware juggernaut Nvidia last week
  &lt;/a&gt;
  . Nvidia was not terribly specific about the money Meta Platforms was spending, but said that the company would be buying access to millions of “Blackwell” and “Rubin” GPU accelerators from Nvidia, but the fine print, as we pointed out and that many people missed, is that some of this would be for cloud capacity, not on premises gear. Our best guess was that the Nvidia deal was for 2 million to 3 million GPUs, and lots of “Vera” Arm server CPUs both inside the AI systems and outside of them, as well as NVSwitch scale up networks for the GPUs and SpectrumX networks for scale out clustering, this might represent somewhere between $110 billion and $167 billion in revenue for GB300 NVL72 equivalents alone. (The Vera CPU clusters would be above and beyond this, but probably not enough to move the needle.)
 &lt;/p&gt;
 &lt;p&gt;
  How much of this investment by Meta Platforms was already on the books at the clouds and neoclouds  over what we expect to be the next four or five years is unknown. Neither company was precise on the term of the deal, except to say that it was a “multiyear, multigenerational strategic partnership.”
 &lt;/p&gt;
 &lt;p&gt;
  AMD and Meta Platforms are being a little more precise, saying that the deal runs for five years, starting in the second half of 2026 when the first 1 gigawatt of systems based on a custom MI450 GPU accelerator and the “Helios” Open Rack Wide v3 rackscale system that was co-designed with Meta Platforms.
 &lt;/p&gt;
 &lt;p&gt;
  Depending on the accelerator used, 1 gigawatt of capacity is reckoned to be somewhere between 500,000 to 600,000 GPUs. Take the midpoint of that, and you are talking 3.3 million MI400 series equivalents across the 6 gigawatts installed over five years. At an average price of $35,000, that is $115.5 billion just for the GPUs, which averages $23.1 billion a year, which is consistent with the “double digit billions per gigawatt” that AMD chief executive officer Lisa Su cited in the conference call with Wall Street. Add in the cost of the rack plus networking (some from AMD, such as the Pensando DPUs) and storage, you are probably talking $35 billion per gigawatt for the iron, with the rest going into facilities, power, and cooling.
 &lt;/p&gt;
 &lt;p&gt;
  On the call, Su said that Meta Platforms was an early adopter of the “Antares” MI300X and MI350X GPUs from AMD, and without such a deal with the social networker she believed that the “Altair” MI450 series “would have done well” selling at Meta Platforms.
 &lt;/p&gt;
 &lt;p&gt;
  “But what we are looking to do is do something transformational,” Su explained. “And when you talk about gigawatt-scale deployment and six gigawatts over five years, that is transformational in terms of where we see our business. And in addition to that, they are at the forefront of what is happening with models. They are optimizing workloads for the for their future, and we are optimized alongside with them.”
 &lt;/p&gt;
 &lt;p&gt;
  Which brings up another unique part of this deal. Meta Platforms is getting semicustom MI450 series GPUs from AMD as part of the deal, and starting with the initial shipments of Helios racks in the second half of 2026. This is the first custom part of the MI400 series generation; Lawrence Livermore National Laboratory got a hybrid CPU-GPU part, the “Antares-A” MI300A with six GPU chiplets and two CPU chiplets, during the MI300 series generation. There could be other customers who get custom MI400 series parts, which is possible thanks to the chiplet design, provided they have significant enough volumes to warrant manufacturing a special part.
 &lt;/p&gt;
 &lt;p&gt;
  The exact nature of this custom MI450X part that Meta Platforms has commissioned is unclear, but Jean Hsu, AMD’s chief financial officer, said that this custom part did not require any additional tapeout during the MI400 cycle. We also know that the custom MI450X tuned for Meta Platform’s own inference workloads.
 &lt;/p&gt;
 &lt;p&gt;
  So it could have more or less HBM stacked memory than the standard parts or higher or lower clock speeds on the GPUs, depending on if Meta Platforms is optimizing for cost per performance per watt or just trying to drive the absolute best performance. If you wanted to get better balance on HBM capacity and bandwidth against the inherent compute in a socket, you might want to actually gear down the number of GPU chiplets, run them at slightly higher clocks, and ramp up the amount and speed of the HBM memory to get more capacity and bandwidth per unit of compute while also lowering the thermals on compute.
 &lt;/p&gt;
 &lt;p&gt;
  There are many different levers to pull – especially if AMD is separating out vector and tensor cores onto different tiles, as we hope it is. If this is the case, then Meta could dial up the tensor cores and dial down the vector cores, for instance, if that helped its inference workload and still stay in the MI450 socket.
 &lt;/p&gt;
 &lt;p&gt;
  Meta Platforms is also an early and big adopter of the impending “Venice” Zen 6 Epyc 9006 CPUs, and will be also scooping up lots of the future “Verrano” Zen 7 Epyc 9007 CPUs. These CPUs will be used in the Helios AI racks, of course, but will also be deployed to run more generic applications at Meta Platforms, supporting Facebook, Instagram, and other applications.
 &lt;/p&gt;
 &lt;p&gt;
  The first 1 gigawatt of capacity has been committed, and the remain five will be put under contract successively between now and 2030. That gives AMD 2 gigawatts of commitments from OpenAI and Meta Platforms alone, which means it can sign deals with its suppliers to fulfill those orders without much of a risk. Each new signing of the next batch of gigawatts – it averages 1.25 gigawatts a year from 2027 through 2030 – gives it confidence to do the manufacturing for each batch.
 &lt;/p&gt;
 &lt;p&gt;
  As we pointed out back in our OpenAI deal analysis, if you assume those warrants come available more or less linearly and that AMD can move its stock fairly linearly to $600 by 2030, then the value of that 160 million shares for both OpenAI and Meta Platforms would be somewhere around $69 billion by the end of the period of these contracts. (The details of this stock deal are in
  &lt;a href=&quot;https://www.sec.gov/ix?doc=/Archives/edgar/data/0000002488/000000248826000045/amd-20260223.htm&quot;&gt;
   an 8-K filed by AMD with the US Securities and Exchange Commission
  &lt;/a&gt;
  .) That is somewhere around 2 gigawatts of GPU system capacity right there if AMD is undercutting Nvidia slightly on AI compute engines and racks, which we believe will be the case. This amounts to a discount on the hardware paid for with stock funny money instead of real cash. Getting AMD shares to more than triple in five years might be a challenge, but it is certainly doable.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the neat bit: With this deal, AMD could have around 40 percent revenue share for AI accelerators at Meta Platforms, compared to around 50 percent for Nvidia and 10 percent for the company’s own MTIA devices and maybe some Google TPUs should that rumored deal come to pass. These accelerators together would represent a little more than half ($327 billion) of the $600 billion in datacenter investments that Meta Platforms chief executive officer Mark Zuckerberg has committed the company to spend out to the end of the decade. That’s some rough math on the back of a drink’s napkin, we realize, but so is the budgeting out four years from now.
 &lt;/p&gt;
 &lt;p&gt;
  We shall see what really happens. But one thing for sure is what Lisa Su said at the end of the call: “We are making a big bet on Meta, and Meta is making a big bet on AMD.”
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> AMD Says “Helios” Racks And MI400 Series GPUs On Track For 2H 2026 </title>
<link>https://www.nextplatform.com/2026/02/23/amd-says-helios-racks-and-mi400-series-gpus-on-track-for-2h-2026/#respond</link>
<pubDate>Tue, 24 Feb 2026 03:47:05 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/01/CES-Su-Helios-system-1030x438.png&quot; title=&quot;CES Su Helios system&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  While releasing an update to its InferenceX AI inference benchmark test, formerly known as InferenceMax and thus far only having Nvidia and AMD testing systems using it,
  &lt;a href=&quot;https://newsletter.semianalysis.com/p/inferencex-v2-nvidia-blackwell-vs&quot;&gt;
   the analysts SemiAnalysis declared correctly
  &lt;/a&gt;
  that thus far only Nvidia, AWS, and Google have created rackscale systems that are deployed today, and that AMD was working on it.
 &lt;/p&gt;
 &lt;p&gt;
  And to be more precise, let’s name the rackscale platforms. Nvidia has its “Oberon” NVL72 rackscale systems using its “Blackwell” B200 and B300 GPU accelerators. AWS has its Trainium2 and Trainium3 accelerators, and Google with seven generations of TPU systems that have been brought to market for more than a decade. (It depends on how you want to draw the lines with memory coherency and flexibility with TPUs – you could argue for fewer generations because TPUs v1, v2, and v3 were hard-wired torus interconnects and only with TPU v4 did Google employ its “Palomar” optical circuit switch in reconfigurable layers in the torus interconnect.)
 &lt;/p&gt;
 &lt;p&gt;
  As you well know, AMD has been working with Meta Platforms on the Open Rack Wide v3 specification, a double-wide rack that AMD calls “Helios” and that is the platform for delivering rackscale compute based on
  &lt;a href=&quot;https://www.nextplatform.com/2025/11/14/amd-solid-roadmaps-beget-money-which-beget-better-roadmaps-and-even-more-money/&quot;&gt;
   the “Altair” MI400 series GPU accelerators
  &lt;/a&gt;
  . This series includes the MI450, the MI430X, and the MI455X for the Helios rackscale systems, which will have 64, 72, or 128 GPUs per system, and possibly the MI440X for eight-way system nodes.
 &lt;/p&gt;
 &lt;p&gt;
  Here are the feeds and speeds for the Helios rack:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-specs.jpg&quot; rel=&quot;attachment wp-att-147032&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;396&quot; sizes=&quot;(max-width: 740px) 100vw, 740px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-specs.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-specs.jpg 740w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-specs-600x321.jpg 600w&quot; width=&quot;740&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the SemiAnalysis report, which we saw because we subscribe to the publication, the authors made this statement:
 &lt;/p&gt;
 &lt;p&gt;
  “
  &lt;em&gt;
   Engineering samples and low volume production of AMD’s first rack scale MI455X UALoE72 system will be in H2 2026, while due to manufacturing delays, the mass production ramp and first production tokens will only be generated on an MI455X UALoE72 by Q2 2027.
  &lt;/em&gt;
  ”
 &lt;/p&gt;
 &lt;p&gt;
  This was a bit perplexing to us, since
  &lt;a href=&quot;https://www.nextplatform.com/2026/02/04/amd-finally-makes-more-money-on-gpus-than-cpus-in-a-quarter/&quot;&gt;
   AMD did not mention any delays in its most recent quarterly results from a few weeks ago
  &lt;/a&gt;
  . This would no doubt be a material delay, and one that would run the company afoul of the US Securities and Exchange Commission should it have been true at the time. It was possible that the delay could have hit after the results were posted on February 3. The further rumors on the street were that the Helios rack system delays were due to some thermal issue in the design.
 &lt;/p&gt;
 &lt;p&gt;
  This is one of the reasons why we sat in on a conference call hosted by New Street Research last week, where Forrest Norrod, general manager of the Data Center Solutions business group, and Doug Huang, one of the co-founders of ZT Systems, which was acquired by AMD last year to underpin its rackscale system engineering efforts, and now its senior vice president in charge of the Data Center Platform Engineering group.
 &lt;/p&gt;
 &lt;p&gt;
  Norrod was having none of it when asked about a delay with the Helios racks:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “I have no idea where this purported issue around thermals is coming from,” Norrod said unequivocally. “I literally have no idea. We have no significant thermal issue. The risk around the thermal design at the component level all the way through the rack level was retired quite some time ago. So, no idea where that’s coming from. I think the meta question that you are asking is when we expect to see the ramp, and are we on track. Lisa showed the first silicon, we are right on track with where we thought we would be, both in terms of the readiness of the overall solution as well as the readiness of the silicon. And we are highly confident of ramping Helios in high volume in the second half of the year.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  So that is that. The reason why AMD can be confident was explained by Huang, who showed off two charts, and explained that they use what amounts to dummy hot plates to simulate the CPUs and GPUs long before they come back from the fabs so they know the physical specs and thermals of the chips that will come back will fit into the racks and the racks will work right.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-1.jpg&quot; rel=&quot;attachment wp-att-147030&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;346&quot; sizes=&quot;(max-width: 744px) 100vw, 744px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-1.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-1.jpg 744w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-1-600x279.jpg 600w&quot; width=&quot;744&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  And here is the Helios development and manufacturing model:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-2.jpg&quot; rel=&quot;attachment wp-att-147031&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;346&quot; sizes=&quot;(max-width: 697px) 100vw, 697px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-2.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-2.jpg 697w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-2-600x298.jpg 600w&quot; width=&quot;697&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This may be AMD’s first rackscale design, just like the NVL36 and NVL72 rackscale machines that were supposed to be based on Hopper were – and never came to market except for one machine sold to AWS. But this is not the first rackscale machine that ZT Systems has developed, and it is not the first one for Meta Platforms, either, which started the Open Compute Project back in 2011 with server, rack, and datacenter designs. Getting Helios out the door on time and being manufactured in volume is why
  &lt;a href=&quot;https://www.nextplatform.com/2024/08/19/why-amd-spent-4-9-billion-to-buy-zt-systems/&quot;&gt;
   AMD spent $4.9 billion to acquire ZT Systems back in August 2024
  &lt;/a&gt;
  . Not wanting to be in the server manufacturing business is why
  &lt;a href=&quot;https://www.nextplatform.com/2025/11/04/amd-is-coiled-to-hockey-stick-in-the-ai-datacenter/&quot;&gt;
   AMD sold the manufacturing arm of ZT Systems to Sanmina for $3 billion last fall
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  One of the big changes is that with rackscale systems, AMD will be delivering whole rackscale systems through what the industry calls a New Product Introduction, or NPI, partner. We are reasonably certain that this initial Helios NPI partner is none other than Sanmina, which has the ZT Systems manufacturing business, which as we pointed out a year and a half ago was a $10 billion system manufacturer with rackscale system cred when AMD bought it. (The biggest server maker you probably never heard of and that never made the OEM or ODM lists.)
 &lt;/p&gt;
 &lt;p&gt;
  Every single component is tested, and is then tested when it is assembled into racks, and then the racks are tested before they ship, and the shipping process is tested before the real racks go in so they can handle the stress of travel to customer datacenters. All of this has to be simulated and mocked up long before the first real rack is assembled in the NPI facility – what Huang called “early risk retirement.”
 &lt;/p&gt;
 &lt;p&gt;
  The issue is that all of this mockup and testing has to be done in parallel because time needs to be compressed. You cannot waiting for the real components to be available for prototypes and then early builds and then hope the rack works out.
 &lt;/p&gt;
 &lt;p&gt;
  “The factories nowadays are really extensions to the labs, because we go so fast that we don’t have the time that we have had before to do everything in series,” Huang explained. “So it is super-important to have an environment and a partner understands that, and that the supply chain has so many components in this complex solution. We are focusing a lot of energy to make sure that the whole ecosystem is enabled.”
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> AMD Says “Helios” Racks And MI400 Series GPUs On Track For 2H 2026 </title>
<link>https://www.nextplatform.com/2026/02/23/amd-says-helios-racks-and-mi400-series-gpus-on-track-for-2h-2026/</link>
<pubDate>Tue, 24 Feb 2026 03:47:03 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/01/CES-Su-Helios-system-1030x438.png&quot; title=&quot;CES Su Helios system&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  While releasing an update to its InferenceX AI inference benchmark test, formerly known as InferenceMax and thus far only having Nvidia and AMD testing systems using it,
  &lt;a href=&quot;https://newsletter.semianalysis.com/p/inferencex-v2-nvidia-blackwell-vs&quot;&gt;
   the analysts SemiAnalysis declared correctly
  &lt;/a&gt;
  that thus far only Nvidia, AWS, and Google have created rackscale systems that are deployed today, and that AMD was working on it.
 &lt;/p&gt;
 &lt;p&gt;
  And to be more precise, let’s name the rackscale platforms. Nvidia has its “Oberon” NVL72 rackscale systems using its “Blackwell” B200 and B300 GPU accelerators. AWS has its Trainium2 and Trainium3 accelerators, and Google with seven generations of TPU systems that have been brought to market for more than a decade. (It depends on how you want to draw the lines with memory coherency and flexibility with TPUs – you could argue for fewer generations because TPUs v1, v2, and v3 were hard-wired torus interconnects and only with TPU v4 did Google employ its “Palomar” optical circuit switch in reconfigurable layers in the torus interconnect.)
 &lt;/p&gt;
 &lt;p&gt;
  As you well know, AMD has been working with Meta Platforms on the Open Rack Wide v3 specification, a double-wide rack that AMD calls “Helios” and that is the platform for delivering rackscale compute based on
  &lt;a href=&quot;https://www.nextplatform.com/2025/11/14/amd-solid-roadmaps-beget-money-which-beget-better-roadmaps-and-even-more-money/&quot;&gt;
   the “Altair” MI400 series GPU accelerators
  &lt;/a&gt;
  . This series includes the MI450, the MI430X, and the MI455X for the Helios rackscale systems, which will have 64, 72, or 128 GPUs per system, and possibly the MI440X for eight-way system nodes.
 &lt;/p&gt;
 &lt;p&gt;
  Here are the feeds and speeds for the Helios rack:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-specs.jpg&quot; rel=&quot;attachment wp-att-147032&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;396&quot; sizes=&quot;(max-width: 740px) 100vw, 740px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-specs.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-specs.jpg 740w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-specs-600x321.jpg 600w&quot; width=&quot;740&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the SemiAnalysis report, which we saw because we subscribe to the publication, the authors made this statement:
 &lt;/p&gt;
 &lt;p&gt;
  “
  &lt;em&gt;
   Engineering samples and low volume production of AMD’s first rack scale MI455X UALoE72 system will be in H2 2026, while due to manufacturing delays, the mass production ramp and first production tokens will only be generated on an MI455X UALoE72 by Q2 2027.
  &lt;/em&gt;
  ”
 &lt;/p&gt;
 &lt;p&gt;
  This was a bit perplexing to us, since
  &lt;a href=&quot;https://www.nextplatform.com/2026/02/04/amd-finally-makes-more-money-on-gpus-than-cpus-in-a-quarter/&quot;&gt;
   AMD did not mention any delays in its most recent quarterly results from a few weeks ago
  &lt;/a&gt;
  . This would no doubt be a material delay, and one that would run the company afoul of the US Securities and Exchange Commission should it have been true at the time. It was possible that the delay could have hit after the results were posted on February 3. The further rumors on the street were that the Helios rack system delays were due to some thermal issue in the design.
 &lt;/p&gt;
 &lt;p&gt;
  This is one of the reasons why we sat in on a conference call hosted by New Street Research last week, where Forrest Norrod, general manager of the Data Center Solutions business group, and Doug Huang, one of the co-founders of ZT Systems, which was acquired by AMD last year to underpin its rackscale system engineering efforts, and now its senior vice president in charge of the Data Center Platform Engineering group.
 &lt;/p&gt;
 &lt;p&gt;
  Norrod was having none of it when asked about a delay with the Helios racks:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “I have no idea where this purported issue around thermals is coming from,” Norrod said unequivocally. “I literally have no idea. We have no significant thermal issue. The risk around the thermal design at the component level all the way through the rack level was retired quite some time ago. So, no idea where that’s coming from. I think the meta question that you are asking is when we expect to see the ramp, and are we on track. Lisa showed the first silicon, we are right on track with where we thought we would be, both in terms of the readiness of the overall solution as well as the readiness of the silicon. And we are highly confident of ramping Helios in high volume in the second half of the year.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  So that is that. The reason why AMD can be confident was explained by Huang, who showed off two charts, and explained that they use what amounts to dummy hot plates to simulate the CPUs and GPUs long before they come back from the fabs so they know the physical specs and thermals of the chips that will come back will fit into the racks and the racks will work right.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-1.jpg&quot; rel=&quot;attachment wp-att-147030&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;346&quot; sizes=&quot;(max-width: 744px) 100vw, 744px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-1.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-1.jpg 744w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-1-600x279.jpg 600w&quot; width=&quot;744&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  And here is the Helios development and manufacturing model:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-2.jpg&quot; rel=&quot;attachment wp-att-147031&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;346&quot; sizes=&quot;(max-width: 697px) 100vw, 697px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-2.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-2.jpg 697w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-helios-rack-design-deploy-2-600x298.jpg 600w&quot; width=&quot;697&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This may be AMD’s first rackscale design, just like the NVL36 and NVL72 rackscale machines that were supposed to be based on Hopper were – and never came to market except for one machine sold to AWS. But this is not the first rackscale machine that ZT Systems has developed, and it is not the first one for Meta Platforms, either, which started the Open Compute Project back in 2011 with server, rack, and datacenter designs. Getting Helios out the door on time and being manufactured in volume is why
  &lt;a href=&quot;https://www.nextplatform.com/2024/08/19/why-amd-spent-4-9-billion-to-buy-zt-systems/&quot;&gt;
   AMD spent $4.9 billion to acquire ZT Systems back in August 2024
  &lt;/a&gt;
  . Not wanting to be in the server manufacturing business is why
  &lt;a href=&quot;https://www.nextplatform.com/2025/11/04/amd-is-coiled-to-hockey-stick-in-the-ai-datacenter/&quot;&gt;
   AMD sold the manufacturing arm of ZT Systems to Sanmina for $3 billion last fall
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  One of the big changes is that with rackscale systems, AMD will be delivering whole rackscale systems through what the industry calls a New Product Introduction, or NPI, partner. We are reasonably certain that this initial Helios NPI partner is none other than Sanmina, which has the ZT Systems manufacturing business, which as we pointed out a year and a half ago was a $10 billion system manufacturer with rackscale system cred when AMD bought it. (The biggest server maker you probably never heard of and that never made the OEM or ODM lists.)
 &lt;/p&gt;
 &lt;p&gt;
  Every single component is tested, and is then tested when it is assembled into racks, and then the racks are tested before they ship, and the shipping process is tested before the real racks go in so they can handle the stress of travel to customer datacenters. All of this has to be simulated and mocked up long before the first real rack is assembled in the NPI facility – what Huang called “early risk retirement.”
 &lt;/p&gt;
 &lt;p&gt;
  The issue is that all of this mockup and testing has to be done in parallel because time needs to be compressed. You cannot waiting for the real components to be available for prototypes and then early builds and then hope the rack works out.
 &lt;/p&gt;
 &lt;p&gt;
  “The factories nowadays are really extensions to the labs, because we go so fast that we don’t have the time that we have had before to do everything in series,” Huang explained. “So it is super-important to have an environment and a partner understands that, and that the supply chain has so many components in this complex solution. We are focusing a lot of energy to make sure that the whole ecosystem is enabled.”
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> CPU-Only Compute Still Matters To A Lot Of HPC Centers </title>
<link>https://www.nextplatform.com/2026/02/23/cpu-only-compute-still-matters-to-a-lot-of-hpc-centers/#respond</link>
<pubDate>Mon, 23 Feb 2026 16:36:21 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2021/10/aws-datacenter-racks-746x438.jpg&quot; title=&quot;aws-datacenter-racks&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It has taken three decades for HPC to move to the cloud, and the truth is that a lot of simulation and modeling applications are still coded to run on CPUs. The good news is that the major clouds and some of the smaller ones have tuned up their CPU estates with fast networking and proper HPC software stacks so they can be of good use for HPC centers that, for whatever reason or another, desire to rent rather than to buy some compute capacity.
 &lt;/p&gt;
 &lt;p&gt;
  This week, Amazon Web Services trotted out the latest of its HPC-tuned virtual servers, the HPC8a instances, which are based on a customized versions of AMD’s “Turin” Epyc 9005 series processors. The Turin CPUs.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/2024/10/10/amd-turns-the-screws-with-turin-server-cpus/&quot;&gt;
   The Turin CPUs were launched in October 2024
  &lt;/a&gt;
  , and come in two flavors: One set based on the regular Zen 5 cores that span from 8 to 128 cores, and from 16 to 256 threads, per socket and another set based on the L3 cache-halved Zen 5c cores that span from 96 to 192 cores and 192 to 384 threads. We think the new HPC8a instances – there is really only one at the moment, but that could change – are based on a custom Epyc 9R15 processor, much as the prior HPC6a and HPC7a instances were based on custom
  &lt;a href=&quot;https://www.nextplatform.com/2021/03/15/the-third-time-charm-of-amds-milan-epyc-processors/&quot;&gt;
   “Milan” Epyc 7R13
  &lt;/a&gt;
  from March 2021 and
  &lt;a href=&quot;https://www.nextplatform.com/2022/11/10/amd-genoa-epyc-server-cpus-take-the-heavyweight-title/&quot;&gt;
   “Genoa” Epyc 9R14 processors
  &lt;/a&gt;
  from November 2022. The Epyc 9R15 chip seems to be based on a the Epyc 9655 processor, which has the same base clock speed of 2.6 GHz and the same turbo clock speed of 4.5 GHz that the Epyc 9R15 is said to have.
 &lt;/p&gt;
 &lt;p&gt;
  We know that these HPC instances, with the exception of those based on the Graviton3E, are two socket machines because we know that all of the HPC instances have simultaneous multithreading turned off. On HPC workloads using the Message Passing Interface (MPI) protocol to share work across loosely couple memory pools composed of the HPC cluster nodes, the MPI communications is very sensitive to cache latencies, which in turn are adversely affected by SMT. When you turn SMT off, you are only using one thread per core and the cache behaves more predictably and performance doesn’t degrade.
 &lt;/p&gt;
 &lt;p&gt;
  So, the new hpc8a-96xlarge is not a single 96-core processor, but rather a pair of them that deliver 192 physical cores that are in turn virtualized into 192 vCPUs.
 &lt;/p&gt;
 &lt;p&gt;
  The important thing about the HPC8a is that by moving to the Turin design, there are a dozen memory controllers on each Turin processor across the 96 cores (and therefore 96 vCPUs) on the Epyc 9R15 socket instead of only eight memory controllers with the Genoa chips used in the HPC7a instances.
 &lt;/p&gt;
 &lt;p&gt;
  The combination of the higher memory controller count and the move to faster DDR5 memory means that on workloads that are constrained by memory bandwidth, the Turin instances will do up to 40 percent more work than the Genoa instances with the same vCPU count even though. This is despite the fact that the peak theoretical performance at the base clock speed of 2.6 GHz of the Epyc 9R15 for FP64 floating point precision used in the HPC8a instance is almost exactly the same as the peak FP64 oomph of the Epyc 9R14 used in the HPC7a instances.
 &lt;/p&gt;
 &lt;p&gt;
  You can see this in the AWS HPC instance table we have created below:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances.jpg&quot; rel=&quot;attachment wp-att-147028&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;341&quot; sizes=&quot;(max-width: 1087px) 100vw, 1087px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances.jpg 1087w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances-768x241.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances-600x188.jpg 600w&quot; width=&quot;1087&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  If you do the math, then, which is what HPC centers do for a living, then almost all of the performance gain and nearly all of the 25 percent price/performance gain that AWS cites in its HPC8a announcement when compared to the top-end HPC7a instance comes from that extra memory bandwidth from more channels and faster DDR5. Clearly, from the table, you see the trouble with using peak theoretical performance alone to gauge real-world performance and therefore price/performance. If you based your buying decisions on this table alone, you would go with the Genoa instance, not the Turin instance.
 &lt;/p&gt;
 &lt;p&gt;
  Both the HPC8a and HP7a instances make use of the 300 Gb/sec EFA-2 Ethernet adapters on the HPC8a instances, so there is no network advantage driving performance across the latest Turin instances. We are surprised that AWS has not yet delivered a 400 Gb/sec or better still 800 Gb/sec EFA-3 adapter for its formal HPC instances, to be honest, which might contribute to cluster throughput in a big way for Genoa and Turin instances used at scale.
 &lt;/p&gt;
 &lt;p&gt;
  You will notice a few things odd about the HPC instances that AWS has put together. The HPC6 instances came in two flavors: One based on a “Ice Lake” Xeon SP v3 processor from Intel and one based on the custom Milan Epyc 7R13 processor from AMD. The name of the instance tells you the core count and the vCPU number tells you the thread count. There is one instance per type, and it has the maximum core and thread count.
 &lt;/p&gt;
 &lt;p&gt;
  With the HPC7g instances, which are based on the HPC-tuned Graviton3E variant of the AWS-designed Graviton3 processor and
  &lt;a href=&quot;https://www.nextplatform.com/2022/12/02/aws-tunes-up-compute-and-network-for-hpc/&quot;&gt;
   which were launched in December 2022
  &lt;/a&gt;
  , and with the HPC7a instances based on the custom Genoa Epyc 9R14 processors, which launched in August 2023, AWS did something different. Rather than just have a single instance with maximum cores and a reasonably large memory, AWS made the memory capacity static but allowed for the core counts to be reduced so that customers could pick a ratio of memory capacity and memory bandwidth to cores as they often do when they configure server nodes in their on-premises CPU clusters.
 &lt;/p&gt;
 &lt;p&gt;
  So, with the Graviton3E-based HPC7g instances, customers could configure cores against that 128 GB of main with 2 GB, 4 GB, or 8 GB of memory across each of those cores. (As the core counts shrank, the amount of memory bandwidth per core grew in the same proportion.) Interestingly, AWS didn’t change the cost of on demand instances based on the core count or these other metrics. The price was the same at $1.68 per hour.
 &lt;/p&gt;
 &lt;p&gt;
  Ditto for the HPC7a instances based on Genoa. By reducing the core count by selecting a smaller instance slice (but with a fixed memory capacity no matter what the core count is), customers could configure 4 GB, 8 GB, 16 GB, or 32 GB of main memory per core and, again, a proportionally smaller amount of memory bandwidth per core as the core counts go up.
 &lt;/p&gt;
 &lt;p&gt;
  With the HPC8a instance just announced, AWS is back to just selling the full, fat configuration, in this case with 96 cores against 768 GB of main memory, or the 4 GB per core that is often used with nodes in HPC clustered systems.
 &lt;/p&gt;
 &lt;p&gt;
  One other thing you will also note: The Elastic Block Storage (EBS) bandwidth and throughput is the same across all of these instances, and it is not particularly high, at 87 Mb/sec and 500 I/O operations per second (IOPS). There is a special turbo mode for EBS that allows HPC customers to run at 2,085 Mb/sec bandwidth and drive 11,000 IOPS for a 30 minute period within every 24 hours. It is not clear if this is being used for snapshotting of state for the virtual HPC clusters, which do not have local storage with the exception of the one HPC6id instance launched many years ago.
 &lt;/p&gt;
 &lt;p&gt;
  Clearly, AWS does not want customers to set up parallel file systems atop its EBS block storage, although there is probably not a technical reason it could be done. And clearly, AWS very much wants HPC shops running their ModSim in the cloud to use is FSx for Lustre, a fully managed implementation of the open source Lustre parallel file system. That said, there is nothing that will stop you from setting up a VAST Data or WekaIO cluster on AWS iron and link that to an HPC instance cluster if you want to.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> CPU-Only Compute Still Matters To A Lot Of HPC Centers </title>
<link>https://www.nextplatform.com/2026/02/23/cpu-only-compute-still-matters-to-a-lot-of-hpc-centers/</link>
<pubDate>Mon, 23 Feb 2026 16:36:19 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2021/10/aws-datacenter-racks-746x438.jpg&quot; title=&quot;aws-datacenter-racks&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It has taken three decades for HPC to move to the cloud, and the truth is that a lot of simulation and modeling applications are still coded to run on CPUs. The good news is that the major clouds and some of the smaller ones have tuned up their CPU estates with fast networking and proper HPC software stacks so they can be of good use for HPC centers that, for whatever reason or another, desire to rent rather than to buy some compute capacity.
 &lt;/p&gt;
 &lt;p&gt;
  This week, Amazon Web Services trotted out the latest of its HPC-tuned virtual servers, the HPC8a instances, which are based on a customized versions of AMD’s “Turin” Epyc 9005 series processors. The Turin CPUs.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/2024/10/10/amd-turns-the-screws-with-turin-server-cpus/&quot;&gt;
   The Turin CPUs were launched in October 2024
  &lt;/a&gt;
  , and come in two flavors: One set based on the regular Zen 5 cores that span from 8 to 128 cores, and from 16 to 256 threads, per socket and another set based on the L3 cache-halved Zen 5c cores that span from 96 to 192 cores and 192 to 384 threads. We think the new HPC8a instances – there is really only one at the moment, but that could change – are based on a custom Epyc 9R15 processor, much as the prior HPC6a and HPC7a instances were based on custom
  &lt;a href=&quot;https://www.nextplatform.com/2021/03/15/the-third-time-charm-of-amds-milan-epyc-processors/&quot;&gt;
   “Milan” Epyc 7R13
  &lt;/a&gt;
  from March 2021 and
  &lt;a href=&quot;https://www.nextplatform.com/2022/11/10/amd-genoa-epyc-server-cpus-take-the-heavyweight-title/&quot;&gt;
   “Genoa” Epyc 9R14 processors
  &lt;/a&gt;
  from November 2022. The Epyc 9R15 chip seems to be based on a the Epyc 9655 processor, which has the same base clock speed of 2.6 GHz and the same turbo clock speed of 4.5 GHz that the Epyc 9R15 is said to have.
 &lt;/p&gt;
 &lt;p&gt;
  We know that these HPC instances, with the exception of those based on the Graviton3E, are two socket machines because we know that all of the HPC instances have simultaneous multithreading turned off. On HPC workloads using the Message Passing Interface (MPI) protocol to share work across loosely couple memory pools composed of the HPC cluster nodes, the MPI communications is very sensitive to cache latencies, which in turn are adversely affected by SMT. When you turn SMT off, you are only using one thread per core and the cache behaves more predictably and performance doesn’t degrade.
 &lt;/p&gt;
 &lt;p&gt;
  So, the new hpc8a-96xlarge is not a single 96-core processor, but rather a pair of them that deliver 192 physical cores that are in turn virtualized into 192 vCPUs.
 &lt;/p&gt;
 &lt;p&gt;
  The important thing about the HPC8a is that by moving to the Turin design, there are a dozen memory controllers on each Turin processor across the 96 cores (and therefore 96 vCPUs) on the Epyc 9R15 socket instead of only eight memory controllers with the Genoa chips used in the HPC7a instances.
 &lt;/p&gt;
 &lt;p&gt;
  The combination of the higher memory controller count and the move to faster DDR5 memory means that on workloads that are constrained by memory bandwidth, the Turin instances will do up to 40 percent more work than the Genoa instances with the same vCPU count even though. This is despite the fact that the peak theoretical performance at the base clock speed of 2.6 GHz of the Epyc 9R15 for FP64 floating point precision used in the HPC8a instance is almost exactly the same as the peak FP64 oomph of the Epyc 9R14 used in the HPC7a instances.
 &lt;/p&gt;
 &lt;p&gt;
  You can see this in the AWS HPC instance table we have created below:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances.jpg&quot; rel=&quot;attachment wp-att-147028&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;341&quot; sizes=&quot;(max-width: 1087px) 100vw, 1087px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances.jpg 1087w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances-768x241.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-hpc-instances-600x188.jpg 600w&quot; width=&quot;1087&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  If you do the math, then, which is what HPC centers do for a living, then almost all of the performance gain and nearly all of the 25 percent price/performance gain that AWS cites in its HPC8a announcement when compared to the top-end HPC7a instance comes from that extra memory bandwidth from more channels and faster DDR5. Clearly, from the table, you see the trouble with using peak theoretical performance alone to gauge real-world performance and therefore price/performance. If you based your buying decisions on this table alone, you would go with the Genoa instance, not the Turin instance.
 &lt;/p&gt;
 &lt;p&gt;
  Both the HPC8a and HP7a instances make use of the 300 Gb/sec EFA-2 Ethernet adapters on the HPC8a instances, so there is no network advantage driving performance across the latest Turin instances. We are surprised that AWS has not yet delivered a 400 Gb/sec or better still 800 Gb/sec EFA-3 adapter for its formal HPC instances, to be honest, which might contribute to cluster throughput in a big way for Genoa and Turin instances used at scale.
 &lt;/p&gt;
 &lt;p&gt;
  You will notice a few things odd about the HPC instances that AWS has put together. The HPC6 instances came in two flavors: One based on a “Ice Lake” Xeon SP v3 processor from Intel and one based on the custom Milan Epyc 7R13 processor from AMD. The name of the instance tells you the core count and the vCPU number tells you the thread count. There is one instance per type, and it has the maximum core and thread count.
 &lt;/p&gt;
 &lt;p&gt;
  With the HPC7g instances, which are based on the HPC-tuned Graviton3E variant of the AWS-designed Graviton3 processor and
  &lt;a href=&quot;https://www.nextplatform.com/2022/12/02/aws-tunes-up-compute-and-network-for-hpc/&quot;&gt;
   which were launched in December 2022
  &lt;/a&gt;
  , and with the HPC7a instances based on the custom Genoa Epyc 9R14 processors, which launched in August 2023, AWS did something different. Rather than just have a single instance with maximum cores and a reasonably large memory, AWS made the memory capacity static but allowed for the core counts to be reduced so that customers could pick a ratio of memory capacity and memory bandwidth to cores as they often do when they configure server nodes in their on-premises CPU clusters.
 &lt;/p&gt;
 &lt;p&gt;
  So, with the Graviton3E-based HPC7g instances, customers could configure cores against that 128 GB of main with 2 GB, 4 GB, or 8 GB of memory across each of those cores. (As the core counts shrank, the amount of memory bandwidth per core grew in the same proportion.) Interestingly, AWS didn’t change the cost of on demand instances based on the core count or these other metrics. The price was the same at $1.68 per hour.
 &lt;/p&gt;
 &lt;p&gt;
  Ditto for the HPC7a instances based on Genoa. By reducing the core count by selecting a smaller instance slice (but with a fixed memory capacity no matter what the core count is), customers could configure 4 GB, 8 GB, 16 GB, or 32 GB of main memory per core and, again, a proportionally smaller amount of memory bandwidth per core as the core counts go up.
 &lt;/p&gt;
 &lt;p&gt;
  With the HPC8a instance just announced, AWS is back to just selling the full, fat configuration, in this case with 96 cores against 768 GB of main memory, or the 4 GB per core that is often used with nodes in HPC clustered systems.
 &lt;/p&gt;
 &lt;p&gt;
  One other thing you will also note: The Elastic Block Storage (EBS) bandwidth and throughput is the same across all of these instances, and it is not particularly high, at 87 Mb/sec and 500 I/O operations per second (IOPS). There is a special turbo mode for EBS that allows HPC customers to run at 2,085 Mb/sec bandwidth and drive 11,000 IOPS for a 30 minute period within every 24 hours. It is not clear if this is being used for snapshotting of state for the virtual HPC clusters, which do not have local storage with the exception of the one HPC6id instance launched many years ago.
 &lt;/p&gt;
 &lt;p&gt;
  Clearly, AWS does not want customers to set up parallel file systems atop its EBS block storage, although there is probably not a technical reason it could be done. And clearly, AWS very much wants HPC shops running their ModSim in the cloud to use is FSx for Lustre, a fully managed implementation of the open source Lustre parallel file system. That said, there is nothing that will stop you from setting up a VAST Data or WekaIO cluster on AWS iron and link that to an HPC instance cluster if you want to.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Taalas Etches AI Models Onto Transistors To Rocket Boost Inference </title>
<link>https://www.nextplatform.com/2026/02/19/taalas-etches-ai-models-onto-transistors-to-rocket-boost-inference/#respond</link>
<pubDate>Thu, 19 Feb 2026 20:12:14 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hc1-logo.jpg&quot; title=&quot;taalas-hc1-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Adding big blocks of SRAM to collections of AI tensor engines, or better still, a waferscale collection of such engines, turbocharges AI inference, as has been shown time and again by AI upstarts Cerebras Systems, SambaNova Systems (which
  &lt;a href=&quot;bloomberg.com/news/articles/2025-12-12/intel-nears-1-6-billion-deal-for-ai-chip-startup-sambanova&quot;&gt;
   Intel is rumored to have taken a run at
  &lt;/a&gt;
  late last year), Groq (
  &lt;a href=&quot;https://www.nextplatform.com/2026/01/16/is-nvidia-assembling-the-parts-for-its-next-inference-platform/&quot;&gt;
   just eaten by Nvidia for $20 billion
  &lt;/a&gt;
  ), and Graphcore (
  &lt;a href=&quot;https://www.nextplatform.com/2024/07/12/can-softbank-be-an-ai-supercomputer-player-will-arm-lend-a-hand/&quot;&gt;
   eaten by SoftBank for $600 million
  &lt;/a&gt;
  a year and a half ago) as they compare against GPUs from Nvidia and AMD.
 &lt;/p&gt;
 &lt;p&gt;
  But if you really want to push the envelope of AI inference, says startup Taalas, which dropped out of stealth mode today, then the thing to do is stop screwing around and encode the weights of a finished AI inference right into the transistors of a chip and get rid of all of the software cruft that comes with trying to make compute engines malleable so companies can constantly tweak and tune their models.
 &lt;/p&gt;
 &lt;p&gt;
  By doing so, you can also radically simplify the architecture of the AI device and, the way that Taalas has done it, you can eliminate the wall between compute and memory that plagues all serial and parallel compute engines – and especially GPUs and AI XPUs that have had to resort to HBM stacked DRAM to get bandwidth commensurate with their floating point and integer performance.
 &lt;/p&gt;
 &lt;p&gt;
  Taalas is two and a half years old and has raised over $200 million in three rounds of venture funding. The company is located in Toronto, one of the hotbeds of AI research and also where there is plenty of chip expertise, including Tenstorrent, where the company’s three founders all worked.
  &lt;a href=&quot;https://www.linkedin.com/in/ljubisa-bajic-1b3608/&quot;&gt;
   Ljubisa Bajic
  &lt;/a&gt;
  is the co-founder who has the chief executive officer job at Taalas, and is well known as the founder of Tenstorrent.
 &lt;/p&gt;
 &lt;p&gt;
  What might be less known is that Bajic spent a few years after the Dot Com Boom designing video encoders for Teralogic and Oak Technology before moving over to AMD and rising through the engineering ranks to be the architect and senior manager of the company’s hybrid CPU-GPU chip designs for PCs and servers. Bajic did a one-year stint at Nvidia as s senior architect, bounced back to AMD as a director of integrated circuit design for two years, and then started Tenstorrent. When chip luminary Jim Keller was brought in in the fall of 2022, Bajic decided to leave and after a six-month break got to work on a completely different idea for AI inference computing and started Taalas in Toronto.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/lejla-bajic-8177a762/&quot;&gt;
   Lejla Bajic
  &lt;/a&gt;
  , who is Ljubisa’s wife, is the chief operating officer at Taalas, and she was a software engineer at FPGA maker Altera in the wake of the Dot Com Boom, and then became a senior engineer at ATI, the Canadian GPU maker
  &lt;a href=&quot;https://www.theregister.com/2006/07/24/amd_to_buy_ati/&quot;&gt;
   that AMD bought in July 2006 for $5.4 billion
  &lt;/a&gt;
  . Lejla Bajic also rose through the AMD engineering ranks over the years, eventually becoming senior manager of systems engineering. She joined Tenstorrent in October 2017 to do the same job and left when her husband did.
 &lt;/p&gt;
 &lt;p&gt;
  The third co-founder at Taalas is
  &lt;a href=&quot;https://www.linkedin.com/in/drago-ignjatovic-5347948/&quot;&gt;
   Drago Ignjatovic
  &lt;/a&gt;
  , who was a senior design engineer working on AMD APUs and GPUs and took over for Ljubisa Bajic as director of ASIC design when the latter left to start Tenstorrent. Nine months later, Ignjatovic joined Tenstorrent as its vice president of hardware engineering, and he started Taalas with the Bajices as the startup’s chief technology officer.
 &lt;/p&gt;
 &lt;p&gt;
  Significantly,
  &lt;a href=&quot;https://www.linkedin.com/in/pareshkharya/&quot;&gt;
   Paresh Kharya
  &lt;/a&gt;
  , who was senior director of product management and marketing for the datacenter business for three years and then director of AI infrastructure product management at Google Cloud (managing its GPU and TPU hardware and their software stacks), has joined Taalas as vice president of products. The company has 25 employees at the moment, most of them engineers who worked at AMD, Apple, Google, Nvidia, and Tenstorrent, and they have plenty of experience bringing chips from idea to systems. The company has only spent $30 million on research and development to get to the launch today, and has more than $170 million still in the bank.
 &lt;/p&gt;
 &lt;h3&gt;
  Mashing Up ROM And SRAM, Ditching HBM And Crazy I/O
 &lt;/h3&gt;
 &lt;p&gt;
  Most good ideas seem obvious in hindsight, and creating a dataflow engine that can embody the weights and algorithms of an AI model and then pouring context and queries through it is also not a new idea. To a certain extent, that is what FPGAs and the first generation of AI accelerators do, and it is what GPUs and special accelerators like TPUs and Trainiums also do.
 &lt;/p&gt;
 &lt;p&gt;
  For now, Taalas is keeping the precise workings of its Hard Coded Inference architecture secret, but Bajic and Kharya game me a high level overview of how this works. But before we get into that, Kharya is a history buff like we all are and showed this funny picture that is very much “
  &lt;em&gt;
   plus ça change, plus c’est la même chose
  &lt;/em&gt;
  .” Take a look:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-ibm-stretch-sperry-eniac.jpg&quot; rel=&quot;attachment wp-att-147024&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;379&quot; sizes=&quot;(max-width: 646px) 100vw, 646px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-ibm-stretch-sperry-eniac.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-ibm-stretch-sperry-eniac.jpg 646w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-ibm-stretch-sperry-eniac-600x352.jpg 600w&quot; width=&quot;646&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  On the upper left, that is the massive copper cabling to interconnect the transistor compute frames of the IBM 7030 Stretch supercomputer from 1961, and the bottom right is the racks and racks of the ENIAC vacuum-tube powered supercomputer from 1946, which eventually spawned the Sperry Rand computer business (now part of Unisys).
 &lt;/p&gt;
 &lt;p&gt;
  The joke is we had massive copper cables and 150 kilowatts per rack back then, and the way GPUs and XPUs have evolved, we are back to the future. (Don’t overanalyze that – it is meant to be funny.)
 &lt;/p&gt;
 &lt;p&gt;
  So what, precisely, is the Hard Coded Inference chip, and how does it work?
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture.jpg&quot; rel=&quot;attachment wp-att-147020&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;477&quot; sizes=&quot;(max-width: 1303px) 100vw, 1303px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture.jpg 1303w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture-768x281.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture-600x220.jpg 600w&quot; width=&quot;1303&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Kharya explains it this way:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “We basically have an architecture where we are embedding the models, and we are hard coding the models and the weights into our what we call the mask ROM recall fabric, which is paired with an SRAM recall fabric. Together, they are able to store both the model as well as do all the computations of KV cache. We have adapters and customizations – we support all of that. This design allows us to be super-dense in terms of compute and in terms of storage, and we can do compute on that storage incredibly fast, which is what drives density up and cost down.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “In the current generation, our density is 8 billion parameters on the hard wired part of the chip., plus the SRAM to allow us to do KV caches, adaptations like fine tuning, and etc. In our next generation, we would have the ability to go up to 20 billion parameters in a chip. Even with trillions of parameters, we’re talking about few tens of chips, which is a very, very small compared to anything else out there on the market today.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Without being specific about the architecture – Taalas wants it to be a bit of a black box for now – Bajic added this:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “We have got this scheme for the mask ROM recall fabric – the hard-wired part – where we can store four bits away and do the multiply related to it – everything – with a single transistor. So the density is basically insane. And this is not nuclear physics – it is fully digital. It is just a clever trick that we don’t want to broadcast. But once you hardwire everything, you get this opportunity to stuff very differently than if you have to deal with changing things. The important thing is that we can put a weight and do the multiply associated with it all in one transistor. And you know the multipliers are kind of the big boy piece of the computer.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “What we invented is not particularly difficult, either. It’s just a clever thing that nobody saw because nobody went down this path. We showed up more than two years ago, and we wanted to remove the barrier between memory and compute altogether. That was the genesis of this whole thing. Now, the first way we came up with to do it – and basically the only way we could see at the time that would produce a product on a predictable timeline, because we didn’t want to be research profs and three years down the line have something that doesn’t work – was to quickly veered off into this ROM-based approach. We started studying it in detail and then we realized that actually this was even better than we thought.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “We actually designed all this stuff from scratch internally. We didn’t use off the shelf anything, we did lots of transistor level design, hand layout – basically our whole effort ended up being a throwback to the 1970s.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  What is immediately obvious is that every change in model, from say Llama 3.1 to Llama 4, will require a new spin of the HC chip. For now, Taalas is focusing on etching the weights for open source models onto its HC chips, but it is not hard to imagine Anthropic and OpenAI picking up the phone and ordering custom accelerators for their models. Even Google might want to give it a try. By the way,
  &lt;a href=&quot;https://patents.google.com/?inventor=Bajic&amp;assignee=Taalas&amp;oq=Taalas+Bajic&quot;&gt;
   Taalas has filed 14 patents under Bajic to cover its technology as far as we can see
  &lt;/a&gt;
  ; there could be more because patent searching is very bad – even Google Patents.
 &lt;/p&gt;
 &lt;p&gt;
  To etch a new model on an HC inference engine involves changing a two layers of metal in the HC chip design, not a complete scrapping of it. And with the cost of training models running into the billions of dollars, paying a relatively nominal fee to adapt an HC inference engine to a new release of a model or for an entirely different model is not a big deal. Kharya says it costs 100X as much to train a model then to get a customize HC chip in reasonable volumes from Taalas.
 &lt;/p&gt;
 &lt;p&gt;
  Perhaps equally importantly, the time between major model releases is lengthening and people are getting attached to their models – there was plenty of gnashing of teeth when OpenAI moved customers from GPT 4.5 to GPT 5, for instance, because the latest release is a bit sycophantic. Given this, it may make sense to order a few hundred thousand to a few million of the HC inference engines.
 &lt;/p&gt;
 &lt;p&gt;
  With the “foundry optimal workflow” that Taalas has created in conjunction with Taiwan Semiconductor Manufacturing Co, customers can go from model weights to deployable PCI-Express cards, actually doing inference, in two months.
 &lt;/p&gt;
 &lt;p&gt;
  The first generation HC1 chip is implemented in the 6 nanometer N6 process from TSMC. At 815 mm
  &lt;sup&gt;
   2
  &lt;/sup&gt;
  it is pushing up against the reticle limit of chips these days (before we move to High NA processes that will cut the reticle size in half, which is not at all desirable). Each HC1 chip has 53 billion transistors on the package, most of it very likely for ROM and SRAM memory. The HC1 card burns about 200 watts, says Bajic, and a two-socket X86 server with ten HC1 cards in it runs 2,500 watts.
 &lt;/p&gt;
 &lt;p&gt;
  By the way, because the HC1 card is so fast, to get low latency inference does not requiring batching up queries, and that means the bandwidth pressure on the Taalas devices is low. So low that the PCI-Express bus is fine if you want to gang cards up to run a larger model, which Taalas will allow customers to do later this year using pipeline parallelism to spread work across the HC cards. By summer, in fact, it will have a Llama 3.1 model with 20 billion parameters hard coded into an HC chip, and by the end of the year it will have a frontier-class large language model – maybe Llama, maybe DeepSeek, maybe both – running inference across a collection of HC cards. This architecture will be called HC2.
 &lt;/p&gt;
 &lt;p&gt;
  So, just how fast and just how cheap is this Taalas HC1 card? Let’s take a look, starting with the latest throughput for Llama 3.1 8B models as assessed by
  &lt;a href=&quot;https://artificialanalysis.ai/&quot;&gt;
   Artificial Analysis
  &lt;/a&gt;
  :
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-1.jpg&quot; rel=&quot;attachment wp-att-147021&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;464&quot; sizes=&quot;(max-width: 723px) 100vw, 723px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-1.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-1.jpg 723w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-1-600x385.jpg 600w&quot; width=&quot;723&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  These initial performance results for the HC1 were run by Taalas itself, not Artificial Analysis, but you can play around with the chatbot demo
  &lt;a href=&quot;https://chatjimmy.ai/&quot;&gt;
   at this link
  &lt;/a&gt;
  and request developer API access
  &lt;a href=&quot;https://taalas.com/api-request-form/&quot;&gt;
   at this other link
  &lt;/a&gt;
  and run your own tests.
 &lt;/p&gt;
 &lt;p&gt;
  That’s a pretty big gap with a “Blackwell” B200 GPU (which Taalas itself ran), and even a substantial gap with what Grow, SambaNova, and Cerebras can deliver with their SRAM-heavy AI compute engines.
 &lt;/p&gt;
 &lt;p&gt;
  For fun, Taalas took the Llama 3.1 8B and DeepSeek R1 671B models and compared the Nvidia B200 against its HC card. (Our guess is that it took around 35 cards to load up the memory to run DeepSeek R1 671B on the Taalas boxes.) Here is how they stack up:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-2.jpg&quot; rel=&quot;attachment wp-att-147022&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;464&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 723px) 100vw, 723px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-2.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-2.jpg 723w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-2-600x385.jpg 600w&quot; width=&quot;723&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Now, what you want to know is throughput, latency, and cost per token, and this chart brings it all together:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3.jpg&quot; rel=&quot;attachment wp-att-147023&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;517&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1239px) 100vw, 1239px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3.jpg 1239w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3-768x320.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3-600x250.jpg 600w&quot; width=&quot;1239&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  On GPU systems, the interactivity – how many users you can support concurrently asking queries and getting fed answers – depends on the latency you want. If you want low latency, you can’t have a lot of users, and if you want lower cost, you have to pay for it with increased latency of tokens processed as input or output.
 &lt;/p&gt;
 &lt;p&gt;
  As you can see, Taalas is showing much lower costs and incredibly lower latencies on these two models tested.
 &lt;/p&gt;
 &lt;p&gt;
  We look forward to independent testing as the HC cards ramp into production, and to see what Taalas will charge for these AI inference engines. This sure looks like a game changer for AI inference.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Taalas Etches AI Models Onto Transistors To Rocket Boost Inference </title>
<link>https://www.nextplatform.com/2026/02/19/taalas-etches-ai-models-onto-transistors-to-rocket-boost-inference/</link>
<pubDate>Thu, 19 Feb 2026 20:12:11 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hc1-logo.jpg&quot; title=&quot;taalas-hc1-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Adding big blocks of SRAM to collections of AI tensor engines, or better still, a waferscale collection of such engines, turbocharges AI inference, as has been shown time and again by AI upstarts Cerebras Systems, SambaNova Systems (which
  &lt;a href=&quot;bloomberg.com/news/articles/2025-12-12/intel-nears-1-6-billion-deal-for-ai-chip-startup-sambanova&quot;&gt;
   Intel is rumored to have taken a run at
  &lt;/a&gt;
  late last year), Groq (
  &lt;a href=&quot;https://www.nextplatform.com/2026/01/16/is-nvidia-assembling-the-parts-for-its-next-inference-platform/&quot;&gt;
   just eaten by Nvidia for $20 billion
  &lt;/a&gt;
  ), and Graphcore (
  &lt;a href=&quot;https://www.nextplatform.com/2024/07/12/can-softbank-be-an-ai-supercomputer-player-will-arm-lend-a-hand/&quot;&gt;
   eaten by SoftBank for $600 million
  &lt;/a&gt;
  a year and a half ago) as they compare against GPUs from Nvidia and AMD.
 &lt;/p&gt;
 &lt;p&gt;
  But if you really want to push the envelope of AI inference, says startup Taalas, which dropped out of stealth mode today, then the thing to do is stop screwing around and encode the weights of a finished AI inference right into the transistors of a chip and get rid of all of the software cruft that comes with trying to make compute engines malleable so companies can constantly tweak and tune their models.
 &lt;/p&gt;
 &lt;p&gt;
  By doing so, you can also radically simplify the architecture of the AI device and, the way that Taalas has done it, you can eliminate the wall between compute and memory that plagues all serial and parallel compute engines – and especially GPUs and AI XPUs that have had to resort to HBM stacked DRAM to get bandwidth commensurate with their floating point and integer performance.
 &lt;/p&gt;
 &lt;p&gt;
  Taalas is two and a half years old and has raised over $200 million in three rounds of venture funding. The company is located in Toronto, one of the hotbeds of AI research and also where there is plenty of chip expertise, including Tenstorrent, where the company’s three founders all worked.
  &lt;a href=&quot;https://www.linkedin.com/in/ljubisa-bajic-1b3608/&quot;&gt;
   Ljubisa Bajic
  &lt;/a&gt;
  is the co-founder who has the chief executive officer job at Taalas, and is well known as the founder of Tenstorrent.
 &lt;/p&gt;
 &lt;p&gt;
  What might be less known is that Bajic spent a few years after the Dot Com Boom designing video encoders for Teralogic and Oak Technology before moving over to AMD and rising through the engineering ranks to be the architect and senior manager of the company’s hybrid CPU-GPU chip designs for PCs and servers. Bajic did a one-year stint at Nvidia as s senior architect, bounced back to AMD as a director of integrated circuit design for two years, and then started Tenstorrent. When chip luminary Jim Keller was brought in in the fall of 2022, Bajic decided to leave and after a six-month break got to work on a completely different idea for AI inference computing and started Taalas in Toronto.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.linkedin.com/in/lejla-bajic-8177a762/&quot;&gt;
   Lejla Bajic
  &lt;/a&gt;
  , who is Ljubisa’s wife, is the chief operating officer at Taalas, and she was a software engineer at FPGA maker Altera in the wake of the Dot Com Boom, and then became a senior engineer at ATI, the Canadian GPU maker
  &lt;a href=&quot;https://www.theregister.com/2006/07/24/amd_to_buy_ati/&quot;&gt;
   that AMD bought in July 2006 for $5.4 billion
  &lt;/a&gt;
  . Lejla Bajic also rose through the AMD engineering ranks over the years, eventually becoming senior manager of systems engineering. She joined Tenstorrent in October 2017 to do the same job and left when her husband did.
 &lt;/p&gt;
 &lt;p&gt;
  The third co-founder at Taalas is
  &lt;a href=&quot;https://www.linkedin.com/in/drago-ignjatovic-5347948/&quot;&gt;
   Drago Ignjatovic
  &lt;/a&gt;
  , who was a senior design engineer working on AMD APUs and GPUs and took over for Ljubisa Bajic as director of ASIC design when the latter left to start Tenstorrent. Nine months later, Ignjatovic joined Tenstorrent as its vice president of hardware engineering, and he started Taalas with the Bajices as the startup’s chief technology officer.
 &lt;/p&gt;
 &lt;p&gt;
  Significantly,
  &lt;a href=&quot;https://www.linkedin.com/in/pareshkharya/&quot;&gt;
   Paresh Kharya
  &lt;/a&gt;
  , who was senior director of product management and marketing for the datacenter business for three years and then director of AI infrastructure product management at Google Cloud (managing its GPU and TPU hardware and their software stacks), has joined Taalas as vice president of products. The company has 25 employees at the moment, most of them engineers who worked at AMD, Apple, Google, Nvidia, and Tenstorrent, and they have plenty of experience bringing chips from idea to systems. The company has only spent $30 million on research and development to get to the launch today, and has more than $170 million still in the bank.
 &lt;/p&gt;
 &lt;h3&gt;
  Mashing Up ROM And SRAM, Ditching HBM And Crazy I/O
 &lt;/h3&gt;
 &lt;p&gt;
  Most good ideas seem obvious in hindsight, and creating a dataflow engine that can embody the weights and algorithms of an AI model and then pouring context and queries through it is also not a new idea. To a certain extent, that is what FPGAs and the first generation of AI accelerators do, and it is what GPUs and special accelerators like TPUs and Trainiums also do.
 &lt;/p&gt;
 &lt;p&gt;
  For now, Taalas is keeping the precise workings of its Hard Coded Inference architecture secret, but Bajic and Kharya game me a high level overview of how this works. But before we get into that, Kharya is a history buff like we all are and showed this funny picture that is very much “
  &lt;em&gt;
   plus ça change, plus c’est la même chose
  &lt;/em&gt;
  .” Take a look:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-ibm-stretch-sperry-eniac.jpg&quot; rel=&quot;attachment wp-att-147024&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;379&quot; sizes=&quot;(max-width: 646px) 100vw, 646px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-ibm-stretch-sperry-eniac.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-ibm-stretch-sperry-eniac.jpg 646w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-ibm-stretch-sperry-eniac-600x352.jpg 600w&quot; width=&quot;646&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  On the upper left, that is the massive copper cabling to interconnect the transistor compute frames of the IBM 7030 Stretch supercomputer from 1961, and the bottom right is the racks and racks of the ENIAC vacuum-tube powered supercomputer from 1946, which eventually spawned the Sperry Rand computer business (now part of Unisys).
 &lt;/p&gt;
 &lt;p&gt;
  The joke is we had massive copper cables and 150 kilowatts per rack back then, and the way GPUs and XPUs have evolved, we are back to the future. (Don’t overanalyze that – it is meant to be funny.)
 &lt;/p&gt;
 &lt;p&gt;
  So what, precisely, is the Hard Coded Inference chip, and how does it work?
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture.jpg&quot; rel=&quot;attachment wp-att-147020&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;477&quot; sizes=&quot;(max-width: 1303px) 100vw, 1303px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture.jpg 1303w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture-768x281.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-architecture-600x220.jpg 600w&quot; width=&quot;1303&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Kharya explains it this way:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “We basically have an architecture where we are embedding the models, and we are hard coding the models and the weights into our what we call the mask ROM recall fabric, which is paired with an SRAM recall fabric. Together, they are able to store both the model as well as do all the computations of KV cache. We have adapters and customizations – we support all of that. This design allows us to be super-dense in terms of compute and in terms of storage, and we can do compute on that storage incredibly fast, which is what drives density up and cost down.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “In the current generation, our density is 8 billion parameters on the hard wired part of the chip., plus the SRAM to allow us to do KV caches, adaptations like fine tuning, and etc. In our next generation, we would have the ability to go up to 20 billion parameters in a chip. Even with trillions of parameters, we’re talking about few tens of chips, which is a very, very small compared to anything else out there on the market today.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Without being specific about the architecture – Taalas wants it to be a bit of a black box for now – Bajic added this:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “We have got this scheme for the mask ROM recall fabric – the hard-wired part – where we can store four bits away and do the multiply related to it – everything – with a single transistor. So the density is basically insane. And this is not nuclear physics – it is fully digital. It is just a clever trick that we don’t want to broadcast. But once you hardwire everything, you get this opportunity to stuff very differently than if you have to deal with changing things. The important thing is that we can put a weight and do the multiply associated with it all in one transistor. And you know the multipliers are kind of the big boy piece of the computer.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “What we invented is not particularly difficult, either. It’s just a clever thing that nobody saw because nobody went down this path. We showed up more than two years ago, and we wanted to remove the barrier between memory and compute altogether. That was the genesis of this whole thing. Now, the first way we came up with to do it – and basically the only way we could see at the time that would produce a product on a predictable timeline, because we didn’t want to be research profs and three years down the line have something that doesn’t work – was to quickly veered off into this ROM-based approach. We started studying it in detail and then we realized that actually this was even better than we thought.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   “We actually designed all this stuff from scratch internally. We didn’t use off the shelf anything, we did lots of transistor level design, hand layout – basically our whole effort ended up being a throwback to the 1970s.”
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  What is immediately obvious is that every change in model, from say Llama 3.1 to Llama 4, will require a new spin of the HC chip. For now, Taalas is focusing on etching the weights for open source models onto its HC chips, but it is not hard to imagine Anthropic and OpenAI picking up the phone and ordering custom accelerators for their models. Even Google might want to give it a try. By the way,
  &lt;a href=&quot;https://patents.google.com/?inventor=Bajic&amp;assignee=Taalas&amp;oq=Taalas+Bajic&quot;&gt;
   Taalas has filed 14 patents under Bajic to cover its technology as far as we can see
  &lt;/a&gt;
  ; there could be more because patent searching is very bad – even Google Patents.
 &lt;/p&gt;
 &lt;p&gt;
  To etch a new model on an HC inference engine involves changing a two layers of metal in the HC chip design, not a complete scrapping of it. And with the cost of training models running into the billions of dollars, paying a relatively nominal fee to adapt an HC inference engine to a new release of a model or for an entirely different model is not a big deal. Kharya says it costs 100X as much to train a model then to get a customize HC chip in reasonable volumes from Taalas.
 &lt;/p&gt;
 &lt;p&gt;
  Perhaps equally importantly, the time between major model releases is lengthening and people are getting attached to their models – there was plenty of gnashing of teeth when OpenAI moved customers from GPT 4.5 to GPT 5, for instance, because the latest release is a bit sycophantic. Given this, it may make sense to order a few hundred thousand to a few million of the HC inference engines.
 &lt;/p&gt;
 &lt;p&gt;
  With the “foundry optimal workflow” that Taalas has created in conjunction with Taiwan Semiconductor Manufacturing Co, customers can go from model weights to deployable PCI-Express cards, actually doing inference, in two months.
 &lt;/p&gt;
 &lt;p&gt;
  The first generation HC1 chip is implemented in the 6 nanometer N6 process from TSMC. At 815 mm
  &lt;sup&gt;
   2
  &lt;/sup&gt;
  it is pushing up against the reticle limit of chips these days (before we move to High NA processes that will cut the reticle size in half, which is not at all desirable). Each HC1 chip has 53 billion transistors on the package, most of it very likely for ROM and SRAM memory. The HC1 card burns about 200 watts, says Bajic, and a two-socket X86 server with ten HC1 cards in it runs 2,500 watts.
 &lt;/p&gt;
 &lt;p&gt;
  By the way, because the HC1 card is so fast, to get low latency inference does not requiring batching up queries, and that means the bandwidth pressure on the Taalas devices is low. So low that the PCI-Express bus is fine if you want to gang cards up to run a larger model, which Taalas will allow customers to do later this year using pipeline parallelism to spread work across the HC cards. By summer, in fact, it will have a Llama 3.1 model with 20 billion parameters hard coded into an HC chip, and by the end of the year it will have a frontier-class large language model – maybe Llama, maybe DeepSeek, maybe both – running inference across a collection of HC cards. This architecture will be called HC2.
 &lt;/p&gt;
 &lt;p&gt;
  So, just how fast and just how cheap is this Taalas HC1 card? Let’s take a look, starting with the latest throughput for Llama 3.1 8B models as assessed by
  &lt;a href=&quot;https://artificialanalysis.ai/&quot;&gt;
   Artificial Analysis
  &lt;/a&gt;
  :
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-1.jpg&quot; rel=&quot;attachment wp-att-147021&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;464&quot; sizes=&quot;(max-width: 723px) 100vw, 723px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-1.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-1.jpg 723w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-1-600x385.jpg 600w&quot; width=&quot;723&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  These initial performance results for the HC1 were run by Taalas itself, not Artificial Analysis, but you can play around with the chatbot demo
  &lt;a href=&quot;https://chatjimmy.ai/&quot;&gt;
   at this link
  &lt;/a&gt;
  and request developer API access
  &lt;a href=&quot;https://taalas.com/api-request-form/&quot;&gt;
   at this other link
  &lt;/a&gt;
  and run your own tests.
 &lt;/p&gt;
 &lt;p&gt;
  That’s a pretty big gap with a “Blackwell” B200 GPU (which Taalas itself ran), and even a substantial gap with what Grow, SambaNova, and Cerebras can deliver with their SRAM-heavy AI compute engines.
 &lt;/p&gt;
 &lt;p&gt;
  For fun, Taalas took the Llama 3.1 8B and DeepSeek R1 671B models and compared the Nvidia B200 against its HC card. (Our guess is that it took around 35 cards to load up the memory to run DeepSeek R1 671B on the Taalas boxes.) Here is how they stack up:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-2.jpg&quot; rel=&quot;attachment wp-att-147022&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;464&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 723px) 100vw, 723px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-2.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-2.jpg 723w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-2-600x385.jpg 600w&quot; width=&quot;723&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Now, what you want to know is throughput, latency, and cost per token, and this chart brings it all together:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3.jpg&quot; rel=&quot;attachment wp-att-147023&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;517&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1239px) 100vw, 1239px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3.jpg 1239w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3-768x320.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/taalas-hci-performance-3-600x250.jpg 600w&quot; width=&quot;1239&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  On GPU systems, the interactivity – how many users you can support concurrently asking queries and getting fed answers – depends on the latency you want. If you want low latency, you can’t have a lot of users, and if you want lower cost, you have to pay for it with increased latency of tokens processed as input or output.
 &lt;/p&gt;
 &lt;p&gt;
  As you can see, Taalas is showing much lower costs and incredibly lower latencies on these two models tested.
 &lt;/p&gt;
 &lt;p&gt;
  We look forward to independent testing as the HC cards ramp into production, and to see what Taalas will charge for these AI inference engines. This sure looks like a game changer for AI inference.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Some Game Theory On That Nvidia-Meta Platforms Partnership </title>
<link>https://www.nextplatform.com/2026/02/18/some-game-theory-on-that-nvidia-meta-platforms-partnership/#respond</link>
<pubDate>Wed, 18 Feb 2026 20:17:21 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2022/05/meta-facebook-logo-1030x438.jpg&quot; title=&quot;meta-facebook-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  When Meta Platforms does a big AI system deal with Nvidia, that usually means that some other open hardware plan that the company had can’t meet an urgent need for compute. This is not the same thing as falling behind schedule, but it has the same effect. We don’t have a lot of data points on such things, mind you, but now we have the third one with the very big deal announced between the social network and AI model maker and AI hardware juggernaut Nvidia.
 &lt;/p&gt;
 &lt;p&gt;
  This one is a much bigger deal than Meta Platforms cut with Nvidia the last time this happened, worth tens of billions of dollars to Nvidia at the very least, plus whatever vig an original design manufacturer can get out of integrating the Nvidia parts into systems for Meta Platforms.
 &lt;/p&gt;
 &lt;p&gt;
  In the past two cases for sure and likely for the third new case, Meta Platforms has been willing to abandon its own Open Compute Project designs when the AI compute need is pressing enough.
 &lt;/p&gt;
 &lt;p&gt;
  Meta Platforms is a little different among the hyperscalers and the model builders in that is it not just adding AI to search or making generic and powerful models that can compete with the likes of OpenAI and Anthropic, carrying the open source banner high. (At least for now, anyway.) The company also has a large fleet of high performance clusters that are recommendation engines for its various services, and these require tightly coupled CPUs and accelerators to give the latter access to the former’s memory that has been stuffed with high-dimensional embedding vectors that make the recommendations for each of us individually. The “Grace-Hopper” superchip pairing of the CG100 CPU and the H100 GPU accelerator was very much aimed at recommendation engines.
 &lt;/p&gt;
 &lt;p&gt;
  For all we know, Meta Platform has scads of them.
 &lt;/p&gt;
 &lt;p&gt;
  What we know for sure is that despite the desire of Meta Platforms to make its own AI chips, as evidenced by
  &lt;a href=&quot;https://www.nextplatform.com/2024/04/10/with-mtia-v2-chip-meta-can-do-ai-training-as-well-as-inference/&quot;&gt;
   its MTIA AI inference chip design efforts
  &lt;/a&gt;
  as well as
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/02/meta-buys-rivos-to-accelerate-compute-engine-engineering/&quot;&gt;
   its acquisition of RISC-V CPU and GPU maker Rivos
  &lt;/a&gt;
  , is that Meta Platforms has spent big bucks with Nvidia, sometimes for whole systems and other times for allocations of GPUs and NVSwitch interconnects and sometimes scale out InfiniBand networks.
 &lt;/p&gt;
 &lt;p&gt;
  When it became clear that Intel was not going to get
  &lt;a href=&quot;https://www.nextplatform.com/2021/08/24/intels-ponte-vecchio-gpu-better-not-be-a-bridge-too-far/&quot;&gt;
   its “Ponte Vecchio” Max Series GPUs
  &lt;/a&gt;
  into the field in a timely fashion and that
  &lt;a href=&quot;https://www.nextplatform.com/2021/11/09/the-aldebaran-amd-gpu-that-won-exascale/&quot;&gt;
   AMD’s “Aldebaran” MI250X GPU accelerators
  &lt;/a&gt;
  could not ship in significant enough volumes to satisfy all of the social network’s needs, Meta Platforms had no choice but to do a deal with Nvidia for its Research Super Computer, based on Nvidia’s “Ampere” A100 GPUs and significantly not on its then-impending “Hopper” H100 accelerators. The backbreaker for Meta Platforms was that these two GPUs supported the Open Accelerator Module (OAM) socket format created by Microsoft and Meta Platforms, but because of the lack of volume, Meta Platforms had no choice but to not use
  &lt;a href=&quot;https://www.nextplatform.com/2022/10/20/the-iron-that-will-drive-ai-at-meta-platforms/&quot;&gt;
   its homegrown “Grand Teton” CPU-GPU systems.
  &lt;/a&gt;
  Intel’s Gaudi compute engines also supported OAM modules, but Nvidia has its own SXM socket designs and system boards linking out to NVSwitch infrastructure.
 &lt;/p&gt;
 &lt;p&gt;
  And so Nvidia got the deal for the 2,000 node RSC machine, crammed with 4,000 AMD CPUs and 16,000 Nvidia A100 GPU accelerators
  &lt;a href=&quot;https://www.nextplatform.com/2022/01/24/meta-buys-rather-than-builds-and-opens-its-massive-ai-supercomputer/&quot;&gt;
   was done in January 2022 and completed in several phases throughout the year
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  In March 2022, Meta Platforms finally talked about how it was going to invest in A100 and H100 accelerators
  &lt;a href=&quot;https://www.nextplatform.com/2024/03/13/inside-the-massive-gpu-buildout-at-meta-platforms/&quot;&gt;
   to build a fleet with over 500,000 H100 equivalents of performance
  &lt;/a&gt;
  , including two clusters with 24,576 GPUs each based on its Grand Teton server platforms – one using Ethernet from Arista Networks, the other using InfiniBand from Nvidia and explicitly to pit the two switching architectures against each other. And in May, still scrambling for immediate AI capacity, Meta Platforms ironed out
  &lt;a href=&quot;https://www.nextplatform.com/2022/05/25/once-again-meta-buys-rather-than-builds-a-supercomputer/&quot;&gt;
   a deal with Microsoft to buy virtual supercomputer on the Azure cloud
  &lt;/a&gt;
  based on its NDm A100 v4-series instances, which are very similar to the nodes used in the RSC system it had acquired.
 &lt;/p&gt;
 &lt;p&gt;
  Clearly, Meta Platforms did not initially seek out big GPU allocations from Nvidia. But that tune changed pretty fast.
 &lt;/p&gt;
 &lt;p&gt;
  More recently, as Meta Platforms seemed to be trying to decrease its dependence on Nvidia, it launched a homegrown MTIA v2 inference accelerator and it also collaborated with AMD to create
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/14/oracle-first-in-line-for-amd-altair-mi450-gpus-helios-racks/&quot;&gt;
   the “Helios” Open Rack Wide 3 double-wide rack design
  &lt;/a&gt;
  , which is half as dense as the Nvidia “Oberon” racks used in its GB200 NVL72 and GB300 NVL72 rackscale systems, but that might be an asset considering the weight and power density that Nvidia is driving with Oberon and will increase with its future “Kyber” racks.
 &lt;/p&gt;
 &lt;p&gt;
  That Nvidia rack density is driven in large part by the low latency needs of the NVSwitch fabric linking the memories of the 72 GPUs in the rack. The Helios rack has UALink tunneling over Ethernet and the latency of the GPU fabric is a lot higher – and in part because the copper cables in the Helios racks are a bit longer. But the latency was going to be higher and bandwidth was going to be lower for the first generation Helios racks no matter what, just like PCI-Express switchery in earlier AMD and Meta Platforms AI node designs had higher latency and lower bandwidth than the NVSwitchery at the same time.
 &lt;/p&gt;
 &lt;p&gt;
  Under this week’s deal, Meta Platforms will be buying Nvidia CPUs and GPUs as well as porting its FBOSS network operating system to Nvidia’s Spectrum-X switch ASICs and systems. Precise numbers were not given, but Meta Platforms will apparently buy “millions of Nvidia Blackwell and Rubin GPUs,” but if you look at the fine print, some of that GPU capacity will come from GPUs that will be installed in on premises datacenters while the other (unknown) part will be rented on (unnamed) Nvidia cloud partners. That could mean AWS, Microsoft, Google, and Oracle clouds, or it could mean the neoclouds such as CoreWeave, Crusoe, Lambda, Nebius, and others.
 &lt;/p&gt;
 &lt;p&gt;
  The initial deployments will be for GB300 systems – do not assume they are GB300 NVL72 rackscale systems – and that means they are focused on inference with maybe a bit of training. If Meta Platforms is working on large-scale mixture of expert models, then the machines it is getting from Nvidia might be GB300 NVL72 rackscale systems. But we have to believe that Meta Platforms also wants to scale up Grand Teton boxes, or make a modified Grand Teton system that can support the NVL4 nodes that are popular with the HPC crowd or the NVL8 nodes that have been more common in the past and that Grand Teton is a good example of.
 &lt;/p&gt;
 &lt;p&gt;
  You will note that InfiniBand is not mentioned in this announcement. ‘Nuff said. Meta Platforms has apparently made its long-term choice.
 &lt;/p&gt;
 &lt;p&gt;
  The deal also has what Nvidia is calling “the first large-scale, Grace-only deployment,” by which we presume it means Grace-Grace superchips. These 144-core processor pairs, which run at 3.2 GHz, are linked by NVLink ports into a NUMA configuration and deliver 7.6 gigaflops across the SVE vector units integrated into the cores embodied in that superchip.
 &lt;/p&gt;
 &lt;p&gt;
  More than a few HPC clusters that have CPU-only codes use Grace CPUs for a big portion of their hardware.
  &lt;a href=&quot;https://www.nextplatform.com/2023/11/14/will-isambard-4-be-the-uks-first-true-exascale-machine/&quot;&gt;
   The latest “Isambard” machine at the University of Bristol
  &lt;/a&gt;
  and
  &lt;a href=&quot;https://www.nextplatform.com/2024/09/04/tacc-fires-up-vista-bridge-to-future-horizon-supercomputer/&quot;&gt;
   the “Vista” machine at the University of Texas
  &lt;/a&gt;
  are good examples. And the Texas Advanced Computing Center has
  &lt;a href=&quot;https://www.nextplatform.com/2026/02/03/tacc-explores-mixed-precision-and-fp64-emulation-for-hpc-with-horizon/&quot;&gt;
   a big partition comprised of Vera 88-core CPUs in the upcoming “Horizon” supercomputer
  &lt;/a&gt;
  that is being installed now. We think that TACC will have 836,352 cores across 4,752 Vera-Vera superchips for a total of 131.8 petaflops at FP64 precision. That is the biggest CPU-only complex based on Nvidia Arm server chips that we have heard of. Nvidia and Meta Platforms say that they are collaborating on how the latter might deploy Vero-only compute, with a potential to do a big installation in 2027.
 &lt;/p&gt;
 &lt;p&gt;
  Now, what would be fun – and what probably is not going to happen – is that Meta Platforms is working with Nvidia to bring its CPUs, GPUs, DPUs, and switch ASICs to the Helios racks. There is no reason that this could not happen, but it might require an OAM version of the Rubin GPUs and a slightly different Vera GPU design that allows it to link more GPUs to a CPU. (Many of us have questioned why the pairing for Grace-Hopper was one to one and why it was one to two for Grace-Blackwell. For a lot of workloads, it might work best for it to be two to eight – such as the way Meta Platforms likes to do it in its Grand Teton designs and how the DGX and HGX server designs were for many generations of Nvidia GPU system boards.
 &lt;/p&gt;
 &lt;p&gt;
  The amount of money this partnership encompassed was not announced, and very likely because this is a commitment to buy parts from Nvidia and to also buy capacity from clouds and/or neoclouds. A lot depends on the ratio there, and how much room the Meta Platforms operational budget has to go outside of its own datacenters.
 &lt;/p&gt;
 &lt;p&gt;
  Assuming this is a ramping deal – meaning the GPU units grow each year – and that it sums up to 2 million to 3 million units, the value of those GPUs if it was only GB300 compute complexes at a cost in excess of $4 million for a GB300 NVL72 machine, you get somewhere between $110 billion and $167 billion to acquire 2 million to 3 million GPUs. Meta Platforms wants to rent as little capacity as possible because this approach would not use Meta Platforms own datacenters (which it is spending a fortune on as well) and because that rented GPU capacity is 4X to 6X more expensive to rent than it is to buy over a four year term.
 &lt;/p&gt;
 &lt;p&gt;
  Without knowing the ratio of rent to buy that Meta Platforms is planning, we can’t say much. But we can remind you that renting capacity is an operational expense that does not come out of the capital expenses budget, which for 2026 is projected to be $125 billion.
 &lt;/p&gt;
 &lt;p&gt;
  You can see why all of the hyperscalers and cloud builders want their own CPUs and XPUs – including Meta Platforms,
  &lt;a href=&quot;https://www.theregister.com/2025/11/25/nvidia_google_tpu_meta/&quot;&gt;
   which is also rumored to be working on a deal to rent TPU capacity from the search giant and eventually get its hands on its own TPUs for its own systems
  &lt;/a&gt;
  . This deal mirrors one that Anthropic has cut with Google.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Some Game Theory On That Nvidia-Meta Platforms Partnership </title>
<link>https://www.nextplatform.com/2026/02/18/some-game-theory-on-that-nvidia-meta-platforms-partnership/</link>
<pubDate>Wed, 18 Feb 2026 20:17:18 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2022/05/meta-facebook-logo-1030x438.jpg&quot; title=&quot;meta-facebook-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  When Meta Platforms does a big AI system deal with Nvidia, that usually means that some other open hardware plan that the company had can’t meet an urgent need for compute. This is not the same thing as falling behind schedule, but it has the same effect. We don’t have a lot of data points on such things, mind you, but now we have the third one with the very big deal announced between the social network and AI model maker and AI hardware juggernaut Nvidia.
 &lt;/p&gt;
 &lt;p&gt;
  This one is a much bigger deal than Meta Platforms cut with Nvidia the last time this happened, worth tens of billions of dollars to Nvidia at the very least, plus whatever vig an original design manufacturer can get out of integrating the Nvidia parts into systems for Meta Platforms.
 &lt;/p&gt;
 &lt;p&gt;
  In the past two cases for sure and likely for the third new case, Meta Platforms has been willing to abandon its own Open Compute Project designs when the AI compute need is pressing enough.
 &lt;/p&gt;
 &lt;p&gt;
  Meta Platforms is a little different among the hyperscalers and the model builders in that is it not just adding AI to search or making generic and powerful models that can compete with the likes of OpenAI and Anthropic, carrying the open source banner high. (At least for now, anyway.) The company also has a large fleet of high performance clusters that are recommendation engines for its various services, and these require tightly coupled CPUs and accelerators to give the latter access to the former’s memory that has been stuffed with high-dimensional embedding vectors that make the recommendations for each of us individually. The “Grace-Hopper” superchip pairing of the CG100 CPU and the H100 GPU accelerator was very much aimed at recommendation engines.
 &lt;/p&gt;
 &lt;p&gt;
  For all we know, Meta Platform has scads of them.
 &lt;/p&gt;
 &lt;p&gt;
  What we know for sure is that despite the desire of Meta Platforms to make its own AI chips, as evidenced by
  &lt;a href=&quot;https://www.nextplatform.com/2024/04/10/with-mtia-v2-chip-meta-can-do-ai-training-as-well-as-inference/&quot;&gt;
   its MTIA AI inference chip design efforts
  &lt;/a&gt;
  as well as
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/02/meta-buys-rivos-to-accelerate-compute-engine-engineering/&quot;&gt;
   its acquisition of RISC-V CPU and GPU maker Rivos
  &lt;/a&gt;
  , is that Meta Platforms has spent big bucks with Nvidia, sometimes for whole systems and other times for allocations of GPUs and NVSwitch interconnects and sometimes scale out InfiniBand networks.
 &lt;/p&gt;
 &lt;p&gt;
  When it became clear that Intel was not going to get
  &lt;a href=&quot;https://www.nextplatform.com/2021/08/24/intels-ponte-vecchio-gpu-better-not-be-a-bridge-too-far/&quot;&gt;
   its “Ponte Vecchio” Max Series GPUs
  &lt;/a&gt;
  into the field in a timely fashion and that
  &lt;a href=&quot;https://www.nextplatform.com/2021/11/09/the-aldebaran-amd-gpu-that-won-exascale/&quot;&gt;
   AMD’s “Aldebaran” MI250X GPU accelerators
  &lt;/a&gt;
  could not ship in significant enough volumes to satisfy all of the social network’s needs, Meta Platforms had no choice but to do a deal with Nvidia for its Research Super Computer, based on Nvidia’s “Ampere” A100 GPUs and significantly not on its then-impending “Hopper” H100 accelerators. The backbreaker for Meta Platforms was that these two GPUs supported the Open Accelerator Module (OAM) socket format created by Microsoft and Meta Platforms, but because of the lack of volume, Meta Platforms had no choice but to not use
  &lt;a href=&quot;https://www.nextplatform.com/2022/10/20/the-iron-that-will-drive-ai-at-meta-platforms/&quot;&gt;
   its homegrown “Grand Teton” CPU-GPU systems.
  &lt;/a&gt;
  Intel’s Gaudi compute engines also supported OAM modules, but Nvidia has its own SXM socket designs and system boards linking out to NVSwitch infrastructure.
 &lt;/p&gt;
 &lt;p&gt;
  And so Nvidia got the deal for the 2,000 node RSC machine, crammed with 4,000 AMD CPUs and 16,000 Nvidia A100 GPU accelerators
  &lt;a href=&quot;https://www.nextplatform.com/2022/01/24/meta-buys-rather-than-builds-and-opens-its-massive-ai-supercomputer/&quot;&gt;
   was done in January 2022 and completed in several phases throughout the year
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  In March 2022, Meta Platforms finally talked about how it was going to invest in A100 and H100 accelerators
  &lt;a href=&quot;https://www.nextplatform.com/2024/03/13/inside-the-massive-gpu-buildout-at-meta-platforms/&quot;&gt;
   to build a fleet with over 500,000 H100 equivalents of performance
  &lt;/a&gt;
  , including two clusters with 24,576 GPUs each based on its Grand Teton server platforms – one using Ethernet from Arista Networks, the other using InfiniBand from Nvidia and explicitly to pit the two switching architectures against each other. And in May, still scrambling for immediate AI capacity, Meta Platforms ironed out
  &lt;a href=&quot;https://www.nextplatform.com/2022/05/25/once-again-meta-buys-rather-than-builds-a-supercomputer/&quot;&gt;
   a deal with Microsoft to buy virtual supercomputer on the Azure cloud
  &lt;/a&gt;
  based on its NDm A100 v4-series instances, which are very similar to the nodes used in the RSC system it had acquired.
 &lt;/p&gt;
 &lt;p&gt;
  Clearly, Meta Platforms did not initially seek out big GPU allocations from Nvidia. But that tune changed pretty fast.
 &lt;/p&gt;
 &lt;p&gt;
  More recently, as Meta Platforms seemed to be trying to decrease its dependence on Nvidia, it launched a homegrown MTIA v2 inference accelerator and it also collaborated with AMD to create
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/14/oracle-first-in-line-for-amd-altair-mi450-gpus-helios-racks/&quot;&gt;
   the “Helios” Open Rack Wide 3 double-wide rack design
  &lt;/a&gt;
  , which is half as dense as the Nvidia “Oberon” racks used in its GB200 NVL72 and GB300 NVL72 rackscale systems, but that might be an asset considering the weight and power density that Nvidia is driving with Oberon and will increase with its future “Kyber” racks.
 &lt;/p&gt;
 &lt;p&gt;
  That Nvidia rack density is driven in large part by the low latency needs of the NVSwitch fabric linking the memories of the 72 GPUs in the rack. The Helios rack has UALink tunneling over Ethernet and the latency of the GPU fabric is a lot higher – and in part because the copper cables in the Helios racks are a bit longer. But the latency was going to be higher and bandwidth was going to be lower for the first generation Helios racks no matter what, just like PCI-Express switchery in earlier AMD and Meta Platforms AI node designs had higher latency and lower bandwidth than the NVSwitchery at the same time.
 &lt;/p&gt;
 &lt;p&gt;
  Under this week’s deal, Meta Platforms will be buying Nvidia CPUs and GPUs as well as porting its FBOSS network operating system to Nvidia’s Spectrum-X switch ASICs and systems. Precise numbers were not given, but Meta Platforms will apparently buy “millions of Nvidia Blackwell and Rubin GPUs,” but if you look at the fine print, some of that GPU capacity will come from GPUs that will be installed in on premises datacenters while the other (unknown) part will be rented on (unnamed) Nvidia cloud partners. That could mean AWS, Microsoft, Google, and Oracle clouds, or it could mean the neoclouds such as CoreWeave, Crusoe, Lambda, Nebius, and others.
 &lt;/p&gt;
 &lt;p&gt;
  The initial deployments will be for GB300 systems – do not assume they are GB300 NVL72 rackscale systems – and that means they are focused on inference with maybe a bit of training. If Meta Platforms is working on large-scale mixture of expert models, then the machines it is getting from Nvidia might be GB300 NVL72 rackscale systems. But we have to believe that Meta Platforms also wants to scale up Grand Teton boxes, or make a modified Grand Teton system that can support the NVL4 nodes that are popular with the HPC crowd or the NVL8 nodes that have been more common in the past and that Grand Teton is a good example of.
 &lt;/p&gt;
 &lt;p&gt;
  You will note that InfiniBand is not mentioned in this announcement. ‘Nuff said. Meta Platforms has apparently made its long-term choice.
 &lt;/p&gt;
 &lt;p&gt;
  The deal also has what Nvidia is calling “the first large-scale, Grace-only deployment,” by which we presume it means Grace-Grace superchips. These 144-core processor pairs, which run at 3.2 GHz, are linked by NVLink ports into a NUMA configuration and deliver 7.6 gigaflops across the SVE vector units integrated into the cores embodied in that superchip.
 &lt;/p&gt;
 &lt;p&gt;
  More than a few HPC clusters that have CPU-only codes use Grace CPUs for a big portion of their hardware.
  &lt;a href=&quot;https://www.nextplatform.com/2023/11/14/will-isambard-4-be-the-uks-first-true-exascale-machine/&quot;&gt;
   The latest “Isambard” machine at the University of Bristol
  &lt;/a&gt;
  and
  &lt;a href=&quot;https://www.nextplatform.com/2024/09/04/tacc-fires-up-vista-bridge-to-future-horizon-supercomputer/&quot;&gt;
   the “Vista” machine at the University of Texas
  &lt;/a&gt;
  are good examples. And the Texas Advanced Computing Center has
  &lt;a href=&quot;https://www.nextplatform.com/2026/02/03/tacc-explores-mixed-precision-and-fp64-emulation-for-hpc-with-horizon/&quot;&gt;
   a big partition comprised of Vera 88-core CPUs in the upcoming “Horizon” supercomputer
  &lt;/a&gt;
  that is being installed now. We think that TACC will have 836,352 cores across 4,752 Vera-Vera superchips for a total of 131.8 petaflops at FP64 precision. That is the biggest CPU-only complex based on Nvidia Arm server chips that we have heard of. Nvidia and Meta Platforms say that they are collaborating on how the latter might deploy Vero-only compute, with a potential to do a big installation in 2027.
 &lt;/p&gt;
 &lt;p&gt;
  Now, what would be fun – and what probably is not going to happen – is that Meta Platforms is working with Nvidia to bring its CPUs, GPUs, DPUs, and switch ASICs to the Helios racks. There is no reason that this could not happen, but it might require an OAM version of the Rubin GPUs and a slightly different Vera GPU design that allows it to link more GPUs to a CPU. (Many of us have questioned why the pairing for Grace-Hopper was one to one and why it was one to two for Grace-Blackwell. For a lot of workloads, it might work best for it to be two to eight – such as the way Meta Platforms likes to do it in its Grand Teton designs and how the DGX and HGX server designs were for many generations of Nvidia GPU system boards.
 &lt;/p&gt;
 &lt;p&gt;
  The amount of money this partnership encompassed was not announced, and very likely because this is a commitment to buy parts from Nvidia and to also buy capacity from clouds and/or neoclouds. A lot depends on the ratio there, and how much room the Meta Platforms operational budget has to go outside of its own datacenters.
 &lt;/p&gt;
 &lt;p&gt;
  Assuming this is a ramping deal – meaning the GPU units grow each year – and that it sums up to 2 million to 3 million units, the value of those GPUs if it was only GB300 compute complexes at a cost in excess of $4 million for a GB300 NVL72 machine, you get somewhere between $110 billion and $167 billion to acquire 2 million to 3 million GPUs. Meta Platforms wants to rent as little capacity as possible because this approach would not use Meta Platforms own datacenters (which it is spending a fortune on as well) and because that rented GPU capacity is 4X to 6X more expensive to rent than it is to buy over a four year term.
 &lt;/p&gt;
 &lt;p&gt;
  Without knowing the ratio of rent to buy that Meta Platforms is planning, we can’t say much. But we can remind you that renting capacity is an operational expense that does not come out of the capital expenses budget, which for 2026 is projected to be $125 billion.
 &lt;/p&gt;
 &lt;p&gt;
  You can see why all of the hyperscalers and cloud builders want their own CPUs and XPUs – including Meta Platforms,
  &lt;a href=&quot;https://www.theregister.com/2025/11/25/nvidia_google_tpu_meta/&quot;&gt;
   which is also rumored to be working on a deal to rent TPU capacity from the search giant and eventually get its hands on its own TPUs for its own systems
  &lt;/a&gt;
  . This deal mirrors one that Anthropic has cut with Google.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> AI Eats The World, And Most Of Its Flash Storage </title>
<link>https://www.nextplatform.com/2026/02/17/ai-eats-the-world-and-most-of-its-flash-storage/#respond</link>
<pubDate>Tue, 17 Feb 2026 18:43:02 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/solidigm-logo-873x438.jpg&quot; title=&quot;solidigm-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  If you want to be in the DRAM and flash memory markets, you had better enjoy rollercoasters. Because the boom-bust cycles in these businesses are true white-knuckle events.
 &lt;/p&gt;
 &lt;p&gt;
  Just as the GenAI market was having its ChatGPT mainstreaming moment in November 2022, the buildout in both personal and datacenter infrastructure that had been driven by the coronavirus pandemic for nearly three years at that point had not just run out of gas and prices for both DRAM and flash dropped by half or more as demand dried up across the IT sector. The memory and flash players took it on the chin, and inventories piled up sky high.
 &lt;/p&gt;
 &lt;p&gt;
  The hyperscalers, cloud builders, and model builders that are now driving the GenAI boom are these days probably wishing that they had time machines because demand for DRAM and flash memory is now far outstripping supply, and prices are once again going through the roof and up into the clouds.
 &lt;/p&gt;
 &lt;p&gt;
  On the DRAM front, more than half of the servers in the world need to ship with hundreds of gigabytes of HBM stacked memory for the millions of devices that ship. For eight high HBM3 memory stacks it took three DRAM chips for every one that ended up in a working stack to make the stack because the stacking often doesn’t work right and you can’t peel the memory apart in a junk stack and reuse it. The taller the stack, the more difficult it is to get a working stack and the lower the yield, and with each new HBM generation, the inherent yield is also lower. So while HBM is in hot demand, it burns a lot of chips, and that is capacity in the memory fab that might be allocated to high performance DDR5 memory for servers but isn’t.
 &lt;/p&gt;
 &lt;p&gt;
  The flash shortage that is now plaguing the IT sector has a different issue. As with DRAM, capacity at the foundries that make flash – Kioxia, Micron Technology, Samsung, SanDisk (fab partner to Kioxia), Solidigm, and YMTC if you want to count the indigenous Chinese supplier – cannot be ramped up quickly. They rebalance their production to chase the most money when they can. The big issue here is that demand is way outstripping supply.
 &lt;/p&gt;
 &lt;p&gt;
  “You know that 2023 was pretty bad, and was in fact the worst downturn in memory market history,” Greg Matson, head of marketing at Solidigm, tells
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  . At the time, Solidigm’s fattest flash drives were 30 TB and 60 TB in capacity. “At the end of September 2023, products started shipping again and then suddenly in Q1 2024, products started flying off the shelves. At the same time, we build the highest capacity drive, and we thought it might be just a very small portion of our demand. As it turns out, it rapidly became one of the highest growing portions of our demand.”
 &lt;/p&gt;
 &lt;p&gt;
  This was happily perplexing to Solidigm, and its peers in the flash chip and flash drive businesses that no doubt were seeing a rapid ramp of flash storage revenues in 2024, which has continued through 2025 and now into 2026. Those flash makers benefitted nicely from price increases on the order of 50 percent to 70 percent over the past two years (end of 2023 compared to end of 2025) as demand for flash drives has outpaced supply.
 &lt;/p&gt;
 &lt;p&gt;
  What is driving this demand? Tiered storage for what Nvidia calls AI factories and what we still call AI supercomputers. (A datacenter has
  &lt;em&gt;
   always been
  &lt;/em&gt;
  an information factory.) While storage does not dominate the budget of AI supercomputers that are contracted in gigawatt units of capacity these days, storage – and particularly HBM, DRAM, and flash storage – are as important as raw serial, vector, and tensor compute to the AI supercomputing architecture.
 &lt;/p&gt;
 &lt;p&gt;
  Just for fun, Matson walked us through the math that he recently used to explain the current situation to upper management at Solidigm.
 &lt;/p&gt;
 &lt;p&gt;
  The Nvidia AI factory architecture has four tiers of storage, designated by the letter G for reasons that are not obvious but perhaps because everything in the Nvidia universe is subservient to the GPU perhaps that is it. The G1 level is the HBM memory on the GPU accelerator package, and G2 is the DRAM memory on the host server. The recommendation is that G2 be somewhere between 2X to 4X the size of G1 so it can absorb the overflow from G1 for large context windows as AI is processing.
 &lt;/p&gt;
 &lt;p&gt;
  Flash comes into play in the next two tiers of storage. The G3 storage is node-level, which in the case of Nvidia NVL72 machines or AMD Helios racks will be a rackscale node. This G3 tier is used to store the intermediate processing data that is created and checkpointed periodically. This is checkpointing is important because AI supercomputers run with synchronous communications between the GPUs and XPUs in the system, which means if one of them fails, then the calculation – which might take days to months – fails. By checkpointing periodically, you can load intermediate data back into the GPUs and restart the calculation before the point of failure and not have to start the AI training run from the beginning.
 &lt;/p&gt;
 &lt;p&gt;
  With the “Vera” VC100 CPUs and “Rubin” R200 GPUs in
  &lt;a href=&quot;https://www.nextplatform.com/2026/01/05/nvidias-vera-rubin-platform-obsoletes-current-ai-iron-six-months-ahead-of-launch/&quot;&gt;
   the Vera-Rubin platforms coming later this year
  &lt;/a&gt;
  , Nvidia will introduce a new G3.5 tier that is called inference context memory storage, which is basically use BlueField-4 DPUs used as a storage controller and put inside of the node/rack to be even faster as well as to deliver some local processing on the data.
 &lt;/p&gt;
 &lt;p&gt;
  The G4 level of storage in the Nvidia AI supercomputer architecture is the network storage that stores objects and files outside of the node or rackscale system (which is just a big node).
  &lt;a href=&quot;https://www.nextplatform.com/2021/01/21/storage-cant-be-an-afterthought-with-ai-systems/&quot;&gt;
   VAST Data has tweaked its architecture so it can absorb the job of G3 storage for checkpointing
  &lt;/a&gt;
  , which is an interesting architectural choice and one that can save AI system architects some money.
 &lt;/p&gt;
 &lt;p&gt;
  We think the Nvidia architecture should include a G5 level of storage, based on very fat disk drives. This would follow the practices of the hyperscalers and cloud builders, who buy something like 95 percent of the world’s disk drive shipments these days. As far as we know, there is no G5 storage tier in the Nvidia reference architecture.
 &lt;/p&gt;
 &lt;p&gt;
  With that out of the way, let’s do some math on flash storage. For a 1 gigawatt installation using Nvidia “Grace” GC100 CPUs and “Blackwell” B200 or B300 GPUs, depending on who you ask it can power between 500,000 to 600,000 GPUs, depending on the options in the system and the cooling methods used. Matson took 550,000 as an average, which seems reasonable. Nvidia recommends that there is 15 TB per GPU of G3 storage in the nodes for checkpointing and other functions and 30 TB per GPU of external networked storage to hold bulk data.
 &lt;/p&gt;
 &lt;p&gt;
  If you do the math, that’s 8.5 exabytes of internal flash capacity and 16.5 exabytes of networked flash capacity, for a total of 25 exabytes of capacity, for a 1 gigawatt installation.
 &lt;/p&gt;
 &lt;p&gt;
  After poking around on the Internet for estimates and statements by the big GPU and XPU makers, it looks like there were somewhere around 3 million compute engines (but which I mean sockets) shipped in 2023, around 7 million in 2024, and around 10 million in 2025. At 45 TB of flash per GPU/XPU, using Nvidia as a guideline, that means around 135 exabytes of flash was consumed for these AI supercomputers in 2023, around 315 exabytes in 2024, and 450 exabytes in 2025.
 &lt;/p&gt;
 &lt;p&gt;
  That’s a lot of flash. And 2026 will get worse as a lot more demand chases a modestly growing supply, and prices will rise accordingly. The flash chip and flash drive makers are going to be making some big bucks.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> AI Eats The World, And Most Of Its Flash Storage </title>
<link>https://www.nextplatform.com/2026/02/17/ai-eats-the-world-and-most-of-its-flash-storage/</link>
<pubDate>Tue, 17 Feb 2026 18:42:59 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/solidigm-logo-873x438.jpg&quot; title=&quot;solidigm-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  If you want to be in the DRAM and flash memory markets, you had better enjoy rollercoasters. Because the boom-bust cycles in these businesses are true white-knuckle events.
 &lt;/p&gt;
 &lt;p&gt;
  Just as the GenAI market was having its ChatGPT mainstreaming moment in November 2022, the buildout in both personal and datacenter infrastructure that had been driven by the coronavirus pandemic for nearly three years at that point had not just run out of gas and prices for both DRAM and flash dropped by half or more as demand dried up across the IT sector. The memory and flash players took it on the chin, and inventories piled up sky high.
 &lt;/p&gt;
 &lt;p&gt;
  The hyperscalers, cloud builders, and model builders that are now driving the GenAI boom are these days probably wishing that they had time machines because demand for DRAM and flash memory is now far outstripping supply, and prices are once again going through the roof and up into the clouds.
 &lt;/p&gt;
 &lt;p&gt;
  On the DRAM front, more than half of the servers in the world need to ship with hundreds of gigabytes of HBM stacked memory for the millions of devices that ship. For eight high HBM3 memory stacks it took three DRAM chips for every one that ended up in a working stack to make the stack because the stacking often doesn’t work right and you can’t peel the memory apart in a junk stack and reuse it. The taller the stack, the more difficult it is to get a working stack and the lower the yield, and with each new HBM generation, the inherent yield is also lower. So while HBM is in hot demand, it burns a lot of chips, and that is capacity in the memory fab that might be allocated to high performance DDR5 memory for servers but isn’t.
 &lt;/p&gt;
 &lt;p&gt;
  The flash shortage that is now plaguing the IT sector has a different issue. As with DRAM, capacity at the foundries that make flash – Kioxia, Micron Technology, Samsung, SanDisk (fab partner to Kioxia), Solidigm, and YMTC if you want to count the indigenous Chinese supplier – cannot be ramped up quickly. They rebalance their production to chase the most money when they can. The big issue here is that demand is way outstripping supply.
 &lt;/p&gt;
 &lt;p&gt;
  “You know that 2023 was pretty bad, and was in fact the worst downturn in memory market history,” Greg Matson, head of marketing at Solidigm, tells
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  . At the time, Solidigm’s fattest flash drives were 30 TB and 60 TB in capacity. “At the end of September 2023, products started shipping again and then suddenly in Q1 2024, products started flying off the shelves. At the same time, we build the highest capacity drive, and we thought it might be just a very small portion of our demand. As it turns out, it rapidly became one of the highest growing portions of our demand.”
 &lt;/p&gt;
 &lt;p&gt;
  This was happily perplexing to Solidigm, and its peers in the flash chip and flash drive businesses that no doubt were seeing a rapid ramp of flash storage revenues in 2024, which has continued through 2025 and now into 2026. Those flash makers benefitted nicely from price increases on the order of 50 percent to 70 percent over the past two years (end of 2023 compared to end of 2025) as demand for flash drives has outpaced supply.
 &lt;/p&gt;
 &lt;p&gt;
  What is driving this demand? Tiered storage for what Nvidia calls AI factories and what we still call AI supercomputers. (A datacenter has
  &lt;em&gt;
   always been
  &lt;/em&gt;
  an information factory.) While storage does not dominate the budget of AI supercomputers that are contracted in gigawatt units of capacity these days, storage – and particularly HBM, DRAM, and flash storage – are as important as raw serial, vector, and tensor compute to the AI supercomputing architecture.
 &lt;/p&gt;
 &lt;p&gt;
  Just for fun, Matson walked us through the math that he recently used to explain the current situation to upper management at Solidigm.
 &lt;/p&gt;
 &lt;p&gt;
  The Nvidia AI factory architecture has four tiers of storage, designated by the letter G for reasons that are not obvious but perhaps because everything in the Nvidia universe is subservient to the GPU perhaps that is it. The G1 level is the HBM memory on the GPU accelerator package, and G2 is the DRAM memory on the host server. The recommendation is that G2 be somewhere between 2X to 4X the size of G1 so it can absorb the overflow from G1 for large context windows as AI is processing.
 &lt;/p&gt;
 &lt;p&gt;
  Flash comes into play in the next two tiers of storage. The G3 storage is node-level, which in the case of Nvidia NVL72 machines or AMD Helios racks will be a rackscale node. This G3 tier is used to store the intermediate processing data that is created and checkpointed periodically. This is checkpointing is important because AI supercomputers run with synchronous communications between the GPUs and XPUs in the system, which means if one of them fails, then the calculation – which might take days to months – fails. By checkpointing periodically, you can load intermediate data back into the GPUs and restart the calculation before the point of failure and not have to start the AI training run from the beginning.
 &lt;/p&gt;
 &lt;p&gt;
  With the “Vera” VC100 CPUs and “Rubin” R200 GPUs in
  &lt;a href=&quot;https://www.nextplatform.com/2026/01/05/nvidias-vera-rubin-platform-obsoletes-current-ai-iron-six-months-ahead-of-launch/&quot;&gt;
   the Vera-Rubin platforms coming later this year
  &lt;/a&gt;
  , Nvidia will introduce a new G3.5 tier that is called inference context memory storage, which is basically use BlueField-4 DPUs used as a storage controller and put inside of the node/rack to be even faster as well as to deliver some local processing on the data.
 &lt;/p&gt;
 &lt;p&gt;
  The G4 level of storage in the Nvidia AI supercomputer architecture is the network storage that stores objects and files outside of the node or rackscale system (which is just a big node).
  &lt;a href=&quot;https://www.nextplatform.com/2021/01/21/storage-cant-be-an-afterthought-with-ai-systems/&quot;&gt;
   VAST Data has tweaked its architecture so it can absorb the job of G3 storage for checkpointing
  &lt;/a&gt;
  , which is an interesting architectural choice and one that can save AI system architects some money.
 &lt;/p&gt;
 &lt;p&gt;
  We think the Nvidia architecture should include a G5 level of storage, based on very fat disk drives. This would follow the practices of the hyperscalers and cloud builders, who buy something like 95 percent of the world’s disk drive shipments these days. As far as we know, there is no G5 storage tier in the Nvidia reference architecture.
 &lt;/p&gt;
 &lt;p&gt;
  With that out of the way, let’s do some math on flash storage. For a 1 gigawatt installation using Nvidia “Grace” GC100 CPUs and “Blackwell” B200 or B300 GPUs, depending on who you ask it can power between 500,000 to 600,000 GPUs, depending on the options in the system and the cooling methods used. Matson took 550,000 as an average, which seems reasonable. Nvidia recommends that there is 15 TB per GPU of G3 storage in the nodes for checkpointing and other functions and 30 TB per GPU of external networked storage to hold bulk data.
 &lt;/p&gt;
 &lt;p&gt;
  If you do the math, that’s 8.5 exabytes of internal flash capacity and 16.5 exabytes of networked flash capacity, for a total of 25 exabytes of capacity, for a 1 gigawatt installation.
 &lt;/p&gt;
 &lt;p&gt;
  After poking around on the Internet for estimates and statements by the big GPU and XPU makers, it looks like there were somewhere around 3 million compute engines (but which I mean sockets) shipped in 2023, around 7 million in 2024, and around 10 million in 2025. At 45 TB of flash per GPU/XPU, using Nvidia as a guideline, that means around 135 exabytes of flash was consumed for these AI supercomputers in 2023, around 315 exabytes in 2024, and 450 exabytes in 2025.
 &lt;/p&gt;
 &lt;p&gt;
  That’s a lot of flash. And 2026 will get worse as a lot more demand chases a modestly growing supply, and prices will rise accordingly. The flash chip and flash drive makers are going to be making some big bucks.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Current AI Networking Wave Will Be A Tsunami Of Money By 2027 </title>
<link>https://www.nextplatform.com/2026/02/13/the-current-ai-networking-wave-will-be-a-tsunami-of-money-by-2027/#respond</link>
<pubDate>Fri, 13 Feb 2026 21:36:36 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/11/arista-r4-logo-673x438.jpg&quot; title=&quot;arista-r4-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  $230.70. That’s it.
 &lt;/p&gt;
 &lt;p&gt;
  If you take the $34.6 billion that Arista Networks has made in product revenue since it was founded way back in 2004 by Andy Bechtolsheim, David Cheriton, and Kenneth Duda and divide it by the 150 million cumulative ports that it has shipped (with
  &lt;a href=&quot;https://www.theregister.com/2010/04/19/arista_7500_modular_switch/&quot;&gt;
   the product ramp really starting in 2010
  &lt;/a&gt;
  after the company dropped out of stealth mode in 2009) This is a remarkable number give the fact that Arista has tended to ship very expensive ports that often cost $1,000 or more without services on top of them.
 &lt;/p&gt;
 &lt;p&gt;
  It looks like somebody has been getting some very deep discounts on those ports, and it also looks like the big customers using Arista switches are not paying for the network operating system, which represents more than half the value of the switch back in the old days
  &lt;a href=&quot;https://www.nextplatform.com/2015/03/16/facebook-ops-director-on-breaking-open-the-switch/&quot;&gt;
   before we started disaggregating the software from the switch
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  And even if you layer on the costs of the $6.4 billion in services that Arista has sold over that time – call it a decade and a half for the bulk of those services sales – then that still only drives it up to $273.60 per port.
 &lt;/p&gt;
 &lt;p&gt;
  And still, when it comes to high speed ports, Arista has been able to eat a lot of the lunch and some of the dinner of Cisco Systems, and it is Arista’s championship of merchant silicon and its willingness to support the network operating systems created by the hyperscalers and cloud builders that has compelled Cisco to launch its Silicon One effort and also to create special whiteboxes (the Cisco 8000 series) that it can sell against Arista switching and routing iron.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-rev-income-cash.jpg&quot; rel=&quot;attachment wp-att-147012&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;394&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-rev-income-cash.jpg&quot; width=&quot;590&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As far as we can tell from the financial report released by Arista this week, the business of this company is accelerating and it will be breaking through $10 billion a year in sales two years early. (Which is what chief executive officer Jayshree Ullal said could happen a few months ago.) Back end and front end networks in AI systems – the former being used to glue together compute engines, the latter being used to feed data from the outside world into those clusters – are driving revenues and perhaps profits.
 &lt;/p&gt;
 &lt;p&gt;
  But Arista’s nascent campus switching business is hitting its targets and its core cloud and hyperscale switching market – the one that the company was founded to take on – is stable over the long view of a few years, its routing business is growing in the double digits, and its edge business is chugging along. And it looks like these trajectories will keep going out into 2026 and beyond.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-product-services.jpg&quot; rel=&quot;attachment wp-att-147009&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;413&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-product-services.jpg&quot; width=&quot;595&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in December, Arista had a tad under $2.1 billion in product revenues, up 30.3 percent, with services revenues growing at a 21.6 percent clip to hit $392.1 million. Software and services together came to 17.1 percent of revenues, which works out to $425.4 million, up 20.4 percent, and if you back out software subscriptions, you get $33.3 million in the December quarter, up only 7.7 percent.
 &lt;/p&gt;
 &lt;p&gt;
  Like we said, the big customers who have spent a fortune on NOS software and telemetry and management tools for their datacenters are not using Arista’s EOS stack, so the subscription revenue is low. But that does not mean that the vast majority of Arista’s more than 10,000 customers (we think it is very close to 11,000 customers) are not using its Extensible Operating System. It is hard to say if it is a majority by customer or not. What does matter is that Arista is agnostic when it comes to NOSes. In fact, it lets customers pick their NOSes. (You know what they say:
  &lt;em&gt;
   You can pick your friends, you can pick your NOS, but you can’t pick your friend’s NOS. . . .
  &lt;/em&gt;
  )
 &lt;/p&gt;
 &lt;p&gt;
  Total revenue for Q4 2025 was $2.49 billion, up 28.9 percent, operating income was $1.03 billion, up 29.2 percent, and net income was $956 million, up 19.3 percent and representing a very healthy 38.4 percent of revenue (a little less than average, frankly).
 &lt;/p&gt;
 &lt;p&gt;
  Of the slightly more than $9 billion in sales that Arista had for 2025, 48 percent of that came from the “cloud titans,” what we call the hyperscalers and cloud builders. That works out to $4.32 billion, up 28.6 percent compared to 2024’s sales to these elite customers. Enterprise &amp; Financial customers represented 32 percent of the Arista pie in 2025, which works out to $2.88 billion, up 17.6 percent year on year. The service providers and neocloud group (which now includes Apple and Oracle), represented $1.8 billion in sales in 2025, up 51.3 percent.
 &lt;/p&gt;
 &lt;p&gt;
  Arista ended the year with $10.74 billion in the bank, an increase of 29.4 percent from a year ago, and has a revenue backlog of $6.8 billion. It doesn’t have to build expensive datacenters or buy GPUs, so it can actually hang onto its cash hoard and make strategic acquisitions as it sees fit, and spend some of that money to prebuy components like flash and main memory as supplies keep tightening.
 &lt;/p&gt;
 &lt;p&gt;
  Here is how the revenues for 2025 for Arista stack up to the prior dozen years for which we have financial data:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025.jpg&quot; rel=&quot;attachment wp-att-147011&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;286&quot; sizes=&quot;(max-width: 960px) 100vw, 960px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025.jpg 960w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025-768x229.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025-600x179.jpg 600w&quot; width=&quot;960&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Arista went public in 2014, so it is hard to get data that covers 2012 and earlier because it was not in the filing documents for the initial public offering.
 &lt;/p&gt;
 &lt;p&gt;
  What does matter is how the company is growing its revenue per customer and that it has expanded its sales to its key biggest customers, Microsoft and Meta Platforms. It also matters that this year, according to Arista, it may have one or two more customers that comprise more than 10 percent of its revenues, which triggers a reporting requirement due to concentration risk from the US Security and Exchange Commission. Arista was burned when Meta Platforms – then known as Facebook – skipped the 200 Gb/sec Ethernet generation back in 2020 and 2021, as you can see from the numbers in the table above.
 &lt;/p&gt;
 &lt;p&gt;
  Arista only gives the breakdown of sales by customer type and the details for its big customers at the end of every year, so we never know what is going on until it is done. In 2025, Microsoft represented 26 percent of Arista’s sales, or $2.34 billion, which was an increase of 67.2 percent compared to the software and cloud giant’s spending in the prior year. Meta Platforms represented 16 percent of sales for the 2025 year, which works out to $1.44 billion, up 40.9 percent year on year. This is a relatively normal concentration of these two companies over the years since Meta Platforms joined the 10 percent club in 2019. Microsoft has been a 10 percenter for as long as there have been public numbers.
 &lt;/p&gt;
 &lt;p&gt;
  We like to try to figure out the portion of Arista’s revenues and operating income that are derived from datacenter products, and here is how we think that has changed over time:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-rev-op.jpg&quot; rel=&quot;attachment wp-att-147006&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;402&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-rev-op.jpg&quot; width=&quot;600&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Including both products and services, we reckon the datacenter drove $2.36 billion in sales in Q4 2025, up 28.9 percent, and operating income was $981 million, up 29.2 percent and representing 41.5 percent of that revenue.
 &lt;/p&gt;
 &lt;p&gt;
  We also like to break Arista’s sales down by product groups and here is what we think has happened over the past several years based on statements that the company has put out:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-product-groups.jpg&quot; rel=&quot;attachment wp-att-147008&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;403&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-product-groups.jpg&quot; width=&quot;589&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Arista had set a goal for itself of selling at least $1.5 billion in AI networking and at least $800 million in campus switching in 2025, and Ullal said that both of those goals were exceeded. Considering that Ullal also said that Arista would double its AI-related sales to $3.25 billion in 2026, we can back that out and know that it had around $1.63 billion in AI-related sales in 2025. We think Arista did around $815 million in campus switch sales this past year and the company said point blank that it will do somewhere around $1.25 billion in campus gear in 2026.
 &lt;/p&gt;
 &lt;p&gt;
  If you want to see our model’s numbers, shown in the chart above, here they are:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table.jpg&quot; rel=&quot;attachment wp-att-147010&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;379&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 772px) 100vw, 772px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table.jpg 772w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table-768x377.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table-600x295.jpg 600w&quot; width=&quot;772&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  What this means is that the non-AI part of the Arista business will grow by 12.3 percent to $8 billion in 2026 if the company hits its $11.25 billion guidance, and if you back out the campus switching business, then the remainder – cloud switching, routing, edge and branch – will only grow by 7 percent to $6.75 billion. We wonder how much of this is about switching components away from enterprises and towards the AI systems, with memory and flash getting harder and harder to come by. Arista has to keep those accounts to get its volumes, and given that the price increases for components are out of its costs, presumably it can make the hyperscalers and cloud builders make up the difference. Probably not more than that, though.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time.jpg&quot; rel=&quot;attachment wp-att-147013&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;424&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 834px) 100vw, 834px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time.jpg 834w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time-768x390.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time-600x305.jpg 600w&quot; width=&quot;834&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  If you take a longer view, as Ullal &amp; Co do, the total addressable market for Arista just keeps getting bigger as its reaches out into campus, branch, and edge and adds on AI networking as a new category. The company is now chasing a TAM of $105 billion by 2029, which is a hell of a lot bigger than the $60 billion TAM was looking at a little more than two years ago.
 &lt;/p&gt;
 &lt;p&gt;
  The AI networking opportunity is enormous, as seen in the forecast for revenues by port speed for Ethernet switching from Dell’Oro Group:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro.jpg&quot; rel=&quot;attachment wp-att-147007&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;529&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1033px) 100vw, 1033px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro.jpg 1033w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro-768x393.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro-600x307.jpg 600w&quot; width=&quot;1033&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The 1.6 TB/sec Ethernet wave is a tsunami that was a little wave, which will grow considerably this year and be a wall of water as tall as skyscrapers in 2027 and 2028. And The 3.2 Tb/sec wave will start in 2027 and be its own wall on top of 1.6 Tb/sec Ethernet switching gear from 2028 and beyond.
 &lt;/p&gt;
 &lt;p&gt;
  The amazing thing in this forecast is how 100 Gb/sec, 400 Gb/sec, and 800 Gb/sec devices will persist in the revenue stream out to the end of the decade, even as most of the money – being spent by hyperscalers, cloud builders, and model builders – goes towards switches with much faster ports.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Current AI Networking Wave Will Be A Tsunami Of Money By 2027 </title>
<link>https://www.nextplatform.com/2026/02/13/the-current-ai-networking-wave-will-be-a-tsunami-of-money-by-2027/</link>
<pubDate>Fri, 13 Feb 2026 21:36:33 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/11/arista-r4-logo-673x438.jpg&quot; title=&quot;arista-r4-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  $230.70. That’s it.
 &lt;/p&gt;
 &lt;p&gt;
  If you take the $34.6 billion that Arista Networks has made in product revenue since it was founded way back in 2004 by Andy Bechtolsheim, David Cheriton, and Kenneth Duda and divide it by the 150 million cumulative ports that it has shipped (with
  &lt;a href=&quot;https://www.theregister.com/2010/04/19/arista_7500_modular_switch/&quot;&gt;
   the product ramp really starting in 2010
  &lt;/a&gt;
  after the company dropped out of stealth mode in 2009) This is a remarkable number give the fact that Arista has tended to ship very expensive ports that often cost $1,000 or more without services on top of them.
 &lt;/p&gt;
 &lt;p&gt;
  It looks like somebody has been getting some very deep discounts on those ports, and it also looks like the big customers using Arista switches are not paying for the network operating system, which represents more than half the value of the switch back in the old days
  &lt;a href=&quot;https://www.nextplatform.com/2015/03/16/facebook-ops-director-on-breaking-open-the-switch/&quot;&gt;
   before we started disaggregating the software from the switch
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  And even if you layer on the costs of the $6.4 billion in services that Arista has sold over that time – call it a decade and a half for the bulk of those services sales – then that still only drives it up to $273.60 per port.
 &lt;/p&gt;
 &lt;p&gt;
  And still, when it comes to high speed ports, Arista has been able to eat a lot of the lunch and some of the dinner of Cisco Systems, and it is Arista’s championship of merchant silicon and its willingness to support the network operating systems created by the hyperscalers and cloud builders that has compelled Cisco to launch its Silicon One effort and also to create special whiteboxes (the Cisco 8000 series) that it can sell against Arista switching and routing iron.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-rev-income-cash.jpg&quot; rel=&quot;attachment wp-att-147012&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;394&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-rev-income-cash.jpg&quot; width=&quot;590&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As far as we can tell from the financial report released by Arista this week, the business of this company is accelerating and it will be breaking through $10 billion a year in sales two years early. (Which is what chief executive officer Jayshree Ullal said could happen a few months ago.) Back end and front end networks in AI systems – the former being used to glue together compute engines, the latter being used to feed data from the outside world into those clusters – are driving revenues and perhaps profits.
 &lt;/p&gt;
 &lt;p&gt;
  But Arista’s nascent campus switching business is hitting its targets and its core cloud and hyperscale switching market – the one that the company was founded to take on – is stable over the long view of a few years, its routing business is growing in the double digits, and its edge business is chugging along. And it looks like these trajectories will keep going out into 2026 and beyond.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-product-services.jpg&quot; rel=&quot;attachment wp-att-147009&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;413&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-product-services.jpg&quot; width=&quot;595&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in December, Arista had a tad under $2.1 billion in product revenues, up 30.3 percent, with services revenues growing at a 21.6 percent clip to hit $392.1 million. Software and services together came to 17.1 percent of revenues, which works out to $425.4 million, up 20.4 percent, and if you back out software subscriptions, you get $33.3 million in the December quarter, up only 7.7 percent.
 &lt;/p&gt;
 &lt;p&gt;
  Like we said, the big customers who have spent a fortune on NOS software and telemetry and management tools for their datacenters are not using Arista’s EOS stack, so the subscription revenue is low. But that does not mean that the vast majority of Arista’s more than 10,000 customers (we think it is very close to 11,000 customers) are not using its Extensible Operating System. It is hard to say if it is a majority by customer or not. What does matter is that Arista is agnostic when it comes to NOSes. In fact, it lets customers pick their NOSes. (You know what they say:
  &lt;em&gt;
   You can pick your friends, you can pick your NOS, but you can’t pick your friend’s NOS. . . .
  &lt;/em&gt;
  )
 &lt;/p&gt;
 &lt;p&gt;
  Total revenue for Q4 2025 was $2.49 billion, up 28.9 percent, operating income was $1.03 billion, up 29.2 percent, and net income was $956 million, up 19.3 percent and representing a very healthy 38.4 percent of revenue (a little less than average, frankly).
 &lt;/p&gt;
 &lt;p&gt;
  Of the slightly more than $9 billion in sales that Arista had for 2025, 48 percent of that came from the “cloud titans,” what we call the hyperscalers and cloud builders. That works out to $4.32 billion, up 28.6 percent compared to 2024’s sales to these elite customers. Enterprise &amp; Financial customers represented 32 percent of the Arista pie in 2025, which works out to $2.88 billion, up 17.6 percent year on year. The service providers and neocloud group (which now includes Apple and Oracle), represented $1.8 billion in sales in 2025, up 51.3 percent.
 &lt;/p&gt;
 &lt;p&gt;
  Arista ended the year with $10.74 billion in the bank, an increase of 29.4 percent from a year ago, and has a revenue backlog of $6.8 billion. It doesn’t have to build expensive datacenters or buy GPUs, so it can actually hang onto its cash hoard and make strategic acquisitions as it sees fit, and spend some of that money to prebuy components like flash and main memory as supplies keep tightening.
 &lt;/p&gt;
 &lt;p&gt;
  Here is how the revenues for 2025 for Arista stack up to the prior dozen years for which we have financial data:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025.jpg&quot; rel=&quot;attachment wp-att-147011&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;286&quot; sizes=&quot;(max-width: 960px) 100vw, 960px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025.jpg 960w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025-768x229.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-customers-2013-2025-600x179.jpg 600w&quot; width=&quot;960&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Arista went public in 2014, so it is hard to get data that covers 2012 and earlier because it was not in the filing documents for the initial public offering.
 &lt;/p&gt;
 &lt;p&gt;
  What does matter is how the company is growing its revenue per customer and that it has expanded its sales to its key biggest customers, Microsoft and Meta Platforms. It also matters that this year, according to Arista, it may have one or two more customers that comprise more than 10 percent of its revenues, which triggers a reporting requirement due to concentration risk from the US Security and Exchange Commission. Arista was burned when Meta Platforms – then known as Facebook – skipped the 200 Gb/sec Ethernet generation back in 2020 and 2021, as you can see from the numbers in the table above.
 &lt;/p&gt;
 &lt;p&gt;
  Arista only gives the breakdown of sales by customer type and the details for its big customers at the end of every year, so we never know what is going on until it is done. In 2025, Microsoft represented 26 percent of Arista’s sales, or $2.34 billion, which was an increase of 67.2 percent compared to the software and cloud giant’s spending in the prior year. Meta Platforms represented 16 percent of sales for the 2025 year, which works out to $1.44 billion, up 40.9 percent year on year. This is a relatively normal concentration of these two companies over the years since Meta Platforms joined the 10 percent club in 2019. Microsoft has been a 10 percenter for as long as there have been public numbers.
 &lt;/p&gt;
 &lt;p&gt;
  We like to try to figure out the portion of Arista’s revenues and operating income that are derived from datacenter products, and here is how we think that has changed over time:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-rev-op.jpg&quot; rel=&quot;attachment wp-att-147006&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;402&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-rev-op.jpg&quot; width=&quot;600&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Including both products and services, we reckon the datacenter drove $2.36 billion in sales in Q4 2025, up 28.9 percent, and operating income was $981 million, up 29.2 percent and representing 41.5 percent of that revenue.
 &lt;/p&gt;
 &lt;p&gt;
  We also like to break Arista’s sales down by product groups and here is what we think has happened over the past several years based on statements that the company has put out:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-product-groups.jpg&quot; rel=&quot;attachment wp-att-147008&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;403&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-product-groups.jpg&quot; width=&quot;589&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Arista had set a goal for itself of selling at least $1.5 billion in AI networking and at least $800 million in campus switching in 2025, and Ullal said that both of those goals were exceeded. Considering that Ullal also said that Arista would double its AI-related sales to $3.25 billion in 2026, we can back that out and know that it had around $1.63 billion in AI-related sales in 2025. We think Arista did around $815 million in campus switch sales this past year and the company said point blank that it will do somewhere around $1.25 billion in campus gear in 2026.
 &lt;/p&gt;
 &lt;p&gt;
  If you want to see our model’s numbers, shown in the chart above, here they are:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table.jpg&quot; rel=&quot;attachment wp-att-147010&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;379&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 772px) 100vw, 772px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table.jpg 772w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table-768x377.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-revenue-2020-2026-table-600x295.jpg 600w&quot; width=&quot;772&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  What this means is that the non-AI part of the Arista business will grow by 12.3 percent to $8 billion in 2026 if the company hits its $11.25 billion guidance, and if you back out the campus switching business, then the remainder – cloud switching, routing, edge and branch – will only grow by 7 percent to $6.75 billion. We wonder how much of this is about switching components away from enterprises and towards the AI systems, with memory and flash getting harder and harder to come by. Arista has to keep those accounts to get its volumes, and given that the price increases for components are out of its costs, presumably it can make the hyperscalers and cloud builders make up the difference. Probably not more than that, though.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time.jpg&quot; rel=&quot;attachment wp-att-147013&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;424&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 834px) 100vw, 834px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time.jpg 834w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time-768x390.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-tam-over-time-600x305.jpg 600w&quot; width=&quot;834&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  If you take a longer view, as Ullal &amp; Co do, the total addressable market for Arista just keeps getting bigger as its reaches out into campus, branch, and edge and adds on AI networking as a new category. The company is now chasing a TAM of $105 billion by 2029, which is a hell of a lot bigger than the $60 billion TAM was looking at a little more than two years ago.
 &lt;/p&gt;
 &lt;p&gt;
  The AI networking opportunity is enormous, as seen in the forecast for revenues by port speed for Ethernet switching from Dell’Oro Group:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro.jpg&quot; rel=&quot;attachment wp-att-147007&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;529&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1033px) 100vw, 1033px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro.jpg 1033w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro-768x393.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/arista-q4-2025-datacenter-switch-forecast-dell-oro-600x307.jpg 600w&quot; width=&quot;1033&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The 1.6 TB/sec Ethernet wave is a tsunami that was a little wave, which will grow considerably this year and be a wall of water as tall as skyscrapers in 2027 and 2028. And The 3.2 Tb/sec wave will start in 2027 and be its own wall on top of 1.6 Tb/sec Ethernet switching gear from 2028 and beyond.
 &lt;/p&gt;
 &lt;p&gt;
  The amazing thing in this forecast is how 100 Gb/sec, 400 Gb/sec, and 800 Gb/sec devices will persist in the revenue stream out to the end of the decade, even as most of the money – being spent by hyperscalers, cloud builders, and model builders – goes towards switches with much faster ports.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Memory Crunch Pinches Cisco’s Profits </title>
<link>https://www.nextplatform.com/2026/02/12/the-memory-crunch-pinches-ciscos-profits/#respond</link>
<pubDate>Thu, 12 Feb 2026 20:29:51 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics-1030x438.jpg&quot; title=&quot;Cisco-G300_Systems_Optics&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It has taken many years for the AI boom to reach the general ledgers and balance sheets of the world’s largest original equipment manufacturers, and one might say that it has taken particularly long for Cisco Systems, the dominant supplier of switching and routing in the enterprise and traditional telco/service provider spaces as well as a respectable systems supplier with over 90,000 customers using its UCS converged server-switch platforms.
 &lt;/p&gt;
 &lt;p&gt;
  But Cisco has been playing a very long game under chief executive officer Chuck Robbins, starting with rearchitecting and unifying its switch and router ASICs with the Silicon One chips, which it uses in its own gear and which it also makes available as merchant silicon that hyperscalers and cloud builders can acquire and use in their own whitebox gear. The acquisition of Acacia Communications back in 2021 for $4.5 billion also gives Cisco the optical transceivers that appeal to the hyperscalers and cloud builders that are driving the lion’s share of AI spending. And now, Cisco is getting traction as a provider of servers and switching to neoclouds and sovereigns need as they build out their infrastructure for the GenAI boom.
 &lt;/p&gt;
 &lt;p&gt;
  This AI business is still relatively small, but it is growing:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-ai-orders.jpg&quot; rel=&quot;attachment wp-att-147000&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;355&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-ai-orders.jpg&quot; width=&quot;588&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As fiscal 2025 was rolling along and AI orders to hyperscalers and cloud builders were starting to ramp, the company originally expected only about $1 billion in AI orders for fiscal 2025. But when the fiscal year ended last July, it was more than double that – our model says around $2.1 billion. Well, that is as much as Cisco booked in AI orders to these elite customers, and Robbins confirmed that number in a conference call with Wall Street analysts, which let us know that our model was spot on.
  &lt;em&gt;
   So hooray for that!
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  So far this fiscal year, Cisco has $1.3 billion in AI orders with hyperscalers and cloud builders in Q1 F2026 and $2.1 billion in Q2 F2026; the company also confirmed that it had $350 million in additional AI orders to neoclouds, sovereigns, and enterprises.
 &lt;/p&gt;
 &lt;p&gt;
  This is the first time that Robbins &amp; Co broke out the latter category, and we did a little backwards extrapolating that pretty much shows why only know is sales to this latter group becoming material. We think most of that is for a combination of servers and switching sold to these customers, with a fair amount of optical transceivers in the mix. But
  &lt;a href=&quot;https://www.nextplatform.com/2026/01/05/nvidias-vera-rubin-platform-obsoletes-current-ai-iron-six-months-ahead-of-launch/&quot;&gt;
   at $3.35 million for each rack of GB200 NVL72 iron
  &lt;/a&gt;
  (72 GPU sockets per rack) and maybe a little bit less for two racks of less dense DGX NVL8 machinery with a total of 64 GPUs, we are only talking about maybe 100 racks or 224 racks of iron, with either 7,200 or 7,168 GPUs, respectively.
 &lt;/p&gt;
 &lt;p&gt;
  A couple of hundred racks is one HPC supercomputer or one risk quantification cluster at a big bank or, heck, one large Hadoop cluster from a decade ago. This is material for Cisco, which should be commended for being specific. But this had better be the beginning of a very big ramp outside of the hyperscalers and the cloud builders.
 &lt;/p&gt;
 &lt;p&gt;
  Now, of that $2.1 billion booked so far with the hyperscalers and cloud builders, none of that is for systems, which stands to reason since these companies design their own iron and have it custom built by original design manufacturers (ODMs), thereby cutting out the OEMs and their profit margins from the picture. But these hyperscalers and customers do want to buy Cisco Silicon One chips and sometimes switches or routers – the Cisco 8000 whitebox series of switches and routers based on Silicon One are explicitly designed to run the open source SONiC network operating system or custom ones made by the hyperscalers and cloud builders.
 &lt;/p&gt;
 &lt;p&gt;
  In Q2 F2026, Robbins said that of that $2.1 billion in orders from the tech titans, about 60 percent was for switch and router silicon or systems and 40 percent was for optics. Some quarters it is half and half, sometimes it is two thirds to one third, and if you do the math for the trailing twelve months, then there was $2.89 billion in Silicon One stuff sold and $1.93 billion of optics stuff (a lot of it from the Acacia line) sold.
 &lt;/p&gt;
 &lt;p&gt;
  Cisco is telling Wall Street that it will do more than $5 billion in sales of AI stuff to the hyperscalers and cloud builders, to which we say: “Yup, Cisco is low-balling it to be careful again.” Just as it did at the beginning of fiscal 2025. Cisco does not include sales of
  &lt;a href=&quot;https://www.nextplatform.com/2026/02/10/cisco-doubles-up-the-switch-bandwidth-to-take-on-ai-scale-up-and-scale-out/&quot;&gt;
   the just-announced G300 and P200 Silicon One chips and switches and routers based on them
  &lt;/a&gt;
  in this forecast.
 &lt;/p&gt;
 &lt;p&gt;
  Our most pessimistic forecast for Cisco, shows AI orders coming in at around $6.2 billion for Silicon One chips and switches and optics sold to the tech titans in fiscal 2026, which is a factor of 3X growth year on year. Cisco could do another $1 billion in AI orders from neoclouds, sovereigns, and enterprises (
  &lt;a href=&quot;https://www.nextplatform.com/2025/05/14/saudi-arabia-has-the-wealth-and-desire-to-become-an-ai-player/&quot;&gt;
   anchored by Saudi Arabia’s Humain effort
  &lt;/a&gt;
  ).
 &lt;/p&gt;
 &lt;p&gt;
  With that, let’s drill down further into Cisco’s financials for the second quarter of fiscal 2026.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-rev-income-cash.jpg&quot; rel=&quot;attachment wp-att-147004&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;447&quot; sizes=&quot;(max-width: 666px) 100vw, 666px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-rev-income-cash.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-rev-income-cash.jpg 666w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-rev-income-cash-600x403.jpg 600w&quot; width=&quot;666&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter, Cisco brought in $15.35 billion in sales, up 9.7 percent year on year and up 3.1 sequentially. Operating income was $3.78 billion, up 21.5 percent thanks to ongoing cost cutting efforts, and net income was $3.18 billion thanks to an incremental $25 million in other net income compared to a $60 million in extra charges this time last year.
 &lt;/p&gt;
 &lt;p&gt;
  Cisco ended the quarter with $15.78 billion in cash and equivalents in the bank, and had a revenue backlog of $43.4 billion, up 5.1 percent. Cisco’s revenue backlog grows slowly and every once in a while shrinks a bit.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-products-services.jpg&quot; rel=&quot;attachment wp-att-147002&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;377&quot; sizes=&quot;(max-width: 664px) 100vw, 664px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-products-services.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-products-services.jpg 664w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-products-services-600x341.jpg 600w&quot; width=&quot;664&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Sales of services, as you can see, just kind of hums along at Cisco, and products, being a capital expense instead of a recurring operating expense, are choppy as they have been for the past six decades of enterprise computing. Product sales were $11.64 billion, up 13.8 percent, while services revenues were $3.71 billion, down 1.3 percent.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-groups.jpg&quot; rel=&quot;attachment wp-att-147001&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;404&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 656px) 100vw, 656px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-groups.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-groups.jpg 656w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-groups-600x370.jpg 600w&quot; width=&quot;656&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The group that Cisco calls Networking – which includes datacenter switching and routing, campus switching, wireless stuff, servers, and IoT gear – was the star of the quarter, with revenues up 21.1 percent to $8.29 billion. All of the categories mentioned above in the networking group all saw double digit growth year on year.
 &lt;/p&gt;
 &lt;p&gt;
  We wish that Cisco separated out edge and IoT stuff from datacenter stuff, but it no longer does. But we have some past trend data that dates from the Great Recession when Cisco entered the datacenter systems market, and since that time we have been trying to figure out what Cisco’s “real” datacenter business looks like. Here is the trendline for that:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-real-systems.jpg&quot; rel=&quot;attachment wp-att-147003&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;329&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 641px) 100vw, 641px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-real-systems.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-real-systems.jpg 641w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-real-systems-600x308.jpg 600w&quot; width=&quot;641&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As best we can figure, Cisco’s core datacenter business was around $7.47 billion in Q2 F2026, with an operating income of $1.82 billion, representing about 24.4 percent of revenues.
 &lt;/p&gt;
 &lt;p&gt;
  Those operating margins took a bit of a hit thanks to DRAM and flash memory shortages, which are driving up prices. Robbins said that Cisco is changing contracts to have provisions for flex pricing on memory so it can pass through higher costs – memory prices have doubled in a year and probably will triple before the year is out is what we are hearing – to customers. Cisco has been buying up DRAM and flash memory as much as it can, which is why its advanced purchase commitments had another $1.8 billion added in the quarter. That is up 73 percent year on year, and “a big chunk of that,” according to Mark Patterson, Cisco’s chief financial officer.
 &lt;/p&gt;
 &lt;p&gt;
  “As we talk to customers, I think they understand it,” Patterson said on the call about the memory price increases. “They understand this is industry-wide. And I haven’t talked to any customers that are really willing to delay or defer any sort of strategic investments that they are making in technology. And I think there’s no concern that we’ve seen there yet whatsoever.”
 &lt;/p&gt;
 &lt;p&gt;
  Just another way you know the GenAI boom is exceptional. Companies will just defer acquisitions of general purpose machines to buy AI iron, we think. It has happened before.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Memory Crunch Pinches Cisco’s Profits </title>
<link>https://www.nextplatform.com/2026/02/12/the-memory-crunch-pinches-ciscos-profits/</link>
<pubDate>Thu, 12 Feb 2026 20:29:48 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics-1030x438.jpg&quot; title=&quot;Cisco-G300_Systems_Optics&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It has taken many years for the AI boom to reach the general ledgers and balance sheets of the world’s largest original equipment manufacturers, and one might say that it has taken particularly long for Cisco Systems, the dominant supplier of switching and routing in the enterprise and traditional telco/service provider spaces as well as a respectable systems supplier with over 90,000 customers using its UCS converged server-switch platforms.
 &lt;/p&gt;
 &lt;p&gt;
  But Cisco has been playing a very long game under chief executive officer Chuck Robbins, starting with rearchitecting and unifying its switch and router ASICs with the Silicon One chips, which it uses in its own gear and which it also makes available as merchant silicon that hyperscalers and cloud builders can acquire and use in their own whitebox gear. The acquisition of Acacia Communications back in 2021 for $4.5 billion also gives Cisco the optical transceivers that appeal to the hyperscalers and cloud builders that are driving the lion’s share of AI spending. And now, Cisco is getting traction as a provider of servers and switching to neoclouds and sovereigns need as they build out their infrastructure for the GenAI boom.
 &lt;/p&gt;
 &lt;p&gt;
  This AI business is still relatively small, but it is growing:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-ai-orders.jpg&quot; rel=&quot;attachment wp-att-147000&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;355&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-ai-orders.jpg&quot; width=&quot;588&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As fiscal 2025 was rolling along and AI orders to hyperscalers and cloud builders were starting to ramp, the company originally expected only about $1 billion in AI orders for fiscal 2025. But when the fiscal year ended last July, it was more than double that – our model says around $2.1 billion. Well, that is as much as Cisco booked in AI orders to these elite customers, and Robbins confirmed that number in a conference call with Wall Street analysts, which let us know that our model was spot on.
  &lt;em&gt;
   So hooray for that!
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  So far this fiscal year, Cisco has $1.3 billion in AI orders with hyperscalers and cloud builders in Q1 F2026 and $2.1 billion in Q2 F2026; the company also confirmed that it had $350 million in additional AI orders to neoclouds, sovereigns, and enterprises.
 &lt;/p&gt;
 &lt;p&gt;
  This is the first time that Robbins &amp; Co broke out the latter category, and we did a little backwards extrapolating that pretty much shows why only know is sales to this latter group becoming material. We think most of that is for a combination of servers and switching sold to these customers, with a fair amount of optical transceivers in the mix. But
  &lt;a href=&quot;https://www.nextplatform.com/2026/01/05/nvidias-vera-rubin-platform-obsoletes-current-ai-iron-six-months-ahead-of-launch/&quot;&gt;
   at $3.35 million for each rack of GB200 NVL72 iron
  &lt;/a&gt;
  (72 GPU sockets per rack) and maybe a little bit less for two racks of less dense DGX NVL8 machinery with a total of 64 GPUs, we are only talking about maybe 100 racks or 224 racks of iron, with either 7,200 or 7,168 GPUs, respectively.
 &lt;/p&gt;
 &lt;p&gt;
  A couple of hundred racks is one HPC supercomputer or one risk quantification cluster at a big bank or, heck, one large Hadoop cluster from a decade ago. This is material for Cisco, which should be commended for being specific. But this had better be the beginning of a very big ramp outside of the hyperscalers and the cloud builders.
 &lt;/p&gt;
 &lt;p&gt;
  Now, of that $2.1 billion booked so far with the hyperscalers and cloud builders, none of that is for systems, which stands to reason since these companies design their own iron and have it custom built by original design manufacturers (ODMs), thereby cutting out the OEMs and their profit margins from the picture. But these hyperscalers and customers do want to buy Cisco Silicon One chips and sometimes switches or routers – the Cisco 8000 whitebox series of switches and routers based on Silicon One are explicitly designed to run the open source SONiC network operating system or custom ones made by the hyperscalers and cloud builders.
 &lt;/p&gt;
 &lt;p&gt;
  In Q2 F2026, Robbins said that of that $2.1 billion in orders from the tech titans, about 60 percent was for switch and router silicon or systems and 40 percent was for optics. Some quarters it is half and half, sometimes it is two thirds to one third, and if you do the math for the trailing twelve months, then there was $2.89 billion in Silicon One stuff sold and $1.93 billion of optics stuff (a lot of it from the Acacia line) sold.
 &lt;/p&gt;
 &lt;p&gt;
  Cisco is telling Wall Street that it will do more than $5 billion in sales of AI stuff to the hyperscalers and cloud builders, to which we say: “Yup, Cisco is low-balling it to be careful again.” Just as it did at the beginning of fiscal 2025. Cisco does not include sales of
  &lt;a href=&quot;https://www.nextplatform.com/2026/02/10/cisco-doubles-up-the-switch-bandwidth-to-take-on-ai-scale-up-and-scale-out/&quot;&gt;
   the just-announced G300 and P200 Silicon One chips and switches and routers based on them
  &lt;/a&gt;
  in this forecast.
 &lt;/p&gt;
 &lt;p&gt;
  Our most pessimistic forecast for Cisco, shows AI orders coming in at around $6.2 billion for Silicon One chips and switches and optics sold to the tech titans in fiscal 2026, which is a factor of 3X growth year on year. Cisco could do another $1 billion in AI orders from neoclouds, sovereigns, and enterprises (
  &lt;a href=&quot;https://www.nextplatform.com/2025/05/14/saudi-arabia-has-the-wealth-and-desire-to-become-an-ai-player/&quot;&gt;
   anchored by Saudi Arabia’s Humain effort
  &lt;/a&gt;
  ).
 &lt;/p&gt;
 &lt;p&gt;
  With that, let’s drill down further into Cisco’s financials for the second quarter of fiscal 2026.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-rev-income-cash.jpg&quot; rel=&quot;attachment wp-att-147004&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;447&quot; sizes=&quot;(max-width: 666px) 100vw, 666px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-rev-income-cash.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-rev-income-cash.jpg 666w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-rev-income-cash-600x403.jpg 600w&quot; width=&quot;666&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter, Cisco brought in $15.35 billion in sales, up 9.7 percent year on year and up 3.1 sequentially. Operating income was $3.78 billion, up 21.5 percent thanks to ongoing cost cutting efforts, and net income was $3.18 billion thanks to an incremental $25 million in other net income compared to a $60 million in extra charges this time last year.
 &lt;/p&gt;
 &lt;p&gt;
  Cisco ended the quarter with $15.78 billion in cash and equivalents in the bank, and had a revenue backlog of $43.4 billion, up 5.1 percent. Cisco’s revenue backlog grows slowly and every once in a while shrinks a bit.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-products-services.jpg&quot; rel=&quot;attachment wp-att-147002&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;377&quot; sizes=&quot;(max-width: 664px) 100vw, 664px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-products-services.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-products-services.jpg 664w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-products-services-600x341.jpg 600w&quot; width=&quot;664&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Sales of services, as you can see, just kind of hums along at Cisco, and products, being a capital expense instead of a recurring operating expense, are choppy as they have been for the past six decades of enterprise computing. Product sales were $11.64 billion, up 13.8 percent, while services revenues were $3.71 billion, down 1.3 percent.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-groups.jpg&quot; rel=&quot;attachment wp-att-147001&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;404&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 656px) 100vw, 656px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-groups.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-groups.jpg 656w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-groups-600x370.jpg 600w&quot; width=&quot;656&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The group that Cisco calls Networking – which includes datacenter switching and routing, campus switching, wireless stuff, servers, and IoT gear – was the star of the quarter, with revenues up 21.1 percent to $8.29 billion. All of the categories mentioned above in the networking group all saw double digit growth year on year.
 &lt;/p&gt;
 &lt;p&gt;
  We wish that Cisco separated out edge and IoT stuff from datacenter stuff, but it no longer does. But we have some past trend data that dates from the Great Recession when Cisco entered the datacenter systems market, and since that time we have been trying to figure out what Cisco’s “real” datacenter business looks like. Here is the trendline for that:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-real-systems.jpg&quot; rel=&quot;attachment wp-att-147003&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;329&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 641px) 100vw, 641px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-real-systems.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-real-systems.jpg 641w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-q2-f2026-real-systems-600x308.jpg 600w&quot; width=&quot;641&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As best we can figure, Cisco’s core datacenter business was around $7.47 billion in Q2 F2026, with an operating income of $1.82 billion, representing about 24.4 percent of revenues.
 &lt;/p&gt;
 &lt;p&gt;
  Those operating margins took a bit of a hit thanks to DRAM and flash memory shortages, which are driving up prices. Robbins said that Cisco is changing contracts to have provisions for flex pricing on memory so it can pass through higher costs – memory prices have doubled in a year and probably will triple before the year is out is what we are hearing – to customers. Cisco has been buying up DRAM and flash memory as much as it can, which is why its advanced purchase commitments had another $1.8 billion added in the quarter. That is up 73 percent year on year, and “a big chunk of that,” according to Mark Patterson, Cisco’s chief financial officer.
 &lt;/p&gt;
 &lt;p&gt;
  “As we talk to customers, I think they understand it,” Patterson said on the call about the memory price increases. “They understand this is industry-wide. And I haven’t talked to any customers that are really willing to delay or defer any sort of strategic investments that they are making in technology. And I think there’s no concern that we’ve seen there yet whatsoever.”
 &lt;/p&gt;
 &lt;p&gt;
  Just another way you know the GenAI boom is exceptional. Companies will just defer acquisitions of general purpose machines to buy AI iron, we think. It has happened before.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Only A Few AI Platforms Can Survive </title>
<link>https://www.nextplatform.com/2026/02/11/only-a-few-ai-platforms-can-survive/#respond</link>
<pubDate>Thu, 12 Feb 2026 01:59:52 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/11/supermicro-ai-racks-logo-759x438.jpg&quot; title=&quot;supermicro-ai-racks-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It does not happen very often in the history of business that an orthogonal product is invented that almost immediately doubles the revenue pool of a market and has the prospect of tripling it over the next handful of years. But that is precisely what GenAI has done for the information technology sector.
 &lt;/p&gt;
 &lt;p&gt;
  It is an amazing thing to behold, a second wave of computing overlaying and possibly replacing a lot of the functionality of the original wave.
 &lt;/p&gt;
 &lt;p&gt;
  The question is, will this GenAI proliferation just allow the big to just get bigger, or will there be a proliferation of suppliers in the AI stacks of the world, a true Cambrian explosion? We are optimistic at
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  , and favor competition and diversification wherever and whenever it is possible. But economic pressure always forces a consolidation and a die-off, and only a few – we will not call them the strongest – survive. Sometimes it is the small or the clever is the one that survives.
 &lt;/p&gt;
 &lt;p&gt;
  If you look at the ever-increasing amounts of money that are pooling in the AI market, both venture funding for startups and private equity for projects, there has never been a better time to pitch a new technology that has some hope of improving the compute, storage, or networking hardware that is the foundation of large language models. Every day, there seems to be another company that is raking in the dough from investors to provide its variation on the AI theme.
 &lt;/p&gt;
 &lt;p&gt;
  While only a few AI platforms can survive to have a reasonable share of the vast sums that are being thrown at AI infrastructure projects over the long haul, the good news is that the IT sector has learned a few lessons and despite the desire of companies to have vertically integrated stacks, from chips all the way up through racks to software, there are very few such stacks and still others can emerge that will allow of different compute, networking, and storage to plug into these devices.
 &lt;/p&gt;
 &lt;p&gt;
  As was the case with traditional supercomputing for running HPC simulations and models, there is a nationalistic aspect to all of this, too, and hence the idea of data sovereignty and computational sovereignty – having a nation control, through its labs or its indigenous chip and system makers, the hardware supporting its codes – is top of mind outside of the United States, which utterly dominates AI processing in terms of architecture and capacity. But that won’t last forever. The rest of the world will catch up and spend its share, and countries and their companies will often pay a premium to control their own fates and not have projects delayed or denied because a foreign country has decided to deny them access to technology for national security reasons.
 &lt;/p&gt;
 &lt;p&gt;
  It is pretty clear that AI is a national security issue for the major nations of the world. The issue is that designing AI hardware from top to bottom is an expensive proposition, and there is really only one foundry that can make and package the compute engine and networking chips, only three suppliers of HBM and high-end DRAM, and the capacity for main memory and flash memory and advanced packaging is pretty tight.
 &lt;/p&gt;
 &lt;p&gt;
  Given this, it will be very hard for the upstarts to wedge their way into the market, even if they are backed by SoftBank/Arm or a sovereign wealth fund in the Middle East or the South Korean or Japanese government. China can do what it has been doing: Make it up in volume with the chips that SMIC can make and being crappy about getting its hands of memory. Eventually, China will come up with a new memory scheme that it can manufacture – stacked LPDDR memory or Z-Angle memory or a large number of banks of HBM3E memory attached to relatively modest AI compute engines and just scale it all out.
 &lt;/p&gt;
 &lt;p&gt;
  But know this: Platforms always get undercut by newer, cheaper platforms, and in recent decades, distributed computing has been the means of doing this. We have no reason to believe it will not happen again – particularly with the enormous revenues and even more amazing profits that Nvidia is generating. IBM was raking in money hand over fist with the System/360 and then the System/370 mainframes six and then five decades ago, too, and vanquished just about all of its competition in mainframes except a very pesky Amdahl (eventually teamed with Fujitsu) and a persistent Hitachi – both running IBM’s own mainframe software because the antitrust authorities of the world said IBM had to.
 &lt;/p&gt;
 &lt;p&gt;
  This pattern may repeat itself, with governments of the world deciding that the CUDA-X stack has to be able to run on other AI accelerators, or that it is absolutely permissible to create a bug-for-bug compatible clone of an Nvidia GPU. Which would be about the time Nvidia decides to roll out a new Groq-ish, non-GPU architecture for inference some years hence, should this come to pass.
 &lt;/p&gt;
 &lt;p&gt;
  It is not hard to see what is already happening. The hyperscalers and the cloud builders can afford to make their own CPUs and their own AI XPUs, and they can pay the iron price – hell, the gold price – for HBM memory or CoWoS-L packaging or whatever they need to make an accelerator that is definitely less expensive than paying for a “Blackwell” or “Rubin” GPU that can do so much more than an AI XPU. (Run 64-bit simulations, crunch data analytics, do visualization and graphics, etc. . . . ) They started with Arm-based server CPUs several years ago and are moving into AI XPUs. (Meta Platforms seems to have
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/02/meta-buys-rivos-to-accelerate-compute-engine-engineering/&quot;&gt;
   an interest in RISC-V CPUs and GPUs based on its acquisition of Rivos
  &lt;/a&gt;
  last fall, but it is not clear how serious it is. This may be more about acquiring a team than endorsing an architecture.)
 &lt;/p&gt;
 &lt;p&gt;
  The hyperscalers and cloud builders can do this not only because they make money on advertising and media, but because they also need the kind of volumes that make it economical to design and manufacture a compute engine independent of an Intel, AMD, or Nvidia. These companies are, in effect, platform providers akin to the proprietary minicomputer and Unix system makers of days gone by, and in many cases, they have integrated hardware and systems software stacks just like IBM, Sun Microsystems, Digital Equipment, Data General, Hewlett Packard, Siemens, Fujitsu, Hitachi, NEC, and a slew of others used to build. The clouds are OEMs for rental access for IaaS compute and the hyperscalers are OEMs for rental access for SaaS compute and sometimes PaaS compute, in a way.
 &lt;/p&gt;
 &lt;p&gt;
  It is
  &lt;em&gt;
   this
  &lt;/em&gt;
  vertical integration that matters, and any AI compute engine designer that hopes to have a business has to somehow get its devices into an integrated platform like this at a scale that makes it economically feasible. The other kind of vertical integration – being one of many suppliers in an Open Compute  rackscale system or its analogs from AMD, for example – is another way.
 &lt;/p&gt;
 &lt;p&gt;
  But ultimately, this is about volume, and that is why you see Anthropic and Meta Platforms using datacenters full of TPUs or Anthropic and OpenAI using AWS Trainiums. And it is why OpenAI has tested Google TPUs and its still developing its “Titan” inference chip in conjunction with Broadcom to keep the heat on Nvidia and AMD and their respective GPUs and on Google and AWS as well. This is why the Tesla-SpaceX-xAI triumvirate is blowing the dust off its “Dojo” accelerator and promising to do its own AI accelerator. Just saying that is a negotiating tactic to get cheaper GPUs.
 &lt;/p&gt;
 &lt;p&gt;
  Everybody is competing and cooperating with everyone else, because the scale of compute
  &lt;em&gt;
   is the advantage
  &lt;/em&gt;
  right now. And companies will get scale any way they can. And despite what anyone says, the limiting factor is not getting compute engine wafers back from the fab, it is getting HBM memory and the advanced packing to attach it to compute engines and getting the power to turn the on.
 &lt;/p&gt;
 &lt;p&gt;
  Having said all that, we are keeping an eye out for differentiated innovation in both AI models and in XPU architecture to see if some upstart can shake things up.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Only A Few AI Platforms Can Survive </title>
<link>https://www.nextplatform.com/2026/02/11/only-a-few-ai-platforms-can-survive/</link>
<pubDate>Thu, 12 Feb 2026 01:59:49 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/11/supermicro-ai-racks-logo-759x438.jpg&quot; title=&quot;supermicro-ai-racks-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It does not happen very often in the history of business that an orthogonal product is invented that almost immediately doubles the revenue pool of a market and has the prospect of tripling it over the next handful of years. But that is precisely what GenAI has done for the information technology sector.
 &lt;/p&gt;
 &lt;p&gt;
  It is an amazing thing to behold, a second wave of computing overlaying and possibly replacing a lot of the functionality of the original wave.
 &lt;/p&gt;
 &lt;p&gt;
  The question is, will this GenAI proliferation just allow the big to just get bigger, or will there be a proliferation of suppliers in the AI stacks of the world, a true Cambrian explosion? We are optimistic at
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  , and favor competition and diversification wherever and whenever it is possible. But economic pressure always forces a consolidation and a die-off, and only a few – we will not call them the strongest – survive. Sometimes it is the small or the clever is the one that survives.
 &lt;/p&gt;
 &lt;p&gt;
  If you look at the ever-increasing amounts of money that are pooling in the AI market, both venture funding for startups and private equity for projects, there has never been a better time to pitch a new technology that has some hope of improving the compute, storage, or networking hardware that is the foundation of large language models. Every day, there seems to be another company that is raking in the dough from investors to provide its variation on the AI theme.
 &lt;/p&gt;
 &lt;p&gt;
  While only a few AI platforms can survive to have a reasonable share of the vast sums that are being thrown at AI infrastructure projects over the long haul, the good news is that the IT sector has learned a few lessons and despite the desire of companies to have vertically integrated stacks, from chips all the way up through racks to software, there are very few such stacks and still others can emerge that will allow of different compute, networking, and storage to plug into these devices.
 &lt;/p&gt;
 &lt;p&gt;
  As was the case with traditional supercomputing for running HPC simulations and models, there is a nationalistic aspect to all of this, too, and hence the idea of data sovereignty and computational sovereignty – having a nation control, through its labs or its indigenous chip and system makers, the hardware supporting its codes – is top of mind outside of the United States, which utterly dominates AI processing in terms of architecture and capacity. But that won’t last forever. The rest of the world will catch up and spend its share, and countries and their companies will often pay a premium to control their own fates and not have projects delayed or denied because a foreign country has decided to deny them access to technology for national security reasons.
 &lt;/p&gt;
 &lt;p&gt;
  It is pretty clear that AI is a national security issue for the major nations of the world. The issue is that designing AI hardware from top to bottom is an expensive proposition, and there is really only one foundry that can make and package the compute engine and networking chips, only three suppliers of HBM and high-end DRAM, and the capacity for main memory and flash memory and advanced packaging is pretty tight.
 &lt;/p&gt;
 &lt;p&gt;
  Given this, it will be very hard for the upstarts to wedge their way into the market, even if they are backed by SoftBank/Arm or a sovereign wealth fund in the Middle East or the South Korean or Japanese government. China can do what it has been doing: Make it up in volume with the chips that SMIC can make and being crappy about getting its hands of memory. Eventually, China will come up with a new memory scheme that it can manufacture – stacked LPDDR memory or Z-Angle memory or a large number of banks of HBM3E memory attached to relatively modest AI compute engines and just scale it all out.
 &lt;/p&gt;
 &lt;p&gt;
  But know this: Platforms always get undercut by newer, cheaper platforms, and in recent decades, distributed computing has been the means of doing this. We have no reason to believe it will not happen again – particularly with the enormous revenues and even more amazing profits that Nvidia is generating. IBM was raking in money hand over fist with the System/360 and then the System/370 mainframes six and then five decades ago, too, and vanquished just about all of its competition in mainframes except a very pesky Amdahl (eventually teamed with Fujitsu) and a persistent Hitachi – both running IBM’s own mainframe software because the antitrust authorities of the world said IBM had to.
 &lt;/p&gt;
 &lt;p&gt;
  This pattern may repeat itself, with governments of the world deciding that the CUDA-X stack has to be able to run on other AI accelerators, or that it is absolutely permissible to create a bug-for-bug compatible clone of an Nvidia GPU. Which would be about the time Nvidia decides to roll out a new Groq-ish, non-GPU architecture for inference some years hence, should this come to pass.
 &lt;/p&gt;
 &lt;p&gt;
  It is not hard to see what is already happening. The hyperscalers and the cloud builders can afford to make their own CPUs and their own AI XPUs, and they can pay the iron price – hell, the gold price – for HBM memory or CoWoS-L packaging or whatever they need to make an accelerator that is definitely less expensive than paying for a “Blackwell” or “Rubin” GPU that can do so much more than an AI XPU. (Run 64-bit simulations, crunch data analytics, do visualization and graphics, etc. . . . ) They started with Arm-based server CPUs several years ago and are moving into AI XPUs. (Meta Platforms seems to have
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/02/meta-buys-rivos-to-accelerate-compute-engine-engineering/&quot;&gt;
   an interest in RISC-V CPUs and GPUs based on its acquisition of Rivos
  &lt;/a&gt;
  last fall, but it is not clear how serious it is. This may be more about acquiring a team than endorsing an architecture.)
 &lt;/p&gt;
 &lt;p&gt;
  The hyperscalers and cloud builders can do this not only because they make money on advertising and media, but because they also need the kind of volumes that make it economical to design and manufacture a compute engine independent of an Intel, AMD, or Nvidia. These companies are, in effect, platform providers akin to the proprietary minicomputer and Unix system makers of days gone by, and in many cases, they have integrated hardware and systems software stacks just like IBM, Sun Microsystems, Digital Equipment, Data General, Hewlett Packard, Siemens, Fujitsu, Hitachi, NEC, and a slew of others used to build. The clouds are OEMs for rental access for IaaS compute and the hyperscalers are OEMs for rental access for SaaS compute and sometimes PaaS compute, in a way.
 &lt;/p&gt;
 &lt;p&gt;
  It is
  &lt;em&gt;
   this
  &lt;/em&gt;
  vertical integration that matters, and any AI compute engine designer that hopes to have a business has to somehow get its devices into an integrated platform like this at a scale that makes it economically feasible. The other kind of vertical integration – being one of many suppliers in an Open Compute  rackscale system or its analogs from AMD, for example – is another way.
 &lt;/p&gt;
 &lt;p&gt;
  But ultimately, this is about volume, and that is why you see Anthropic and Meta Platforms using datacenters full of TPUs or Anthropic and OpenAI using AWS Trainiums. And it is why OpenAI has tested Google TPUs and its still developing its “Titan” inference chip in conjunction with Broadcom to keep the heat on Nvidia and AMD and their respective GPUs and on Google and AWS as well. This is why the Tesla-SpaceX-xAI triumvirate is blowing the dust off its “Dojo” accelerator and promising to do its own AI accelerator. Just saying that is a negotiating tactic to get cheaper GPUs.
 &lt;/p&gt;
 &lt;p&gt;
  Everybody is competing and cooperating with everyone else, because the scale of compute
  &lt;em&gt;
   is the advantage
  &lt;/em&gt;
  right now. And companies will get scale any way they can. And despite what anyone says, the limiting factor is not getting compute engine wafers back from the fab, it is getting HBM memory and the advanced packing to attach it to compute engines and getting the power to turn the on.
 &lt;/p&gt;
 &lt;p&gt;
  Having said all that, we are keeping an eye out for differentiated innovation in both AI models and in XPU architecture to see if some upstart can shake things up.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Attending GTC? Join us for an exclusive roundtable dinner on AI data platforms </title>
<link>https://www.nextplatform.com/2026/02/11/gtc-ai-data-platform-roundtable-dinner/#respond</link>
<pubDate>Wed, 11 Feb 2026 17:59:28 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;p&gt;
  AI projects don’t fail because models don’t work or GPUs lack performance. They fail because data can’t keep pace.
 &lt;/p&gt;
 &lt;p&gt;
  Enterprise teams have foundation models working. They have GPU capacity. But when they try to scale AI across hybrid and multi-cloud environments, data becomes the bottleneck. Distributed data stays fragmented. Real-time inference stalls waiting for pipelines to deliver. GPUs sit underutilized.
 &lt;/p&gt;
 &lt;p&gt;
  This is the problem keeping infrastructure leaders awake at night.
 &lt;/p&gt;
 &lt;h2&gt;
  Off the Record, No Filter
 &lt;/h2&gt;
 &lt;p&gt;
  During GTC week, Next Platform, The Register, Hammerspace, and NVIDIA are hosting an off-the-record executive roundtable with infrastructure leaders actually operating AI at scale. It will be a candid conversation about what’s breaking and how to fix it and it’s hosted by our very own Tim Prickett Morgan.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.theregister.com/Page/events/2026-03-16/?source=nextplatform&quot;&gt;
   &lt;strong&gt;
    Monday, March 16, 2026 | 6:00pm – 8:30pm | San Jose
   &lt;/strong&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The roundtable will explore where AI initiatives actually break down, how enterprises are enabling real-time inference across hybrid environments, and what effective AI data platforms look like in practice. Conducted under Chatham House Rules to ensure participants can speak freely.
 &lt;/p&gt;
 &lt;p&gt;
  Places are limited. If you’re leading infrastructure, data, or AI initiatives,
  &lt;a href=&quot;https://www.theregister.com/Page/events/2026-03-16/?source=nextplatform&quot;&gt;
   register your interest here
  &lt;/a&gt;
  and we’ll confirm your place.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Attending GTC? Join us for an exclusive roundtable dinner on AI data platforms </title>
<link>https://www.nextplatform.com/2026/02/11/gtc-ai-data-platform-roundtable-dinner/</link>
<pubDate>Wed, 11 Feb 2026 17:59:26 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;p&gt;
  AI projects don’t fail because models don’t work or GPUs lack performance. They fail because data can’t keep pace.
 &lt;/p&gt;
 &lt;p&gt;
  Enterprise teams have foundation models working. They have GPU capacity. But when they try to scale AI across hybrid and multi-cloud environments, data becomes the bottleneck. Distributed data stays fragmented. Real-time inference stalls waiting for pipelines to deliver. GPUs sit underutilized.
 &lt;/p&gt;
 &lt;p&gt;
  This is the problem keeping infrastructure leaders awake at night.
 &lt;/p&gt;
 &lt;h2&gt;
  Off the Record, No Filter
 &lt;/h2&gt;
 &lt;p&gt;
  During GTC week, Next Platform, The Register, Hammerspace, and NVIDIA are hosting an off-the-record executive roundtable with infrastructure leaders actually operating AI at scale. It will be a candid conversation about what’s breaking and how to fix it and it’s hosted by our very own Tim Prickett Morgan.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.theregister.com/Page/events/2026-03-16/?source=nextplatform&quot;&gt;
   &lt;strong&gt;
    Monday, March 16, 2026 | 6:00pm – 8:30pm | San Jose
   &lt;/strong&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The roundtable will explore where AI initiatives actually break down, how enterprises are enabling real-time inference across hybrid environments, and what effective AI data platforms look like in practice. Conducted under Chatham House Rules to ensure participants can speak freely.
 &lt;/p&gt;
 &lt;p&gt;
  Places are limited. If you’re leading infrastructure, data, or AI initiatives,
  &lt;a href=&quot;https://www.theregister.com/Page/events/2026-03-16/?source=nextplatform&quot;&gt;
   register your interest here
  &lt;/a&gt;
  and we’ll confirm your place.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Cisco Doubles Up The Switch Bandwidth To Take On AI Scale Up And Scale Out </title>
<link>https://www.nextplatform.com/2026/02/10/cisco-doubles-up-the-switch-bandwidth-to-take-on-ai-scale-up-and-scale-out/#respond</link>
<pubDate>Tue, 10 Feb 2026 18:47:09 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-g300-logo-1030x438.jpg&quot; title=&quot;Cisco-g300-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  In the modern AI datacenter – really,
  &lt;a href=&quot;https://www.nextplatform.com/2018/10/16/a-new-datacenter-compels-arm-to-create-a-new-chip-line/&quot;&gt;
   a data galaxy at this point
  &lt;/a&gt;
  because AI processing needs have broken well beyond the bounds of a single datacenter or even multiple datacenters in a region in a few extreme cases – has two pinch points in the network. There is the datacenter interconnect that creates a router backbone to lash multiple datacenters together into a single working compute complex, and then there is the back-end network that creates a single memory domain across dozens to someday hundreds or thousands of GPUs or XPUs as the most useful granularity for mixture of expert training and inference.
 &lt;/p&gt;
 &lt;p&gt;
  Last fall,
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/08/cisco-takes-on-broadcom-nvidia-for-fat-ai-datacenter-interconnects/&quot;&gt;
   Cisco took care of the datacenter interconnect with its “Dark Pyramid” P200 router chip
  &lt;/a&gt;
  , part of the ever-embiggening and enfattening Silicon One product line, which is used by Cisco in its own switches and routers as well as by hyperscalers and cloud builders in their custom gear.
 &lt;/p&gt;
 &lt;p&gt;
  And this week, at the Cisco Live conference in Amsterdam, Cisco is ratcheting up the bandwidth on the switch side with its G300 ASIC, which delivers 102.4 Tb/sec of aggregate bandwidth and which is meant to take on Broadcom and Nvidia in the rush towards 1.6 Tb/sec ports to link GPUs and XPUs in back-end networks as well as goose the front end networks with 800 Gb/sec ports delivered through higher radix switches that flatten the network and radically lower the cost of it, too.
 &lt;/p&gt;
 &lt;p&gt;
  We don’t know the codename of the G300 chip, but we do know that it doubles up on the basic feeds and speeds of
  &lt;a href=&quot;https://www.nextplatform.com/2023/06/22/cisco-guns-for-infiniband-with-silicon-one-g200/&quot;&gt;
   the G200 switch ASIC that launched in June 2023
  &lt;/a&gt;
  and that was explicitly brought to market to take on InfiniBand in scale-out networks for AI and HPC clusters.
  &lt;a href=&quot;https://www.nextplatform.com/2025/05/21/silicon-one-g200-finally-drives-ciscos-ai-networking-business/&quot;&gt;
   The G200 started getting traction in this work back in May 2025
  &lt;/a&gt;
  as a more scalable and cheaper alternative to InfiniBand, but make no mistake: Nvidia is still selling a hell of a lot of InfiniBand scale out networks and has made a fortune off of NVSwitch interconnects for back-end, scale-up networks for linking GPU memories.
 &lt;/p&gt;
 &lt;p&gt;
  The G200 and now G300 ASICs are explicitly designed to deliver a much-improved and stripped-down Ethernet that can do what InfiniBand does well – high bandwidth, low latency, adaptive routing, and congestion control – while at the same time preserving many Ethernet advantages that are lacking in InfiniBand – security, microsegmentation, and multiple vendors competing against each other. InfiniBand has one source, and this has become a sticking point for some.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements.jpg&quot; rel=&quot;attachment wp-att-146986&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;516&quot; sizes=&quot;(max-width: 1208px) 100vw, 1208px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements.jpg 1208w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements-768x328.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements-600x256.jpg 600w&quot; width=&quot;1208&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The G300 is coming out today along with new Nexus 9000 and Cisco 8000 systems, new pluggable optics modules that plug into them, and network operating system and control plane updates to make it easier to manage larger scale-up networks or beefier front-end networks that feed into AI systems using G300-based switches.
 &lt;/p&gt;
 &lt;p&gt;
  Cisco has not yet provided a technical deep dive on the G300 ASIC, so we cannot share a block diagram of the chip or the schematics of the G300 switches. But we do have some basic feeds and speeds and insight from
  &lt;a href=&quot;https://www.linkedin.com/in/rakesh-chopra/&quot;&gt;
   Rakesh Chopra
  &lt;/a&gt;
  , senior vice president and Cisco Fellow in charge of Silicon One development.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs.jpg&quot; rel=&quot;attachment wp-att-146985&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;502&quot; sizes=&quot;(max-width: 1141px) 100vw, 1141px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs.jpg 1141w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs-768x338.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs-600x264.jpg 600w&quot; width=&quot;1141&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Like the G200, the G300 has 512 SerDes circuit blocks for creating the ports, all wrapped around a packet processing engine. Both also have a single Ethernet MAC address per SerDes, which allows ports and MACs to be mapped at a one to one ratio. (Many switch ASICs have half as many MAC addresses per SerDes, which is fine if you are aggregating SerDes to create ports of a given bandwidth. But what if you want to cut it down to make a high radix switch with a lot more lower bandwidth ports? Well, you run out of MAC addresses.)
 &lt;/p&gt;
 &lt;p&gt;
  And like the P200 that was announced last fall, the G300 is what is called a “lidless” chip design, which means the packaging lid has been removed so that heat sinks for air cooling and blocks for liquid cooling can be mounted directly on the chip and more efficiently remove its heat.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/2022/10/19/cisco-figures-out-how-to-sell-switches-to-hyperscalers-and-enterprises-alike/#:~:text=Foundational%20to%20what%20Cisco%20is,with%20Broadcom%20among%20these%20customers.&quot;&gt;
   The 25.6 Tb/sec G100 switch ASIC from October 2022
  &lt;/a&gt;
  was etched in 7 nanometer processes from Taiwan Semiconductor Manufacturing Co, and the G200 from the summer of 2023 was etched in 5 nanometer processes from Taiwan Semiconductor Manufacturing Co. With an SRAM cache buffer of 252 MB on the G300, which we think is at least 2X higher than the buffer in the G200, and a constant number of faster SerDes, we think Cisco has chosen a mix of processes from TSMC to etch the G300. The G300 is a multichip design, and if you forced us to guess, we would say that the packet processing engine chip block and its SRAM buffer were made using 3 nanometer processes and the SerDes chiplets wrapping around it were etched in the reasonably mature 4 nanometer refinement of the 5 nanometer process from TSMC. It is harder to shrink signaling circuits than it is to shrink computational circuits, and it also turns out to be more expensive to use 3 nanometer than 4 nanometer technology.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic.jpg&quot; rel=&quot;attachment wp-att-146987&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1053&quot; sizes=&quot;(max-width: 1609px) 100vw, 1609px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic.jpg 1609w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic-768x503.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic-1536x1005.jpg 1536w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic-600x393.jpg 600w&quot; width=&quot;1609&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  That 252 MB buffer is a single, unified, shared buffer for all 512 SerDes, and is not segmented to support groups of SerDes on a switch ASIC as is the case with some other designs, says Chopra. So not only is it bigger, but it is a single shared memory for all the SerDes, which improves the efficiency of operations, particularly when there is a lot of congestion in the network and the buffer is used to keep Ethernet from dropping packets on the floor and letting them shatter.
 &lt;/p&gt;
 &lt;p&gt;
  That deep buffer is married to a set of on-chip hardware load balancing agents that can look inside the switch and understand what is flowing through the switch and create a map of flows, with their congestion and transmission failures, how to optimize traffic across all of the G300s in the network. This software is algorithmic, but it is not, technically speaking, AI. And, like all Silicon One chips, the G300 is fully programmable in the P4 network programming language and new features and functions can be created on the fly as they emerge in the market.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics.jpg&quot; rel=&quot;attachment wp-att-146982&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1247&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 2047px) 100vw, 2047px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics.jpg 2047w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics-768x468.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics-1536x936.jpg 1536w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics-600x366.jpg 600w&quot; width=&quot;2047&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The SerDes used in the G300 are designed by Cisco and deliver 224 Gb/sec before encoding and 200 Gb/sec after encoding. (We suspect that the native signaling clock of the SerDes is 112 Gb/sec and it gets doubled up to 224 Gb/sec using PAM4 modulation, which can pump two bits per signal.) This is the first 200 Gb/sec SerDes to come from Cisco, by the way.
 &lt;/p&gt;
 &lt;p&gt;
  If you wanted to, you could create a 512-port switch with each port running at 200 Gb/sec using the G300 chip, or one with 256 ports running at 400 Gb/sec, 128 ports running at 800 Gb/sec, or 64 ports running at what still sounds like an amazing 1.6 Tb/sec.
 &lt;/p&gt;
 &lt;p&gt;
  By the way, the G300 can drive linear pluggable optics (LPO) directly at 800 Gb/sec per port and Cisco has an LPO module that marries to switches using the G300. Cisco also has its own 1.6 Tb/sec OSFP pluggable optics module for those choosing to use the G300 to drive 1.6 Tb/sec ports. This are optics modules are based on Cisco’s own silicon, not stuff brought in by third parties. (Nvidia and Broadcom are also one-stop shops for these components.) But in an interesting twist, Cisco is also building pluggable modules based on third party DSPs for customers who need choice or desire second sourcing to thin out the risk across a wider supply chain.
 &lt;/p&gt;
 &lt;p&gt;
  By moving to LPO, there is a huge power savings, which can in turn allow more power to be allocated to compute engines. Chopra says that it is about a 50 percent power savings for the optics and about a 30 percent savings across the switch infrastructure in an AI cluster. This is a big deal.
 &lt;/p&gt;
 &lt;p&gt;
  Some customers need higher bandwidth for forthcoming GPUs and XPUs that are expected in the second half of 2026, so they will spend the extra power to get 1.6 Tb/sec ports. Other customers are hanging back and 800 Gb/sec is fine and therefore the lower-power LPO modules are the better option.
 &lt;/p&gt;
 &lt;p&gt;
  When you add it all up and normalize for bandwidth, the G300 delivers 33 percent higher network utilization and 28 percent faster job completion time than the G200 and many of its competitors out there in the field, says Chopra.
 &lt;/p&gt;
 &lt;p&gt;
  “Of the big things in this announcement, we have the raw technology, but how we actually test and qualify our optics is completely unique,” brags Chopra. “We have a vastly more comprehensive test structure than others in the industry. And the reason is that AI workloads are synchronous, and a single failure of an optical module is not like a failure in the front end network. It forces a AI job restart – you have to go back to a checkpoint and restart.”
 &lt;/p&gt;
 &lt;p&gt;
  Cisco is putting the G300 in a number of devices. For air cooled devices, the G300 is in the Nexus N9364-SG3 running the company’s own NX-OS operating system and the Cisco 8133 running the open source SONiC network operating system.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches.jpg&quot; rel=&quot;attachment wp-att-146989&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;520&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1220px) 100vw, 1220px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches.jpg 1220w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches-768x327.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches-1030x438.jpg 1030w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches-600x256.jpg 600w&quot; width=&quot;1220&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  These switches take up 3U of rack space and have 64 ports running at 1.6 Tb/sec.
 &lt;/p&gt;
 &lt;p&gt;
  If you want to go more compact and have liquid cooling as well as an Open Rack setup, then there is the Nexus N9363-SG2 running NX-OS and the Cisco 8132 running SONiC. This is a 21-inch wide unit that adheres to the Orv3N specification from the Open Compute Project.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s a closer look at the water cooled switch:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-scaled.jpg&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;853&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 2560px) 100vw, 2560px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-scaled.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-scaled.jpg 2560w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-768x256.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-1536x512.jpg 1536w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-2048x682.jpg 2048w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-600x200.jpg 600w&quot; width=&quot;2560&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
 &lt;/p&gt;
 &lt;p&gt;
 &lt;/p&gt;
 &lt;p&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The upshot of this is that if you want to provide an aggregate of 102.4 Tb/sec of connectivity between GPUs in a scale up network or across server nodes and their GPUs linking to each other across a scale out network, it would take six G200 devices cross-coupled and now it only takes one G300.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation.jpg&quot; rel=&quot;attachment wp-att-146988&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;507&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1220px) 100vw, 1220px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation.jpg 1220w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation-768x319.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation-600x249.jpg 600w&quot; width=&quot;1220&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This also probably means the G300 can cost three or four times as much and still feel like a bargain compared to the G200. We have no idea what it actually costs, but the old ways before AI came along were for the new ASIC to cost somewhere around 1.5X to deliver 2X the bandwidth per ASIC unit.
 &lt;/p&gt;
 &lt;p&gt;
  That 70 percent better power efficiency is a big, big deal when the companies driving the GenAI revolution are buying capacity in blocks of gigawatts.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update.jpg&quot; rel=&quot;attachment wp-att-146990&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;565&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1220px) 100vw, 1220px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update.jpg 1220w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update-768x356.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update-600x278.jpg 600w&quot; width=&quot;1220&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In addition to the G300 and its switches and optics, Cisco this week is also fleshing out the lineup of P200 devices. Last fall, Cisco debuted the P200 in a 51.2 Tb/sec router for DCI links with 64 800 Gb/sec ports, and now it is delivering the P200 in commercial-grade Nexus gear as well as in line cards for its Nexus and whitebox modular switches.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Cisco Doubles Up The Switch Bandwidth To Take On AI Scale Up And Scale Out </title>
<link>https://www.nextplatform.com/2026/02/10/cisco-doubles-up-the-switch-bandwidth-to-take-on-ai-scale-up-and-scale-out/</link>
<pubDate>Tue, 10 Feb 2026 18:47:06 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-g300-logo-1030x438.jpg&quot; title=&quot;Cisco-g300-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  In the modern AI datacenter – really,
  &lt;a href=&quot;https://www.nextplatform.com/2018/10/16/a-new-datacenter-compels-arm-to-create-a-new-chip-line/&quot;&gt;
   a data galaxy at this point
  &lt;/a&gt;
  because AI processing needs have broken well beyond the bounds of a single datacenter or even multiple datacenters in a region in a few extreme cases – has two pinch points in the network. There is the datacenter interconnect that creates a router backbone to lash multiple datacenters together into a single working compute complex, and then there is the back-end network that creates a single memory domain across dozens to someday hundreds or thousands of GPUs or XPUs as the most useful granularity for mixture of expert training and inference.
 &lt;/p&gt;
 &lt;p&gt;
  Last fall,
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/08/cisco-takes-on-broadcom-nvidia-for-fat-ai-datacenter-interconnects/&quot;&gt;
   Cisco took care of the datacenter interconnect with its “Dark Pyramid” P200 router chip
  &lt;/a&gt;
  , part of the ever-embiggening and enfattening Silicon One product line, which is used by Cisco in its own switches and routers as well as by hyperscalers and cloud builders in their custom gear.
 &lt;/p&gt;
 &lt;p&gt;
  And this week, at the Cisco Live conference in Amsterdam, Cisco is ratcheting up the bandwidth on the switch side with its G300 ASIC, which delivers 102.4 Tb/sec of aggregate bandwidth and which is meant to take on Broadcom and Nvidia in the rush towards 1.6 Tb/sec ports to link GPUs and XPUs in back-end networks as well as goose the front end networks with 800 Gb/sec ports delivered through higher radix switches that flatten the network and radically lower the cost of it, too.
 &lt;/p&gt;
 &lt;p&gt;
  We don’t know the codename of the G300 chip, but we do know that it doubles up on the basic feeds and speeds of
  &lt;a href=&quot;https://www.nextplatform.com/2023/06/22/cisco-guns-for-infiniband-with-silicon-one-g200/&quot;&gt;
   the G200 switch ASIC that launched in June 2023
  &lt;/a&gt;
  and that was explicitly brought to market to take on InfiniBand in scale-out networks for AI and HPC clusters.
  &lt;a href=&quot;https://www.nextplatform.com/2025/05/21/silicon-one-g200-finally-drives-ciscos-ai-networking-business/&quot;&gt;
   The G200 started getting traction in this work back in May 2025
  &lt;/a&gt;
  as a more scalable and cheaper alternative to InfiniBand, but make no mistake: Nvidia is still selling a hell of a lot of InfiniBand scale out networks and has made a fortune off of NVSwitch interconnects for back-end, scale-up networks for linking GPU memories.
 &lt;/p&gt;
 &lt;p&gt;
  The G200 and now G300 ASICs are explicitly designed to deliver a much-improved and stripped-down Ethernet that can do what InfiniBand does well – high bandwidth, low latency, adaptive routing, and congestion control – while at the same time preserving many Ethernet advantages that are lacking in InfiniBand – security, microsegmentation, and multiple vendors competing against each other. InfiniBand has one source, and this has become a sticking point for some.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements.jpg&quot; rel=&quot;attachment wp-att-146986&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;516&quot; sizes=&quot;(max-width: 1208px) 100vw, 1208px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements.jpg 1208w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements-768x328.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-stack-announcements-600x256.jpg 600w&quot; width=&quot;1208&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The G300 is coming out today along with new Nexus 9000 and Cisco 8000 systems, new pluggable optics modules that plug into them, and network operating system and control plane updates to make it easier to manage larger scale-up networks or beefier front-end networks that feed into AI systems using G300-based switches.
 &lt;/p&gt;
 &lt;p&gt;
  Cisco has not yet provided a technical deep dive on the G300 ASIC, so we cannot share a block diagram of the chip or the schematics of the G300 switches. But we do have some basic feeds and speeds and insight from
  &lt;a href=&quot;https://www.linkedin.com/in/rakesh-chopra/&quot;&gt;
   Rakesh Chopra
  &lt;/a&gt;
  , senior vice president and Cisco Fellow in charge of Silicon One development.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs.jpg&quot; rel=&quot;attachment wp-att-146985&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;502&quot; sizes=&quot;(max-width: 1141px) 100vw, 1141px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs.jpg 1141w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs-768x338.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-specs-600x264.jpg 600w&quot; width=&quot;1141&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Like the G200, the G300 has 512 SerDes circuit blocks for creating the ports, all wrapped around a packet processing engine. Both also have a single Ethernet MAC address per SerDes, which allows ports and MACs to be mapped at a one to one ratio. (Many switch ASICs have half as many MAC addresses per SerDes, which is fine if you are aggregating SerDes to create ports of a given bandwidth. But what if you want to cut it down to make a high radix switch with a lot more lower bandwidth ports? Well, you run out of MAC addresses.)
 &lt;/p&gt;
 &lt;p&gt;
  And like the P200 that was announced last fall, the G300 is what is called a “lidless” chip design, which means the packaging lid has been removed so that heat sinks for air cooling and blocks for liquid cooling can be mounted directly on the chip and more efficiently remove its heat.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/2022/10/19/cisco-figures-out-how-to-sell-switches-to-hyperscalers-and-enterprises-alike/#:~:text=Foundational%20to%20what%20Cisco%20is,with%20Broadcom%20among%20these%20customers.&quot;&gt;
   The 25.6 Tb/sec G100 switch ASIC from October 2022
  &lt;/a&gt;
  was etched in 7 nanometer processes from Taiwan Semiconductor Manufacturing Co, and the G200 from the summer of 2023 was etched in 5 nanometer processes from Taiwan Semiconductor Manufacturing Co. With an SRAM cache buffer of 252 MB on the G300, which we think is at least 2X higher than the buffer in the G200, and a constant number of faster SerDes, we think Cisco has chosen a mix of processes from TSMC to etch the G300. The G300 is a multichip design, and if you forced us to guess, we would say that the packet processing engine chip block and its SRAM buffer were made using 3 nanometer processes and the SerDes chiplets wrapping around it were etched in the reasonably mature 4 nanometer refinement of the 5 nanometer process from TSMC. It is harder to shrink signaling circuits than it is to shrink computational circuits, and it also turns out to be more expensive to use 3 nanometer than 4 nanometer technology.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic.jpg&quot; rel=&quot;attachment wp-att-146987&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1053&quot; sizes=&quot;(max-width: 1609px) 100vw, 1609px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic.jpg 1609w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic-768x503.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic-1536x1005.jpg 1536w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-asic-600x393.jpg 600w&quot; width=&quot;1609&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  That 252 MB buffer is a single, unified, shared buffer for all 512 SerDes, and is not segmented to support groups of SerDes on a switch ASIC as is the case with some other designs, says Chopra. So not only is it bigger, but it is a single shared memory for all the SerDes, which improves the efficiency of operations, particularly when there is a lot of congestion in the network and the buffer is used to keep Ethernet from dropping packets on the floor and letting them shatter.
 &lt;/p&gt;
 &lt;p&gt;
  That deep buffer is married to a set of on-chip hardware load balancing agents that can look inside the switch and understand what is flowing through the switch and create a map of flows, with their congestion and transmission failures, how to optimize traffic across all of the G300s in the network. This software is algorithmic, but it is not, technically speaking, AI. And, like all Silicon One chips, the G300 is fully programmable in the P4 network programming language and new features and functions can be created on the fly as they emerge in the market.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics.jpg&quot; rel=&quot;attachment wp-att-146982&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1247&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 2047px) 100vw, 2047px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics.jpg 2047w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics-768x468.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics-1536x936.jpg 1536w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300_Systems_Optics-600x366.jpg 600w&quot; width=&quot;2047&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The SerDes used in the G300 are designed by Cisco and deliver 224 Gb/sec before encoding and 200 Gb/sec after encoding. (We suspect that the native signaling clock of the SerDes is 112 Gb/sec and it gets doubled up to 224 Gb/sec using PAM4 modulation, which can pump two bits per signal.) This is the first 200 Gb/sec SerDes to come from Cisco, by the way.
 &lt;/p&gt;
 &lt;p&gt;
  If you wanted to, you could create a 512-port switch with each port running at 200 Gb/sec using the G300 chip, or one with 256 ports running at 400 Gb/sec, 128 ports running at 800 Gb/sec, or 64 ports running at what still sounds like an amazing 1.6 Tb/sec.
 &lt;/p&gt;
 &lt;p&gt;
  By the way, the G300 can drive linear pluggable optics (LPO) directly at 800 Gb/sec per port and Cisco has an LPO module that marries to switches using the G300. Cisco also has its own 1.6 Tb/sec OSFP pluggable optics module for those choosing to use the G300 to drive 1.6 Tb/sec ports. This are optics modules are based on Cisco’s own silicon, not stuff brought in by third parties. (Nvidia and Broadcom are also one-stop shops for these components.) But in an interesting twist, Cisco is also building pluggable modules based on third party DSPs for customers who need choice or desire second sourcing to thin out the risk across a wider supply chain.
 &lt;/p&gt;
 &lt;p&gt;
  By moving to LPO, there is a huge power savings, which can in turn allow more power to be allocated to compute engines. Chopra says that it is about a 50 percent power savings for the optics and about a 30 percent savings across the switch infrastructure in an AI cluster. This is a big deal.
 &lt;/p&gt;
 &lt;p&gt;
  Some customers need higher bandwidth for forthcoming GPUs and XPUs that are expected in the second half of 2026, so they will spend the extra power to get 1.6 Tb/sec ports. Other customers are hanging back and 800 Gb/sec is fine and therefore the lower-power LPO modules are the better option.
 &lt;/p&gt;
 &lt;p&gt;
  When you add it all up and normalize for bandwidth, the G300 delivers 33 percent higher network utilization and 28 percent faster job completion time than the G200 and many of its competitors out there in the field, says Chopra.
 &lt;/p&gt;
 &lt;p&gt;
  “Of the big things in this announcement, we have the raw technology, but how we actually test and qualify our optics is completely unique,” brags Chopra. “We have a vastly more comprehensive test structure than others in the industry. And the reason is that AI workloads are synchronous, and a single failure of an optical module is not like a failure in the front end network. It forces a AI job restart – you have to go back to a checkpoint and restart.”
 &lt;/p&gt;
 &lt;p&gt;
  Cisco is putting the G300 in a number of devices. For air cooled devices, the G300 is in the Nexus N9364-SG3 running the company’s own NX-OS operating system and the Cisco 8133 running the open source SONiC network operating system.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches.jpg&quot; rel=&quot;attachment wp-att-146989&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;520&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1220px) 100vw, 1220px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches.jpg 1220w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches-768x327.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches-1030x438.jpg 1030w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switches-600x256.jpg 600w&quot; width=&quot;1220&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  These switches take up 3U of rack space and have 64 ports running at 1.6 Tb/sec.
 &lt;/p&gt;
 &lt;p&gt;
  If you want to go more compact and have liquid cooling as well as an Open Rack setup, then there is the Nexus N9363-SG2 running NX-OS and the Cisco 8132 running SONiC. This is a 21-inch wide unit that adheres to the Orv3N specification from the Open Compute Project.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s a closer look at the water cooled switch:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-scaled.jpg&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;853&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 2560px) 100vw, 2560px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-scaled.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-scaled.jpg 2560w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-768x256.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-1536x512.jpg 1536w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-2048x682.jpg 2048w, https://www.nextplatform.com/wp-content/uploads/2026/02/Cisco-G300-liquid-cooled-switch-600x200.jpg 600w&quot; width=&quot;2560&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
 &lt;/p&gt;
 &lt;p&gt;
 &lt;/p&gt;
 &lt;p&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The upshot of this is that if you want to provide an aggregate of 102.4 Tb/sec of connectivity between GPUs in a scale up network or across server nodes and their GPUs linking to each other across a scale out network, it would take six G200 devices cross-coupled and now it only takes one G300.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation.jpg&quot; rel=&quot;attachment wp-att-146988&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;507&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1220px) 100vw, 1220px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation.jpg 1220w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation-768x319.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-g300-switch-consolidation-600x249.jpg 600w&quot; width=&quot;1220&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This also probably means the G300 can cost three or four times as much and still feel like a bargain compared to the G200. We have no idea what it actually costs, but the old ways before AI came along were for the new ASIC to cost somewhere around 1.5X to deliver 2X the bandwidth per ASIC unit.
 &lt;/p&gt;
 &lt;p&gt;
  That 70 percent better power efficiency is a big, big deal when the companies driving the GenAI revolution are buying capacity in blocks of gigawatts.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update.jpg&quot; rel=&quot;attachment wp-att-146990&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;565&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1220px) 100vw, 1220px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update.jpg 1220w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update-768x356.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/cisco-P200-switch-update-600x278.jpg 600w&quot; width=&quot;1220&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In addition to the G300 and its switches and optics, Cisco this week is also fleshing out the lineup of P200 devices. Last fall, Cisco debuted the P200 in a 51.2 Tb/sec router for DCI links with 64 800 Gb/sec ports, and now it is delivering the P200 in commercial-grade Nexus gear as well as in line cards for its Nexus and whitebox modular switches.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Greatest AI Show On Earth </title>
<link>https://www.nextplatform.com/2026/02/10/greatest-ai-show-on-earth/</link>
<pubDate>Tue, 10 Feb 2026 14:40:15 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/03/nvidia-jensen-grace-blackwell-logo.jpg&quot; title=&quot;nvidia-jensen-grace-blackwell-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  The NVIDIA GTC conference has a reputation for delivering announcements that reshape industry roadmaps. At this year’s event, from March 16 to 19 in downtown San Jose, the AI community will converge to explore what comes next.
 &lt;/p&gt;
 &lt;p&gt;
  The marquee event remains chief executive officer Jensen Huang’s keynote at SAP Center on Monday, March 16. His presentations are now an industry ritual; part product launch, part strategic insight, always surprising. This year’s keynote follows the
  &lt;em&gt;
   GTC Live 2026
  &lt;/em&gt;
  pregame show, where industry figures will gather for a warm-up discussion on trends in accelerated computing. The keynote and pregame will both be livestreamed for those who can’t make it in person.
 &lt;/p&gt;
 &lt;p&gt;
  Beyond the main stage, the agenda dives into the topics shaping real-world AI decisions today. Attendees can expect:
 &lt;/p&gt;
 &lt;ul&gt;
  &lt;li&gt;
   &lt;strong&gt;
    Physical AI and robotics sessions
   &lt;/strong&gt;
   exploring how simulation-driven training is moving autonomous systems from pilots into production.
  &lt;/li&gt;
  &lt;li&gt;
   &lt;strong&gt;
    Agentic AI tracks
   &lt;/strong&gt;
   examining the reasoning models reshaping how enterprises design and manage complex, multi-step workflows.
  &lt;/li&gt;
  &lt;li&gt;
   &lt;strong&gt;
    AI factory sessions
   &lt;/strong&gt;
   focused on the infrastructure layer, unpacking what purpose-built compute looks like at scale.
  &lt;/li&gt;
 &lt;/ul&gt;
 &lt;p&gt;
  These sessions focus on real-world implementation. Speakers include industry leaders, developers, and researchers sharing what’s working in production today.
 &lt;/p&gt;
 &lt;h3&gt;
  Rich Learning And Networking Opportunities
 &lt;/h3&gt;
 &lt;p&gt;
  Beyond the presentations, expert-led workshops deliver hands-on training on NVIDIA platforms, with certification tracks that carry real advantages in hiring. Developer Days and hackathons offer faster-paced, build-first formats for those who learn best by doing.
 &lt;/p&gt;
 &lt;p&gt;
  The startup showcase highlights emerging companies, with founders demonstrating prototypes and engaging early adopters.
 &lt;/p&gt;
 &lt;p&gt;
  And, like any good conference, much of the value happens outside the sessions. The networking opportunities range from structured meetups to the conversations that happen between sessions, connecting developers, startups, and the people shaping where AI goes from here.
 &lt;/p&gt;
 &lt;p&gt;
  Register today to secure your in-person pass or join virtually.
 &lt;/p&gt;
 &lt;p&gt;
  Find out more about the event at
  &lt;a href=&quot;https://www.nvidia.com/gtc/?ncid=cont-gvt-239-prsp-txt-en-us-1-l1&quot;&gt;
   AI Conference | Mar 16-19, 2026 San Jose | NVIDIA GTC
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   Sponsored by NVIDIA.
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Datacenter Spending Forecast Revised Upwards – Yet Again </title>
<link>https://www.nextplatform.com/2026/02/09/datacenter-spending-forecast-revised-upwards-yet-again/#respond</link>
<pubDate>Mon, 09 Feb 2026 21:53:17 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2022/03/microsoft-quincy-datacenter-1030x438.jpg&quot; title=&quot;Microsoft Data Center - Quincy&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  This is turning into a “dog bites man” story, but the forecasts for spending in the datacenter for this year keep going up and up, and a few days ago Gartner’s economists and prognosticators finished up their tea and looked at the leaves at the common of a cup through a polished crystal ball and predicted that datacenter spending this year would go up.
 &lt;/p&gt;
 &lt;p&gt;
  This comes as no surprise with Amazon, Google, Meta Platforms, Microsoft, and Oracle all saying in the past two weeks that they would collectively be increasing their capital expenditures this year by an average of 80 percent to $705 billion, injecting $230 billion more spending into this year. About 90 percent of that combined capex is for datacenter gear, and the lion’s share of that is for AI gear, so the wonder is why IDC’s forecast for spending on datacenter systems in 2026 was only revised upwards by 10.9 percent and representing $71 billion in spending.
 &lt;/p&gt;
 &lt;p&gt;
  Apparently Gartner was already anticipating some of this increase in spending.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-little-table-forecast.jpg&quot; rel=&quot;attachment wp-att-146976&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;361&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-little-table-forecast.jpg&quot; width=&quot;531&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Now, says the market researcher, spending in the datacenter will tally up to $653.4 billion in 2026, an increase of 31.7 percent following a 48.9 percent bump in datacenter gear spending in 2025, to $496.2 billion. It could turn out that Gartner makes another upwards revision for 2026 when its next forecast comes out in April.
 &lt;/p&gt;
 &lt;p&gt;
  For now, all the other parts of the IT sector as defined by Gartner have more or less the same forecast that the company put together back in October 2025. Spending on client devices – smartphones, PC, and laptops – has held steady, at $836.4 billion, and so has spending on enterprise software, at $1.43 trillion. Growth in IT services spending is down a fraction of a point compared to the October 2025 prediction, rising 8.7 percent to $1.87 trillion. Telecom services spending is nearly identical in the current forecast, up 4.7 percent to $1.37 trillion.
 &lt;/p&gt;
 &lt;p&gt;
  As you know, we like to take a longest view we can on datasets, so here is the Gartner IT spending forecasts between now and all the way back to 2014:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast.jpg&quot; rel=&quot;attachment wp-att-146972&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;370&quot; sizes=&quot;(max-width: 1069px) 100vw, 1069px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast.jpg 1069w, https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast-768x266.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast-600x208.jpg 600w&quot; width=&quot;1069&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We like to track core IT spending – datacenter systems, enterprise software, and IT services – separately from spending on devices and telecom services because this gives a better sense of the external spending by organizations on stuff they get from third parties.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   &lt;u&gt;
    Important note:
   &lt;/u&gt;
  &lt;/em&gt;
  To our knowledge, no one has ever tried to pair
  &lt;em&gt;
   internal
  &lt;/em&gt;
  IT spending on systems support and system and application software development with
  &lt;em&gt;
   external
  &lt;/em&gt;
  IT spending data from Gartner or IDC, but with maybe 50 million people worldwide involved in datacenter computing in some fashion, this would easily add up to $4 trillion dollars or more annually at prevailing compensation rates for such employees worldwide.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-core-it.jpg&quot; rel=&quot;attachment wp-att-146973&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;419&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-core-it.jpg&quot; width=&quot;573&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Gartner now says that the spending on core IT in 2025 was $3.46 trillion, up 13 percent from 2024, which tells you how sluggish software and services are growing during the GenAI boom compared to systems. In 2026, Gartner is forecasting that core IT spending, as we call it, will rise by 14.2 percent to $3.95 trillion.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-it-versus-gdp.jpg&quot; rel=&quot;attachment wp-att-146975&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;371&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 608px) 100vw, 608px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-it-versus-gdp.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-it-versus-gdp.jpg 608w, https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-it-versus-gdp-600x366.jpg 600w&quot; width=&quot;608&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The core IT spending growth is many times the rate of growth of global gross domestic product from the World Bank, as you can see in the monster table we have built above.
 &lt;/p&gt;
 &lt;p&gt;
  Whenever you have datasets that span more than a decade, particular during a period where there has been high inflation, it is wise to inflation adjust the data to see how “real” spending has changed over time. We set the pivot point on inflation adjustment in 2021, and as you can see, in 2021 dollars, the further you go back, the more the past data is inflated to 2021 dollars and the more you go into the future beyond 2021, the more it is deflated. This means that over that bakers dozen of years shown in the monster table, the spending curve is a little bit flatter than the raw data suggests.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-datacenter-inflation-adjusted.jpg&quot; rel=&quot;attachment wp-att-146974&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;400&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-datacenter-inflation-adjusted.jpg&quot; width=&quot;582&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  But you can also see that while datacenter systems spending averaged around $200 billion a year, give or take until 2023, with the peak in 2018 filling in a lot of the gap on 2012 and 2013, from then onwards we are on a wildly different trajectory, even with inflation adjustment, with datacenter systems spending going up at around a 45 degree angle, adding an average of $160 billion a year to datacenter systems spending in both 2025 and 2026, if the forecasts play out. Each incremental spend in datacenter stuff is a little less than what was spent each in 2012 and 2013 after adjusting for inflation.
 &lt;/p&gt;
 &lt;p&gt;
  Each successive year adds a new 2012 yearlet to the prior year at this rate. And the rate could very well accelerate in 2027 and beyond until we get to the end of the decade.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Datacenter Spending Forecast Revised Upwards – Yet Again </title>
<link>https://www.nextplatform.com/2026/02/09/datacenter-spending-forecast-revised-upwards-yet-again/</link>
<pubDate>Mon, 09 Feb 2026 21:53:14 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2022/03/microsoft-quincy-datacenter-1030x438.jpg&quot; title=&quot;Microsoft Data Center - Quincy&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  This is turning into a “dog bites man” story, but the forecasts for spending in the datacenter for this year keep going up and up, and a few days ago Gartner’s economists and prognosticators finished up their tea and looked at the leaves at the common of a cup through a polished crystal ball and predicted that datacenter spending this year would go up.
 &lt;/p&gt;
 &lt;p&gt;
  This comes as no surprise with Amazon, Google, Meta Platforms, Microsoft, and Oracle all saying in the past two weeks that they would collectively be increasing their capital expenditures this year by an average of 80 percent to $705 billion, injecting $230 billion more spending into this year. About 90 percent of that combined capex is for datacenter gear, and the lion’s share of that is for AI gear, so the wonder is why IDC’s forecast for spending on datacenter systems in 2026 was only revised upwards by 10.9 percent and representing $71 billion in spending.
 &lt;/p&gt;
 &lt;p&gt;
  Apparently Gartner was already anticipating some of this increase in spending.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-little-table-forecast.jpg&quot; rel=&quot;attachment wp-att-146976&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;361&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-little-table-forecast.jpg&quot; width=&quot;531&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Now, says the market researcher, spending in the datacenter will tally up to $653.4 billion in 2026, an increase of 31.7 percent following a 48.9 percent bump in datacenter gear spending in 2025, to $496.2 billion. It could turn out that Gartner makes another upwards revision for 2026 when its next forecast comes out in April.
 &lt;/p&gt;
 &lt;p&gt;
  For now, all the other parts of the IT sector as defined by Gartner have more or less the same forecast that the company put together back in October 2025. Spending on client devices – smartphones, PC, and laptops – has held steady, at $836.4 billion, and so has spending on enterprise software, at $1.43 trillion. Growth in IT services spending is down a fraction of a point compared to the October 2025 prediction, rising 8.7 percent to $1.87 trillion. Telecom services spending is nearly identical in the current forecast, up 4.7 percent to $1.37 trillion.
 &lt;/p&gt;
 &lt;p&gt;
  As you know, we like to take a longest view we can on datasets, so here is the Gartner IT spending forecasts between now and all the way back to 2014:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast.jpg&quot; rel=&quot;attachment wp-att-146972&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;370&quot; sizes=&quot;(max-width: 1069px) 100vw, 1069px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast.jpg 1069w, https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast-768x266.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-big-table-forecast-600x208.jpg 600w&quot; width=&quot;1069&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We like to track core IT spending – datacenter systems, enterprise software, and IT services – separately from spending on devices and telecom services because this gives a better sense of the external spending by organizations on stuff they get from third parties.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   &lt;u&gt;
    Important note:
   &lt;/u&gt;
  &lt;/em&gt;
  To our knowledge, no one has ever tried to pair
  &lt;em&gt;
   internal
  &lt;/em&gt;
  IT spending on systems support and system and application software development with
  &lt;em&gt;
   external
  &lt;/em&gt;
  IT spending data from Gartner or IDC, but with maybe 50 million people worldwide involved in datacenter computing in some fashion, this would easily add up to $4 trillion dollars or more annually at prevailing compensation rates for such employees worldwide.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-core-it.jpg&quot; rel=&quot;attachment wp-att-146973&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;419&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-core-it.jpg&quot; width=&quot;573&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Gartner now says that the spending on core IT in 2025 was $3.46 trillion, up 13 percent from 2024, which tells you how sluggish software and services are growing during the GenAI boom compared to systems. In 2026, Gartner is forecasting that core IT spending, as we call it, will rise by 14.2 percent to $3.95 trillion.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-it-versus-gdp.jpg&quot; rel=&quot;attachment wp-att-146975&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;371&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 608px) 100vw, 608px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-it-versus-gdp.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-it-versus-gdp.jpg 608w, https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-it-versus-gdp-600x366.jpg 600w&quot; width=&quot;608&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The core IT spending growth is many times the rate of growth of global gross domestic product from the World Bank, as you can see in the monster table we have built above.
 &lt;/p&gt;
 &lt;p&gt;
  Whenever you have datasets that span more than a decade, particular during a period where there has been high inflation, it is wise to inflation adjust the data to see how “real” spending has changed over time. We set the pivot point on inflation adjustment in 2021, and as you can see, in 2021 dollars, the further you go back, the more the past data is inflated to 2021 dollars and the more you go into the future beyond 2021, the more it is deflated. This means that over that bakers dozen of years shown in the monster table, the spending curve is a little bit flatter than the raw data suggests.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-datacenter-inflation-adjusted.jpg&quot; rel=&quot;attachment wp-att-146974&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;400&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/gartner-it-spending-feb-2026-datacenter-inflation-adjusted.jpg&quot; width=&quot;582&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  But you can also see that while datacenter systems spending averaged around $200 billion a year, give or take until 2023, with the peak in 2018 filling in a lot of the gap on 2012 and 2013, from then onwards we are on a wildly different trajectory, even with inflation adjustment, with datacenter systems spending going up at around a 45 degree angle, adding an average of $160 billion a year to datacenter systems spending in both 2025 and 2026, if the forecasts play out. Each incremental spend in datacenter stuff is a little less than what was spent each in 2012 and 2013 after adjusting for inflation.
 &lt;/p&gt;
 &lt;p&gt;
  Each successive year adds a new 2012 yearlet to the prior year at this rate. And the rate could very well accelerate in 2027 and beyond until we get to the end of the decade.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Twin Engine Strategy That Propels AWS Is Working Well </title>
<link>https://www.nextplatform.com/2026/02/08/the-twin-engine-strategy-that-propels-aws-is-working-well/#respond</link>
<pubDate>Sun, 08 Feb 2026 19:47:09 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-rack-servers-logo-3-927x438.jpg&quot; title=&quot;aws-rack-servers-logo-3&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Like Google and Meta Platforms, Amazon knows exactly how to infuse AI into its business operations such as online retail, transportation, advertising, and even the Amazon Web Services cloud. Just like Google and IBM have been their own Customer Zero for AI efforts, Amazon has been learning how to use AI to replace or enhance business functions, not just for itself but so it can better understand how to sell such expertise to external customers and get them using the AWS cloud.
 &lt;/p&gt;
 &lt;p&gt;
  So it is not, we think, a coincidence at all that Amazon has laid off what looks like more than 60,000 people in its corporate employee base. Some of these layoffs have to do with over-hiring during coronavirus pandemic that started in 2020; some of it is just “workforce rebalancing” as IBM has called it for years. But, the fact remains that there are more corporate workers today than there were five years ago, and on top of that, there are another 1.2 million warehouse employees at Amazon (down about 100,000 from 2021 levels.
 &lt;/p&gt;
 &lt;p&gt;
  It is easy – far too easy – to make a one-to-one correlation between the 60,000 recent corporate layoffs and the rise of GenAI as a worker within Amazon. The world’s biggest online retailer, one of its biggest advertisers and media companies, and the world’s biggest cloud also wants to flatten its corporate organization – and would have needed to do this anyway. But as go the physical robots in the Amazon warehouses, so will go the AI agents in the Amazon Web Services data warehouses. Amazon had around 200,000 robots helping move and pack stuff into cardboard boxes in 2019 before, and now it is well over 1 million and probably well on its way to being much higher. You can bet that is Jeff Bezos and Andy Jassy could replace every person in a warehouse with a robot, they would. We have no idea how practical that is, but we do know that with every passing year it gets more likely.
 &lt;/p&gt;
 &lt;p&gt;
  It is harder to say that about the corporate operations running the Amazon businesses. People are still buying from people. People are still making the calls. We don’t know what the equilibrium is, and a lot depends on how good the AI agents are several years from now. The Rufus shopping agent that Amazon wants me to employ is but a first step. If money gets tight enough, people will use Rufus to do opportunistic buying. If profits get hard enough to come by, Amazon will use AI agents to do opportunistic firing. It is that simple.
 &lt;/p&gt;
 &lt;p&gt;
  All we know for sure about modeling the future is that it is important to watch what Amazon does and how AWS will benefit from the company’s deployment of AI technologies and how that experience will infuse its experience with millions of other companies. Likewise, the experience of AWS with millions of customers playing around with AI will infuse what the Amazon mothership does and does not do.
 &lt;/p&gt;
 &lt;p&gt;
  The good news for Amazon is that it has built a large advertising business that can not only buy AI services from AWS as a showcase, but, we think, is so wildly popular and profitable that it can also pay more than its fair share of the enormous capital expenses that Bezos &amp; Co are committing to this year and beyond.
 &lt;/p&gt;
 &lt;p&gt;
  Take a look at this chart and you will see what we mean:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-amazon-ads-versus-aws-systems.jpg&quot; rel=&quot;attachment wp-att-146965&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;383&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-amazon-ads-versus-aws-systems.jpg&quot; width=&quot;563&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In this chart above, we compare revenues of what we think of as the core AWS systems business – compute, networking, and storage – against the relatively new Amazon Ads business, for which we only have five years of data based on things the company says in its quarterly reports and guesses from third party advertising market research companies that fill in the blanks in the earlier years.
 &lt;/p&gt;
 &lt;p&gt;
  We fully realize that separating out the underlying AWS systems business from the rest of the AWS stack, including platform services and software rented out, is dubious, but we have been making our estimates long before this advertising business even existed. These are two distinct datasets, across time. But look at how they twine! It is not causation, but it is correlation for sure.
 &lt;/p&gt;
 &lt;p&gt;
  The important thing is that Amazon Ads probably has much higher operating margins than the AWS core hardware business, and it can help pay for the AI buildout.
 &lt;/p&gt;
 &lt;p&gt;
  “We expect to invest about $200 billion in capital expenditures across Amazon, but predominantly in AWS because we have very high demand customers who really want AWS for core and AI workloads,” Amazon chief executive officer Jassy explained on the call with Wall Street. “And we are monetizing capacity as fast as we can install it. We have deep experience understanding demand signals in the AWS business and then turning that capacity into strong return on invested capital. We are confident this will be the case here as well.”
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-capex.jpg&quot; rel=&quot;attachment wp-att-146967&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;382&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-capex.jpg&quot; width=&quot;561&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the fourth quarter of 2025, Amazon spent $40.47 billion on capital expenses, an increase of 43.1 percent over the year ago period, and for all of 2025, it shelled out $134.73 billion, an increase of 60.5 percent compared to the tad bit less than $84 billion that Amazon spent on infrastructure in 2024. Our model suggests that AWS spent around $115 billion on IT infrastructure, and of this around $105 billion was for AI infrastructure. So AI was 78 percent or so of all capex, with other IT being around 7 percent and the remaining 14.5 percent being for warehouses and transportation equipment for the Amazon network of retail operations.
 &lt;/p&gt;
 &lt;p&gt;
  “We are growing at really an unprecedented rate yet,” Jassy said about the capex spending. “I think every provider would tell you, including us, that we could actually grow faster if we had all the supply that we could take. And so we are being incredibly scrappy around that.”
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-backlog-capex.jpg&quot; rel=&quot;attachment wp-att-146966&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;396&quot; sizes=&quot;(max-width: 518px) 100vw, 518px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-backlog-capex.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-backlog-capex.jpg 518w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-backlog-capex-80x60.jpg 80w&quot; width=&quot;518&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the past twelve months, said Jassy, Amazon has added 3.9 gigawatts of datacenter capacity, which averages out to a very scrappy $29.5 billion per gigawatt where the big model builders like Anthropic and OpenAI are spending anywhere from $45 billion to $60 billion per gigawatt. Jassy added that Amazon added 1.2 gigawatts in Q4, and said further that only back in 2022, when AWS was at an annualized run rate of $80 billion at year end, it only had around 2 gigawatts of capacity installed. When we model the in-between years, that means AWS has around 6 gigawatts of total capacity installed as 2025 came to an end, and Jassy has said in past statements that it will double again by 2027, which will be 12 gigawatts. At the prevailing price that AWS is paying – call it $30 billion per gigawatt – that is $180 billion to add 6 gigawatts. So, the $200 billion that Amazon will spend on capex in 2026 will cover the bulk of the cost of the gear (warehouse and transportation stuff) that will be installed and ready by 2027 it looks like.
 &lt;/p&gt;
 &lt;p&gt;
  If the spending is doubling, the compute capacity of the gear is also probably doubling to quadrupling, depending on the computing precision used, as AWS moves through the Nvidia roadmap and its own Trainium roadmap. Software improvements over the two years should yield somewhere between 3X and 4X more performance if history is any guide. So the amount of compute AWS will have for AI will be vastly more than the increase in spending alone indicates.
 &lt;/p&gt;
 &lt;p&gt;
  Which is, we think, the key driver of renewed revenue growth for AWS. As inference processing gets cheaper, the elasticity of demand will increase faster and burn a lot more compute, fueling another reinvestment cycle:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-growth-over-time.jpg&quot; rel=&quot;attachment wp-att-146969&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;377&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 602px) 100vw, 602px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-growth-over-time.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-growth-over-time.jpg 602w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-growth-over-time-600x376.jpg 600w&quot; width=&quot;602&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It remains to be seen if growth can go as high as 30 percent or 35 percent year on year – or even higher. A lot depends on how GenAI bots and agents are adopted by enterprises and how much they customize their training.
 &lt;/p&gt;
 &lt;p&gt;
  Some interesting tidbits: Jassy said on the call that the custom compute engines at AWS – the Graviton Arm server CPU and the Trainium1 and Trainium2 AI XPU engines – ended 2025 with a $10 billion annualized run rate. That means those instanced brought in $2.5 billion in rent, and this was driven in large part by the fleet of 1.4 million Tranium2 million chips. Trainium3 installations are ramping now and all of the Trainium3 capacity for the chips it will install will be allocated by the middle of 2026. Presumably these Trainium3 systems will be installed this year and next, and probably in the millions and maybe accounting for a very large share of the $200 billion in spending this year.
 &lt;/p&gt;
 &lt;p&gt;
  What that means, in our model, is that X86 CPUs and Nvidia and sometimes AMD GPUs accounted for around $12 billion in spending for instances on AWS in Q4 2025. Some years hence, Amazon will spend more money on Graviton and Trainium than it does on external chips – when it hard to say.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-revenue-income.jpg&quot; rel=&quot;attachment wp-att-146970&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;393&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 626px) 100vw, 626px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-revenue-income.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-revenue-income.jpg 626w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-revenue-income-600x377.jpg 600w&quot; width=&quot;626&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In Q4 2025, AWS brought in $35.58 billion, up 23.6 percent, and operating income was under pressure a bit from chip design and manufacturing costs and only rose by 17.2 percent to $12.47 billion.
 &lt;/p&gt;
 &lt;p&gt;
  For the full year, AWS had $128.73 billion in sales, up 19.7 percent, with operating income of $45.61 billion, up 14.5 percent.
 &lt;/p&gt;
 &lt;p&gt;
  Here is how we think the breakdown of compute, storage, networking, and software revenues has broken down at AWS over the years:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-categories.jpg&quot; rel=&quot;attachment wp-att-146968&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;396&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-categories.jpg&quot; width=&quot;563&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As we have pointed out in the past, Amazon has never, ever given any indication of how AWS revenues break down across the four buckets we have created in that chart above, and it has never called us up to confirm or deny our model. We do this merely because we need to separate these out to understand what drives the AWS business.
 &lt;/p&gt;
 &lt;p&gt;
  For many years, the hardware was the main driver, but as AWS started building a complete platform, with networking and data services as well as development tools and sometimes full-blow applications, software came to dominate the revenue stream. But with GenAI hardware costing do damned much, and having access to it is so dear that Nvidia and AMD can charge a lot for it and AWS can turn around and charge an even higher premium to rent it, the compute part of the revenue stream has been skyrocketing at AWS and, we think, now drives more revenue than the software stack does.
 &lt;/p&gt;
 &lt;p&gt;
  And, because AWS is smart with Trainium as it has been with Graviton with core compute, homegrown chips can undercut Nvidia and AMD capacity rentals and still probably run AWS an equal or better operating profit.
 &lt;/p&gt;
 &lt;p&gt;
  AWS is in a win-win scenario here, as long as the money holds out. And until GenAI normalizes, it can invest in a twin engine approach for compute and keep the pressure on third party engine suppliers. In the long run, there is no reason to believe that the bulk of AWS DPU, CPU, and GPU/XPU compute power will come from its Annapurna Labs designers, not outside the company. And if you want to rent a third party device on the AWS cloud, you will pay a premium for that.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Twin Engine Strategy That Propels AWS Is Working Well </title>
<link>https://www.nextplatform.com/2026/02/08/the-twin-engine-strategy-that-propels-aws-is-working-well/</link>
<pubDate>Sun, 08 Feb 2026 19:47:05 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-rack-servers-logo-3-927x438.jpg&quot; title=&quot;aws-rack-servers-logo-3&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Like Google and Meta Platforms, Amazon knows exactly how to infuse AI into its business operations such as online retail, transportation, advertising, and even the Amazon Web Services cloud. Just like Google and IBM have been their own Customer Zero for AI efforts, Amazon has been learning how to use AI to replace or enhance business functions, not just for itself but so it can better understand how to sell such expertise to external customers and get them using the AWS cloud.
 &lt;/p&gt;
 &lt;p&gt;
  So it is not, we think, a coincidence at all that Amazon has laid off what looks like more than 60,000 people in its corporate employee base. Some of these layoffs have to do with over-hiring during coronavirus pandemic that started in 2020; some of it is just “workforce rebalancing” as IBM has called it for years. But, the fact remains that there are more corporate workers today than there were five years ago, and on top of that, there are another 1.2 million warehouse employees at Amazon (down about 100,000 from 2021 levels.
 &lt;/p&gt;
 &lt;p&gt;
  It is easy – far too easy – to make a one-to-one correlation between the 60,000 recent corporate layoffs and the rise of GenAI as a worker within Amazon. The world’s biggest online retailer, one of its biggest advertisers and media companies, and the world’s biggest cloud also wants to flatten its corporate organization – and would have needed to do this anyway. But as go the physical robots in the Amazon warehouses, so will go the AI agents in the Amazon Web Services data warehouses. Amazon had around 200,000 robots helping move and pack stuff into cardboard boxes in 2019 before, and now it is well over 1 million and probably well on its way to being much higher. You can bet that is Jeff Bezos and Andy Jassy could replace every person in a warehouse with a robot, they would. We have no idea how practical that is, but we do know that with every passing year it gets more likely.
 &lt;/p&gt;
 &lt;p&gt;
  It is harder to say that about the corporate operations running the Amazon businesses. People are still buying from people. People are still making the calls. We don’t know what the equilibrium is, and a lot depends on how good the AI agents are several years from now. The Rufus shopping agent that Amazon wants me to employ is but a first step. If money gets tight enough, people will use Rufus to do opportunistic buying. If profits get hard enough to come by, Amazon will use AI agents to do opportunistic firing. It is that simple.
 &lt;/p&gt;
 &lt;p&gt;
  All we know for sure about modeling the future is that it is important to watch what Amazon does and how AWS will benefit from the company’s deployment of AI technologies and how that experience will infuse its experience with millions of other companies. Likewise, the experience of AWS with millions of customers playing around with AI will infuse what the Amazon mothership does and does not do.
 &lt;/p&gt;
 &lt;p&gt;
  The good news for Amazon is that it has built a large advertising business that can not only buy AI services from AWS as a showcase, but, we think, is so wildly popular and profitable that it can also pay more than its fair share of the enormous capital expenses that Bezos &amp; Co are committing to this year and beyond.
 &lt;/p&gt;
 &lt;p&gt;
  Take a look at this chart and you will see what we mean:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-amazon-ads-versus-aws-systems.jpg&quot; rel=&quot;attachment wp-att-146965&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;383&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-amazon-ads-versus-aws-systems.jpg&quot; width=&quot;563&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In this chart above, we compare revenues of what we think of as the core AWS systems business – compute, networking, and storage – against the relatively new Amazon Ads business, for which we only have five years of data based on things the company says in its quarterly reports and guesses from third party advertising market research companies that fill in the blanks in the earlier years.
 &lt;/p&gt;
 &lt;p&gt;
  We fully realize that separating out the underlying AWS systems business from the rest of the AWS stack, including platform services and software rented out, is dubious, but we have been making our estimates long before this advertising business even existed. These are two distinct datasets, across time. But look at how they twine! It is not causation, but it is correlation for sure.
 &lt;/p&gt;
 &lt;p&gt;
  The important thing is that Amazon Ads probably has much higher operating margins than the AWS core hardware business, and it can help pay for the AI buildout.
 &lt;/p&gt;
 &lt;p&gt;
  “We expect to invest about $200 billion in capital expenditures across Amazon, but predominantly in AWS because we have very high demand customers who really want AWS for core and AI workloads,” Amazon chief executive officer Jassy explained on the call with Wall Street. “And we are monetizing capacity as fast as we can install it. We have deep experience understanding demand signals in the AWS business and then turning that capacity into strong return on invested capital. We are confident this will be the case here as well.”
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-capex.jpg&quot; rel=&quot;attachment wp-att-146967&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;382&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-capex.jpg&quot; width=&quot;561&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the fourth quarter of 2025, Amazon spent $40.47 billion on capital expenses, an increase of 43.1 percent over the year ago period, and for all of 2025, it shelled out $134.73 billion, an increase of 60.5 percent compared to the tad bit less than $84 billion that Amazon spent on infrastructure in 2024. Our model suggests that AWS spent around $115 billion on IT infrastructure, and of this around $105 billion was for AI infrastructure. So AI was 78 percent or so of all capex, with other IT being around 7 percent and the remaining 14.5 percent being for warehouses and transportation equipment for the Amazon network of retail operations.
 &lt;/p&gt;
 &lt;p&gt;
  “We are growing at really an unprecedented rate yet,” Jassy said about the capex spending. “I think every provider would tell you, including us, that we could actually grow faster if we had all the supply that we could take. And so we are being incredibly scrappy around that.”
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-backlog-capex.jpg&quot; rel=&quot;attachment wp-att-146966&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;396&quot; sizes=&quot;(max-width: 518px) 100vw, 518px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-backlog-capex.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-backlog-capex.jpg 518w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-backlog-capex-80x60.jpg 80w&quot; width=&quot;518&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the past twelve months, said Jassy, Amazon has added 3.9 gigawatts of datacenter capacity, which averages out to a very scrappy $29.5 billion per gigawatt where the big model builders like Anthropic and OpenAI are spending anywhere from $45 billion to $60 billion per gigawatt. Jassy added that Amazon added 1.2 gigawatts in Q4, and said further that only back in 2022, when AWS was at an annualized run rate of $80 billion at year end, it only had around 2 gigawatts of capacity installed. When we model the in-between years, that means AWS has around 6 gigawatts of total capacity installed as 2025 came to an end, and Jassy has said in past statements that it will double again by 2027, which will be 12 gigawatts. At the prevailing price that AWS is paying – call it $30 billion per gigawatt – that is $180 billion to add 6 gigawatts. So, the $200 billion that Amazon will spend on capex in 2026 will cover the bulk of the cost of the gear (warehouse and transportation stuff) that will be installed and ready by 2027 it looks like.
 &lt;/p&gt;
 &lt;p&gt;
  If the spending is doubling, the compute capacity of the gear is also probably doubling to quadrupling, depending on the computing precision used, as AWS moves through the Nvidia roadmap and its own Trainium roadmap. Software improvements over the two years should yield somewhere between 3X and 4X more performance if history is any guide. So the amount of compute AWS will have for AI will be vastly more than the increase in spending alone indicates.
 &lt;/p&gt;
 &lt;p&gt;
  Which is, we think, the key driver of renewed revenue growth for AWS. As inference processing gets cheaper, the elasticity of demand will increase faster and burn a lot more compute, fueling another reinvestment cycle:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-growth-over-time.jpg&quot; rel=&quot;attachment wp-att-146969&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;377&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 602px) 100vw, 602px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-growth-over-time.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-growth-over-time.jpg 602w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-growth-over-time-600x376.jpg 600w&quot; width=&quot;602&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It remains to be seen if growth can go as high as 30 percent or 35 percent year on year – or even higher. A lot depends on how GenAI bots and agents are adopted by enterprises and how much they customize their training.
 &lt;/p&gt;
 &lt;p&gt;
  Some interesting tidbits: Jassy said on the call that the custom compute engines at AWS – the Graviton Arm server CPU and the Trainium1 and Trainium2 AI XPU engines – ended 2025 with a $10 billion annualized run rate. That means those instanced brought in $2.5 billion in rent, and this was driven in large part by the fleet of 1.4 million Tranium2 million chips. Trainium3 installations are ramping now and all of the Trainium3 capacity for the chips it will install will be allocated by the middle of 2026. Presumably these Trainium3 systems will be installed this year and next, and probably in the millions and maybe accounting for a very large share of the $200 billion in spending this year.
 &lt;/p&gt;
 &lt;p&gt;
  What that means, in our model, is that X86 CPUs and Nvidia and sometimes AMD GPUs accounted for around $12 billion in spending for instances on AWS in Q4 2025. Some years hence, Amazon will spend more money on Graviton and Trainium than it does on external chips – when it hard to say.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-revenue-income.jpg&quot; rel=&quot;attachment wp-att-146970&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;393&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 626px) 100vw, 626px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-revenue-income.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-revenue-income.jpg 626w, https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-revenue-income-600x377.jpg 600w&quot; width=&quot;626&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In Q4 2025, AWS brought in $35.58 billion, up 23.6 percent, and operating income was under pressure a bit from chip design and manufacturing costs and only rose by 17.2 percent to $12.47 billion.
 &lt;/p&gt;
 &lt;p&gt;
  For the full year, AWS had $128.73 billion in sales, up 19.7 percent, with operating income of $45.61 billion, up 14.5 percent.
 &lt;/p&gt;
 &lt;p&gt;
  Here is how we think the breakdown of compute, storage, networking, and software revenues has broken down at AWS over the years:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-categories.jpg&quot; rel=&quot;attachment wp-att-146968&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;396&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/aws-q4-2025-categories.jpg&quot; width=&quot;563&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As we have pointed out in the past, Amazon has never, ever given any indication of how AWS revenues break down across the four buckets we have created in that chart above, and it has never called us up to confirm or deny our model. We do this merely because we need to separate these out to understand what drives the AWS business.
 &lt;/p&gt;
 &lt;p&gt;
  For many years, the hardware was the main driver, but as AWS started building a complete platform, with networking and data services as well as development tools and sometimes full-blow applications, software came to dominate the revenue stream. But with GenAI hardware costing do damned much, and having access to it is so dear that Nvidia and AMD can charge a lot for it and AWS can turn around and charge an even higher premium to rent it, the compute part of the revenue stream has been skyrocketing at AWS and, we think, now drives more revenue than the software stack does.
 &lt;/p&gt;
 &lt;p&gt;
  And, because AWS is smart with Trainium as it has been with Graviton with core compute, homegrown chips can undercut Nvidia and AMD capacity rentals and still probably run AWS an equal or better operating profit.
 &lt;/p&gt;
 &lt;p&gt;
  AWS is in a win-win scenario here, as long as the money holds out. And until GenAI normalizes, it can invest in a twin engine approach for compute and keep the pressure on third party engine suppliers. In the long run, there is no reason to believe that the bulk of AWS DPU, CPU, and GPU/XPU compute power will come from its Annapurna Labs designers, not outside the company. And if you want to rent a third party device on the AWS cloud, you will pay a premium for that.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> With GenAI Turbochargers, Google Is Shifting Its Cloud Into A Higher Gear </title>
<link>https://www.nextplatform.com/2026/02/05/with-genai-turbochargers-google-is-shifting-its-cloud-into-a-higher-gear/#respond</link>
<pubDate>Thu, 05 Feb 2026 19:21:57 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/09/google-ai-infra-ironwood-board-531x438.jpg&quot; title=&quot;google-ai-infra-ironwood-board&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Here is how we know computing could eventually be a peer to energy, transportation, sustenance, and healthcare as a basic infrastructure need – and will be a bigger part of our lives in the future, if the hyperscalers and cloud builders have their way: The front loading of enormous capital expenses.
 &lt;/p&gt;
 &lt;p&gt;
  Before we dive into the Google numbers for the final quarter of 2025, we wanted to level set with some comparative data, and spent a while combining the Internet with the assistance of the Google search engine and its Gemini adjunct to come up with some numbers. These numbers are for illustrative purposes, of course, and there is a tremendous amount of wiggle in what you might get depending on what you count and what you don’t count in each sector’s revenue and capital expenses.
 &lt;/p&gt;
 &lt;p&gt;
  We thought the energy sector was capital intensive, and it certainly is. Depending on the timing and the development of new energy sources, these companies can spend a large portions of their revenue on capex. But on average, throughout the energy production and distribution business, it looks like the capex intensity is something like 25 percent of revenues in the United States:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-gemini-capex-intensity.jpg&quot; rel=&quot;attachment wp-att-146958&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;196&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-gemini-capex-intensity.jpg&quot; width=&quot;406&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  All of the numbers in this table are in bold red italics because heaven only knows how Gemini took data out of Google searches and made its estimates. We are suspicious of such round numbers, but we are really just trying to use Gemini as a thought experiment and to make a point. We built a table with the help of Google and Gemini, but we don’t know if it can be trusted down to the details. We think the data has the right shape to it, but this is just confirming our pre-existing bias that some infrastructure sectors are more capital intensive than others.
 &lt;/p&gt;
 &lt;p&gt;
  Welcome to the modern AI world, where the machines act like they know a lot, and depending on how you ask you can get different answers. And we are not sure how much Apples to Oracles comparisons are being made here. (Yes, that was a joke. Sort of.)
 &lt;/p&gt;
 &lt;p&gt;
  Anyway, to continue in this experiment. Both the healthcare and farm/food sectors are very people-intensive and also have a lot of capital expenses, but it is a lot less than the transportation sector (more machines, less people) and the energy sector (a lot more machines still and fewer people by comparison).
 &lt;/p&gt;
 &lt;p&gt;
  But the capex spending by the Big Five hyperscalers and cloud builders – Amazon, Google, Meta Platforms, Microsoft, and Oracle – as a share of revenue makes energy look capex stingy by comparison. The fact that the hyperscalers and cloud builders are spending around half of their revenue on capex is indisputable. But as we said, we prefer to work with hard numbers that we have vetted, and building a table like that one above and verifying the underlying data and assumptions would probably take a day the old fashioned way.
 &lt;/p&gt;
 &lt;p&gt;
  With that experiment done, let’s dig into Google’s final quarter of 2025 and see what 2026 is going to look like for revenue and capex.
 &lt;/p&gt;
 &lt;p&gt;
  The thing to remember about Google is that it has been embedding AI functions into its core search and ads businesses for more than a decade and it has been making its own TPU accelerators to support these functions for most of that time precisely because it was too expensive to add voice translation to search using CPU inference or GPU inference at the time way back in the 2010s when AI was relatively young. If only a portion of people used voice search a few times a day, Google’s datacenters would have melted, and hence the Tensor Processing Unit, which is now in its seventh generation with the “Ironwood” devices, was born.
 &lt;/p&gt;
 &lt;p&gt;
  The Gemini model was trained on TPUs and most of the inference that Google performs through Gemini APIs is done on its vast TPU fleet. In Q3 2025, Google was processing tokens (for inference, we presume) at a rate of 7 billion tokens per minute; in Q4, that rate jumped by 43 percent to more than 10 billion tokens per minute. If you do the math, Google processed 917.3 trillion tokens in Q3 and 1,310.4 trillion tokens in Q4 for its “first party” application applications. This new data revealed by Google chief executive officer Sundar Pichai does not include tokens processed for GenAI training, and it doesn’t look like it includes Google’s internal services use of Gemini because it does not map
  &lt;a href=&quot;https://www.nextplatform.com/2025/09/17/google-shows-off-its-inference-scale-and-prowess/&quot;&gt;
   to the data Google’s Mark Lohmeyer showed off at the AI Hardware Summit last September
  &lt;/a&gt;
  . By our math at that time, we thought Google was processing around 1,460 trillion total tokens in August alone.
 &lt;/p&gt;
 &lt;p&gt;
  The point is, Google’s processing needs are growing fast because of actual use of GenAI, and the Gemini 3 models, which are arguably the best in the world now for certain things, with Anthropic’s Claude variants being best at certain other things, is growing rapidly.
 &lt;/p&gt;
 &lt;p&gt;
  To cover that processing demand and address a revenue backlog that stands at $240 billion, Google has to buy an incredible amount of iron in 2026, and Anat Ashkenazi, chief financial officer at Google, laid out the plan.
 &lt;/p&gt;
 &lt;p&gt;
  “The investment we have been making in AI are already translating into strong performance across the business, as you have seen in our financial results,” Ashkenazi said on the call with Wall Street analysts. (We will get to those numbers in a second. “Our successful execution, coupled with strong performance, reinforces our conviction to make the investments required to further capitalize on the AI opportunity. For the full year 2026, we expect capex to be in the range of $175 billion to $185 billion, with investments ramping over the course of the year. We are investing in AI compute capacity to support frontier model development by Google DeepMind, ongoing efforts to improve the user experience and drive higher advertiser ROI in Google services, and significant cloud customer demand as well as strategic investments in Other Bets.”
 &lt;/p&gt;
 &lt;p&gt;
  Ashkenazi also explained the timing of capex spend and what Google gets for it would depend on component supplies and pricing, and the timing of payments is what causes the variability between $175 billion to $185 billion. Call it $180 billion at the midpoint, and that is nearly double the $91.45 billion that Google spent on capex in 2025, which was 1.74X that which was spent in 2024, which was 1.63X of that spent in 2023, and so on back through time a decade ago.
 &lt;/p&gt;
 &lt;p&gt;
  If you look at Google’s revenue backlog growth against capex spending, you can see immediately that something has got to give:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-backlog-capex.jpg&quot; rel=&quot;attachment wp-att-146959&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;397&quot; sizes=&quot;(max-width: 524px) 100vw, 524px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-backlog-capex.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-backlog-capex.jpg 524w, https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-backlog-capex-80x60.jpg 80w&quot; width=&quot;524&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The wonder is that Google has not spent more, frankly, given the spread of the two. But you can rest assured that the company will not spend a dime on infrastructure until it knows it can get it into a datacenter and know someone is ready to rent it almost the second it is fired up.
 &lt;/p&gt;
 &lt;p&gt;
  That spread between backlog and capex is getting wider, and that may be an effect of longer deals for cloud capacity being on the books. (It would be interesting if Google disclosed such data.) No matter what, it is clear to us that for Google to meet its future processing commitments to itself, the model builders like Anthropic and OpenAI, and to its hundreds of thousands of enterprise AI customers, it is not only going to have to double capex, but it is going to have to get another 1.5X performance boost from software improvements and other efficiency gains. This is a tall order after already getting a 1.8X boost from software tweaks to the Gemini models in 2025.
 &lt;/p&gt;
 &lt;p&gt;
  The good news is that functional GenAI from the Gemini 3 model is feeding back into Google products and services, and it is driving revenues and usage, which in turn is paying for the increased capacity. Google has kept the AI horse in front of the application cart, which it has been able to do thanks to its vast and wildly profitable search and ads businesses. YouTube streaming is a solid business in its own right, too.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   Only the big can afford to get further embiggened. . . .
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We only care about those businesses inasmuch as they give Google the data on which to train models and the money to afford to be a cloud provider and a model builder as well as what is arguably the largest user of GenAI on the planet.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-cloud-rev-income.jpg&quot; rel=&quot;attachment wp-att-146960&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;392&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-cloud-rev-income.jpg&quot; width=&quot;568&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The dotted line red line is meant to convey we were estimating the operating losses for the Google Cloud business.
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter, Google booked $113.83 billion in sales, up 18 percent year on year, with net income of $34.46 billion, up 29.8 percent. Despite spending $27.85 billion on capex, the company existed the quarter with $95.66 billion in cash and equivalents in the bank, which is about half the cash it wants to spend on capex in 2026.
 &lt;/p&gt;
 &lt;p&gt;
  Google Cloud, the company’s cloud service as the name suggests, had $17.66 billion in sales, up 47.8 percent, with an operating income of $5.31 billion, up by a factor of 2.54X compared to the year ago period. That operating income rate was 30.1 percent of Google Cloud revenues, which is the highest profitability level Google has ever seen in its cloud business and nearly double the rate of only a year ago.
 &lt;/p&gt;
 &lt;p&gt;
  While it has been tough for Microsoft and Amazon Web Services to drive huge revenues directly with GenAI, Meta Platforms certainly has shown a knack for it and so has Google. Even if Google’s customers need time to figure it out, Google’s own businesses already long-since know how to use AI. We wonder how much internal backlog for AI hardware and services there is for Google’s own businesses, which have their own infrastructure and which do not officially use the Google Cloud.
 &lt;/p&gt;
 &lt;p&gt;
  As we have pointed out many times, all Google would have to do is a bookkeeping trick, calling all infrastructure Google Cloud and having its search, ads, and video businesses pay it for services and it would be the largest cloud in the world. But that would just be a stunt – even if it would be very funny.
 &lt;/p&gt;
 &lt;p&gt;
  Google seems intent on having its cloud grow in its own way and separate from the mothership, Alphabet, and those other quasi-independent businesses. The word on the street is somewhere between 30 percent and 50 percent top line growth for Google Cloud in 2026, and we wonder if it can’t be more.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> With GenAI Turbochargers, Google Is Shifting Its Cloud Into A Higher Gear </title>
<link>https://www.nextplatform.com/2026/02/05/with-genai-turbochargers-google-is-shifting-its-cloud-into-a-higher-gear/</link>
<pubDate>Thu, 05 Feb 2026 19:21:54 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/09/google-ai-infra-ironwood-board-531x438.jpg&quot; title=&quot;google-ai-infra-ironwood-board&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Here is how we know computing could eventually be a peer to energy, transportation, sustenance, and healthcare as a basic infrastructure need – and will be a bigger part of our lives in the future, if the hyperscalers and cloud builders have their way: The front loading of enormous capital expenses.
 &lt;/p&gt;
 &lt;p&gt;
  Before we dive into the Google numbers for the final quarter of 2025, we wanted to level set with some comparative data, and spent a while combining the Internet with the assistance of the Google search engine and its Gemini adjunct to come up with some numbers. These numbers are for illustrative purposes, of course, and there is a tremendous amount of wiggle in what you might get depending on what you count and what you don’t count in each sector’s revenue and capital expenses.
 &lt;/p&gt;
 &lt;p&gt;
  We thought the energy sector was capital intensive, and it certainly is. Depending on the timing and the development of new energy sources, these companies can spend a large portions of their revenue on capex. But on average, throughout the energy production and distribution business, it looks like the capex intensity is something like 25 percent of revenues in the United States:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-gemini-capex-intensity.jpg&quot; rel=&quot;attachment wp-att-146958&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;196&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-gemini-capex-intensity.jpg&quot; width=&quot;406&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  All of the numbers in this table are in bold red italics because heaven only knows how Gemini took data out of Google searches and made its estimates. We are suspicious of such round numbers, but we are really just trying to use Gemini as a thought experiment and to make a point. We built a table with the help of Google and Gemini, but we don’t know if it can be trusted down to the details. We think the data has the right shape to it, but this is just confirming our pre-existing bias that some infrastructure sectors are more capital intensive than others.
 &lt;/p&gt;
 &lt;p&gt;
  Welcome to the modern AI world, where the machines act like they know a lot, and depending on how you ask you can get different answers. And we are not sure how much Apples to Oracles comparisons are being made here. (Yes, that was a joke. Sort of.)
 &lt;/p&gt;
 &lt;p&gt;
  Anyway, to continue in this experiment. Both the healthcare and farm/food sectors are very people-intensive and also have a lot of capital expenses, but it is a lot less than the transportation sector (more machines, less people) and the energy sector (a lot more machines still and fewer people by comparison).
 &lt;/p&gt;
 &lt;p&gt;
  But the capex spending by the Big Five hyperscalers and cloud builders – Amazon, Google, Meta Platforms, Microsoft, and Oracle – as a share of revenue makes energy look capex stingy by comparison. The fact that the hyperscalers and cloud builders are spending around half of their revenue on capex is indisputable. But as we said, we prefer to work with hard numbers that we have vetted, and building a table like that one above and verifying the underlying data and assumptions would probably take a day the old fashioned way.
 &lt;/p&gt;
 &lt;p&gt;
  With that experiment done, let’s dig into Google’s final quarter of 2025 and see what 2026 is going to look like for revenue and capex.
 &lt;/p&gt;
 &lt;p&gt;
  The thing to remember about Google is that it has been embedding AI functions into its core search and ads businesses for more than a decade and it has been making its own TPU accelerators to support these functions for most of that time precisely because it was too expensive to add voice translation to search using CPU inference or GPU inference at the time way back in the 2010s when AI was relatively young. If only a portion of people used voice search a few times a day, Google’s datacenters would have melted, and hence the Tensor Processing Unit, which is now in its seventh generation with the “Ironwood” devices, was born.
 &lt;/p&gt;
 &lt;p&gt;
  The Gemini model was trained on TPUs and most of the inference that Google performs through Gemini APIs is done on its vast TPU fleet. In Q3 2025, Google was processing tokens (for inference, we presume) at a rate of 7 billion tokens per minute; in Q4, that rate jumped by 43 percent to more than 10 billion tokens per minute. If you do the math, Google processed 917.3 trillion tokens in Q3 and 1,310.4 trillion tokens in Q4 for its “first party” application applications. This new data revealed by Google chief executive officer Sundar Pichai does not include tokens processed for GenAI training, and it doesn’t look like it includes Google’s internal services use of Gemini because it does not map
  &lt;a href=&quot;https://www.nextplatform.com/2025/09/17/google-shows-off-its-inference-scale-and-prowess/&quot;&gt;
   to the data Google’s Mark Lohmeyer showed off at the AI Hardware Summit last September
  &lt;/a&gt;
  . By our math at that time, we thought Google was processing around 1,460 trillion total tokens in August alone.
 &lt;/p&gt;
 &lt;p&gt;
  The point is, Google’s processing needs are growing fast because of actual use of GenAI, and the Gemini 3 models, which are arguably the best in the world now for certain things, with Anthropic’s Claude variants being best at certain other things, is growing rapidly.
 &lt;/p&gt;
 &lt;p&gt;
  To cover that processing demand and address a revenue backlog that stands at $240 billion, Google has to buy an incredible amount of iron in 2026, and Anat Ashkenazi, chief financial officer at Google, laid out the plan.
 &lt;/p&gt;
 &lt;p&gt;
  “The investment we have been making in AI are already translating into strong performance across the business, as you have seen in our financial results,” Ashkenazi said on the call with Wall Street analysts. (We will get to those numbers in a second. “Our successful execution, coupled with strong performance, reinforces our conviction to make the investments required to further capitalize on the AI opportunity. For the full year 2026, we expect capex to be in the range of $175 billion to $185 billion, with investments ramping over the course of the year. We are investing in AI compute capacity to support frontier model development by Google DeepMind, ongoing efforts to improve the user experience and drive higher advertiser ROI in Google services, and significant cloud customer demand as well as strategic investments in Other Bets.”
 &lt;/p&gt;
 &lt;p&gt;
  Ashkenazi also explained the timing of capex spend and what Google gets for it would depend on component supplies and pricing, and the timing of payments is what causes the variability between $175 billion to $185 billion. Call it $180 billion at the midpoint, and that is nearly double the $91.45 billion that Google spent on capex in 2025, which was 1.74X that which was spent in 2024, which was 1.63X of that spent in 2023, and so on back through time a decade ago.
 &lt;/p&gt;
 &lt;p&gt;
  If you look at Google’s revenue backlog growth against capex spending, you can see immediately that something has got to give:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-backlog-capex.jpg&quot; rel=&quot;attachment wp-att-146959&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;397&quot; sizes=&quot;(max-width: 524px) 100vw, 524px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-backlog-capex.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-backlog-capex.jpg 524w, https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-backlog-capex-80x60.jpg 80w&quot; width=&quot;524&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The wonder is that Google has not spent more, frankly, given the spread of the two. But you can rest assured that the company will not spend a dime on infrastructure until it knows it can get it into a datacenter and know someone is ready to rent it almost the second it is fired up.
 &lt;/p&gt;
 &lt;p&gt;
  That spread between backlog and capex is getting wider, and that may be an effect of longer deals for cloud capacity being on the books. (It would be interesting if Google disclosed such data.) No matter what, it is clear to us that for Google to meet its future processing commitments to itself, the model builders like Anthropic and OpenAI, and to its hundreds of thousands of enterprise AI customers, it is not only going to have to double capex, but it is going to have to get another 1.5X performance boost from software improvements and other efficiency gains. This is a tall order after already getting a 1.8X boost from software tweaks to the Gemini models in 2025.
 &lt;/p&gt;
 &lt;p&gt;
  The good news is that functional GenAI from the Gemini 3 model is feeding back into Google products and services, and it is driving revenues and usage, which in turn is paying for the increased capacity. Google has kept the AI horse in front of the application cart, which it has been able to do thanks to its vast and wildly profitable search and ads businesses. YouTube streaming is a solid business in its own right, too.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   Only the big can afford to get further embiggened. . . .
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We only care about those businesses inasmuch as they give Google the data on which to train models and the money to afford to be a cloud provider and a model builder as well as what is arguably the largest user of GenAI on the planet.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-cloud-rev-income.jpg&quot; rel=&quot;attachment wp-att-146960&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;392&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/google-q4-2025-cloud-rev-income.jpg&quot; width=&quot;568&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The dotted line red line is meant to convey we were estimating the operating losses for the Google Cloud business.
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter, Google booked $113.83 billion in sales, up 18 percent year on year, with net income of $34.46 billion, up 29.8 percent. Despite spending $27.85 billion on capex, the company existed the quarter with $95.66 billion in cash and equivalents in the bank, which is about half the cash it wants to spend on capex in 2026.
 &lt;/p&gt;
 &lt;p&gt;
  Google Cloud, the company’s cloud service as the name suggests, had $17.66 billion in sales, up 47.8 percent, with an operating income of $5.31 billion, up by a factor of 2.54X compared to the year ago period. That operating income rate was 30.1 percent of Google Cloud revenues, which is the highest profitability level Google has ever seen in its cloud business and nearly double the rate of only a year ago.
 &lt;/p&gt;
 &lt;p&gt;
  While it has been tough for Microsoft and Amazon Web Services to drive huge revenues directly with GenAI, Meta Platforms certainly has shown a knack for it and so has Google. Even if Google’s customers need time to figure it out, Google’s own businesses already long-since know how to use AI. We wonder how much internal backlog for AI hardware and services there is for Google’s own businesses, which have their own infrastructure and which do not officially use the Google Cloud.
 &lt;/p&gt;
 &lt;p&gt;
  As we have pointed out many times, all Google would have to do is a bookkeeping trick, calling all infrastructure Google Cloud and having its search, ads, and video businesses pay it for services and it would be the largest cloud in the world. But that would just be a stunt – even if it would be very funny.
 &lt;/p&gt;
 &lt;p&gt;
  Google seems intent on having its cloud grow in its own way and separate from the mothership, Alphabet, and those other quasi-independent businesses. The word on the street is somewhere between 30 percent and 50 percent top line growth for Google Cloud in 2026, and we wonder if it can’t be more.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> AMD Finally Makes More Money On GPUs Than CPUs In A Quarter </title>
<link>https://www.nextplatform.com/2026/02/04/amd-finally-makes-more-money-on-gpus-than-cpus-in-a-quarter/#respond</link>
<pubDate>Wed, 04 Feb 2026 20:36:48 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/06/amd-lisa-su-mi355x-logo-673x438.jpg&quot; title=&quot;amd-lisa-su-mi355x-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Pent up demand for MI308 GPUs in China, which AMD has been trying to get a license to sell since early last year, were approved so that $360 million in Instinct GPU sales that were not officially part of the pipeline made their way onto the AMD books in Q4 2025. And with that move, for the first time in the history of the datacenter compute business at the chip maker – and rival to both Intel and Nvidia – that sales of Instinct GPUs outpaced sales of Epyc GPUs.
 &lt;/p&gt;
 &lt;p&gt;
  This is a moment that has been more than a decade in the making, and a testament to all of the hard work that AMD has done with its hardware and its software to be able to compete with Nvidia at all in GPUs.
 &lt;/p&gt;
 &lt;p&gt;
  That assessment comes from our own financial model of AMD, which has its share of educated guesses in it. But so does every other model put together on Wall Street. Our pal Aaron Rakers at Wells Fargo Securities estimates that Instinct GPU sales were between $2.5 billion and $2.6 billion in the quarter, and puts Epyc CPUs slightly ahead of GPUs. Even if he is right and we are wrong, it will not be long before GPU revenues at AMD almost always beat CPU revenues, simply because GPUs are more expensive and in higher demand.
 &lt;/p&gt;
 &lt;p&gt;
  Admittedly, this may be a one-off situation until the “Altair” MI400 series starts ramping in the second half of this year, with three main variations and one spare, if what we hear and see is correct. (AMD uses MI400 and MI450 kind of fluidly as terms.) We went over
  &lt;a href=&quot;https://www.nextplatform.com/2025/11/14/amd-solid-roadmaps-beget-money-which-beget-better-roadmaps-and-even-more-money/&quot;&gt;
   the different variations on the Altair GPU theme back in November 2025
  &lt;/a&gt;
  and are not going over all of that again. Suffice it to say that the Altair GPUs (which we gave their codename because AMD refuses to give us a synonym) and their “Helios” double-wide racks are going to be transformational to the AMD datacenter GPU business and foundational for the next level of that business.
 &lt;/p&gt;
 &lt;p&gt;
  Chief executive officer Lisa Su reiterated on a call with Wall Street analysts that its own datacenter business can grow at more than 60 percent per year over the next three to five years thanks to the two cylinder Epyc and Instinct engine, which will “scale our AI business to tens of billions in annual revenue in 2027.”
 &lt;/p&gt;
 &lt;p&gt;
  We took a stab at modeling AMD’s datacenter AI total addressable market and its business compared to Nvidia
  &lt;a href=&quot;https://www.nextplatform.com/2025/11/14/amd-solid-roadmaps-beget-money-which-beget-better-roadmaps-and-even-more-money/&quot;&gt;
   last November when Su started giving out broader guidance
  &lt;/a&gt;
  . AMD is being very careful not to forecast any particular number because of the vicissitudes of the component supply chain and the capriciousness of some customers. Which is actually the responsible thing to do for a public company. AMD is not alone in hoping that OpenAI comes through on its promises, which has the AI model maker committing to
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/06/did-amd-use-chatgpt-to-come-up-with-its-openai-partnership-deal/&quot;&gt;
   developing 6 gigawatts of AI compute based on AMD engines between the second half of 2026 and through October 2030
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  We are not here to do all of that again, but to put 2025 into the financial history books and move on to 2026, which is 35 days old and counting. Let’s dive in.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-rev-income-cash.jpg&quot; rel=&quot;attachment wp-att-146954&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;483&quot; sizes=&quot;(max-width: 639px) 100vw, 639px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-rev-income-cash.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-rev-income-cash.jpg 639w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-rev-income-cash-326x245.jpg 326w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-rev-income-cash-80x60.jpg 80w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-rev-income-cash-600x454.jpg 600w&quot; width=&quot;639&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in December 2025, AMD’s sales were $10.27 billion, a record quarter and up 34.1 percent year on year. This is the first time that AMD has broken through $10 billion in quarterly sales, and Q1 2026 might be the second if AMD can come in at the high end of its guidance, which is for $9.8 billion in sales, plus or minus $300 million.
 &lt;/p&gt;
 &lt;p&gt;
  After a strong Q4, the client and gaming chip business at AMD is going into a normal seasonal decline, and growth in the datacenter business will probably not be enough to offset that – but there’s a chance, as you see.
 &lt;/p&gt;
 &lt;p&gt;
  In the fourth quarter, the Data Center group at AMD posted sales of $5.38 billion, up 39.4 percent year on year and up 23.9 percent sequentially. Perhaps more importantly, operating income for the Data Center group was up 51.4 percent to $1.75 billion, representing a very healthy 32.6 percent of revenues. The Instinct MI355X ramp as well as healthy sales of both “Genoa” and Turin Epyc server processors are were the main drivers of sales.
 &lt;/p&gt;
 &lt;p&gt;
  Interestingly, all of the operating income for the other AMD groups – Client, Gaming, and Embedded – offset by corporate overhead of $1.08 billion in operating costs were perfectly balanced such that the same $1.75 billion passed through the Data Center group to the middle line for all of AMD. Even so, operating income more than doubled compared to a year ago, despite costs associated with the ZT Systems acquisition and spinout of its manufacturing business.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-groups.jpg&quot; rel=&quot;attachment wp-att-146953&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;455&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-groups.jpg&quot; width=&quot;550&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  For the full year, the Data Center group had $16.64 billion in sales, up 32.2 percent, with $3.6 billion in operating income, up only 3.5 percent year on year. Wall Street is upset about that last bit where operating profits are not rising anywhere as fast for all of 2025 as they did in its final quarter. Hopefully the profitability curve is improving for 2026 and 2027.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-datacenter-other.jpg&quot; rel=&quot;attachment wp-att-146951&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;402&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-datacenter-other.jpg&quot; width=&quot;552&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The interesting thing to do is to calculate the revenue for the rest of AMD. The Other half or so of AMD that is not Datacenter had a tad over $18 billion in sales in 2025, up a healthy 36.3 percent. The rest of AMD was growing faster, on an annual basis, than was the datacenter business. This is why you have to look at Nvidia and AMD and Intel and Broadcom on an annual basis, which accounts for the large ups and downs that comes with the product cycles set up between the chip makers and their hyperscaler and cloud builder customers. The product cycles drive the investment cycles, give or take supply chain static.
 &lt;/p&gt;
 &lt;p&gt;
  The other neat thing we like to do is separate out Epyc CPU sales from Instinct GPU sales, which we have been doing for a number of years now. We extract out a little money for embedded FPGAs aimed at the datacenter and network interface card sales as well, and then use the hints provided by AMD to break CPUs and GPUs apart. This worked reasonably well until a year ago when AMD’s hints got a lot weaker as it got skittish about making too many promises and saying too much about its Instinct GPU business. (That vertical red line delineates this change.)
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-epyc-cpu-vs-instinct-gpu.jpg&quot; rel=&quot;attachment wp-att-146952&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;405&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 541px) 100vw, 541px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-epyc-cpu-vs-instinct-gpu.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-epyc-cpu-vs-instinct-gpu.jpg 541w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-epyc-cpu-vs-instinct-gpu-326x245.jpg 326w, https://www.nextplatform.com/wp-content/uploads/2026/02/amd-q4-2025-epyc-cpu-vs-instinct-gpu-80x60.jpg 80w&quot; width=&quot;541&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As far as we can tell, AMD’s Epyc CPUs and Instinct GPUs both turned in the highest sales in the history of AMD’s datacenter business in Q4 2025, and as we said at the top of this story, the GPU business was helped by a $390 million shipment of MI308 crippled GPUs into China. Without those GPU sales into China, Instinct revenues would have been $2.26 billion, up 29.4 percent, in our model. With that $390 million bump up, Instinct GPU sales for Q4 2025 were $2.65 billion, up 51.7 percent.
 &lt;/p&gt;
 &lt;p&gt;
  We think Epyc CPU sales in the period were $2.51 billion, up 26.4 percent.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
</channel>
</rss>
