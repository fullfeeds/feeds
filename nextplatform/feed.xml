<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" version="2.0">
<channel><title>Next Platform</title>
<lastBuildDate>Sat, 01 Nov 2025 17:55:14 -0000</lastBuildDate>
<item>
<title> AWS “Bullish” On Homegrown Trainium AI Accelerators </title>
<link>https://www.nextplatform.com/2025/10/31/aws-bullish-on-homegrown-trainium-ai-accelerators/#respond</link>
<pubDate>Fri, 31 Oct 2025 17:41:10 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-rack-servers-logo-3-927x438.jpg&quot; title=&quot;aws-rack-servers-logo-3&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  One only need look at the incredible revenues and profits of the datacenter business at Nvidia to know that the world’s biggest compute customers – the hyperscalers, the cloud builders, and now the biggest model builders – need to bend the price/performance curve to boost their own profits.
 &lt;/p&gt;
 &lt;p&gt;
  And Amazon, with the Trainium AI accelerator, which appears to be used to do AI inferencing as well as the AI training that gives the product its name on the company’s SageMaker and Bedrock AI stacks. This seems to imply that AWS is sidelining the related Inferentia line of inference accelerators during the GenAI era. (Perhaps they should just call it
  &lt;em&gt;
   AInium
  &lt;/em&gt;
  ?)
 &lt;/p&gt;
 &lt;p&gt;
  The central them from the datacenter on the call with Wall Street analysts going over the financial results for Amazon and its Amazon Web Services cloud was that Trainium2 is going like gangbusters, and that the Trainium3 accelerator, which has been in development in conjunction with model builder and tight partner Anthropic and
  &lt;a href=&quot;https://www.nextplatform.com/2024/12/03/aws-reaps-the-benefits-of-the-custom-silicon-it-has-sown/&quot;&gt;
   which was previewed last December at the re:Invent 2024 conference
  &lt;/a&gt;
  , it getting ready to ramp.
 &lt;/p&gt;
 &lt;p&gt;
  We did a preview of the Trainium2 chip
  &lt;a href=&quot;https://www.nextplatform.com/2023/12/04/how-aws-can-undercut-nvidia-with-homegrown-ai-compute-engines/&quot;&gt;
   way back in December 2023
  &lt;/a&gt;
  , and we need to update this with the actual specs of the chip. We don’t know much about Trainium3, except that it is etched using 3 nanometer processes from Taiwan Semiconductor Manufacturing Co, will have 2X the performance of the current Trainium2 chip, and deliver 40 percent better energy efficiency (which we presume means better flops per watt).
 &lt;/p&gt;
 &lt;p&gt;
  Amazon, like other clouds, is trying to strike a happy balance using its own accelerators to drive profits and to underpin AI platform services while at the same time offering massive GPU capacity from Nvidia and sometimes AMD for those who want raw capacity to build their own platforms on the cloud. Thus far, only Google with the TPU and AWS with the Trainium have widely deployed their homegrown AI training accelerators. Microsoft is still working on its Maia chips and Meta Platforms is similarly not there yet with the training variants of its MTIA accelerators. (The Chinese hyperscalers and cloud builders are also working on their own CPUs and XPUs in varying degrees, or working with third parties like Huawei Technology’s HiSilicon unit to wean themselves off of Nvidia GPUs.)
 &lt;/p&gt;
 &lt;p&gt;
  Andy Jassy, who is now chief executive officer of Amazon but who ran AWS for more than a decade since its founding, said that Trainium2 capacity is fully subscribed and now represents a business with a multi-billion dollar run rate, with revenues up 2.5X sequentially from the second quarter.
 &lt;/p&gt;
 &lt;p&gt;
  Jassy said that a small number of large customers are using most of the Trainium2 capacity on its clouds, which he claimed offer 30 percent to 40 percent better bang for the buck on AI workloads “than other options out there,” as he put it. And because customers want to get better price/performance as they deploy AI applications in production, there is a lot of demand for Trainium2 instances on AWS. Jassy added that “the majority of token usage in Amazon Bedrock is already running on Trainium,” by which we think he meant to say that the majority of context tokens processed and the majority of output tokens generated on Bedrock are being chewed on and created by computations on Tranium2 and sometimes Trainium1 or Inferentia2.
 &lt;/p&gt;
 &lt;p&gt;
  Jassy also said that Anthropic was training its latest Claude models in the 4.X generation using the “project Ranier” supercluster that
  &lt;a href=&quot;https://www.nextplatform.com/2024/12/03/aws-reaps-the-benefits-of-the-custom-silicon-it-has-sown/&quot;&gt;
   the company revealed back in December 2024
  &lt;/a&gt;
  . At the time, AWS and Anthropic said that Project Ranier would have “hundreds of thousands” of Trainium2 chips and would have 5X the performance of the GPU clusters that Anthropic had used to train its Claude 3 generation of models.
 &lt;/p&gt;
 &lt;p&gt;
  As it turns out, Ranier is more of a beast than people might have thought, with 500,000 Tranium2 chips, according to Jassy and a plan to expand that to 1 million Trainium2 chips by the end of this year.
 &lt;/p&gt;
 &lt;p&gt;
  As for Trainium3, Jassy said that it will be in preview by the end of the year (which means we can expect to see more about it at the re:Invent 2025 conference in December), with “fuller volumes coming in the beginning of 2026,” as he put it, adding that AWS has a lot of large and medium-sized customers “who are quite interested in Trainium3.” That stands to reason if instances on AWS will offer 4X the aggregate capacity and 2X the per chip capacity compared to Trainium2 UltraClusters. Companies like Anthropic can string together much larger collections of instances, much as OpenAI has gotten very large clusters on Microsoft Azure historically than other customers can rent.
 &lt;/p&gt;
 &lt;p&gt;
  “So we have to, of course, deliver the chip,” Jassy quipped, referring to Trainium3. “We have to deliver it in volumes and deliver it quickly. And we have to continue to work on the software ecosystem, which gets better all the time. And as we have more proof points like we have with Project Rainier with what Anthropic is doing on Trainium2, it builds increasing credibility for Trainium. And I think customers are very bullish about it. I’m bullish about it as well.”
 &lt;/p&gt;
 &lt;p&gt;
  The other interesting thing that Jassy talked about on the call with Wall Street was the amount of datacenter capacity that AWS is bringing online. Jassy said that “in the last year,” by which we think he meant the trailing twelve months – a metric Amazon uses a lot – AWS had fired up 3.8 GW of datacenter capacity, and it has another 1 GW coming online in the fourth quarter. Jassy did not give a figure for total installed capacity of AWS datacenters, but said that it would double between now and the end of 2027. It doubled from the end of 2022 to the present.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;strong&gt;
   “
  &lt;/strong&gt;
  So we are bringing in quite a bit of capacity today,” Jassy explained. “And overall in the industry, maybe the bottleneck is power. I think at some point, it may move to chips, but we’re bringing in quite a bit of capacity. And as fast as we’re bringing in right now, we are monetizing it.”
 &lt;/p&gt;
 &lt;p&gt;
  Given this, let’s say that AWS had maybe 4 GW of total datacenter capacity at the end of 2022 and will have 10 GW by the end of 2025. That would imply maybe somewhere around 20 GW total two years from now. For AI datacenters, you are talking about somewhere on the order of $50 billion per GW for Nvidia infrastructure, and maybe $37 billion per GW for homegrown accelerators like the Trainiums. An incremental 10 GW, assuming half GPUs and half Trainiums represents something on the order of $435 billion in datacenter spending for 2026 and 2027.
  &lt;em&gt;
   This sounds nuts
  &lt;/em&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  To match a mere 40 percent growth in GW capacity for both 2026 and 2027, assuming AWS will spend $106.7 billion in IT gear in 2025 – the vast majority of its expected $125 billion in capital expenses for the year, and almost all of that for AI infrastructure – you have to start with 1.95 GW as 2022 comes to a close, you hit 5.9 GW as 2025 closes and that means 11.8 GW by the end of 2027, with $256.7 billion in IT spending over 2026 and 2027 inclusive. This sounds relatively
  &lt;em&gt;
   more sane
  &lt;/em&gt;
  , and it also implies that megawatts very quickly became small potatoes in this modern GenAI era when this was the capacity of big datacenters over the past decade or two.
 &lt;/p&gt;
 &lt;p&gt;
  And with all of that as a backdrop, let’s dig into the Amazon Web Services numbers for the third quarter of 2025.
 &lt;/p&gt;
 &lt;p&gt;
  Amazon overall, including its retail, advertising, media, and cloud businesses, raked in $180.17 billion, up 13.4 percent, with net income of $21.19 billion, up 38.2 percent.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-revenue-income.jpg&quot; rel=&quot;attachment wp-att-146555&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;393&quot; sizes=&quot;(max-width: 626px) 100vw, 626px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-revenue-income.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-revenue-income.jpg 626w, https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-revenue-income-600x377.jpg 600w&quot; width=&quot;626&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The AWS cloud accelerated its revenue growth a bit, thanks in large part to the GenAI boom. AWS revenues rose by 20.2 percent to just a smidgen over $33 billion, and operating income was under pressure and only grew 9.4 percent to $11.43 billion.
 &lt;/p&gt;
 &lt;p&gt;
  That operating income represented 34.6 percent of revenues in the period, still a little bit lower than average but not surprising given the cost of ramping “Blackwell” GPU systems at the same time as the Trainium2 machinery in its datacenters and regions.
 &lt;/p&gt;
 &lt;p&gt;
  Parent company Amazon had a little over $36 billion in capital expenses, and we think that $26.4 billion of that of that was for IT infrastructure, with $28.2 billion of that being for AI clusters and $2.6 billion being for other kinds of IT infrastructure for web and data analytics workloads based on CPU architectures.
 &lt;/p&gt;
 &lt;p&gt;
  It is hard to say for sure, but we think that spending on Trainium accelerators might have been somewhere around 35 percent of the AI spending, with the other 65 percent on GPU-based systems. We would not be surprised if by the end of 2026 or early in 2027 that half of the AI compute engine capacity installed by was for Trainium gear, but more than half of the money being allocated for GPU gear that rents at a much higher price on AWS.
 &lt;/p&gt;
 &lt;p&gt;
  We have been using spreadsheet witchcraft for many years to try to estimate how revenue streams at AWS split across software, compute, networking, and storage, and we think, as you can see in the chart below, compute is catching up to software during the GenAI boom:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-categories.jpg&quot; rel=&quot;attachment wp-att-146553&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;381&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 615px) 100vw, 615px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-categories.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-categories.jpg 615w, https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-categories-600x372.jpg 600w&quot; width=&quot;615&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In fact, it will not be long before we flash back to the future of the past of AWS when EC2 was a bigger business than S3 and compute, in the aggregate across CPU, GPU, and Trainium instances will match software revenues for things like databases and data stores or Lambda serverless processing or AI frameworks and models like those gathered up in the SageMaker and Bedrock services.
 &lt;/p&gt;
 &lt;p&gt;
  One thing: When an AWS revenue line in the chart above is flat, that means it is growing modestly because AWS is continually trying to lower the cost of a unit of compute, storage, or networking capacity to make it easier to consume more. And if a line is going up, capacity is rising faster than that line for the same reason over the course of years.
 &lt;/p&gt;
 &lt;p&gt;
  That leaves us with our estimation of the “real” systems business of AWS, which is the combination of compute, storage, and networking. When you add this up, the core AWS systems business drove $19.47 billion in sales, according to our model, up 33.8 percent, with operating income of $5.73 billion, up 21.8 percent.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> AWS “Bullish” On Homegrown Trainium AI Accelerators </title>
<link>https://www.nextplatform.com/2025/10/31/aws-bullish-on-homegrown-trainium-ai-accelerators/</link>
<pubDate>Fri, 31 Oct 2025 17:41:07 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-rack-servers-logo-3-927x438.jpg&quot; title=&quot;aws-rack-servers-logo-3&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  One only need look at the incredible revenues and profits of the datacenter business at Nvidia to know that the world’s biggest compute customers – the hyperscalers, the cloud builders, and now the biggest model builders – need to bend the price/performance curve to boost their own profits.
 &lt;/p&gt;
 &lt;p&gt;
  And Amazon, with the Trainium AI accelerator, which appears to be used to do AI inferencing as well as the AI training that gives the product its name on the company’s SageMaker and Bedrock AI stacks. This seems to imply that AWS is sidelining the related Inferentia line of inference accelerators during the GenAI era. (Perhaps they should just call it
  &lt;em&gt;
   AInium
  &lt;/em&gt;
  ?)
 &lt;/p&gt;
 &lt;p&gt;
  The central them from the datacenter on the call with Wall Street analysts going over the financial results for Amazon and its Amazon Web Services cloud was that Trainium2 is going like gangbusters, and that the Trainium3 accelerator, which has been in development in conjunction with model builder and tight partner Anthropic and
  &lt;a href=&quot;https://www.nextplatform.com/2024/12/03/aws-reaps-the-benefits-of-the-custom-silicon-it-has-sown/&quot;&gt;
   which was previewed last December at the re:Invent 2024 conference
  &lt;/a&gt;
  , it getting ready to ramp.
 &lt;/p&gt;
 &lt;p&gt;
  We did a preview of the Trainium2 chip
  &lt;a href=&quot;https://www.nextplatform.com/2023/12/04/how-aws-can-undercut-nvidia-with-homegrown-ai-compute-engines/&quot;&gt;
   way back in December 2023
  &lt;/a&gt;
  , and we need to update this with the actual specs of the chip. We don’t know much about Trainium3, except that it is etched using 3 nanometer processes from Taiwan Semiconductor Manufacturing Co, will have 2X the performance of the current Trainium2 chip, and deliver 40 percent better energy efficiency (which we presume means better flops per watt).
 &lt;/p&gt;
 &lt;p&gt;
  Amazon, like other clouds, is trying to strike a happy balance using its own accelerators to drive profits and to underpin AI platform services while at the same time offering massive GPU capacity from Nvidia and sometimes AMD for those who want raw capacity to build their own platforms on the cloud. Thus far, only Google with the TPU and AWS with the Trainium have widely deployed their homegrown AI training accelerators. Microsoft is still working on its Maia chips and Meta Platforms is similarly not there yet with the training variants of its MTIA accelerators. (The Chinese hyperscalers and cloud builders are also working on their own CPUs and XPUs in varying degrees, or working with third parties like Huawei Technology’s HiSilicon unit to wean themselves off of Nvidia GPUs.)
 &lt;/p&gt;
 &lt;p&gt;
  Andy Jassy, who is now chief executive officer of Amazon but who ran AWS for more than a decade since its founding, said that Trainium2 capacity is fully subscribed and now represents a business with a multi-billion dollar run rate, with revenues up 2.5X sequentially from the second quarter.
 &lt;/p&gt;
 &lt;p&gt;
  Jassy said that a small number of large customers are using most of the Trainium2 capacity on its clouds, which he claimed offer 30 percent to 40 percent better bang for the buck on AI workloads “than other options out there,” as he put it. And because customers want to get better price/performance as they deploy AI applications in production, there is a lot of demand for Trainium2 instances on AWS. Jassy added that “the majority of token usage in Amazon Bedrock is already running on Trainium,” by which we think he meant to say that the majority of context tokens processed and the majority of output tokens generated on Bedrock are being chewed on and created by computations on Tranium2 and sometimes Trainium1 or Inferentia2.
 &lt;/p&gt;
 &lt;p&gt;
  Jassy also said that Anthropic was training its latest Claude models in the 4.X generation using the “project Ranier” supercluster that
  &lt;a href=&quot;https://www.nextplatform.com/2024/12/03/aws-reaps-the-benefits-of-the-custom-silicon-it-has-sown/&quot;&gt;
   the company revealed back in December 2024
  &lt;/a&gt;
  . At the time, AWS and Anthropic said that Project Ranier would have “hundreds of thousands” of Trainium2 chips and would have 5X the performance of the GPU clusters that Anthropic had used to train its Claude 3 generation of models.
 &lt;/p&gt;
 &lt;p&gt;
  As it turns out, Ranier is more of a beast than people might have thought, with 500,000 Tranium2 chips, according to Jassy and a plan to expand that to 1 million Trainium2 chips by the end of this year.
 &lt;/p&gt;
 &lt;p&gt;
  As for Trainium3, Jassy said that it will be in preview by the end of the year (which means we can expect to see more about it at the re:Invent 2025 conference in December), with “fuller volumes coming in the beginning of 2026,” as he put it, adding that AWS has a lot of large and medium-sized customers “who are quite interested in Trainium3.” That stands to reason if instances on AWS will offer 4X the aggregate capacity and 2X the per chip capacity compared to Trainium2 UltraClusters. Companies like Anthropic can string together much larger collections of instances, much as OpenAI has gotten very large clusters on Microsoft Azure historically than other customers can rent.
 &lt;/p&gt;
 &lt;p&gt;
  “So we have to, of course, deliver the chip,” Jassy quipped, referring to Trainium3. “We have to deliver it in volumes and deliver it quickly. And we have to continue to work on the software ecosystem, which gets better all the time. And as we have more proof points like we have with Project Rainier with what Anthropic is doing on Trainium2, it builds increasing credibility for Trainium. And I think customers are very bullish about it. I’m bullish about it as well.”
 &lt;/p&gt;
 &lt;p&gt;
  The other interesting thing that Jassy talked about on the call with Wall Street was the amount of datacenter capacity that AWS is bringing online. Jassy said that “in the last year,” by which we think he meant the trailing twelve months – a metric Amazon uses a lot – AWS had fired up 3.8 GW of datacenter capacity, and it has another 1 GW coming online in the fourth quarter. Jassy did not give a figure for total installed capacity of AWS datacenters, but said that it would double between now and the end of 2027. It doubled from the end of 2022 to the present.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;strong&gt;
   “
  &lt;/strong&gt;
  So we are bringing in quite a bit of capacity today,” Jassy explained. “And overall in the industry, maybe the bottleneck is power. I think at some point, it may move to chips, but we’re bringing in quite a bit of capacity. And as fast as we’re bringing in right now, we are monetizing it.”
 &lt;/p&gt;
 &lt;p&gt;
  Given this, let’s say that AWS had maybe 4 GW of total datacenter capacity at the end of 2022 and will have 10 GW by the end of 2025. That would imply maybe somewhere around 20 GW total two years from now. For AI datacenters, you are talking about somewhere on the order of $50 billion per GW for Nvidia infrastructure, and maybe $37 billion per GW for homegrown accelerators like the Trainiums. An incremental 10 GW, assuming half GPUs and half Trainiums represents something on the order of $435 billion in datacenter spending for 2026 and 2027.
  &lt;em&gt;
   This sounds nuts
  &lt;/em&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  To match a mere 40 percent growth in GW capacity for both 2026 and 2027, assuming AWS will spend $106.7 billion in IT gear in 2025 – the vast majority of its expected $125 billion in capital expenses for the year, and almost all of that for AI infrastructure – you have to start with 1.95 GW as 2022 comes to a close, you hit 5.9 GW as 2025 closes and that means 11.8 GW by the end of 2027, with $256.7 billion in IT spending over 2026 and 2027 inclusive. This sounds relatively
  &lt;em&gt;
   more sane
  &lt;/em&gt;
  , and it also implies that megawatts very quickly became small potatoes in this modern GenAI era when this was the capacity of big datacenters over the past decade or two.
 &lt;/p&gt;
 &lt;p&gt;
  And with all of that as a backdrop, let’s dig into the Amazon Web Services numbers for the third quarter of 2025.
 &lt;/p&gt;
 &lt;p&gt;
  Amazon overall, including its retail, advertising, media, and cloud businesses, raked in $180.17 billion, up 13.4 percent, with net income of $21.19 billion, up 38.2 percent.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-revenue-income.jpg&quot; rel=&quot;attachment wp-att-146555&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;393&quot; sizes=&quot;(max-width: 626px) 100vw, 626px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-revenue-income.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-revenue-income.jpg 626w, https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-revenue-income-600x377.jpg 600w&quot; width=&quot;626&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The AWS cloud accelerated its revenue growth a bit, thanks in large part to the GenAI boom. AWS revenues rose by 20.2 percent to just a smidgen over $33 billion, and operating income was under pressure and only grew 9.4 percent to $11.43 billion.
 &lt;/p&gt;
 &lt;p&gt;
  That operating income represented 34.6 percent of revenues in the period, still a little bit lower than average but not surprising given the cost of ramping “Blackwell” GPU systems at the same time as the Trainium2 machinery in its datacenters and regions.
 &lt;/p&gt;
 &lt;p&gt;
  Parent company Amazon had a little over $36 billion in capital expenses, and we think that $26.4 billion of that of that was for IT infrastructure, with $28.2 billion of that being for AI clusters and $2.6 billion being for other kinds of IT infrastructure for web and data analytics workloads based on CPU architectures.
 &lt;/p&gt;
 &lt;p&gt;
  It is hard to say for sure, but we think that spending on Trainium accelerators might have been somewhere around 35 percent of the AI spending, with the other 65 percent on GPU-based systems. We would not be surprised if by the end of 2026 or early in 2027 that half of the AI compute engine capacity installed by was for Trainium gear, but more than half of the money being allocated for GPU gear that rents at a much higher price on AWS.
 &lt;/p&gt;
 &lt;p&gt;
  We have been using spreadsheet witchcraft for many years to try to estimate how revenue streams at AWS split across software, compute, networking, and storage, and we think, as you can see in the chart below, compute is catching up to software during the GenAI boom:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-categories.jpg&quot; rel=&quot;attachment wp-att-146553&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;381&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 615px) 100vw, 615px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-categories.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-categories.jpg 615w, https://www.nextplatform.com/wp-content/uploads/2025/10/aws-q3-2025-categories-600x372.jpg 600w&quot; width=&quot;615&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In fact, it will not be long before we flash back to the future of the past of AWS when EC2 was a bigger business than S3 and compute, in the aggregate across CPU, GPU, and Trainium instances will match software revenues for things like databases and data stores or Lambda serverless processing or AI frameworks and models like those gathered up in the SageMaker and Bedrock services.
 &lt;/p&gt;
 &lt;p&gt;
  One thing: When an AWS revenue line in the chart above is flat, that means it is growing modestly because AWS is continually trying to lower the cost of a unit of compute, storage, or networking capacity to make it easier to consume more. And if a line is going up, capacity is rising faster than that line for the same reason over the course of years.
 &lt;/p&gt;
 &lt;p&gt;
  That leaves us with our estimation of the “real” systems business of AWS, which is the combination of compute, storage, and networking. When you add this up, the core AWS systems business drove $19.47 billion in sales, according to our model, up 33.8 percent, with operating income of $5.73 billion, up 21.8 percent.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Microsoft: Getting Margins From AI Means Sometimes Saying No </title>
<link>https://www.nextplatform.com/2025/10/31/microsoft-getting-margins-from-ai-means-sometimes-saying-no/#respond</link>
<pubDate>Fri, 31 Oct 2025 13:08:28 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2023/11/microsoft-ignite-maiai-rack-607x438.jpg&quot; title=&quot;microsoft-ignite-maiai-rack&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Hot on the heels of an updated contract with OpenAI that will see the AI model builder commit to spending an incremental $250 billion for Azure infrastructure, Microsoft has reported that it has a revenue backlog dominated by its Azure cloud that stood at $392 billion as the first quarter of its fiscal 2026 year ended in September. It may not be clear how OpenAI is going to get the money to honor those contracts, but Big Bill is sitting pretty with a $642 billion revenue backlog this week that is still growing as we speak.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://blogs.microsoft.com/blog/2025/10/28/the-next-chapter-of-the-microsoft-openai-partnership/&quot;&gt;
   Under that deal
  &lt;/a&gt;
  , Microsoft will have a 27 percent stake in OpenAI worth $135 billion based on the current $500 billion valuation that OpenAI had after its latest funding round in March this year, bringing its total to $57.9 billion not including share sales by early investors. That deal also includes giving Microsoft exclusive IP rights and model API exclusivity on the Azure cloud until artificial general intelligence – models matching the skills of humans – is reached. Microsoft also has secured GPT model rights out to 2032 in the event that AGI is not reached by 2030. Microsoft no longer has the right of first refusal on OpenAI buying compute capacity, and OpenAI can give API access to GPT models to an US national security customer (this was not defined in the statement, but was no doubt outlined clearly in the contract to include DHS, NSA, FBI, and CIA users).
 &lt;/p&gt;
 &lt;p&gt;
  The number of years for that incremental $250 billion to be spent was not revealed, but we think, as do others, that this runs out to 2032.
 &lt;/p&gt;
 &lt;p&gt;
  Satya Nadella, Microsoft’s chief executive officer, explained on the call with Wall Street analysts going over the Q1 numbers that Microsoft is not just running around trying to buy every GPU it can, building every datacenter it can, and chasing every GenAI customer that it can. The company has a decidedly calmer and methodical approach, which is why the company’s margin is not as diluted as the OpenAI deal (which no doubt has equity as a compensation for what would otherwise be a higher price for compute) and a heavy GPU fleet dominated by a few customers might imply. Take a listen:
 &lt;/p&gt;
 &lt;blockquote&gt;
  &lt;p&gt;
   &lt;em&gt;
    “For us, again, it just always goes back to the core principle, which is build a fleet that is fungible across the planet and works for third party and first party and research. So that is essentially what we have done.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “And so when some demand comes in a shape that doesn’t fit that goal, where it’s too concentrated, not just by customer, by location, by type of skewing. . . . When you think about the margin profile of a hyperscaler, you have got to remember this: The AI accelerator piece, but there’s compute, there’s storage. And so if all of the demand just comes for just one metric, that’s really not a long-term business we want to be in. That’s even from a third party. We have to balance it with all of our first-party stuff because that is,  after all, a different margin stack for us. And then we have to fund our own R&amp;D and model capability because in the long run, that is what is going to differentiate us.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “And so I look at all of those, and we use all of that to make sure we are saying ‘Yes’ to all the demand that we want, we say ‘No’ to some of the demand that may be something that we could serve but it’s not in our long-term interest. And so that’s sort of the decision-making we have done, and we feel very, very good about the decisions. In some sense, I feel even better. Each time we say ‘No,’ the day after, I feel better.”
   &lt;/em&gt;
  &lt;/p&gt;
 &lt;/blockquote&gt;
 &lt;p&gt;
  To fulfill that expanded OpenAI contract – as well as thousands of other “smaller” ones that would be very large in any other context – Microsoft is going to have to spend big bucks on infrastructure for Azure, and it indeed has done that in its first quarter. Microsoft spent an incredible $34.9 billion on capital expenses in the September quarter, which was 74.5 percent larger than the year ago period. And even with that, thanks to its $72.3 billion portfolio of investments that keeps swelling, the company was able to end the quarter with $102 billion in cash and equivalents as the September quarter came to a close.
 &lt;/p&gt;
 &lt;p&gt;
  One of the reasons why this is the case is that Microsoft has a vast base of productivity and enterprise application customers as well as the Windows platform on PCs and the Windows Server platform on servers, and now compute and storage capacity rentals on the Azure cloud.
 &lt;/p&gt;
 &lt;p&gt;
  In the September quarter, Microsoft posted sales of $77.67 billion, up 18.4 percent, and operating income was $37.96 billion, up 24.3 percent. After paying taxes and dividends, doing share buybacks, and some other stuff, Microsoft had a net income of $27.75 billion, up 12.5 percent and representing 35.7 percent of revenues.
 &lt;/p&gt;
 &lt;p&gt;
  As
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  , part of Microsoft that we care about most as the chronicler of platforms is the Intelligent Cloud group, which includes the Windows Server stack and Azure together. To be more precise, Intelligent Cloud encompasses Windows Server, SQL Server, Visual Studio, and other middleware and tools sold into the datacenter as well as compute, storage, and networking capacity on the Azure cloud, plus related platform PaaS and SaaS services Microsoft peddles into the datacenter and on Azure.
 &lt;/p&gt;
 &lt;p&gt;
  The Intelligent Cloud revenues were up 28.2 percent to $30.9 billion, with operating income of $13.39 billion, representing 43.3 percent of revenues and up 27.5 percent year on year.
 &lt;/p&gt;
 &lt;p&gt;
  To further confused everyone, Microsoft lumps all of its cloud-based software and services into a single category called Microsoft Cloud. This category (which is not a formal business group), includes Azure but it is not solely Azure. It absolutely includes anything sold under a utility model from Microsoft using its infrastructure – so think of it as infrastructure, software, and platform as a service for both servers and PCs all rolled up into one.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2025-microsoft-cloud.jpg&quot; rel=&quot;attachment wp-att-146548&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;413&quot; sizes=&quot;(max-width: 541px) 100vw, 541px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2025-microsoft-cloud.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2025-microsoft-cloud.jpg 541w, https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2025-microsoft-cloud-80x60.jpg 80w&quot; width=&quot;541&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Microsoft Cloud revenues were just a tad over $49 billion, up 26 percent, with gross profits of 68 percent, which works out to $33.35 billion. (With all those billions, you can see how Microsoft can play the GenAI platform game.)
 &lt;/p&gt;
 &lt;p&gt;
  Microsoft does not break the Azure cloud out separately from this generic cloud category, but our model shows Azure brought in $20.72 billion, up 39 percent, and posted an operating income of $9.43 billion, up 45.1 percent and comprising 45.5 percent of Azure revenues. Azure has doubled in size in two years by our math, and there is no reason to believe it will not double in half the time given the GenAI boom.
 &lt;/p&gt;
 &lt;p&gt;
  Azure is only one way of using a Microsoft platform. The other way is to buy some servers, switches, and storage and build it yourself on a Windows Server stack. If you add up Azure with the Windows Server platform, you get what we call Microsoft’s “real” systems revenues and operating profit.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems.jpg&quot; rel=&quot;attachment wp-att-146550&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;405&quot; sizes=&quot;(max-width: 541px) 100vw, 541px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems.jpg 541w, https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems-326x245.jpg 326w, https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems-80x60.jpg 80w&quot; width=&quot;541&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We think, based on our model, that Microsoft’s “real” systems sales in Q1 F2026 were $23.67 billion, up 43.3 percent, and operating profit for this collection of products and services had an operating income of $10.26 billion, up 42.4 percent and accounting for 43.3 percent of sales. This includes base operating systems and middleware, not databases and development tools that run on Windows Server, whether it is on premises or on the Azure cloud.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Microsoft: Getting Margins From AI Means Sometimes Saying No </title>
<link>https://www.nextplatform.com/2025/10/31/microsoft-getting-margins-from-ai-means-sometimes-saying-no/</link>
<pubDate>Fri, 31 Oct 2025 13:08:25 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2023/11/microsoft-ignite-maiai-rack-607x438.jpg&quot; title=&quot;microsoft-ignite-maiai-rack&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Hot on the heels of an updated contract with OpenAI that will see the AI model builder commit to spending an incremental $250 billion for Azure infrastructure, Microsoft has reported that it has a revenue backlog dominated by its Azure cloud that stood at $392 billion as the first quarter of its fiscal 2026 year ended in September. It may not be clear how OpenAI is going to get the money to honor those contracts, but Big Bill is sitting pretty with a $642 billion revenue backlog this week that is still growing as we speak.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://blogs.microsoft.com/blog/2025/10/28/the-next-chapter-of-the-microsoft-openai-partnership/&quot;&gt;
   Under that deal
  &lt;/a&gt;
  , Microsoft will have a 27 percent stake in OpenAI worth $135 billion based on the current $500 billion valuation that OpenAI had after its latest funding round in March this year, bringing its total to $57.9 billion not including share sales by early investors. That deal also includes giving Microsoft exclusive IP rights and model API exclusivity on the Azure cloud until artificial general intelligence – models matching the skills of humans – is reached. Microsoft also has secured GPT model rights out to 2032 in the event that AGI is not reached by 2030. Microsoft no longer has the right of first refusal on OpenAI buying compute capacity, and OpenAI can give API access to GPT models to an US national security customer (this was not defined in the statement, but was no doubt outlined clearly in the contract to include DHS, NSA, FBI, and CIA users).
 &lt;/p&gt;
 &lt;p&gt;
  The number of years for that incremental $250 billion to be spent was not revealed, but we think, as do others, that this runs out to 2032.
 &lt;/p&gt;
 &lt;p&gt;
  Satya Nadella, Microsoft’s chief executive officer, explained on the call with Wall Street analysts going over the Q1 numbers that Microsoft is not just running around trying to buy every GPU it can, building every datacenter it can, and chasing every GenAI customer that it can. The company has a decidedly calmer and methodical approach, which is why the company’s margin is not as diluted as the OpenAI deal (which no doubt has equity as a compensation for what would otherwise be a higher price for compute) and a heavy GPU fleet dominated by a few customers might imply. Take a listen:
 &lt;/p&gt;
 &lt;blockquote&gt;
  &lt;p&gt;
   &lt;em&gt;
    “For us, again, it just always goes back to the core principle, which is build a fleet that is fungible across the planet and works for third party and first party and research. So that is essentially what we have done.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “And so when some demand comes in a shape that doesn’t fit that goal, where it’s too concentrated, not just by customer, by location, by type of skewing. . . . When you think about the margin profile of a hyperscaler, you have got to remember this: The AI accelerator piece, but there’s compute, there’s storage. And so if all of the demand just comes for just one metric, that’s really not a long-term business we want to be in. That’s even from a third party. We have to balance it with all of our first-party stuff because that is,  after all, a different margin stack for us. And then we have to fund our own R&amp;D and model capability because in the long run, that is what is going to differentiate us.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “And so I look at all of those, and we use all of that to make sure we are saying ‘Yes’ to all the demand that we want, we say ‘No’ to some of the demand that may be something that we could serve but it’s not in our long-term interest. And so that’s sort of the decision-making we have done, and we feel very, very good about the decisions. In some sense, I feel even better. Each time we say ‘No,’ the day after, I feel better.”
   &lt;/em&gt;
  &lt;/p&gt;
 &lt;/blockquote&gt;
 &lt;p&gt;
  To fulfill that expanded OpenAI contract – as well as thousands of other “smaller” ones that would be very large in any other context – Microsoft is going to have to spend big bucks on infrastructure for Azure, and it indeed has done that in its first quarter. Microsoft spent an incredible $34.9 billion on capital expenses in the September quarter, which was 74.5 percent larger than the year ago period. And even with that, thanks to its $72.3 billion portfolio of investments that keeps swelling, the company was able to end the quarter with $102 billion in cash and equivalents as the September quarter came to a close.
 &lt;/p&gt;
 &lt;p&gt;
  One of the reasons why this is the case is that Microsoft has a vast base of productivity and enterprise application customers as well as the Windows platform on PCs and the Windows Server platform on servers, and now compute and storage capacity rentals on the Azure cloud.
 &lt;/p&gt;
 &lt;p&gt;
  In the September quarter, Microsoft posted sales of $77.67 billion, up 18.4 percent, and operating income was $37.96 billion, up 24.3 percent. After paying taxes and dividends, doing share buybacks, and some other stuff, Microsoft had a net income of $27.75 billion, up 12.5 percent and representing 35.7 percent of revenues.
 &lt;/p&gt;
 &lt;p&gt;
  As
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  , part of Microsoft that we care about most as the chronicler of platforms is the Intelligent Cloud group, which includes the Windows Server stack and Azure together. To be more precise, Intelligent Cloud encompasses Windows Server, SQL Server, Visual Studio, and other middleware and tools sold into the datacenter as well as compute, storage, and networking capacity on the Azure cloud, plus related platform PaaS and SaaS services Microsoft peddles into the datacenter and on Azure.
 &lt;/p&gt;
 &lt;p&gt;
  The Intelligent Cloud revenues were up 28.2 percent to $30.9 billion, with operating income of $13.39 billion, representing 43.3 percent of revenues and up 27.5 percent year on year.
 &lt;/p&gt;
 &lt;p&gt;
  To further confused everyone, Microsoft lumps all of its cloud-based software and services into a single category called Microsoft Cloud. This category (which is not a formal business group), includes Azure but it is not solely Azure. It absolutely includes anything sold under a utility model from Microsoft using its infrastructure – so think of it as infrastructure, software, and platform as a service for both servers and PCs all rolled up into one.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2025-microsoft-cloud.jpg&quot; rel=&quot;attachment wp-att-146548&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;413&quot; sizes=&quot;(max-width: 541px) 100vw, 541px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2025-microsoft-cloud.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2025-microsoft-cloud.jpg 541w, https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2025-microsoft-cloud-80x60.jpg 80w&quot; width=&quot;541&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Microsoft Cloud revenues were just a tad over $49 billion, up 26 percent, with gross profits of 68 percent, which works out to $33.35 billion. (With all those billions, you can see how Microsoft can play the GenAI platform game.)
 &lt;/p&gt;
 &lt;p&gt;
  Microsoft does not break the Azure cloud out separately from this generic cloud category, but our model shows Azure brought in $20.72 billion, up 39 percent, and posted an operating income of $9.43 billion, up 45.1 percent and comprising 45.5 percent of Azure revenues. Azure has doubled in size in two years by our math, and there is no reason to believe it will not double in half the time given the GenAI boom.
 &lt;/p&gt;
 &lt;p&gt;
  Azure is only one way of using a Microsoft platform. The other way is to buy some servers, switches, and storage and build it yourself on a Windows Server stack. If you add up Azure with the Windows Server platform, you get what we call Microsoft’s “real” systems revenues and operating profit.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems.jpg&quot; rel=&quot;attachment wp-att-146550&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;405&quot; sizes=&quot;(max-width: 541px) 100vw, 541px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems.jpg 541w, https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems-326x245.jpg 326w, https://www.nextplatform.com/wp-content/uploads/2025/10/microsoft-q1-f2026-real-systems-80x60.jpg 80w&quot; width=&quot;541&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We think, based on our model, that Microsoft’s “real” systems sales in Q1 F2026 were $23.67 billion, up 43.3 percent, and operating profit for this collection of products and services had an operating income of $10.26 billion, up 42.4 percent and accounting for 43.3 percent of sales. This includes base operating systems and middleware, not databases and development tools that run on Windows Server, whether it is on premises or on the Azure cloud.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Google Spends More On Servers Than The Whole World Used To </title>
<link>https://www.nextplatform.com/2025/10/30/google-spends-more-on-servers-than-the-whole-world-used-to/#respond</link>
<pubDate>Thu, 30 Oct 2025 21:58:50 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/08/google-tpu-racks-550x438.jpg&quot; title=&quot;google-tpu-racks&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Here is a funny number to chew on. Sometime in the early part of 2026, if current trends persist, Google will have a spending rate on servers that is in excess of the inflation adjusted spending levels set by the entire world in the wake of the Dot Com bust. And it will not be long before Google’s spending rate on servers – all by its lonesome – matches the spending rate during the Dot Com boom.
 &lt;/p&gt;
 &lt;p&gt;
  Right now, given all of the numbers that Google has put out, we project that it will spend $55.2 billion on servers in 2025, and that beats the anemic $51.5 billion the world spent on servers in 2009 when the Great Recession caused a 20 percent decline in server spending (these figures are expressed in 2021 US dollars) and it nearly matches the $57.7 billion that was spent worldwide on all kinds of iron in the following year.
 &lt;/p&gt;
 &lt;p&gt;
  That spending rate by Google is what happens when all of the hyperscalers, cloud builders, and now the model builders have to spend $3 million, or $4 million, or even $5 million for a rackscale accelerated computer that has coherent memory across its CPUs and GPUs to run AI training and inference so that we can augment (many think replace is a better word) the parts of human thinking involved with data retrieval and synthesis for many orders of magnitude more energy (145 kilowatts per rack compared to 20 watts for the human brain) but with a lot more speed and arguably a lot less complaining.
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in September, the top brass at Google said that the company, which includes the Alphabet shell company and some do-dads that are not ad serving, video serving, and cloud computing, spent $23.95 billion on capital expenditures, with 60 percent of that being for servers and the remaining amount for datacenter facilities and other gear such as networking. (For Google, it seems, a server is tuned for storage but not distinct, which we get.) If you do the math on that, you get $14.37 billion spent on servers in Q3 2025. If you take the midpoint of the new – and higher – estimate for capital expense spending for 2025, which is between $91 billion and $93 billion, and multiply by the same ratio, you get that $55.2 billion figure. And if you project out spending at the same quarterly sequential rate, which we think is probably a low ball estimate for growth in the coming year unless the GenAI bubble bursts, it is not hard to see Google spending in excess of $80 billion on servers – with the bulk of that being accelerated systems using either its homegrown TPUs or GPUs from Nvidia or sometimes AMD.
 &lt;/p&gt;
 &lt;p&gt;
  We have become so accustomed to people throwing such large numbers around that we can’t feel what an astounding amount of gear this is, and how much aggregate computation it is.
 &lt;/p&gt;
 &lt;p&gt;
  “While we have been working hard to increase capacity and have improved the pace of server deployments and datacenter construction, we still expect to remain in a tight demand/supply environment in Q4 and 2026,” Anat Ashkenazi, chief financial officer at Google, explained on a call with Wall Street analysts going over the third quarter numbers. “Moving to investments: We Are continuing to invest aggressively due to the demand we’re experiencing from cloud customers as well as the growth opportunities we see across the company.” Ashkenazi then went on raise the capex spending to a midpoint of $92 billion, plus or minus $1 billion, and added that for 2026 Google was expecting “a significant increase in capex” and would provide more insight into that after 2025 closes and it reports its number for the year in January.
 &lt;/p&gt;
 &lt;p&gt;
  Our guess is somewhere north of $130 billion, with 2026 being the local maxima in GenAI investments as powerful hardware and rising AI inference demand come together to start creating actual revenue-generating businesses.
 &lt;/p&gt;
 &lt;p&gt;
  By the way, demand is strong both for the homegrown TPU accelerators as well as the GPU accelerators that Google buys from Nvidia and sometimes AMD. (It is a pity that Google does not provide a breakdown on this.) On the call, Sundar Pichai, chief executive officer at Google as well as the Alphabet shell holding company, said that Google had just started renting out capacity to customers on its “Blackwell” GB300 instances to customers in its A4X Max instances. The B300 GPUs are aimed at AI inference and have deprecated high precision floating point processing in favor of boosting lower precision throughput but can also be used for AI training if need be. Pichai said that the “Ironwood” TPU v7p,
  &lt;a href=&quot;https://www.nextplatform.com/2025/04/09/with-ironwood-tpu-google-pushes-the-ai-accelerator-to-the-floor/&quot;&gt;
   which we covered in depth back in April
  &lt;/a&gt;
  , would be rentable soon.
 &lt;/p&gt;
 &lt;p&gt;
  “We are investing in TPU capacity to meet the tremendous demand we are seeing from customers and partners, and we are excited that Anthropic recently shared plans to access up to one million TPUs,” Pichai said on the call.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/2025/09/17/google-shows-off-its-inference-scale-and-prowess/&quot;&gt;
   Back in September
  &lt;/a&gt;
  , based on Google’s comments about token generation rates for its complete application stack and the technical specs of the TPU lineup, we estimated that maybe 1,460 trillion tokens were generated for Google applications in August and that this would take a fleet of over 700,000 Trillium TPU v6e accelerators. It would take progressively larger number of older TPUs to get the same throughput, obviously. The Ironwood TPU system spans 24 rows of gear with 9,216 TPU v7p devices in a shared memory cluster that has 42.5 exaflops of FP8 or INT8 performance and twice the performance per watt of the Trillium TPU v6e, whose shared memory domain was only 256 devices.
 &lt;/p&gt;
 &lt;p&gt;
  Anthropic is no doubt wanting to get access to a million Ironwood devices, not a million Trilliums.
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in September, Google set a new record for revenues and broke through $100 billion for the first time. It raked in $102.34 billion, to be specific, up 15.9 percent year on year and up 6.1 percent sequentially.  Operating income rose by 9.5 percent to $31.23 billion, and thanks to some accounting bennies, net income rose and a by 33 percent to just a hair under $35 billion in the quarter.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/google-q3-2025-cloud-rev-income.jpg&quot; rel=&quot;attachment wp-att-146546&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;391&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/google-q3-2025-cloud-rev-income.jpg&quot; width=&quot;568&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Google Cloud, the infrastructure rental arm of the company, posted $15.16 billion in sales, rising 33.5 percent compared to the year ago period, and largely because GenAI customers (and particularly large ones like model builders OpenAI and Anthropic) are spending big bucks to get their hands on any GPUs and XPUs they can. (We expect much the same story at Microsoft and Amazon Web Services.) Operating income for Google Cloud rose by a very nice 84.6 percent to $3.59 billion, but still only represented 14.8 percent of Google Cloud’s revenues. The trend for profitability has been improving over the years, particularly as Google has kept server iron on the books longer rather than just writing it off after three years as it used to do. That is not economically feasible anymore, not even for a hyperscaler and particularly not one with its core search engine and related advertising business under direct threat from GenAI.
 &lt;/p&gt;
 &lt;p&gt;
  The Google search and ads business still represented 85.1 percent of the company’s sales in Q3 2025, with $87.05 billion coming in. That was a 13.8 percent increase year on year.
 &lt;/p&gt;
 &lt;p&gt;
  We would remind you that Google could turn itself into a much bigger cloud anytime it wants to. All it has to do is designate all of its infrastructure used for its internal workloads as part of Google Cloud, and backcast financials showing how various divisions and groups allocate costs as revenue to Google Cloud. It would be interesting to see just how much Google Search, Google Ads, and YouTube really spend on infrastructure. . . . and how much Google Cloud would rent it, and at what margins.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Google Spends More On Servers Than The Whole World Used To </title>
<link>https://www.nextplatform.com/2025/10/30/google-spends-more-on-servers-than-the-whole-world-used-to/</link>
<pubDate>Thu, 30 Oct 2025 21:58:47 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/08/google-tpu-racks-550x438.jpg&quot; title=&quot;google-tpu-racks&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Here is a funny number to chew on. Sometime in the early part of 2026, if current trends persist, Google will have a spending rate on servers that is in excess of the inflation adjusted spending levels set by the entire world in the wake of the Dot Com bust. And it will not be long before Google’s spending rate on servers – all by its lonesome – matches the spending rate during the Dot Com boom.
 &lt;/p&gt;
 &lt;p&gt;
  Right now, given all of the numbers that Google has put out, we project that it will spend $55.2 billion on servers in 2025, and that beats the anemic $51.5 billion the world spent on servers in 2009 when the Great Recession caused a 20 percent decline in server spending (these figures are expressed in 2021 US dollars) and it nearly matches the $57.7 billion that was spent worldwide on all kinds of iron in the following year.
 &lt;/p&gt;
 &lt;p&gt;
  That spending rate by Google is what happens when all of the hyperscalers, cloud builders, and now the model builders have to spend $3 million, or $4 million, or even $5 million for a rackscale accelerated computer that has coherent memory across its CPUs and GPUs to run AI training and inference so that we can augment (many think replace is a better word) the parts of human thinking involved with data retrieval and synthesis for many orders of magnitude more energy (145 kilowatts per rack compared to 20 watts for the human brain) but with a lot more speed and arguably a lot less complaining.
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in September, the top brass at Google said that the company, which includes the Alphabet shell company and some do-dads that are not ad serving, video serving, and cloud computing, spent $23.95 billion on capital expenditures, with 60 percent of that being for servers and the remaining amount for datacenter facilities and other gear such as networking. (For Google, it seems, a server is tuned for storage but not distinct, which we get.) If you do the math on that, you get $14.37 billion spent on servers in Q3 2025. If you take the midpoint of the new – and higher – estimate for capital expense spending for 2025, which is between $91 billion and $93 billion, and multiply by the same ratio, you get that $55.2 billion figure. And if you project out spending at the same quarterly sequential rate, which we think is probably a low ball estimate for growth in the coming year unless the GenAI bubble bursts, it is not hard to see Google spending in excess of $80 billion on servers – with the bulk of that being accelerated systems using either its homegrown TPUs or GPUs from Nvidia or sometimes AMD.
 &lt;/p&gt;
 &lt;p&gt;
  We have become so accustomed to people throwing such large numbers around that we can’t feel what an astounding amount of gear this is, and how much aggregate computation it is.
 &lt;/p&gt;
 &lt;p&gt;
  “While we have been working hard to increase capacity and have improved the pace of server deployments and datacenter construction, we still expect to remain in a tight demand/supply environment in Q4 and 2026,” Anat Ashkenazi, chief financial officer at Google, explained on a call with Wall Street analysts going over the third quarter numbers. “Moving to investments: We Are continuing to invest aggressively due to the demand we’re experiencing from cloud customers as well as the growth opportunities we see across the company.” Ashkenazi then went on raise the capex spending to a midpoint of $92 billion, plus or minus $1 billion, and added that for 2026 Google was expecting “a significant increase in capex” and would provide more insight into that after 2025 closes and it reports its number for the year in January.
 &lt;/p&gt;
 &lt;p&gt;
  Our guess is somewhere north of $130 billion, with 2026 being the local maxima in GenAI investments as powerful hardware and rising AI inference demand come together to start creating actual revenue-generating businesses.
 &lt;/p&gt;
 &lt;p&gt;
  By the way, demand is strong both for the homegrown TPU accelerators as well as the GPU accelerators that Google buys from Nvidia and sometimes AMD. (It is a pity that Google does not provide a breakdown on this.) On the call, Sundar Pichai, chief executive officer at Google as well as the Alphabet shell holding company, said that Google had just started renting out capacity to customers on its “Blackwell” GB300 instances to customers in its A4X Max instances. The B300 GPUs are aimed at AI inference and have deprecated high precision floating point processing in favor of boosting lower precision throughput but can also be used for AI training if need be. Pichai said that the “Ironwood” TPU v7p,
  &lt;a href=&quot;https://www.nextplatform.com/2025/04/09/with-ironwood-tpu-google-pushes-the-ai-accelerator-to-the-floor/&quot;&gt;
   which we covered in depth back in April
  &lt;/a&gt;
  , would be rentable soon.
 &lt;/p&gt;
 &lt;p&gt;
  “We are investing in TPU capacity to meet the tremendous demand we are seeing from customers and partners, and we are excited that Anthropic recently shared plans to access up to one million TPUs,” Pichai said on the call.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/2025/09/17/google-shows-off-its-inference-scale-and-prowess/&quot;&gt;
   Back in September
  &lt;/a&gt;
  , based on Google’s comments about token generation rates for its complete application stack and the technical specs of the TPU lineup, we estimated that maybe 1,460 trillion tokens were generated for Google applications in August and that this would take a fleet of over 700,000 Trillium TPU v6e accelerators. It would take progressively larger number of older TPUs to get the same throughput, obviously. The Ironwood TPU system spans 24 rows of gear with 9,216 TPU v7p devices in a shared memory cluster that has 42.5 exaflops of FP8 or INT8 performance and twice the performance per watt of the Trillium TPU v6e, whose shared memory domain was only 256 devices.
 &lt;/p&gt;
 &lt;p&gt;
  Anthropic is no doubt wanting to get access to a million Ironwood devices, not a million Trilliums.
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in September, Google set a new record for revenues and broke through $100 billion for the first time. It raked in $102.34 billion, to be specific, up 15.9 percent year on year and up 6.1 percent sequentially.  Operating income rose by 9.5 percent to $31.23 billion, and thanks to some accounting bennies, net income rose and a by 33 percent to just a hair under $35 billion in the quarter.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/google-q3-2025-cloud-rev-income.jpg&quot; rel=&quot;attachment wp-att-146546&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;391&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/google-q3-2025-cloud-rev-income.jpg&quot; width=&quot;568&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Google Cloud, the infrastructure rental arm of the company, posted $15.16 billion in sales, rising 33.5 percent compared to the year ago period, and largely because GenAI customers (and particularly large ones like model builders OpenAI and Anthropic) are spending big bucks to get their hands on any GPUs and XPUs they can. (We expect much the same story at Microsoft and Amazon Web Services.) Operating income for Google Cloud rose by a very nice 84.6 percent to $3.59 billion, but still only represented 14.8 percent of Google Cloud’s revenues. The trend for profitability has been improving over the years, particularly as Google has kept server iron on the books longer rather than just writing it off after three years as it used to do. That is not economically feasible anymore, not even for a hyperscaler and particularly not one with its core search engine and related advertising business under direct threat from GenAI.
 &lt;/p&gt;
 &lt;p&gt;
  The Google search and ads business still represented 85.1 percent of the company’s sales in Q3 2025, with $87.05 billion coming in. That was a 13.8 percent increase year on year.
 &lt;/p&gt;
 &lt;p&gt;
  We would remind you that Google could turn itself into a much bigger cloud anytime it wants to. All it has to do is designate all of its infrastructure used for its internal workloads as part of Google Cloud, and backcast financials showing how various divisions and groups allocate costs as revenue to Google Cloud. It would be interesting to see just how much Google Search, Google Ads, and YouTube really spend on infrastructure. . . . and how much Google Cloud would rent it, and at what margins.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> As Rackscale GPU Systems Boom, Big Green Is Emphatically Red, White, And Blue </title>
<link>https://www.nextplatform.com/2025/10/29/as-rackscale-gpu-systems-boom-big-green-is-emphatically-red-white-and-blue/#respond</link>
<pubDate>Wed, 29 Oct 2025 13:29:19 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Jensen-1-e1761744537879-1030x438.png&quot; title=&quot;Nvidia Jensen 1&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Nvidia co-founder and chief executive officer Jensen Huang paced the stage at the company’s GTC Washington DC event dressed in his standard black leather jacket over a black T-shirt, with black pants and black sneakers, but the messages he delivered during his keynote was decidedly red, white, and blue.
 &lt;/p&gt;
 &lt;p&gt;
  Like a stone skipping across the water, Huang touched on a range of areas where Nvidia and its partners are leaving their mark, from AI and
  &lt;a href=&quot;https://www.nextplatform.com/2025/04/01/nvidia-says-it-will-be-an-accelerator-of-quantum-computing/&quot;&gt;
   quantum computing
  &lt;/a&gt;
  to telecommunications, supercomputers, robotics, and cybersecurity – via a partnership with security firm CrowdStrike – and all in the name of returning the United States to the pole position in the increasingly AI-based IT industry.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US.png&quot; rel=&quot;attachment wp-att-146542&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;817&quot; sizes=&quot;(max-width: 1517px) 100vw, 1517px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US.png 1517w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US-768x414.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US-600x323.png 600w&quot; width=&quot;1517&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The way to do that is to bring technology development and manufacturing back to the country. One place to do that is in telecommunications.
 &lt;/p&gt;
 &lt;p&gt;
  Huang said Nvidia is partnering with Nokia to drive AI deeper into
  &lt;a href=&quot;https://www.nextplatform.com/2025/06/23/nvidia-passes-cisco-and-rivals-arista-in-datacenter-ethernet-sales/&quot;&gt;
   wireless networking infrastructure
  &lt;/a&gt;
  , a move that Huang will reduce America’s reliance on foreign technology in an industry that is central to the country’s economic and national security interests.
 &lt;/p&gt;
 &lt;p&gt;
  “Ever since the beginning of wireless, where we defined the technology, we defined a global standard, we exported American technology all around the world so that the world can build on top of American technology and standards,” he said. “It has been a long time since this happened. Wireless technology around the word largely today [is] deployed on foreign technologies. Our fundamental communication fabric, built on foreign technologies. That has to stop. We have an opportunity to do that.”
 &lt;/p&gt;
 &lt;p&gt;
  That will come in a few ways with the help of Nokia, a Finnish company that Nvidia is investing $1 billion in. Nokia will incorporate Nvidia’s commercial-grade AI-RAN products into its own RAN portfolio, paving the way for a range of communications services providers to run their AI-native 5G and 6G networks on Nvidia technology and giving Nvidia a bigger presence in a multi-trillion-dollar global market.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN.png&quot; rel=&quot;attachment wp-att-146536&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;992&quot; sizes=&quot;(max-width: 1623px) 100vw, 1623px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN.png 1623w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN-768x469.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN-1536x939.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN-600x367.png 600w&quot; width=&quot;1623&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Building on that, Nvidia also unveiled its Aerial RAN Computer Pro (ARC-Pro), an accelerated computing platform for 6G the including connectivity, computing, and sensing capabilities and giving telcos a software-upgrade path to move from 5G-Advanced to 6G.
 &lt;/p&gt;
 &lt;p&gt;
  “ARC is built from three fundamental new technologies: the Grace CPU, the Blackwell GPU, and our Mellanox ConnectX networking designed for this application,” Huang said. “All of that makes it possible for us to run this CUDA-X library called Aerial. Aerial is essentially a wireless communication system running on top of CUDA-X. We’re going to create, for the first time, a software-defined programmable computer that’s able to communicate wirelessly and do AI processing at the same time. . . . and Nokia is going to work with us to integrate our technology [and] rewrite their stack.”
 &lt;/p&gt;
 &lt;p&gt;
  Huang said the global telecom market sits at about $3 trillion and includes hundreds of billions of dollars in infrastructure and millions of base stations, adding that “if we could partner, we could build on top of this incredible new technology, fundamentally based on accelerated computing and AI, and for United States, for America, to be at the center of the next revolution in 6G.”
 &lt;/p&gt;
 &lt;p&gt;
  He also sees this as open the edge even more to AI, which will lead to opportunities on a more powerful AI-based wireless telecommunications network, similar to what Amazon Web Services (AWS) and other cloud providers did by building their business atop the internet, which initially was a communications system. Cloud computing will leverage base stations worldwide – rather than datacenters – to reach the edge.
 &lt;/p&gt;
 &lt;p&gt;
  In quantum computing, Nvidia unveiled NVQLink, an interconnect that links quantum processors with Nvidia’s GPUs. An ongoing challenge in quantum computing is
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/27/aws-cat-qubits-make-quantum-error-correction-effective-affordable/&quot;&gt;
   error correction
  &lt;/a&gt;
  , a technique that enable errors made by the systems to be detected and corrected without disturbing the fragile qubits at the heart of the computers. Huang said what needs to be done is adding extra entangled qubits so that measuring them offers information to find the errors without damaging the qubits. However, doing so calls for conventional computing, in this case GPU accelerators.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink.png&quot; rel=&quot;attachment wp-att-146541&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1067&quot; sizes=&quot;(max-width: 1748px) 100vw, 1748px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink.png 1748w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink-768x469.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink-1536x938.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink-600x366.png 600w&quot; width=&quot;1748&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This is where NVQLink comes in.
 &lt;/p&gt;
 &lt;p&gt;
  It’s “a new interconnect architecture that directly connects quantum processors with Nvidia GPUs,” he said. “Quantum error correction requires reading out information from qubits, calculating where errors occur, and sending data back to correct them. NVQLink is capable of moving terabytes of data to and from quantum hardware the thousands of times every second needed for quantum error correction.”
 &lt;/p&gt;
 &lt;p&gt;
  Foundational to is CUDA-Q, Nvidia’s quantum-classical computing platform, Using the two together will let researchers do more than error correction, but also orchestrate quantum devices and AI supercomputers to run quantum GPU calculations, with Huang adding that “quantum computing won’t replace classical systems. They will work together, fused into one accelerated quantum supercomputing platform.”
 &lt;/p&gt;
 &lt;p&gt;
  In addition, 17 quantum computing companies are supporting NVQLink, and eight supercomputing labs – Brookhaven National Laboratory, Fermi Laboratory, Lawrence Berkeley National Laboratory, Los Alamos National Laboratory, MIT Lincoln Laboratory, Oak Ridge National Laboratory, Pacific Northwest National Laboratory and Sandia National Laboratories – will use it.
 &lt;/p&gt;
 &lt;p&gt;
  Nvidia also will work with Oracle to build “Solstice,” which will be the US Department of Energy’s largest AI supercomputer, powered by 100,000
  &lt;a href=&quot;https://www.nextplatform.com/2024/03/18/with-blackwell-gpus-ai-gets-cheaper-and-easier-competing-with-nvidia-gets-harder/&quot;&gt;
   Blackwell GPUs
  &lt;/a&gt;
  . It’s the largest of seven AI supercomputers Nvidia will build for the DOE. Solstice and “Equinox,” another of the systems that will hold 10,000 Blackwells, will be housed at the Argonne National Laboratory and will deliver a combined 2,200 exaflops of performance.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters.png&quot; rel=&quot;attachment wp-att-146535&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1037&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1775px) 100vw, 1775px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters.png 1775w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters-768x449.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters-1536x897.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters-600x351.png 600w&quot; width=&quot;1775&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Huang gave kudos to the Trump Administration for enable these large systems. An ongoing concern about the rapid adoption of AI is the enormous amounts of power and water that the systems consume. Huang applauded Trump for his administration’s energy policies and Energy Secretary Chris Wright for bringing “a surge of energy, a surge of passion, to make sure that America leads science again.”
 &lt;/p&gt;
 &lt;p&gt;
  Two of the seven systems will be HPE supercomputers based on the OEM’s upcoming GX5000, the successor to the EX4000 architecture that was the foundation for the vendor’s three exascale systems, El Capitan, Frontier, and Aurora. HPE announced this week that
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/27/oak-ridge-discovery-supercomputer-spearheads-new-hpe-cray-gx5000-design/&quot;&gt;
   the GX5000 will be the architecture for “Discovery,”
  &lt;/a&gt;
  the successor to Frontier that will be housed at Oak Ridge National Lab.
 &lt;/p&gt;
 &lt;p&gt;
  At GTC Washington, the systems maker announced the “Mission” and “Vision” that be housed at Los Alamos National Lab and will feature Nvidia’s upcoming “Vera” Arm server CPUs and “Rubin” GPUs combined into superchips, which Huang showed off onstage. Mission will be used to manage the U.S. nuclear stockpile and will go online in 2027. Vision will build on the work of the Venado supercomputer around AI research and national security.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin.png&quot; rel=&quot;attachment wp-att-146543&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1080&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1845px) 100vw, 1845px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin.png 1845w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin-768x450.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin-1536x899.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin-600x351.png 600w&quot; width=&quot;1845&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Huang said Vera Rubin is the result of what Nvidia calls extreme co-design, the idea of designing and engineering all layers of a computing stack simultaneously to create highly optimized AI systems and AI factories.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design.png&quot; rel=&quot;attachment wp-att-146537&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1034&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1798px) 100vw, 1798px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design.png 1798w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design-768x442.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design-1536x883.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design-600x345.png 600w&quot; width=&quot;1798&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “We’re preparing Rubin to be in production this time next year, maybe slightly earlier,” Huang said. “So every single year, we are going to come up with the most extreme co-design system so that we can keep driving up performance and keep driving down the token-generation cost. This is just an incredibly beautiful computer. This is amazing. This is 100 petaflops.”
 &lt;/p&gt;
 &lt;p&gt;
  Right now, Nvidia is seeing only an acceleration in demand for its AI chips, which isn’t surprising given the rapid enterprise adoption of AI, he said. Sales of
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/26/blackwell-is-the-fastest-ramping-compute-engine-in-nvidias-history/&quot;&gt;
   Grace Blackwell GPUs
  &lt;/a&gt;
  illustrate the future trends.
 &lt;/p&gt;
 &lt;p&gt;
  Showing a chart, he noted that with Hopper, Nvida sold four million GPUs. With Blackwell – each one contains two GPUs in a single large package – and Rubin orders thus far, there ae 20 million GPUs on the books.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand.png&quot; rel=&quot;attachment wp-att-146540&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;994&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1820px) 100vw, 1820px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand.png 1820w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand-768x419.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand-1536x839.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand-600x328.png 600w&quot; width=&quot;1820&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “It’s driven by two exponentials,” Huang said. “We now have visibility. I think we’re probably the first technology company in history to have visibility into half a trillion dollars of cumulative Blackwell and early ramps of Rubin through 2026. As you know, 2025 is not over yet and 2026 hasn’t started. This is how much business is on the books. Half a trillion dollars worth, so far. We’ve already shipped 6 million of the Blackwells in the first several quarters. … We still have one more quarter to go for 2025 and then we have four quarters [referring to 2026]. So the next five quarters, there’s $500 billion, half a trillion dollars. That’s five times the growth rate of Hopper.”
 &lt;/p&gt;
 &lt;p&gt;
  As best we can figure, Huang is correctly counting GPU chiplets per socket, so some of this increase is due to adding two reticle limited chiplets per socket. Five times the money is five times the chiplets, more or less. Which means that Nvidia is not commanding much of a price premium for a Blackwell GPU versus Hopper, and that is very likely due to the high concentration of sales among hyperscalers, cloud builders, and model builders who command deeper discounts than large enterprises with much smaller volumes can ask for. As more enterprises start deploying their own GenAI, revenue per GPU chiplet will rise and, we believe, so will profits.
 &lt;/p&gt;
 &lt;p&gt;
  The other thing to note about this chart above is that it does not include all of the Blackwell and Rubin GPUs that go into eight-way NVL8 server nodes or custom architectures that have only four GPUs per node like the many systems installed at HPC centers in the US and Europe.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> As Rackscale GPU Systems Boom, Big Green Is Emphatically Red, White, And Blue </title>
<link>https://www.nextplatform.com/2025/10/29/as-rackscale-gpu-systems-boom-big-green-is-emphatically-red-white-and-blue/</link>
<pubDate>Wed, 29 Oct 2025 13:29:16 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Jensen-1-e1761744537879-1030x438.png&quot; title=&quot;Nvidia Jensen 1&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Nvidia co-founder and chief executive officer Jensen Huang paced the stage at the company’s GTC Washington DC event dressed in his standard black leather jacket over a black T-shirt, with black pants and black sneakers, but the messages he delivered during his keynote was decidedly red, white, and blue.
 &lt;/p&gt;
 &lt;p&gt;
  Like a stone skipping across the water, Huang touched on a range of areas where Nvidia and its partners are leaving their mark, from AI and
  &lt;a href=&quot;https://www.nextplatform.com/2025/04/01/nvidia-says-it-will-be-an-accelerator-of-quantum-computing/&quot;&gt;
   quantum computing
  &lt;/a&gt;
  to telecommunications, supercomputers, robotics, and cybersecurity – via a partnership with security firm CrowdStrike – and all in the name of returning the United States to the pole position in the increasingly AI-based IT industry.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US.png&quot; rel=&quot;attachment wp-att-146542&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;817&quot; sizes=&quot;(max-width: 1517px) 100vw, 1517px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US.png 1517w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US-768x414.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-US-600x323.png 600w&quot; width=&quot;1517&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The way to do that is to bring technology development and manufacturing back to the country. One place to do that is in telecommunications.
 &lt;/p&gt;
 &lt;p&gt;
  Huang said Nvidia is partnering with Nokia to drive AI deeper into
  &lt;a href=&quot;https://www.nextplatform.com/2025/06/23/nvidia-passes-cisco-and-rivals-arista-in-datacenter-ethernet-sales/&quot;&gt;
   wireless networking infrastructure
  &lt;/a&gt;
  , a move that Huang will reduce America’s reliance on foreign technology in an industry that is central to the country’s economic and national security interests.
 &lt;/p&gt;
 &lt;p&gt;
  “Ever since the beginning of wireless, where we defined the technology, we defined a global standard, we exported American technology all around the world so that the world can build on top of American technology and standards,” he said. “It has been a long time since this happened. Wireless technology around the word largely today [is] deployed on foreign technologies. Our fundamental communication fabric, built on foreign technologies. That has to stop. We have an opportunity to do that.”
 &lt;/p&gt;
 &lt;p&gt;
  That will come in a few ways with the help of Nokia, a Finnish company that Nvidia is investing $1 billion in. Nokia will incorporate Nvidia’s commercial-grade AI-RAN products into its own RAN portfolio, paving the way for a range of communications services providers to run their AI-native 5G and 6G networks on Nvidia technology and giving Nvidia a bigger presence in a multi-trillion-dollar global market.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN.png&quot; rel=&quot;attachment wp-att-146536&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;992&quot; sizes=&quot;(max-width: 1623px) 100vw, 1623px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN.png 1623w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN-768x469.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN-1536x939.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-RAN-600x367.png 600w&quot; width=&quot;1623&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Building on that, Nvidia also unveiled its Aerial RAN Computer Pro (ARC-Pro), an accelerated computing platform for 6G the including connectivity, computing, and sensing capabilities and giving telcos a software-upgrade path to move from 5G-Advanced to 6G.
 &lt;/p&gt;
 &lt;p&gt;
  “ARC is built from three fundamental new technologies: the Grace CPU, the Blackwell GPU, and our Mellanox ConnectX networking designed for this application,” Huang said. “All of that makes it possible for us to run this CUDA-X library called Aerial. Aerial is essentially a wireless communication system running on top of CUDA-X. We’re going to create, for the first time, a software-defined programmable computer that’s able to communicate wirelessly and do AI processing at the same time. . . . and Nokia is going to work with us to integrate our technology [and] rewrite their stack.”
 &lt;/p&gt;
 &lt;p&gt;
  Huang said the global telecom market sits at about $3 trillion and includes hundreds of billions of dollars in infrastructure and millions of base stations, adding that “if we could partner, we could build on top of this incredible new technology, fundamentally based on accelerated computing and AI, and for United States, for America, to be at the center of the next revolution in 6G.”
 &lt;/p&gt;
 &lt;p&gt;
  He also sees this as open the edge even more to AI, which will lead to opportunities on a more powerful AI-based wireless telecommunications network, similar to what Amazon Web Services (AWS) and other cloud providers did by building their business atop the internet, which initially was a communications system. Cloud computing will leverage base stations worldwide – rather than datacenters – to reach the edge.
 &lt;/p&gt;
 &lt;p&gt;
  In quantum computing, Nvidia unveiled NVQLink, an interconnect that links quantum processors with Nvidia’s GPUs. An ongoing challenge in quantum computing is
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/27/aws-cat-qubits-make-quantum-error-correction-effective-affordable/&quot;&gt;
   error correction
  &lt;/a&gt;
  , a technique that enable errors made by the systems to be detected and corrected without disturbing the fragile qubits at the heart of the computers. Huang said what needs to be done is adding extra entangled qubits so that measuring them offers information to find the errors without damaging the qubits. However, doing so calls for conventional computing, in this case GPU accelerators.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink.png&quot; rel=&quot;attachment wp-att-146541&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1067&quot; sizes=&quot;(max-width: 1748px) 100vw, 1748px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink.png 1748w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink-768x469.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink-1536x938.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVQLink-600x366.png 600w&quot; width=&quot;1748&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This is where NVQLink comes in.
 &lt;/p&gt;
 &lt;p&gt;
  It’s “a new interconnect architecture that directly connects quantum processors with Nvidia GPUs,” he said. “Quantum error correction requires reading out information from qubits, calculating where errors occur, and sending data back to correct them. NVQLink is capable of moving terabytes of data to and from quantum hardware the thousands of times every second needed for quantum error correction.”
 &lt;/p&gt;
 &lt;p&gt;
  Foundational to is CUDA-Q, Nvidia’s quantum-classical computing platform, Using the two together will let researchers do more than error correction, but also orchestrate quantum devices and AI supercomputers to run quantum GPU calculations, with Huang adding that “quantum computing won’t replace classical systems. They will work together, fused into one accelerated quantum supercomputing platform.”
 &lt;/p&gt;
 &lt;p&gt;
  In addition, 17 quantum computing companies are supporting NVQLink, and eight supercomputing labs – Brookhaven National Laboratory, Fermi Laboratory, Lawrence Berkeley National Laboratory, Los Alamos National Laboratory, MIT Lincoln Laboratory, Oak Ridge National Laboratory, Pacific Northwest National Laboratory and Sandia National Laboratories – will use it.
 &lt;/p&gt;
 &lt;p&gt;
  Nvidia also will work with Oracle to build “Solstice,” which will be the US Department of Energy’s largest AI supercomputer, powered by 100,000
  &lt;a href=&quot;https://www.nextplatform.com/2024/03/18/with-blackwell-gpus-ai-gets-cheaper-and-easier-competing-with-nvidia-gets-harder/&quot;&gt;
   Blackwell GPUs
  &lt;/a&gt;
  . It’s the largest of seven AI supercomputers Nvidia will build for the DOE. Solstice and “Equinox,” another of the systems that will hold 10,000 Blackwells, will be housed at the Argonne National Laboratory and will deliver a combined 2,200 exaflops of performance.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters.png&quot; rel=&quot;attachment wp-att-146535&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1037&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1775px) 100vw, 1775px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters.png 1775w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters-768x449.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters-1536x897.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-AI-datacenters-600x351.png 600w&quot; width=&quot;1775&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Huang gave kudos to the Trump Administration for enable these large systems. An ongoing concern about the rapid adoption of AI is the enormous amounts of power and water that the systems consume. Huang applauded Trump for his administration’s energy policies and Energy Secretary Chris Wright for bringing “a surge of energy, a surge of passion, to make sure that America leads science again.”
 &lt;/p&gt;
 &lt;p&gt;
  Two of the seven systems will be HPE supercomputers based on the OEM’s upcoming GX5000, the successor to the EX4000 architecture that was the foundation for the vendor’s three exascale systems, El Capitan, Frontier, and Aurora. HPE announced this week that
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/27/oak-ridge-discovery-supercomputer-spearheads-new-hpe-cray-gx5000-design/&quot;&gt;
   the GX5000 will be the architecture for “Discovery,”
  &lt;/a&gt;
  the successor to Frontier that will be housed at Oak Ridge National Lab.
 &lt;/p&gt;
 &lt;p&gt;
  At GTC Washington, the systems maker announced the “Mission” and “Vision” that be housed at Los Alamos National Lab and will feature Nvidia’s upcoming “Vera” Arm server CPUs and “Rubin” GPUs combined into superchips, which Huang showed off onstage. Mission will be used to manage the U.S. nuclear stockpile and will go online in 2027. Vision will build on the work of the Venado supercomputer around AI research and national security.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin.png&quot; rel=&quot;attachment wp-att-146543&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1080&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1845px) 100vw, 1845px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin.png 1845w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin-768x450.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin-1536x899.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-Vera-Rubin-600x351.png 600w&quot; width=&quot;1845&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Huang said Vera Rubin is the result of what Nvidia calls extreme co-design, the idea of designing and engineering all layers of a computing stack simultaneously to create highly optimized AI systems and AI factories.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design.png&quot; rel=&quot;attachment wp-att-146537&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1034&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1798px) 100vw, 1798px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design.png 1798w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design-768x442.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design-1536x883.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-extreme-co-design-600x345.png 600w&quot; width=&quot;1798&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “We’re preparing Rubin to be in production this time next year, maybe slightly earlier,” Huang said. “So every single year, we are going to come up with the most extreme co-design system so that we can keep driving up performance and keep driving down the token-generation cost. This is just an incredibly beautiful computer. This is amazing. This is 100 petaflops.”
 &lt;/p&gt;
 &lt;p&gt;
  Right now, Nvidia is seeing only an acceleration in demand for its AI chips, which isn’t surprising given the rapid enterprise adoption of AI, he said. Sales of
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/26/blackwell-is-the-fastest-ramping-compute-engine-in-nvidias-history/&quot;&gt;
   Grace Blackwell GPUs
  &lt;/a&gt;
  illustrate the future trends.
 &lt;/p&gt;
 &lt;p&gt;
  Showing a chart, he noted that with Hopper, Nvida sold four million GPUs. With Blackwell – each one contains two GPUs in a single large package – and Rubin orders thus far, there ae 20 million GPUs on the books.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand.png&quot; rel=&quot;attachment wp-att-146540&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;994&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1820px) 100vw, 1820px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand.png 1820w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand-768x419.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand-1536x839.png 1536w, https://www.nextplatform.com/wp-content/uploads/2025/10/Nvidia-NVL72-demand-600x328.png 600w&quot; width=&quot;1820&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “It’s driven by two exponentials,” Huang said. “We now have visibility. I think we’re probably the first technology company in history to have visibility into half a trillion dollars of cumulative Blackwell and early ramps of Rubin through 2026. As you know, 2025 is not over yet and 2026 hasn’t started. This is how much business is on the books. Half a trillion dollars worth, so far. We’ve already shipped 6 million of the Blackwells in the first several quarters. … We still have one more quarter to go for 2025 and then we have four quarters [referring to 2026]. So the next five quarters, there’s $500 billion, half a trillion dollars. That’s five times the growth rate of Hopper.”
 &lt;/p&gt;
 &lt;p&gt;
  As best we can figure, Huang is correctly counting GPU chiplets per socket, so some of this increase is due to adding two reticle limited chiplets per socket. Five times the money is five times the chiplets, more or less. Which means that Nvidia is not commanding much of a price premium for a Blackwell GPU versus Hopper, and that is very likely due to the high concentration of sales among hyperscalers, cloud builders, and model builders who command deeper discounts than large enterprises with much smaller volumes can ask for. As more enterprises start deploying their own GenAI, revenue per GPU chiplet will rise and, we believe, so will profits.
 &lt;/p&gt;
 &lt;p&gt;
  The other thing to note about this chart above is that it does not include all of the Blackwell and Rubin GPUs that go into eight-way NVL8 server nodes or custom architectures that have only four GPUs per node like the many systems installed at HPC centers in the US and Europe.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> How Qualcomm Can Compete With Nvidia For Datacenter AI Inference </title>
<link>https://www.nextplatform.com/2025/10/28/how-qualcomm-can-compete-with-nvidia-for-datacenter-ai-inference/#respond</link>
<pubDate>Tue, 28 Oct 2025 17:52:43 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-100-ultra-logo.jpg&quot; title=&quot;qualcomm-ai-100-ultra-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Qualcomm had datacenter envy back when Intel ruled the bit barns of the world, and now it has it even worse now that Nvidia has shown how AI processing can utterly transform the finances of a chip maker. Qualcomm is also the volume leader for high-end Arm CPUs and add-on circuits for smartphones, and it has the IP and the talent to create server CPUs and AI accelerators that can take a piece of the enormous AI inference opportunity.
 &lt;/p&gt;
 &lt;p&gt;
  What it doesn’t have is a technology that is going to take much of a bite out of Nvidia’s AI inference workload, no matter how excited Wall Street is about that prospect this week as Saudi Arabia’s Humain AI startup has emerged as a sugar daddy for Qualcomm’s datacenter AI ambitions. And Qualcomm has absolutely no shot of creating anything that can take on Nvidia in AI training, which is where Nvidia will make around half of its $183.5 billion in datacenter revenues in fiscal 2026 ending in January according to our model.
 &lt;/p&gt;
 &lt;p&gt;
  This is the green-tinted lens through which you have to look at the deal with Humain that Qualcomm outlined a little bit more this week.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.qualcomm.com/news/releases/2025/05/qualcomm-and-humain-to-develop-state-of-the-art-ai-data-centers-&quot;&gt;
   Back in May
  &lt;/a&gt;
  , Qualcomm inked a memorandum of understanding with Humain to collaborate on the development of AI technologies at the edge and in the datacenter. That MOU not only included the expected AI chips for inference – we wish they had a better product name and we wish we knew the code name – but edge devices powered by Snapdragon and Dragonwing system-on-chip designs that aim to “accelerate” back-end cloud infrastructure and to tune up Humain’s Arabic large language models for these SoCs. And significantly, the MOU called for Qualcomm to “develop and supply state-of-the-art datacenter CPU and AI solutions,” which unequivocally means that Qualcomm is getting back into the server CPU business while also getting funding to expand its AI accelerator lineup.
 &lt;/p&gt;
 &lt;p&gt;
  Let’s tackle the AI accelerators first, and then ponder the server CPUs after that.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-200-inference-rack.jpg&quot; rel=&quot;attachment wp-att-146529&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;700&quot; sizes=&quot;(max-width: 734px) 100vw, 734px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-200-inference-rack.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-200-inference-rack.jpg 734w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-200-inference-rack-600x572.jpg 600w&quot; width=&quot;734&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Concurrent with the Future Investment Initiative 2025 conference in Riyadh, Saudi Arabia this week, Humain and Qualcomm moved from an MOU to an actual contract, and revealed that two future AI accelerators were in development with Humain as the first customer. Like Broadcom with at least two of its XPU customers, Qualcomm is also providing Humain with complete rackscale systems, not just chips that it has to provide to an original design manufacturer like Quanta, Foxconn, Invensys, Jabil Circuit, Celestica, or WiWynn (just to name a few biggies) to turn into servers and cluster into systems.
 &lt;/p&gt;
 &lt;p&gt;
  The original AI 100 XPUs from Qualcomm were announced way back in 2019 and shipped sometime in the first half of 2021. The only time we ever saw them was before waferscale system supplier Cerebras Systems had tweaked its software stack so it could do inference. But
  &lt;a href=&quot;https://www.nextplatform.com/2024/03/14/cerebras-goes-hyperscale-with-third-gen-waferscale-supercomputers/&quot;&gt;
   in March 2024
  &lt;/a&gt;
  , when the WS-3 compute engines and their CS-3 systems debuted, Cerebras was offloading inference to racks of AI 100 accelerators from Qualcomm to do inference cheaper than could be done at the time with its own systems. By September last year,
  &lt;a href=&quot;https://www.nextplatform.com/2024/09/10/the-battle-begins-for-ai-inference-compute-in-the-datacenter/&quot;&gt;
   Cerebras had tweaked its software to run inference workloads
  &lt;/a&gt;
  , and we never heard about the AI 100 XPUs again.
 &lt;/p&gt;
 &lt;p&gt;
  Frankly, there are so many startups chasing AI inference and there was so much going on we never did circle back. (Apologies for that, Qualcomm.) In any event, Qualcomm published a series of benchmarks on the AI 100 accelerator back in September 2021, which is interesting reading and which showed these devices standing toe-to-toe with low-end and high-end Nvidia “Ampere” GPUs and other inference engines suitable for the edge on ResNet-50 image processing tests. The AI 100 did particularly well on inference per second per watt compared to Nvidia A100 GPUs, which is an important fact.
 &lt;/p&gt;
 &lt;p&gt;
  But inference has moved on, and in a big way, with GenAI, and the compute workload has gotten a lot more intense. But so has the desire to find a cheaper alternative – should one really exist – compared to running mixture of expert, reasoning inference on Nvidia rackscale CPU-GPU hybrids.
 &lt;/p&gt;
 &lt;p&gt;
  In October 2024 – and we cannot find an exact announcement date, which is peculiar – Qualcomm started shipped geared-down versions of the AI 100 called the AI 80 and also created a PCI-Express card that interlinked four AI 100 chips on a single package called the AI 100 Ultra. (There was also an Ultra version of the AI 80 card added.) Qualcomm also started to get better yield on the SRAM on the XPUs, and was able to boost the capacity from 126 MB per chip to 144 MB per chip – we have no idea if this capacity of the SRAM scratchpad memory is the maximum available on the device, but if not, it is probably close.
 &lt;/p&gt;
 &lt;p&gt;
  Just a few days ago, researchers at the University of California at San Diego, just down the road from Qualcomm headquarters,
  &lt;a href=&quot;https://arxiv.org/pdf/2507.00418&quot;&gt;
   put the AI 100 Ultra through the benchmark paces
  &lt;/a&gt;
  against systems with four and eight A100 GPUs, and the Qualcomm XPUs did well. On GPT-2 and Granite 3.2, four A100s burned 60 percent less watts per token generated as a single AI 100 Ultra with four Qualcomm chips, and the A100 did a little bit better on the Neomtron-70B model. But otherwise, a given number of Qualcomm cards offered better performance per watt than a given number of Nvidia cards.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table.jpg&quot; rel=&quot;attachment wp-att-146533&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;263&quot; sizes=&quot;(max-width: 725px) 100vw, 725px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table.jpg 853w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table-768x278.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table-600x217.jpg 600w&quot; width=&quot;725&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It was odd on the paper that UCSD didn’t actually do the math and show how these two sets of devices stacked up explicitly, leaving readers to do the math. But we built the sheet above to show you how they compared.
 &lt;/p&gt;
 &lt;p&gt;
  The other thing the paper does not talk about is the density of the compute and the number of devices you will need to reach a given throughput. We did the math, calculating how many AICs (which is what Qualcomm sometimes calls its cards) it would take to match the performance of either four or eight A100s. As you can see, the numbers add up pretty fast. Hypothetically speaking, if you can get sixteen AIC cards into a 5U server, which is reasonably dense, then in those areas where the AI 100 Ultra is beating the GPUs on efficiency, it will take anywhere from one to four racks of Qualcomm accelerators to match the performance of four or eight A100 GPUs. Matching the performance of even lower precision “Hopper” H100 or H200 or “Blackwell” B100, B200, or B300 GPUs from Nvidia would require 2X or 4X to 6X, respectively, more racks.
 &lt;/p&gt;
 &lt;p&gt;
  As usual, if you have space, you can go cheap if your workload is embarrassingly parallel.
 &lt;/p&gt;
 &lt;p&gt;
  Here is a table comparing the five existing variations of the Qualcomm AI XPUs and our estimation of what the Ultra versions of the future AI 200 and AI 250 accelerators, which were revealed this week as part of the deal with Humain, might look like:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table.jpg&quot; rel=&quot;attachment wp-att-146531&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;390&quot; sizes=&quot;(max-width: 1062px) 100vw, 1062px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table.jpg 1062w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table-768x282.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table-600x220.jpg 600w&quot; width=&quot;1062&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We are reasonably sure that Qualcomm is making its AI XPUs at Taiwan Semiconductor Manufacturing Co; we have guessed about the process used, and as usual, our guesses are shown in bold red italics.
 &lt;/p&gt;
 &lt;p&gt;
  We know that the AI 200 coming sometime next year, with Hussain as the anchor customer, will have 768 GB of LPDDR5 main memory and will use PCI-Express for its scale up network within a rack and Ethernet to scale out across racks. That’s about it. We know that the AI 250 kicker will come in early 2027, and that Qualcomm has committed to an annual cadence for advancements for its AI accelerators.
 &lt;/p&gt;
 &lt;p&gt;
  We made estimated of what the AI 200 Ultra and AI 250 Ultra looked like mostly to amuse ourselves, and to get a feeling for how these might look.
 &lt;/p&gt;
 &lt;p&gt;
  Not much detail is known about the architecture of the chip used in the AI 100 series. It supports FP16 floating point and INT8 integer processing and the performance the AI 100 cards along with board-level SRAM and main memory shown for the number of chips and AI cores. The AI 100 architecture is based on the Hexagon neural network processors (NNPs) that are in Qualcomm’s smartphone CPUs and that are also known as Q6 in some Linux documentation.
 &lt;/p&gt;
 &lt;p&gt;
  Here is what the Qualcomm AI core looks like:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-core-block-diagram.jpg&quot; rel=&quot;attachment wp-att-146530&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;518&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-core-block-diagram.jpg&quot; width=&quot;538&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As you can see, the architecture has scalar, vector and tensor units all on the same core. It represents the seventh generation of neural network processors that Qualcomm has developed for its smartphones. The scalar chip is a four-way VLIW setup with six hardware threads; it has over 1,800 instructions. The scalar circuits have instruction and data caches and even though it doesn’t show it, there are links between the scalar unit and the memory subsystem, which is how the scalar unit offloads work to the vector and tensor units on the core. That memory subsystem has a 1 MB L2 cache that feeds into an 8 MB scratchpad memory shared by the vector and tensor units.
 &lt;/p&gt;
 &lt;p&gt;
  The tensor unit has over 125 instructions suitable for AI operations and has on 8,192 2D multiply-accumulate (MAC) array to do INT8 work and another 4,096 2D MAC array to do FP16 work. The tensor extensions are called HMX, short for Hexagon Matrix Extensions.
 &lt;/p&gt;
 &lt;p&gt;
  The vector unit has accelerators for scatter/gather collective operations and has over 700 instructions for AI, image processing, and other content manipulation functions. It can support 8-bit or 16-bit integer and 16-bit or 32-bit floating point operations. At 8-bits in integer mode, this vector unit can do 512 MAC operations per clock and at 16-bit floating point, 256 MAC operations per clock. This is obviously only a portion of the throughput of the tensor unit, but some algorithms want a vector unit, not a tensor unit. The vector instructions are known collectively as HVX, or Hexagon Vector Extensions, for short.
 &lt;/p&gt;
 &lt;p&gt;
  Here is what the AI 100 SoC looks like when you put sixteen of the AI cores on a die and wrap four LPDDR4X memory controllers around it plus a PCI-Express 4.0 controller with eight lanes of I/O to link to a host system:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-soc-block-diagram.jpg&quot; rel=&quot;attachment wp-att-146532&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;485&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 742px) 100vw, 742px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-soc-block-diagram.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-soc-block-diagram.jpg 742w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-soc-block-diagram-600x392.jpg 600w&quot; width=&quot;742&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It is reasonable to assume that Qualcomm will release a Hexagon 7 architecture with more instructions and other stuff, and also that it will boost the number of AI cores on an SoC with the AI 200 generation. To keep pace, it should be about 2X for the AI 200, etched in 5 nanometer processes perhaps to make it cheap, to get some kind of performance per watt advantage over more current Nvidia GPUs. And we think an AI 250 might boost it by another 50 percent with a shrink to 3 nanometer processes from TSMC in 2027. So 32 cores in 2026 with the AI 200 and 48 cores with the AI 250 in 2027. Clock speeds will be what they will be, based on thermals required. Expect for Qualcomm to emphasis efficiency over performance, which means lower clocks and more devices to get a given level of throughput at the same power draw as a GPU setup. This is, after all, the game that Qualcomm has been playing.
 &lt;/p&gt;
 &lt;p&gt;
  We think that the AI core also has to do at least FP8 if not FP4 precision on the tensor cores, which would double or quadruple the performance per clock cycle compared to the current AI cores based on the Hexagon 6 architecture. It is possible that Qualcomm gets rid of integer support in the tensor core and gooses floating point by a lot. (That’s what we would do.)
 &lt;/p&gt;
 &lt;p&gt;
  That brings us to CPUs. Qualcomm had an Arm server CPU called the “Amberwing” Centriq 2400,
  &lt;a href=&quot;https://www.nextplatform.com/2017/12/12/battle-datacenter-compute-qualcomm-centriq-versus-intel-xeon/&quot;&gt;
   way back in 2017
  &lt;/a&gt;
  . It was a 48-core chip that did pretty good against the “Broadwell” and “Skylake” Xeon SPs of the time. The rumor was that Google was the backer of the Centriq effort, and for whatever reason when Google didn’t buy a lot of them,
  &lt;a href=&quot;https://www.nextplatform.com/2018/05/10/what-qualcomms-exit-from-arm-server-chips-means/&quot;&gt;
   Qualcomm spiked the server CPU effort in May 2018
  &lt;/a&gt;
  . In January 2021,
  &lt;a href=&quot;https://www.nextplatform.com/2021/01/13/qualcomm-probably-not-pursuing-arm-servers-with-nuvia-purchase/&quot;&gt;
   Qualcomm bought Arm server chip designer Nuvia
  &lt;/a&gt;
  , oddly enough not to do servers but to get its hands on its “Phoenix” cores, which are now known as Oryon cores and in contrast to the Snapdragon cores Qualcomm has designed itself.
 &lt;/p&gt;
 &lt;p&gt;
  Qualcomm has been very clear that it is once again working on a datacenter server CPU, per its May announcement with Humain. There is a good chance, we think, that the future AI 200 and AI 250 devices will have integrated server-class Oryon Arm cores integrated in the package, eliminating the need to run external X86 or Arm CPUs as hosts. And we would venture so far as to say that the LPDDR5 memory attached to the AI 200 or the LPDDR6X memory attached to the AI 250 accelerators will be shared coherently with said Oryon cores.
 &lt;/p&gt;
 &lt;p&gt;
  The statement from Qualcomm says that the AI 250 “will debut with an innovative memory architecture based on near-memory computing, providing a generational leap in efficiency and performance for AI inference workloads by delivering greater than 10X higher effective memory bandwidth and much lower power consumption.” We are not sure what that means, but it sounds like it might mean what we said above. We do not expect for Qualcomm to add HBM stacked memory to its devices, which would defeat the purpose of lowering cost and increasing availability.
 &lt;/p&gt;
 &lt;p&gt;
  That brings us to the next issue: How many Qualcomm accelerators is Humain planning to buy, and how much money is this for Qualcomm? (Which is another way of saying how much money can it take away from Nvidia.)
 &lt;/p&gt;
 &lt;p&gt;
  Qualcomm said that it has won a 200 megawatt deployment. At 250 watts for an AI 200 Ultra card with four SoCs, that is 800,000 cards. We know Qualcomm wants to deliver 160 kilowatts per rack, so say that the AI 200 Ultras are 80 percent of that power, which is 128 kilowatts. That is 512 devices per rack, and that is 1,250 racks. At $4,000 per card, that is $3.2 billion, plus maybe another $2 billion for the rack and its cooling, networking, and storage. That’s $5.2 million per rack, and if Qualcomm gets rid of integer math on the tensor cores and only does floating point and it drives the precision down to FP4 on the tensor cores, that is 983 petaflops for that $3.2 million of compute in the rack, which is $2,604 per petaflops and which is $16.30 per petaflops per kilowatt.
 &lt;/p&gt;
 &lt;p&gt;
  What does an Nvidia B300 NVL72 cost per rack, which weighs in at around 120 kilowatts to 145 kilowatts, depending on who you ask and the conditions. Not including storage, but just the scale up networking and host compute, that GB300 NVL72 rack does 1,100 petaflops at FP4 precision (really tuned for inference, not training) and costs around $4 billion. That is $3,636 per petaflops and $25.08 per petaflops per kilowatt using the 145 kilowatts per rack figure. That’s about 35 percent better oomph per watt to the advantage of Qualcomm.
 &lt;/p&gt;
 &lt;p&gt;
  At $6,150 per unit for the AI 200 Ultra – if it looks like we think it might – then the performance per watt is the same between the GB300 rack and the AI 200 Ultra rack. Qualcomm can cut down from there as market conditions dictate, and maybe it will not have to discount much at all because of supply shortages and the desire to have multiple suppliers.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> How Qualcomm Can Compete With Nvidia For Datacenter AI Inference </title>
<link>https://www.nextplatform.com/2025/10/28/how-qualcomm-can-compete-with-nvidia-for-datacenter-ai-inference/</link>
<pubDate>Tue, 28 Oct 2025 17:52:40 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-100-ultra-logo.jpg&quot; title=&quot;qualcomm-ai-100-ultra-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Qualcomm had datacenter envy back when Intel ruled the bit barns of the world, and now it has it even worse now that Nvidia has shown how AI processing can utterly transform the finances of a chip maker. Qualcomm is also the volume leader for high-end Arm CPUs and add-on circuits for smartphones, and it has the IP and the talent to create server CPUs and AI accelerators that can take a piece of the enormous AI inference opportunity.
 &lt;/p&gt;
 &lt;p&gt;
  What it doesn’t have is a technology that is going to take much of a bite out of Nvidia’s AI inference workload, no matter how excited Wall Street is about that prospect this week as Saudi Arabia’s Humain AI startup has emerged as a sugar daddy for Qualcomm’s datacenter AI ambitions. And Qualcomm has absolutely no shot of creating anything that can take on Nvidia in AI training, which is where Nvidia will make around half of its $183.5 billion in datacenter revenues in fiscal 2026 ending in January according to our model.
 &lt;/p&gt;
 &lt;p&gt;
  This is the green-tinted lens through which you have to look at the deal with Humain that Qualcomm outlined a little bit more this week.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.qualcomm.com/news/releases/2025/05/qualcomm-and-humain-to-develop-state-of-the-art-ai-data-centers-&quot;&gt;
   Back in May
  &lt;/a&gt;
  , Qualcomm inked a memorandum of understanding with Humain to collaborate on the development of AI technologies at the edge and in the datacenter. That MOU not only included the expected AI chips for inference – we wish they had a better product name and we wish we knew the code name – but edge devices powered by Snapdragon and Dragonwing system-on-chip designs that aim to “accelerate” back-end cloud infrastructure and to tune up Humain’s Arabic large language models for these SoCs. And significantly, the MOU called for Qualcomm to “develop and supply state-of-the-art datacenter CPU and AI solutions,” which unequivocally means that Qualcomm is getting back into the server CPU business while also getting funding to expand its AI accelerator lineup.
 &lt;/p&gt;
 &lt;p&gt;
  Let’s tackle the AI accelerators first, and then ponder the server CPUs after that.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-200-inference-rack.jpg&quot; rel=&quot;attachment wp-att-146529&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;700&quot; sizes=&quot;(max-width: 734px) 100vw, 734px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-200-inference-rack.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-200-inference-rack.jpg 734w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-200-inference-rack-600x572.jpg 600w&quot; width=&quot;734&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Concurrent with the Future Investment Initiative 2025 conference in Riyadh, Saudi Arabia this week, Humain and Qualcomm moved from an MOU to an actual contract, and revealed that two future AI accelerators were in development with Humain as the first customer. Like Broadcom with at least two of its XPU customers, Qualcomm is also providing Humain with complete rackscale systems, not just chips that it has to provide to an original design manufacturer like Quanta, Foxconn, Invensys, Jabil Circuit, Celestica, or WiWynn (just to name a few biggies) to turn into servers and cluster into systems.
 &lt;/p&gt;
 &lt;p&gt;
  The original AI 100 XPUs from Qualcomm were announced way back in 2019 and shipped sometime in the first half of 2021. The only time we ever saw them was before waferscale system supplier Cerebras Systems had tweaked its software stack so it could do inference. But
  &lt;a href=&quot;https://www.nextplatform.com/2024/03/14/cerebras-goes-hyperscale-with-third-gen-waferscale-supercomputers/&quot;&gt;
   in March 2024
  &lt;/a&gt;
  , when the WS-3 compute engines and their CS-3 systems debuted, Cerebras was offloading inference to racks of AI 100 accelerators from Qualcomm to do inference cheaper than could be done at the time with its own systems. By September last year,
  &lt;a href=&quot;https://www.nextplatform.com/2024/09/10/the-battle-begins-for-ai-inference-compute-in-the-datacenter/&quot;&gt;
   Cerebras had tweaked its software to run inference workloads
  &lt;/a&gt;
  , and we never heard about the AI 100 XPUs again.
 &lt;/p&gt;
 &lt;p&gt;
  Frankly, there are so many startups chasing AI inference and there was so much going on we never did circle back. (Apologies for that, Qualcomm.) In any event, Qualcomm published a series of benchmarks on the AI 100 accelerator back in September 2021, which is interesting reading and which showed these devices standing toe-to-toe with low-end and high-end Nvidia “Ampere” GPUs and other inference engines suitable for the edge on ResNet-50 image processing tests. The AI 100 did particularly well on inference per second per watt compared to Nvidia A100 GPUs, which is an important fact.
 &lt;/p&gt;
 &lt;p&gt;
  But inference has moved on, and in a big way, with GenAI, and the compute workload has gotten a lot more intense. But so has the desire to find a cheaper alternative – should one really exist – compared to running mixture of expert, reasoning inference on Nvidia rackscale CPU-GPU hybrids.
 &lt;/p&gt;
 &lt;p&gt;
  In October 2024 – and we cannot find an exact announcement date, which is peculiar – Qualcomm started shipped geared-down versions of the AI 100 called the AI 80 and also created a PCI-Express card that interlinked four AI 100 chips on a single package called the AI 100 Ultra. (There was also an Ultra version of the AI 80 card added.) Qualcomm also started to get better yield on the SRAM on the XPUs, and was able to boost the capacity from 126 MB per chip to 144 MB per chip – we have no idea if this capacity of the SRAM scratchpad memory is the maximum available on the device, but if not, it is probably close.
 &lt;/p&gt;
 &lt;p&gt;
  Just a few days ago, researchers at the University of California at San Diego, just down the road from Qualcomm headquarters,
  &lt;a href=&quot;https://arxiv.org/pdf/2507.00418&quot;&gt;
   put the AI 100 Ultra through the benchmark paces
  &lt;/a&gt;
  against systems with four and eight A100 GPUs, and the Qualcomm XPUs did well. On GPT-2 and Granite 3.2, four A100s burned 60 percent less watts per token generated as a single AI 100 Ultra with four Qualcomm chips, and the A100 did a little bit better on the Neomtron-70B model. But otherwise, a given number of Qualcomm cards offered better performance per watt than a given number of Nvidia cards.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table.jpg&quot; rel=&quot;attachment wp-att-146533&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;263&quot; sizes=&quot;(max-width: 725px) 100vw, 725px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table.jpg 853w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table-768x278.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ucsd-ai-100-versus-nvidia-a100-table-600x217.jpg 600w&quot; width=&quot;725&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It was odd on the paper that UCSD didn’t actually do the math and show how these two sets of devices stacked up explicitly, leaving readers to do the math. But we built the sheet above to show you how they compared.
 &lt;/p&gt;
 &lt;p&gt;
  The other thing the paper does not talk about is the density of the compute and the number of devices you will need to reach a given throughput. We did the math, calculating how many AICs (which is what Qualcomm sometimes calls its cards) it would take to match the performance of either four or eight A100s. As you can see, the numbers add up pretty fast. Hypothetically speaking, if you can get sixteen AIC cards into a 5U server, which is reasonably dense, then in those areas where the AI 100 Ultra is beating the GPUs on efficiency, it will take anywhere from one to four racks of Qualcomm accelerators to match the performance of four or eight A100 GPUs. Matching the performance of even lower precision “Hopper” H100 or H200 or “Blackwell” B100, B200, or B300 GPUs from Nvidia would require 2X or 4X to 6X, respectively, more racks.
 &lt;/p&gt;
 &lt;p&gt;
  As usual, if you have space, you can go cheap if your workload is embarrassingly parallel.
 &lt;/p&gt;
 &lt;p&gt;
  Here is a table comparing the five existing variations of the Qualcomm AI XPUs and our estimation of what the Ultra versions of the future AI 200 and AI 250 accelerators, which were revealed this week as part of the deal with Humain, might look like:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table.jpg&quot; rel=&quot;attachment wp-att-146531&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;390&quot; sizes=&quot;(max-width: 1062px) 100vw, 1062px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table.jpg 1062w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table-768x282.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-inference-accelerator-big-table-600x220.jpg 600w&quot; width=&quot;1062&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We are reasonably sure that Qualcomm is making its AI XPUs at Taiwan Semiconductor Manufacturing Co; we have guessed about the process used, and as usual, our guesses are shown in bold red italics.
 &lt;/p&gt;
 &lt;p&gt;
  We know that the AI 200 coming sometime next year, with Hussain as the anchor customer, will have 768 GB of LPDDR5 main memory and will use PCI-Express for its scale up network within a rack and Ethernet to scale out across racks. That’s about it. We know that the AI 250 kicker will come in early 2027, and that Qualcomm has committed to an annual cadence for advancements for its AI accelerators.
 &lt;/p&gt;
 &lt;p&gt;
  We made estimated of what the AI 200 Ultra and AI 250 Ultra looked like mostly to amuse ourselves, and to get a feeling for how these might look.
 &lt;/p&gt;
 &lt;p&gt;
  Not much detail is known about the architecture of the chip used in the AI 100 series. It supports FP16 floating point and INT8 integer processing and the performance the AI 100 cards along with board-level SRAM and main memory shown for the number of chips and AI cores. The AI 100 architecture is based on the Hexagon neural network processors (NNPs) that are in Qualcomm’s smartphone CPUs and that are also known as Q6 in some Linux documentation.
 &lt;/p&gt;
 &lt;p&gt;
  Here is what the Qualcomm AI core looks like:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-core-block-diagram.jpg&quot; rel=&quot;attachment wp-att-146530&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;518&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-core-block-diagram.jpg&quot; width=&quot;538&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As you can see, the architecture has scalar, vector and tensor units all on the same core. It represents the seventh generation of neural network processors that Qualcomm has developed for its smartphones. The scalar chip is a four-way VLIW setup with six hardware threads; it has over 1,800 instructions. The scalar circuits have instruction and data caches and even though it doesn’t show it, there are links between the scalar unit and the memory subsystem, which is how the scalar unit offloads work to the vector and tensor units on the core. That memory subsystem has a 1 MB L2 cache that feeds into an 8 MB scratchpad memory shared by the vector and tensor units.
 &lt;/p&gt;
 &lt;p&gt;
  The tensor unit has over 125 instructions suitable for AI operations and has on 8,192 2D multiply-accumulate (MAC) array to do INT8 work and another 4,096 2D MAC array to do FP16 work. The tensor extensions are called HMX, short for Hexagon Matrix Extensions.
 &lt;/p&gt;
 &lt;p&gt;
  The vector unit has accelerators for scatter/gather collective operations and has over 700 instructions for AI, image processing, and other content manipulation functions. It can support 8-bit or 16-bit integer and 16-bit or 32-bit floating point operations. At 8-bits in integer mode, this vector unit can do 512 MAC operations per clock and at 16-bit floating point, 256 MAC operations per clock. This is obviously only a portion of the throughput of the tensor unit, but some algorithms want a vector unit, not a tensor unit. The vector instructions are known collectively as HVX, or Hexagon Vector Extensions, for short.
 &lt;/p&gt;
 &lt;p&gt;
  Here is what the AI 100 SoC looks like when you put sixteen of the AI cores on a die and wrap four LPDDR4X memory controllers around it plus a PCI-Express 4.0 controller with eight lanes of I/O to link to a host system:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-soc-block-diagram.jpg&quot; rel=&quot;attachment wp-att-146532&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;485&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 742px) 100vw, 742px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-soc-block-diagram.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-soc-block-diagram.jpg 742w, https://www.nextplatform.com/wp-content/uploads/2025/10/qualcomm-ai-soc-block-diagram-600x392.jpg 600w&quot; width=&quot;742&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It is reasonable to assume that Qualcomm will release a Hexagon 7 architecture with more instructions and other stuff, and also that it will boost the number of AI cores on an SoC with the AI 200 generation. To keep pace, it should be about 2X for the AI 200, etched in 5 nanometer processes perhaps to make it cheap, to get some kind of performance per watt advantage over more current Nvidia GPUs. And we think an AI 250 might boost it by another 50 percent with a shrink to 3 nanometer processes from TSMC in 2027. So 32 cores in 2026 with the AI 200 and 48 cores with the AI 250 in 2027. Clock speeds will be what they will be, based on thermals required. Expect for Qualcomm to emphasis efficiency over performance, which means lower clocks and more devices to get a given level of throughput at the same power draw as a GPU setup. This is, after all, the game that Qualcomm has been playing.
 &lt;/p&gt;
 &lt;p&gt;
  We think that the AI core also has to do at least FP8 if not FP4 precision on the tensor cores, which would double or quadruple the performance per clock cycle compared to the current AI cores based on the Hexagon 6 architecture. It is possible that Qualcomm gets rid of integer support in the tensor core and gooses floating point by a lot. (That’s what we would do.)
 &lt;/p&gt;
 &lt;p&gt;
  That brings us to CPUs. Qualcomm had an Arm server CPU called the “Amberwing” Centriq 2400,
  &lt;a href=&quot;https://www.nextplatform.com/2017/12/12/battle-datacenter-compute-qualcomm-centriq-versus-intel-xeon/&quot;&gt;
   way back in 2017
  &lt;/a&gt;
  . It was a 48-core chip that did pretty good against the “Broadwell” and “Skylake” Xeon SPs of the time. The rumor was that Google was the backer of the Centriq effort, and for whatever reason when Google didn’t buy a lot of them,
  &lt;a href=&quot;https://www.nextplatform.com/2018/05/10/what-qualcomms-exit-from-arm-server-chips-means/&quot;&gt;
   Qualcomm spiked the server CPU effort in May 2018
  &lt;/a&gt;
  . In January 2021,
  &lt;a href=&quot;https://www.nextplatform.com/2021/01/13/qualcomm-probably-not-pursuing-arm-servers-with-nuvia-purchase/&quot;&gt;
   Qualcomm bought Arm server chip designer Nuvia
  &lt;/a&gt;
  , oddly enough not to do servers but to get its hands on its “Phoenix” cores, which are now known as Oryon cores and in contrast to the Snapdragon cores Qualcomm has designed itself.
 &lt;/p&gt;
 &lt;p&gt;
  Qualcomm has been very clear that it is once again working on a datacenter server CPU, per its May announcement with Humain. There is a good chance, we think, that the future AI 200 and AI 250 devices will have integrated server-class Oryon Arm cores integrated in the package, eliminating the need to run external X86 or Arm CPUs as hosts. And we would venture so far as to say that the LPDDR5 memory attached to the AI 200 or the LPDDR6X memory attached to the AI 250 accelerators will be shared coherently with said Oryon cores.
 &lt;/p&gt;
 &lt;p&gt;
  The statement from Qualcomm says that the AI 250 “will debut with an innovative memory architecture based on near-memory computing, providing a generational leap in efficiency and performance for AI inference workloads by delivering greater than 10X higher effective memory bandwidth and much lower power consumption.” We are not sure what that means, but it sounds like it might mean what we said above. We do not expect for Qualcomm to add HBM stacked memory to its devices, which would defeat the purpose of lowering cost and increasing availability.
 &lt;/p&gt;
 &lt;p&gt;
  That brings us to the next issue: How many Qualcomm accelerators is Humain planning to buy, and how much money is this for Qualcomm? (Which is another way of saying how much money can it take away from Nvidia.)
 &lt;/p&gt;
 &lt;p&gt;
  Qualcomm said that it has won a 200 megawatt deployment. At 250 watts for an AI 200 Ultra card with four SoCs, that is 800,000 cards. We know Qualcomm wants to deliver 160 kilowatts per rack, so say that the AI 200 Ultras are 80 percent of that power, which is 128 kilowatts. That is 512 devices per rack, and that is 1,250 racks. At $4,000 per card, that is $3.2 billion, plus maybe another $2 billion for the rack and its cooling, networking, and storage. That’s $5.2 million per rack, and if Qualcomm gets rid of integer math on the tensor cores and only does floating point and it drives the precision down to FP4 on the tensor cores, that is 983 petaflops for that $3.2 million of compute in the rack, which is $2,604 per petaflops and which is $16.30 per petaflops per kilowatt.
 &lt;/p&gt;
 &lt;p&gt;
  What does an Nvidia B300 NVL72 cost per rack, which weighs in at around 120 kilowatts to 145 kilowatts, depending on who you ask and the conditions. Not including storage, but just the scale up networking and host compute, that GB300 NVL72 rack does 1,100 petaflops at FP4 precision (really tuned for inference, not training) and costs around $4 billion. That is $3,636 per petaflops and $25.08 per petaflops per kilowatt using the 145 kilowatts per rack figure. That’s about 35 percent better oomph per watt to the advantage of Qualcomm.
 &lt;/p&gt;
 &lt;p&gt;
  At $6,150 per unit for the AI 200 Ultra – if it looks like we think it might – then the performance per watt is the same between the GB300 rack and the AI 200 Ultra rack. Qualcomm can cut down from there as market conditions dictate, and maybe it will not have to discount much at all because of supply shortages and the desire to have multiple suppliers.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Oak Ridge “Discovery” Supercomputer Spearheads New HPE Cray GX5000 Design </title>
<link>https://www.nextplatform.com/2025/10/27/oak-ridge-discovery-supercomputer-spearheads-new-hpe-cray-gx5000-design/#respond</link>
<pubDate>Mon, 27 Oct 2025 19:43:50 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design-1030x438.png&quot; title=&quot;HPE pump design&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Hewlett Packard Enterprise PE has a long history in supercomputing, with efforts like its Apollo family of systems aimed at data-intensive workloads like HPC, data analytics, and storage, and its $275 million acquisition in 2016 of SGI to help expand its presence in HPC.
 &lt;/p&gt;
 &lt;p&gt;
  But it was HPE’s
  &lt;a href=&quot;https://www.nextplatform.com/2019/05/17/with-cray-deal-hpe-finally-enters-the-hpc-big-leagues/&quot;&gt;
   $1.3 billion deal
  &lt;/a&gt;
  three years later to buy supercomputing pioneer Cray that propelled it past competitors like IBM and into its current dominant position. And it was HPE’s much deeper pockets and global reach that gave Cray the financial staying power it needed to become dominant.
 &lt;/p&gt;
 &lt;p&gt;
  With Cray in hand, HPE has now built the world’s three fastest supercomputers,
  &lt;a href=&quot;https://top500.org/news/el-capitan-retains-top-spot-65th-top500-list-exascale-era-expands/&quot;&gt;
   all of them exascale systems
  &lt;/a&gt;
  based on the Cray EX4000 architecture and now humming along in three US Department of Energy (DOE) laboratories – “
  &lt;a href=&quot;https://www.nextplatform.com/?s=el+capitan&quot;&gt;
   El Capitan
  &lt;/a&gt;
  ” at Lawrence Livermore National Laboratory in California, “
  &lt;a href=&quot;https://www.nextplatform.com/2024/12/19/gordon-bell-prize-awarded-to-molecular-dynamics-quantum-mechanics-mashup-on-frontier-supercomputer/&quot;&gt;
   Frontier
  &lt;/a&gt;
  ” at Oak Ridge National Lab in Tennessee, and “
  &lt;a href=&quot;https://www.nextplatform.com/2023/06/27/argonne-aurora-a21-alls-well-that-ends-better/&quot;&gt;
   Aurora
  &lt;/a&gt;
  ” at the Argonne Leadership Computing Facility in Illinois.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3.png&quot; rel=&quot;attachment wp-att-146524&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;610&quot; sizes=&quot;(max-width: 1217px) 100vw, 1217px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3.png 1217w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3-768x385.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3-600x301.png 600w&quot; width=&quot;1217&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As with every part of the IT industry, supercomputing is undergoing rapid changes in needs and demands with the accelerating rise of generative AI workloads and models. Those shifts are forcing HPE and other system makers to assess how they need to evolve their architecture and infrastructure in not-too-distant future generations to meet the new AI-driven demands.
 &lt;/p&gt;
 &lt;h3&gt;
  A “Crazy Time” In AI And HPC
 &lt;/h3&gt;
 &lt;p&gt;
  “It’s been a crazy time to be in the HPC and AI business because of the explosion of AI and ChatGPT and everything else,” Trish Damkroger, senior vice president and general manager of HPE’s HPC and AI infrastructure solutions, told journalists during a video briefing on the new GX5000 iron. “We need to build a converged system that really can focus on both of these workload needs. It has to be modeling a simulation, which is core to so many of our customers, but it also has to fit the AI world. What this means is, it’s not only about having the traditional needs and what our traditional customers are looking for. For silicon, it’s a broader silicon need that we’re going to have to house within this infrastructure. We’re also converging with AI. Simulation is now not done alone. It’s truly part of a workflow, and AI is enhancing the modeling and simulation work, speeding it up, making it easier, etc.”
 &lt;/p&gt;
 &lt;p&gt;
  Damkroger added that “we have to make sure that our infrastructure supports both. It’s really about extreme scaling. With the growth of AI and the TDP [thermal design power] of the silicon. We’ve had to rethink what is possible and what we can do so that we can make sure that we maximize both the datacenter space and the energy usage.”
 &lt;/p&gt;
 &lt;p&gt;
  Due to the expanding use of such systems by cloud providers and enterprises adopting infrastructure-as-a-service (IaaS), and the core factors that HPE and others have to think about grows.
 &lt;/p&gt;
 &lt;p&gt;
  To address this, HPE is introducing the GX5000 exascale system – the successor to the Cray “Shasta” EX3000 line that was debuted ahead of SC2018 and that started shipping a year later. The Shasta line was expanded with the EX4000 systems at SC2022, while we were still dealing with the aftermath of the coronavirus pandemic. In addition to the new GX5000 designs, HPE is rolling out a new distributed storage cluster based on the open source Distributed Asynchronous Object Storage (DAOS) software that was pioneered by Intel in the Aurora system. The GX5000s also have the next generation of liquid cooling. All of these changes were made to address the ongoing convergence of AI and HPC workloads.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000.png&quot; rel=&quot;attachment wp-att-146517&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;615&quot; sizes=&quot;(max-width: 1074px) 100vw, 1074px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000.png 1074w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000-768x440.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000-600x344.png 600w&quot; width=&quot;1074&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The GX5000 system will be the architecture for two new systems at Oak Ridge, including “Discovery,” an exascale computer that will be the successor to Frontier and also expected to be installed in 2028 at a cost of around $500 million, with operations starting in 2029.
 &lt;/p&gt;
 &lt;p&gt;
  The other will be an AI cluster called “Lux,” which will be based on the direct liquid-cooled HPE ProLiant Compute XD685 that is powered by AMD technology, including Instinct MI355X GPUs, Epyc CPUs, and Pensando DPUs.
 &lt;/p&gt;
 &lt;p&gt;
  Discovery is aimed at AI, HPC, and quantum computing and will increase the productivity of some applications by 10X, with use cases in such areas as precision medicine, cancer research, nuclear energy, and aerospace. It will be powered by AMD’s upcoming next-generation “Venice” Epyc processors and Instinct MI430X GPUs, and will come with the HPE’s Slingshot 400 interconnect, which promises twice the speed over the 200 Gb/sec Slingshot 200 series. Slingshot 400 was introduced last year along with
  &lt;a href=&quot;https://www.nextplatform.com/2024/11/26/hpe-upgrades-supercomputer-lineup-top-to-bottom-in-2025/&quot;&gt;
   other HPC upgrades
  &lt;/a&gt;
  and with plans to make it available in “Shasta” Cray EX systems this fall.
 &lt;/p&gt;
 &lt;p&gt;
  Meanwhile, Lux will operate in a multi-tenant cloud-platform focusing on AI and machine learning operations.
 &lt;/p&gt;
 &lt;h3&gt;
  Pivots With The GX5000
 &lt;/h3&gt;
 &lt;p&gt;
  Discovery will be the introduction for the GX5000.
 &lt;/p&gt;
 &lt;p&gt;
  “It is purpose-built, like our previous generation, for supercomputing,” Damkroger said. “It includes CPUs, GPUs, networking, software, storage, and cooling. It’s a completely new architecture engineered to deliver this unprecedented performance with higher density and truly supports these growing workloads and workflows. The GX5000 has been in the works for years, but, honestly, we’ve made some pivots over the last year and a half as we’ve seen the growth of TDPs, the growth of different silicon coming out from all the vendors, and the need to be able to support all of these different workloads.”
 &lt;/p&gt;
 &lt;p&gt;
  There is still more information that will be coming out down the road about the GX5000 – which will be on display at SC25 next month in St. Louis – but she said the architecture will offer more compute power – 127 percent more – and up to 25 kilowatts per compute slot. The system will be 42 percent smaller than its predecessor.
 &lt;/p&gt;
 &lt;p&gt;
  Each GX5000 tray will accommodate different TDP parts, though Damkroger said she wasn’t ready to talk in detail about how hot they could get (our guess is several kilowatts each) or about which CPUs, GPUs, and XPUs would be included as options. Those details will emerge when HPE starts to announce customer wins for the system, with system deliveries beginning in early 2027, she said.
 &lt;/p&gt;
 &lt;p&gt;
  The EX system is a double-cabinet wide, but the GX5000 will be smaller, dropping from 95 inches across to 53 inches. It also has a design that will allow organizations to mix and match the processors.
 &lt;/p&gt;
 &lt;p&gt;
  “Where [with] the EX, you had to have the same, so you made sure you had the same load across each one of the blades,” she said. “With the new pump design [below], we’re going to be able to mix and match, which will mean we could have mixed cabinets, which is definitely something that our customers have been interested in.”
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design.png&quot; rel=&quot;attachment wp-att-146523&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;6000&quot; sizes=&quot;(max-width: 8000px) 100vw, 8000px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design.png 8000w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design-768x576.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design-600x450.png 600w&quot; width=&quot;8000&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Augmenting the GX5000 will be the Cray Supercomputing Storage System K3000, which comes with embedded DAOS software. Intel late last year transferred its DAOS development team to HPE. DAOS software is also part of the Aurora system, and with the all-flash K3000, HPE has a factory-built storage system with DAOS embedded that will deliver as much as 75 million IO operations per second (IOPS) per storage rack, 39 percent higher than other systems, according to Damkroger.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000.png&quot; rel=&quot;attachment wp-att-146520&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;595&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1178px) 100vw, 1178px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000.png 1178w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000-768x388.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000-600x303.png 600w&quot; width=&quot;1178&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It complements HPE’s Lustre Cray Supercomputing Storage Systems E2000, which also will be used in Discovery.
 &lt;/p&gt;
 &lt;p&gt;
  The GX5000 also will feature HPE’s latest direct liquid cooling.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling.png&quot; rel=&quot;attachment wp-att-146522&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;599&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1221px) 100vw, 1221px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling.png 1221w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling-768x377.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling-600x294.png 600w&quot; width=&quot;1221&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “We’re bringing liquid coolant to every component of the supercomputing that transmits heat,” she said. “This is important. It’s not just CPUs and GPUs and memory, but it’s also switches, which is unique in our current EX design. The next generation of Cray supercomputing will allow for more efficiency and density. … Basically, the cooling pump is designed to be more compact and can be placed on the side of the system, called a side pump, instead of in the middle, and each pump is going to have redundancy to ensure that there’s an always on operation.”
 &lt;/p&gt;
 &lt;p&gt;
  Users will be able to control the water flow rate, so rather than every single blade having the same flow rate, it can be optimized based on the needs of the blade and what it’s running. The water also will be at 40 degrees Celsius, or just more than 100 degrees Fahrenheit. That’s warmer than the current 25 degrees Celsius (77 Fahrenheit).
 &lt;/p&gt;
 &lt;p&gt;
  “This new thermal capacity meets the new energy requirements for a lot of our customers in Europe and, actually, in other parts of the world,” Damkroger said. “This will ensure that we don’t need additional chillers and refrigerators, which just costs additional power. It’s really going to be a much more energy-efficient system.”
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Oak Ridge “Discovery” Supercomputer Spearheads New HPE Cray GX5000 Design </title>
<link>https://www.nextplatform.com/2025/10/27/oak-ridge-discovery-supercomputer-spearheads-new-hpe-cray-gx5000-design/</link>
<pubDate>Mon, 27 Oct 2025 19:43:47 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design-1030x438.png&quot; title=&quot;HPE pump design&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Hewlett Packard Enterprise PE has a long history in supercomputing, with efforts like its Apollo family of systems aimed at data-intensive workloads like HPC, data analytics, and storage, and its $275 million acquisition in 2016 of SGI to help expand its presence in HPC.
 &lt;/p&gt;
 &lt;p&gt;
  But it was HPE’s
  &lt;a href=&quot;https://www.nextplatform.com/2019/05/17/with-cray-deal-hpe-finally-enters-the-hpc-big-leagues/&quot;&gt;
   $1.3 billion deal
  &lt;/a&gt;
  three years later to buy supercomputing pioneer Cray that propelled it past competitors like IBM and into its current dominant position. And it was HPE’s much deeper pockets and global reach that gave Cray the financial staying power it needed to become dominant.
 &lt;/p&gt;
 &lt;p&gt;
  With Cray in hand, HPE has now built the world’s three fastest supercomputers,
  &lt;a href=&quot;https://top500.org/news/el-capitan-retains-top-spot-65th-top500-list-exascale-era-expands/&quot;&gt;
   all of them exascale systems
  &lt;/a&gt;
  based on the Cray EX4000 architecture and now humming along in three US Department of Energy (DOE) laboratories – “
  &lt;a href=&quot;https://www.nextplatform.com/?s=el+capitan&quot;&gt;
   El Capitan
  &lt;/a&gt;
  ” at Lawrence Livermore National Laboratory in California, “
  &lt;a href=&quot;https://www.nextplatform.com/2024/12/19/gordon-bell-prize-awarded-to-molecular-dynamics-quantum-mechanics-mashup-on-frontier-supercomputer/&quot;&gt;
   Frontier
  &lt;/a&gt;
  ” at Oak Ridge National Lab in Tennessee, and “
  &lt;a href=&quot;https://www.nextplatform.com/2023/06/27/argonne-aurora-a21-alls-well-that-ends-better/&quot;&gt;
   Aurora
  &lt;/a&gt;
  ” at the Argonne Leadership Computing Facility in Illinois.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3.png&quot; rel=&quot;attachment wp-att-146524&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;610&quot; sizes=&quot;(max-width: 1217px) 100vw, 1217px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3.png 1217w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3-768x385.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-top-3-600x301.png 600w&quot; width=&quot;1217&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As with every part of the IT industry, supercomputing is undergoing rapid changes in needs and demands with the accelerating rise of generative AI workloads and models. Those shifts are forcing HPE and other system makers to assess how they need to evolve their architecture and infrastructure in not-too-distant future generations to meet the new AI-driven demands.
 &lt;/p&gt;
 &lt;h3&gt;
  A “Crazy Time” In AI And HPC
 &lt;/h3&gt;
 &lt;p&gt;
  “It’s been a crazy time to be in the HPC and AI business because of the explosion of AI and ChatGPT and everything else,” Trish Damkroger, senior vice president and general manager of HPE’s HPC and AI infrastructure solutions, told journalists during a video briefing on the new GX5000 iron. “We need to build a converged system that really can focus on both of these workload needs. It has to be modeling a simulation, which is core to so many of our customers, but it also has to fit the AI world. What this means is, it’s not only about having the traditional needs and what our traditional customers are looking for. For silicon, it’s a broader silicon need that we’re going to have to house within this infrastructure. We’re also converging with AI. Simulation is now not done alone. It’s truly part of a workflow, and AI is enhancing the modeling and simulation work, speeding it up, making it easier, etc.”
 &lt;/p&gt;
 &lt;p&gt;
  Damkroger added that “we have to make sure that our infrastructure supports both. It’s really about extreme scaling. With the growth of AI and the TDP [thermal design power] of the silicon. We’ve had to rethink what is possible and what we can do so that we can make sure that we maximize both the datacenter space and the energy usage.”
 &lt;/p&gt;
 &lt;p&gt;
  Due to the expanding use of such systems by cloud providers and enterprises adopting infrastructure-as-a-service (IaaS), and the core factors that HPE and others have to think about grows.
 &lt;/p&gt;
 &lt;p&gt;
  To address this, HPE is introducing the GX5000 exascale system – the successor to the Cray “Shasta” EX3000 line that was debuted ahead of SC2018 and that started shipping a year later. The Shasta line was expanded with the EX4000 systems at SC2022, while we were still dealing with the aftermath of the coronavirus pandemic. In addition to the new GX5000 designs, HPE is rolling out a new distributed storage cluster based on the open source Distributed Asynchronous Object Storage (DAOS) software that was pioneered by Intel in the Aurora system. The GX5000s also have the next generation of liquid cooling. All of these changes were made to address the ongoing convergence of AI and HPC workloads.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000.png&quot; rel=&quot;attachment wp-att-146517&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;615&quot; sizes=&quot;(max-width: 1074px) 100vw, 1074px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000.png 1074w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000-768x440.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-GX5000-600x344.png 600w&quot; width=&quot;1074&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The GX5000 system will be the architecture for two new systems at Oak Ridge, including “Discovery,” an exascale computer that will be the successor to Frontier and also expected to be installed in 2028 at a cost of around $500 million, with operations starting in 2029.
 &lt;/p&gt;
 &lt;p&gt;
  The other will be an AI cluster called “Lux,” which will be based on the direct liquid-cooled HPE ProLiant Compute XD685 that is powered by AMD technology, including Instinct MI355X GPUs, Epyc CPUs, and Pensando DPUs.
 &lt;/p&gt;
 &lt;p&gt;
  Discovery is aimed at AI, HPC, and quantum computing and will increase the productivity of some applications by 10X, with use cases in such areas as precision medicine, cancer research, nuclear energy, and aerospace. It will be powered by AMD’s upcoming next-generation “Venice” Epyc processors and Instinct MI430X GPUs, and will come with the HPE’s Slingshot 400 interconnect, which promises twice the speed over the 200 Gb/sec Slingshot 200 series. Slingshot 400 was introduced last year along with
  &lt;a href=&quot;https://www.nextplatform.com/2024/11/26/hpe-upgrades-supercomputer-lineup-top-to-bottom-in-2025/&quot;&gt;
   other HPC upgrades
  &lt;/a&gt;
  and with plans to make it available in “Shasta” Cray EX systems this fall.
 &lt;/p&gt;
 &lt;p&gt;
  Meanwhile, Lux will operate in a multi-tenant cloud-platform focusing on AI and machine learning operations.
 &lt;/p&gt;
 &lt;h3&gt;
  Pivots With The GX5000
 &lt;/h3&gt;
 &lt;p&gt;
  Discovery will be the introduction for the GX5000.
 &lt;/p&gt;
 &lt;p&gt;
  “It is purpose-built, like our previous generation, for supercomputing,” Damkroger said. “It includes CPUs, GPUs, networking, software, storage, and cooling. It’s a completely new architecture engineered to deliver this unprecedented performance with higher density and truly supports these growing workloads and workflows. The GX5000 has been in the works for years, but, honestly, we’ve made some pivots over the last year and a half as we’ve seen the growth of TDPs, the growth of different silicon coming out from all the vendors, and the need to be able to support all of these different workloads.”
 &lt;/p&gt;
 &lt;p&gt;
  There is still more information that will be coming out down the road about the GX5000 – which will be on display at SC25 next month in St. Louis – but she said the architecture will offer more compute power – 127 percent more – and up to 25 kilowatts per compute slot. The system will be 42 percent smaller than its predecessor.
 &lt;/p&gt;
 &lt;p&gt;
  Each GX5000 tray will accommodate different TDP parts, though Damkroger said she wasn’t ready to talk in detail about how hot they could get (our guess is several kilowatts each) or about which CPUs, GPUs, and XPUs would be included as options. Those details will emerge when HPE starts to announce customer wins for the system, with system deliveries beginning in early 2027, she said.
 &lt;/p&gt;
 &lt;p&gt;
  The EX system is a double-cabinet wide, but the GX5000 will be smaller, dropping from 95 inches across to 53 inches. It also has a design that will allow organizations to mix and match the processors.
 &lt;/p&gt;
 &lt;p&gt;
  “Where [with] the EX, you had to have the same, so you made sure you had the same load across each one of the blades,” she said. “With the new pump design [below], we’re going to be able to mix and match, which will mean we could have mixed cabinets, which is definitely something that our customers have been interested in.”
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design.png&quot; rel=&quot;attachment wp-att-146523&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;6000&quot; sizes=&quot;(max-width: 8000px) 100vw, 8000px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design.png 8000w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design-768x576.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-pump-design-600x450.png 600w&quot; width=&quot;8000&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Augmenting the GX5000 will be the Cray Supercomputing Storage System K3000, which comes with embedded DAOS software. Intel late last year transferred its DAOS development team to HPE. DAOS software is also part of the Aurora system, and with the all-flash K3000, HPE has a factory-built storage system with DAOS embedded that will deliver as much as 75 million IO operations per second (IOPS) per storage rack, 39 percent higher than other systems, according to Damkroger.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000.png&quot; rel=&quot;attachment wp-att-146520&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;595&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1178px) 100vw, 1178px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000.png 1178w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000-768x388.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-K3000-600x303.png 600w&quot; width=&quot;1178&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It complements HPE’s Lustre Cray Supercomputing Storage Systems E2000, which also will be used in Discovery.
 &lt;/p&gt;
 &lt;p&gt;
  The GX5000 also will feature HPE’s latest direct liquid cooling.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling.png&quot; rel=&quot;attachment wp-att-146522&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;599&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1221px) 100vw, 1221px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling.png&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling.png 1221w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling-768x377.png 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/HPE-liquid-cooling-600x294.png 600w&quot; width=&quot;1221&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “We’re bringing liquid coolant to every component of the supercomputing that transmits heat,” she said. “This is important. It’s not just CPUs and GPUs and memory, but it’s also switches, which is unique in our current EX design. The next generation of Cray supercomputing will allow for more efficiency and density. … Basically, the cooling pump is designed to be more compact and can be placed on the side of the system, called a side pump, instead of in the middle, and each pump is going to have redundancy to ensure that there’s an always on operation.”
 &lt;/p&gt;
 &lt;p&gt;
  Users will be able to control the water flow rate, so rather than every single blade having the same flow rate, it can be optimized based on the needs of the blade and what it’s running. The water also will be at 40 degrees Celsius, or just more than 100 degrees Fahrenheit. That’s warmer than the current 25 degrees Celsius (77 Fahrenheit).
 &lt;/p&gt;
 &lt;p&gt;
  “This new thermal capacity meets the new energy requirements for a lot of our customers in Europe and, actually, in other parts of the world,” Damkroger said. “This will ensure that we don’t need additional chillers and refrigerators, which just costs additional power. It’s really going to be a much more energy-efficient system.”
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Gartner Radically Raises Datacenter Spending Forecasts </title>
<link>https://www.nextplatform.com/2025/10/27/gartner-radically-raises-datacenter-spending-forecasts/#respond</link>
<pubDate>Mon, 27 Oct 2025 04:46:28 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/08/aws-datacenter-racks-1030x438.jpg&quot; title=&quot;aws-datacenter-racks&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Back in early September, the prognosticators at IDC put out
  &lt;a href=&quot;https://www.nextplatform.com/2025/09/08/idc-makes-ebullient-ai-spending-forecast-out-to-2029/&quot;&gt;
   an ebullient but simple AI spending forecast going out to 2029
  &lt;/a&gt;
  , and Gartner followed suit with a much more detailed AI spending forecast, which we augmented and expanded with the help of the market researcher
  &lt;a href=&quot;https://www.nextplatform.com/2025/09/25/dicing-slicing-and-augmenting-gartners-ai-spending-forecast/&quot;&gt;
   a few weeks later
  &lt;/a&gt;
  . Now, Gartner has circled back and revised its overall IT spending forecast given the explosive spending on GenAI hardware for the model builders and their partners.
 &lt;/p&gt;
 &lt;p&gt;
  This bigtime spending, says Gartner, is finally spurring investments by enterprises, particularly for AI-infused software that is more expensive than prior generations of code that did not have it. People are definitely paying for copilots and code assistants and other AI functions in their ERP, SCM, CRM, and other suites. And that, as well as the humongous datacenter system spending by the AI model builders and their cloud partners, is driving IT spending to new heights, with the expectation for overall global IT spending, as reckoned in US dollars, rising above $6 trillion for the first time in 2026.
 &lt;/p&gt;
 &lt;p&gt;
  That IT spending level is about a year ahead of schedule, based on a Gartner forecast
  &lt;a href=&quot;https://www.nextplatform.com/2025/07/28/how-to-cash-in-on-massive-datacenter-spending/&quot;&gt;
   that came out only in July
  &lt;/a&gt;
  . And based on past statements by Gartner, the datacenter systems spending levels that are now expected in 2026 are higher than the forecast that was made for 2028 earlier this year.
 &lt;/p&gt;
 &lt;p&gt;
  This GenAI boom has gone from chemical to nuclear fission, and perhaps there will be nuclear fusion at some point to really blow the forecasts to kingdom come.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the October 22 forecast for spending for 2025 and 2026 by the usual categories of datacenter systems, enterprise software, IT services, devices, and telecom services:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-little-table-forecast.jpg&quot; rel=&quot;attachment wp-att-146514&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;187&quot; sizes=&quot;(max-width: 550px) 100vw, 550px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-little-table-forecast.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-little-table-forecast.jpg 694w, https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-little-table-forecast-600x204.jpg 600w&quot; width=&quot;550&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We have kept the old category names in some cases because they are better.
 &lt;/p&gt;
 &lt;p&gt;
  In looking at that table above, to paraphrase the great Paul Hogan:
  &lt;em&gt;
   That’s not a table – this is a table:
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  You can get a feel of things from our big table above, which is based on historical Gartner data, a whole lot better than you can from the little one Gartner has put together. Gartner, of course, has a multidimensional database with a zillion categories and geographies that probably has a natural language processing front end by now for those of you who really want to surf historical data. That would be
  &lt;em&gt;
   a freaking sword
  &lt;/em&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  Anyway. The IT spending forecast for 2025 is now $104.9 trillion higher than the prediction that Gartner was making back in July, and is now expected to be $5.54 trillion, up 10 percent from the $5.04 trillion that was spent for all IT hardware, software, and services back in 2024. The level of IT spending expected in 2025 worldwide is now almost precisely what Gartner was expecting in 2026 earlier this year.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-core-it.jpg&quot; rel=&quot;attachment wp-att-146511&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;351&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-core-it.jpg&quot; width=&quot;573&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Gartner’s latest numbers put datacenter systems spending at $333.4 billion in 2024, which is about 2X that of spending levels before the coronavirus pandemic and represents a 40.3 percent growth sequentially from 2023’s $238.6 billion in datacenter systems spending. (Datacenter systems means servers, switches, storage, and other IT gear in the datacenter.)
 &lt;/p&gt;
 &lt;p&gt;
  If you think 2X is a lot, it is going to almost double again in two years. Gartner expects datacenter systems spending to rise by 46.8 percent this year to $489.5 billion in 2025, and while spending will cool next year, it is now expected to still grow by 19 percent to $582.5 billion.
 &lt;/p&gt;
 &lt;p&gt;
  As you can see from our big table above, the growth rates for datacenter systems spending this year and in the coming year are crazy multiples of global gross domestic product increases as expressed in US dollars, which we have obtained from the World Bank. GenAI is absolutely the fuel of this growth, but there are other factors at work. Inflation is one of those factors:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-datacenter-inflation-adjusted.jpg&quot; rel=&quot;attachment wp-att-146512&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;400&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-datacenter-inflation-adjusted.jpg&quot; width=&quot;562&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  If you adjust the Gartner datacenter systems spending by the US Consumer Price Index using the average of the expected inflation rates for 2025 and 2026 coming from various forecasts, what you find is that a lot of the extra money that is being spent is the cumulative effect of inflation.
 &lt;/p&gt;
 &lt;p&gt;
  In our inflation adjustment above, we are using 2021 US dollars and adjusting all data before and after that time. As you can see, inflation adjustment makes past spending look bigger and future spending look smaller. For instance, the cumulative effect of inflation from 2022 through 2026 inclusive adds $103.1 billion to the raw spending numbers. So, to that way of looking at it, the spending is not as high as it looks. But even with the inflation adjustment, the spending increase on datacenter systems from 2019, before COVID hit, and 2026, is a factor of 2.55X. Which is a lot, even if it is not the 3.35X that you get from using the raw numbers that have not been adjusted for inflation by Gartner.
 &lt;/p&gt;
 &lt;p&gt;
  We also like to look at IT spending in general and for datacenter systems in particular as reckoned against global GDP growth, which has had its forecasts recently raised and which we updated in our table above.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-it-versus-gdp.jpg&quot; rel=&quot;attachment wp-att-146513&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;371&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 608px) 100vw, 608px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-it-versus-gdp.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-it-versus-gdp.jpg 608w, https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-it-versus-gdp-600x366.jpg 600w&quot; width=&quot;608&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In this table above, core IT spending means datacenter systems, enterprise software, and IT services. Overall IT spending adds in spending on telecom and datacom services as well as for devices such as PCs, tablet, and smartphones.
 &lt;/p&gt;
 &lt;p&gt;
  As you can see, core IT spending growth tends to be higher than for overall IT spending growth in any given year, which in turn tends to be higher than global GDP growth in a year. It takes a big event – like a pandemic in 2020 – to change the amplitudes on these signals.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Gartner Radically Raises Datacenter Spending Forecasts </title>
<link>https://www.nextplatform.com/2025/10/27/gartner-radically-raises-datacenter-spending-forecasts/</link>
<pubDate>Mon, 27 Oct 2025 04:46:25 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/08/aws-datacenter-racks-1030x438.jpg&quot; title=&quot;aws-datacenter-racks&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Back in early September, the prognosticators at IDC put out
  &lt;a href=&quot;https://www.nextplatform.com/2025/09/08/idc-makes-ebullient-ai-spending-forecast-out-to-2029/&quot;&gt;
   an ebullient but simple AI spending forecast going out to 2029
  &lt;/a&gt;
  , and Gartner followed suit with a much more detailed AI spending forecast, which we augmented and expanded with the help of the market researcher
  &lt;a href=&quot;https://www.nextplatform.com/2025/09/25/dicing-slicing-and-augmenting-gartners-ai-spending-forecast/&quot;&gt;
   a few weeks later
  &lt;/a&gt;
  . Now, Gartner has circled back and revised its overall IT spending forecast given the explosive spending on GenAI hardware for the model builders and their partners.
 &lt;/p&gt;
 &lt;p&gt;
  This bigtime spending, says Gartner, is finally spurring investments by enterprises, particularly for AI-infused software that is more expensive than prior generations of code that did not have it. People are definitely paying for copilots and code assistants and other AI functions in their ERP, SCM, CRM, and other suites. And that, as well as the humongous datacenter system spending by the AI model builders and their cloud partners, is driving IT spending to new heights, with the expectation for overall global IT spending, as reckoned in US dollars, rising above $6 trillion for the first time in 2026.
 &lt;/p&gt;
 &lt;p&gt;
  That IT spending level is about a year ahead of schedule, based on a Gartner forecast
  &lt;a href=&quot;https://www.nextplatform.com/2025/07/28/how-to-cash-in-on-massive-datacenter-spending/&quot;&gt;
   that came out only in July
  &lt;/a&gt;
  . And based on past statements by Gartner, the datacenter systems spending levels that are now expected in 2026 are higher than the forecast that was made for 2028 earlier this year.
 &lt;/p&gt;
 &lt;p&gt;
  This GenAI boom has gone from chemical to nuclear fission, and perhaps there will be nuclear fusion at some point to really blow the forecasts to kingdom come.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the October 22 forecast for spending for 2025 and 2026 by the usual categories of datacenter systems, enterprise software, IT services, devices, and telecom services:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-little-table-forecast.jpg&quot; rel=&quot;attachment wp-att-146514&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;187&quot; sizes=&quot;(max-width: 550px) 100vw, 550px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-little-table-forecast.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-little-table-forecast.jpg 694w, https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-little-table-forecast-600x204.jpg 600w&quot; width=&quot;550&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We have kept the old category names in some cases because they are better.
 &lt;/p&gt;
 &lt;p&gt;
  In looking at that table above, to paraphrase the great Paul Hogan:
  &lt;em&gt;
   That’s not a table – this is a table:
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  You can get a feel of things from our big table above, which is based on historical Gartner data, a whole lot better than you can from the little one Gartner has put together. Gartner, of course, has a multidimensional database with a zillion categories and geographies that probably has a natural language processing front end by now for those of you who really want to surf historical data. That would be
  &lt;em&gt;
   a freaking sword
  &lt;/em&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  Anyway. The IT spending forecast for 2025 is now $104.9 trillion higher than the prediction that Gartner was making back in July, and is now expected to be $5.54 trillion, up 10 percent from the $5.04 trillion that was spent for all IT hardware, software, and services back in 2024. The level of IT spending expected in 2025 worldwide is now almost precisely what Gartner was expecting in 2026 earlier this year.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-core-it.jpg&quot; rel=&quot;attachment wp-att-146511&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;351&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-core-it.jpg&quot; width=&quot;573&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Gartner’s latest numbers put datacenter systems spending at $333.4 billion in 2024, which is about 2X that of spending levels before the coronavirus pandemic and represents a 40.3 percent growth sequentially from 2023’s $238.6 billion in datacenter systems spending. (Datacenter systems means servers, switches, storage, and other IT gear in the datacenter.)
 &lt;/p&gt;
 &lt;p&gt;
  If you think 2X is a lot, it is going to almost double again in two years. Gartner expects datacenter systems spending to rise by 46.8 percent this year to $489.5 billion in 2025, and while spending will cool next year, it is now expected to still grow by 19 percent to $582.5 billion.
 &lt;/p&gt;
 &lt;p&gt;
  As you can see from our big table above, the growth rates for datacenter systems spending this year and in the coming year are crazy multiples of global gross domestic product increases as expressed in US dollars, which we have obtained from the World Bank. GenAI is absolutely the fuel of this growth, but there are other factors at work. Inflation is one of those factors:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-datacenter-inflation-adjusted.jpg&quot; rel=&quot;attachment wp-att-146512&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;400&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-datacenter-inflation-adjusted.jpg&quot; width=&quot;562&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  If you adjust the Gartner datacenter systems spending by the US Consumer Price Index using the average of the expected inflation rates for 2025 and 2026 coming from various forecasts, what you find is that a lot of the extra money that is being spent is the cumulative effect of inflation.
 &lt;/p&gt;
 &lt;p&gt;
  In our inflation adjustment above, we are using 2021 US dollars and adjusting all data before and after that time. As you can see, inflation adjustment makes past spending look bigger and future spending look smaller. For instance, the cumulative effect of inflation from 2022 through 2026 inclusive adds $103.1 billion to the raw spending numbers. So, to that way of looking at it, the spending is not as high as it looks. But even with the inflation adjustment, the spending increase on datacenter systems from 2019, before COVID hit, and 2026, is a factor of 2.55X. Which is a lot, even if it is not the 3.35X that you get from using the raw numbers that have not been adjusted for inflation by Gartner.
 &lt;/p&gt;
 &lt;p&gt;
  We also like to look at IT spending in general and for datacenter systems in particular as reckoned against global GDP growth, which has had its forecasts recently raised and which we updated in our table above.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-it-versus-gdp.jpg&quot; rel=&quot;attachment wp-att-146513&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;371&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 608px) 100vw, 608px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-it-versus-gdp.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-it-versus-gdp.jpg 608w, https://www.nextplatform.com/wp-content/uploads/2025/10/gartner-it-spending-oct-2025-it-versus-gdp-600x366.jpg 600w&quot; width=&quot;608&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In this table above, core IT spending means datacenter systems, enterprise software, and IT services. Overall IT spending adds in spending on telecom and datacom services as well as for devices such as PCs, tablet, and smartphones.
 &lt;/p&gt;
 &lt;p&gt;
  As you can see, core IT spending growth tends to be higher than for overall IT spending growth in any given year, which in turn tends to be higher than global GDP growth in a year. It takes a big event – like a pandemic in 2020 – to change the amplitudes on these signals.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Fixing Intel Foundry Is Like Stopping Tripping Down The Stairs </title>
<link>https://www.nextplatform.com/2025/10/24/fixing-intel-foundry-is-like-stopping-tripping-down-the-stairs/#respond</link>
<pubDate>Fri, 24 Oct 2025 17:49:10 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/08/hc-intel-clearwater-forest-die-shot-1030x438.jpg&quot; title=&quot;hc-intel-clearwater-forest-die-shot&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Technical debt is a real thing, as any IT manager, programmer, system administrator or SRE, or end user will tell you. If you save money in the short run by not keeping up, you always end up paying with interest when you are backed into a place where you do have to catch up. Or, you go out of business.
 &lt;/p&gt;
 &lt;p&gt;
  So it is with Intel, which has come perilously close to irrelevancy in such a short time through a mix of foolishness, arrogance, and honest mistakes. For many decades, Intel was a very good chip manufacturer that also happened to be a chip designer, which just so happened to be the Intel Foundry’s only customer.
 &lt;/p&gt;
 &lt;p&gt;
  The foundry and products groups at Intel really did act like two interdependent but separate companies. The fact that the foundry business would just keep pace with upstart rival Taiwan Semiconductor Manufacturing was just a given in the Intel CPU products groups, even after AMD, IBM, and other chip makers abandoned their foundries because it was just too hard to keep up. And by the time GlobalFoundries (which ate the AMD and IBM foundries, among others) threw in the towel on 10 nanometer and 7 nanometer processes,
  &lt;a href=&quot;https://www.nextplatform.com/2021/06/10/why-ibm-is-suing-globalfoundries-over-chip-roadmap-failures/&quot;&gt;
   leaving IBM’s Power Systems and System z businesses in the lurch
  &lt;/a&gt;
  , Intel was also having its own issues trying to commercialize extreme ultraviolet lithography at 10 nanometers and below.
 &lt;/p&gt;
 &lt;p&gt;
  TSMC did not falter at 7 nanometers or 5 nanometers, but it also had a bit of trouble with the jump from 16 nanometers down to 7 nanometers. In fact, the only server CPU we can find with a 10 nanometer process outside of a desperate Intel with some of its Xeons is
  &lt;a href=&quot;https://www.nextplatform.com/2017/11/08/qualcomms-amberwing-arm-server-chip-finally-takes-flight/&quot;&gt;
   the ill-fated “Amberwing” Centriq 2400 chip from Qualcomm
  &lt;/a&gt;
  , a 48-core Arm chip launched in November 2017, which was etched by Samsung. AMD used the 14 nanometer processes from GlobalFoundries (which was significantly assisted by the IBM Microelectronics acquisition) for
  &lt;a href=&quot;https://www.nextplatform.com/2017/06/20/competition-returns-x86-servers-epyc-fashion/&quot;&gt;
   its “Naples” Epyc 7001
  &lt;/a&gt;
  server CPUs launched in June 2017, and saw the mess that was 10 nanometers and 7 nanometers at GlobalFoundries, Samsung, and TSMC and decided to jump right over it all to 7 nanometer processes from TSMC with
  &lt;a href=&quot;https://www.nextplatform.com/2019/08/07/amd-doubles-down-and-up-with-rome-epyc-server-chips/&quot;&gt;
   the “Rome” Epyc 7002 processors
  &lt;/a&gt;
  in August 2019; AMD processed to the 5 nanometer shrink at TSMC with
  &lt;a href=&quot;https://www.nextplatform.com/2021/03/15/the-third-time-charm-of-amds-milan-epyc-processors/&quot;&gt;
   the “Milan” Epyc 7003s
  &lt;/a&gt;
  in March 2021 and has been eating Intel’s market share lunch in server CPUs since then.
 &lt;/p&gt;
 &lt;p&gt;
  IBM has been stuck in a holding pattern with Power10 and Power11 on Samsung’s 7 nanometer processes, and used 7 nanometer processes for the z16 processor chip for mainframes and shrank down to Samsung’s 5 nanometer processes for the z17 chip follow-on that is ramping now.
 &lt;/p&gt;
 &lt;p&gt;
  Given all of the years of delays that Intel had getting 10 nanometer processes out, culminating with Intel 7 (which is a refined 10 nanometer process), and getting Intel 4 and Intel 3 out, which we think are 7 nanometer and 5 nanometer processes, respectively, but people argue over this, you might have thought that the plan of former Intel chief executive officer, Pat Gelsinger,
  &lt;a href=&quot;https://www.nextplatform.com/2024/02/22/intel-i-was-lostry-but-now-i-am-foundry/&quot;&gt;
   to deliver five new process nodes in four years to get Intel Foundry back on track
  &lt;/a&gt;
  was insane. And maybe it was. 5N4Y, as the effort was called, is a lot.
 &lt;/p&gt;
 &lt;p&gt;
  But what Gelsinger knows, and what his replacement Lip-Bu Tan knows, is that manufacturing knowledge in the chip business is cumulative. You can’t skip steps. You have to stand on them – or, if you trip, as Intel has done, try to land on the one after the one you skipped by accident.
 &lt;/p&gt;
 &lt;p&gt;
  Instead, Intel tripped on the steps with its Intel 4 process, which was never used as the compute cores in a Xeon processor or in a Max Series datacenter GPU. (Intel 4 was used for the I/O dies on the
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/24/intel-rounds-out-granite-rapids-xeon-6-with-a-slew-of-chips/&quot;&gt;
   “Granite Rapids” P-core Xeon 6
  &lt;/a&gt;
  or
  &lt;a href=&quot;https://www.nextplatform.com/2024/06/03/intel-brings-a-big-fork-to-a-server-cpu-knife-fight/&quot;&gt;
   “Sierra Forrest” E-core Xeon 6
  &lt;/a&gt;
  , which had their compute cores etched with Intel 3.
 &lt;/p&gt;
 &lt;p&gt;
  And a few years later, Intel tripped down and skipped a step again – that’s hard on the back – with Intel 20A, the company’s first crack at production EUV lithography married to RibbonFET transistors and PowerVia backside power. The 20A process was snuffed in September last year so the company could focus on trying to ramp and commercialize the 18A process.
 &lt;/p&gt;
 &lt;p&gt;
  Everyone tripped with 10 nanometers, so Intel can get a pass with that one. But if Intel wants for customers to adopt 18A, which the company says will be doing high volumes well into the next decade, and embrace the even better 14A, Intel has to go through all of the steps and not falter.
 &lt;/p&gt;
 &lt;p&gt;
  The trouble is, Intel can’t wait forever for Nvidia to decide that Intel 18A or Intel 14A is good enough to etch its Arm server processors or GPU dies indigenously here in the United States – something that we are certain the Trump administration would love to see – particularly now that the US government has a 10 percent stake in Intel. There has not, to our knowledge, been any political pressure to use the Intel foundry for nationalistic reasons, but there is no reason to believe that this will not happen starting next year as Intel gets better yields for 18A and 14A. The situation is particularly dire for 14A, which
  &lt;a href=&quot;https://www.nextplatform.com/2025/07/25/intel-puts-the-process-horse-back-in-front-of-the-foundry-cart/&quot;&gt;
   Intel said back in July
  &lt;/a&gt;
  may not make it if outside customers do not adopt it. Intel Products cannot shoulder the burden alone.
 &lt;/p&gt;
 &lt;p&gt;
  Intel and Amazon Web Services were talking about collaboration on 18A and 14A exploration in the Intel Ohio fabs last September, when 20A was spiked, but that foundry buildout is now stalled.
  &lt;a href=&quot;https://www.nextplatform.com/2024/09/17/the-irony-of-aws-being-intels-latest-savior/&quot;&gt;
   AWS is getting a custom Xeon 6 processor
  &lt;/a&gt;
  – no one knows if it is a
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/24/intel-rounds-out-granite-rapids-xeon-6-with-a-slew-of-chips/&quot;&gt;
   “Granite Rapids” P-core Xeon 6
  &lt;/a&gt;
  or a
  &lt;a href=&quot;https://www.nextplatform.com/2024/06/03/intel-brings-a-big-fork-to-a-server-cpu-knife-fight/&quot;&gt;
   “Sierra Forrest” E-core Xeon 6
  &lt;/a&gt;
  – and might be working with Intel to create a fabric interconnect chip for its Tranium and Inferentia AI XPU compute engines based on 18A. (The statement was vague, and intentionally so.)
 &lt;/p&gt;
 &lt;p&gt;
  It doesn’t help that Intel’s own “Clearwater Forest” Xeon 7 E-core processor, which uses the 18A technology, which has Intel “Darkmont” Atom-style cores, and which was due this year,
  &lt;a href=&quot;https://www.nextplatform.com/2025/01/31/intel-pushes-out-clearwater-forest-xeon-7-sidelines-falcon-shores-accelerator/&quot;&gt;
   was pushed out back in January 2025
  &lt;/a&gt;
  until sometime in 2026. Back then, Intel said the yields for 18A were progressing just fine but that Clearwater Forest had “some complicated packaging expectations” that would move it into 2026. Intel has said very little about the “Diamond Rapids” P-core Xeon 7 chips, which will use the 18A process, will be based on the “Panther Cove” Xeon cores, will reportedly have four blocks of core tiles with a total of 192 cores (that’s 48 on each tile). There is a rumor that Diamond Rapids does not have hyperthreading on its cores, which we find hard to believe. Diamond Rapids is slated for 2H 2026, and Clearwater Forest might come ahead of it or at the same time. Intel has not said.
 &lt;/p&gt;
 &lt;p&gt;
  The point is, a delay in the Xeon lineup that is not fully explained for an 18A process will taint the outside perception of the 18A process, and that doesn’t help Intel Foundry. But, then again, neither does saying that 18A chips in the pipeline have some packaging challenges.
 &lt;/p&gt;
 &lt;p&gt;
  What seems clear from all of this is that if Intel does not get some 14A customers, it is perfectly happy – well,
  &lt;em&gt;
   willing
  &lt;/em&gt;
  , if not happy – to do future Xeons further down the roadmap on 18A or go to TSMC for cores for its compute engines using its rival’s A16 or A14 processes. If that happens, Intel Foundry will stall at 18A just like it did at 14 nanometers, and this will not be good for either Intel Products or Intel Foundry. It may be the end of the line, as 14 nanometers and its 12 nanometer refinement, was for GlobalFoundries.
 &lt;/p&gt;
 &lt;p&gt;
  Intel is in a very pesky situation, and Tan, Intel’s current chief executive officer, and David Zinsner, its chief financial officer, were pretty blunt about it. When pressed about 18A yields, Zinsner did a lot of the talking, and said this:
 &lt;/p&gt;
 &lt;blockquote&gt;
  &lt;p&gt;
   &lt;em&gt;
    “The yields are adequate to address the supply, but they are not where we need them to be in order to drive the appropriate level of margins. And by the end of next year, we will probably be in that space. And certainly, the year after that, I think they will be in what would be kind of an industry acceptable level on the yields.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “I would tell you, on 14A, we are off to a great start. And if you look at 14A in terms of its maturity relative to 18A at that same point of maturity, we are better in terms of performance and yield. So we are off to an even better start on 14A. We have just got to continue that progress.”
   &lt;/em&gt;
  &lt;/p&gt;
 &lt;/blockquote&gt;
 &lt;p&gt;
  Well, if I was the Intel Products group, I would want 14A over 18A, just like I wanted 18A over 20A and helped kill it off. But that can’t happen this time because Intel needs to ramp a process to good yield so it can make Xeon CPUs for servers and Core CPUs for PCs profitably. No more skipping steps, either. And to that, Zinsner said that the 18A yields are where Intel expected them to be right now, and that they will improve over 2026 to get to the right cost structure.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s the funny bit. Intel said that its server CPU business is supply constrained right now, and in part due to the limited capacity it has for the Intel 3 and Intel 4 processes it uses for current Xeon CPUs and also due to the shortage of substrates, which is affecting every chip maker. When demand is greater than supply, you would expect for margins to rise – one need only look at Nvidia financials to see how wonderfully this can work. And you cannot just blame the older 10 nanometer and 7 nanometer processes, apparently, which were never low-cost nodes. It looks like 18A, right now, is worse on that front.
 &lt;/p&gt;
 &lt;p&gt;
  Here is how Zinsner put it, and we are quoting him at length because it warrants it:
 &lt;/p&gt;
 &lt;blockquote&gt;
  &lt;p&gt;
   &lt;em&gt;
    “I would say there are two dynamics, one of which is the high cost of older processes versus the better cost structure for the newer processes. And that’s obviously meaningful. I mean we are in negative gross margin territory for Foundry. That makes a meaningful improvement if you move it even into the positive territory.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “But the other aspect of our gross margin is a function of just the product quality. We are in reasonably decent shape on client in terms of product performance and competitiveness with a few exceptions. But we are not where we need to be on a cost basis. And so we have got to make improvements there. And we have that on the roadmap. The team recognizes it, but that is a multiyear process to get there.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “It is more pronounced on the datacenter side. Not only do we not have the right cost structure, but we also don’t have the right competitiveness to really get the right margins from our customers. And so we have got work to do there. And so that’s what Lip-Bu and the team has pulled in, are hyper-focused on, is getting great products at the right cost structure to drive better gross margins. That to me, I think, is the linchpin in all this.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “The improvements on the Foundry side are just going to come, I think. We’re going to mix higher and higher to Intel 4, 3, and then 18A, and ultimately 14A. The cost structures of all of those are actually pretty similar. And it will just be a function of the fact that the value that is provided by those leading edge nodes is going to be significantly more and that is going to just materially drive the gross margins up.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “The other thing that I would say is we are seeing a lot of start-up costs by virtue of the fact that we have jammed a whole bunch of new processes in rapid fashion. As we get into 14A, our cadence will be more normalized. And so you won’t see so much start-up costs stacked on top of each other, which is affecting gross margins. And that’s billions of dollars. So I think as you get beyond a few years, that rolls off and will also help.”
   &lt;/em&gt;
  &lt;/p&gt;
 &lt;/blockquote&gt;
 &lt;p&gt;
  So, Intel will stop tripping down the stairs accidentally and stop jumping stairs to try to catch up to TSMC.
 &lt;/p&gt;
 &lt;p&gt;
  Which brings us all the way down to the 14A step. Who is going to use 14A?
 &lt;/p&gt;
 &lt;p&gt;
  Tan didn’t tell any secrets, but as we have said before, it may take some strong-arming from the White House to get some enthusiastic contracts for Intel. And we will not be at all surprised if we do, and perhaps in the longest of runs, this is the best thing to drive down the costs of chip manufacturing. (Assuming this is of value anymore. Such competition used to directly help millions of companies, but now it will help maybe dozens a whole lot more.) Then again, Nvidia has much higher profits than TSMC, and together, these two companies account for the lion’s share of all of the profits from the GenAI boom. Without question.
 &lt;/p&gt;
 &lt;p&gt;
  There is apparently one customer, aside from Intel Products we presume, that is very interested in 14A.
 &lt;/p&gt;
 &lt;p&gt;
  “Since the last quarter, I think our engagement with the customer for 14A increased, and we are very heavily engaging with the customer in terms of defining the technology, the process, the yield, and the IP requirement to serve them,” Tan explained. “And they clearly see the tremendous demand that they need to have Intel to be strong on the 14A. And so we are delighted and more confident. And in the meantime, we are also attracting some of the key talent for the process technology that can really drive success, and that’s why it gives me a lot more confidence to drive that.”
 &lt;/p&gt;
 &lt;p&gt;
  We will be surprised if there are not, at the very least, some gaming GPUs from Nvidia using the 14A process. And Nvidia co-founder and chief executive officer Jensen Huang may have already had dinner with Trump to layout how this might happen.
 &lt;/p&gt;
 &lt;p&gt;
  But here is the thing. Getting Nvidia to buy $5 billion of diluted Intel shares helped the Intel balance sheet, and it is great that Intel is working with Nvidia to add NVLink ports to future Xeon server CPUs so they can be a peer to Nvidia’s own “Grace” and “Vera” Arm server chips in rackscale AI systems. But Intel has walked away from the $200 billion datacenter GPU market that might be worth $500 billion by 2030, and is going to try to make do with a rackscale inference engine that doesn’t upset Nvidia too much. It is hard for us to see this as a win, even if it is what Intel has to do to survive. Clearly, Intel is banking on winning some foundry deals as the trade war with China heats up, but TSMC ramping up foundry operations in Arizona and showing off Nvidia “Blackwell” GPUs coming out id the fabs there, hurt Intel’s cause even if it may eventually avert World War III.
 &lt;/p&gt;
 &lt;p&gt;
  With all of that, let’s talk about Intel’s financial results for the third quarter, which have stopped tripping down the stairs.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s the big table of financial results since Q1 2023, when things started to get ugly for Intel:
 &lt;/p&gt;
 &lt;p&gt;
  With Altera sold off, some of the profits come to Intel without any of the costs, which helps. But there is also less profit coming in because it has to share with Silver Lake Partners, which is the downside. Mobileye is now part of Other revenue, and we don’t care to try to extract it and model it. It has no effect on datacenter product revenues. Network and Edge Group (NEX) has been carved up, with pieces put back into the Datacenter and AI (DCAI) and the Client Computing (CCG) groups.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the same data visually:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-groups.jpg&quot; rel=&quot;attachment wp-att-146507&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;474&quot; sizes=&quot;(max-width: 687px) 100vw, 687px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-groups.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-groups.jpg 687w, https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-groups-600x414.jpg 600w&quot; width=&quot;687&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The situation has not gotten a lot better, but it has stopped getting worse.
 &lt;/p&gt;
 &lt;p&gt;
  Intel’s revenues rose by 2.7 percent to $13.65 billion, and were up 6.2 percent sequentially, and operating profit across its groups was $683 million, or 5 percent of revenues, compared to a whopping $9.1 billion operating loss after writeoffs and layoffs in the year ago period. Net income after some infusions from Nvidia was $4.27 billion, which was a far cray better than the nearly $17 billion loss Intel posted in the year ago period.
 &lt;/p&gt;
 &lt;p&gt;
  Intel ended the quarter, after share sales of $5 billion to Nvidia and $11.1 billion to the US government, with $30.94 billion in cash. It paid off $4.3 billion in debts in the quarter as well.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-datacenter.jpg&quot; rel=&quot;attachment wp-att-146506&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;453&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 695px) 100vw, 695px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-datacenter.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-datacenter.jpg 695w, https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-datacenter-600x391.jpg 600w&quot; width=&quot;695&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Intel’s DCAI group, which sells server CPUs, motherboards, networking stuff, and some other do-dads here and there, had $4.12 billion in sales, down six-tenths of a point year on year but up 4.5 percent sequentially. Significantly, operating profit for DCAI was $964 million, up by a factor of 2.5X from the year ago period and up 1.5X sequentially. This is absolutely a reflection of better product mix and better yields on the Intel 3 and Intel 4 processes used in the current Xeon 6 lineup.
 &lt;/p&gt;
 &lt;p&gt;
  Intel Foundry is limping along with pretty huge losses, as you can see from the big table. Not as bad as Nutanix or Pure Storage had for many of their growth years, but half as bad for sure, and that is still not good.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Fixing Intel Foundry Is Like Stopping Tripping Down The Stairs </title>
<link>https://www.nextplatform.com/2025/10/24/fixing-intel-foundry-is-like-stopping-tripping-down-the-stairs/</link>
<pubDate>Fri, 24 Oct 2025 17:49:07 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/08/hc-intel-clearwater-forest-die-shot-1030x438.jpg&quot; title=&quot;hc-intel-clearwater-forest-die-shot&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Technical debt is a real thing, as any IT manager, programmer, system administrator or SRE, or end user will tell you. If you save money in the short run by not keeping up, you always end up paying with interest when you are backed into a place where you do have to catch up. Or, you go out of business.
 &lt;/p&gt;
 &lt;p&gt;
  So it is with Intel, which has come perilously close to irrelevancy in such a short time through a mix of foolishness, arrogance, and honest mistakes. For many decades, Intel was a very good chip manufacturer that also happened to be a chip designer, which just so happened to be the Intel Foundry’s only customer.
 &lt;/p&gt;
 &lt;p&gt;
  The foundry and products groups at Intel really did act like two interdependent but separate companies. The fact that the foundry business would just keep pace with upstart rival Taiwan Semiconductor Manufacturing was just a given in the Intel CPU products groups, even after AMD, IBM, and other chip makers abandoned their foundries because it was just too hard to keep up. And by the time GlobalFoundries (which ate the AMD and IBM foundries, among others) threw in the towel on 10 nanometer and 7 nanometer processes,
  &lt;a href=&quot;https://www.nextplatform.com/2021/06/10/why-ibm-is-suing-globalfoundries-over-chip-roadmap-failures/&quot;&gt;
   leaving IBM’s Power Systems and System z businesses in the lurch
  &lt;/a&gt;
  , Intel was also having its own issues trying to commercialize extreme ultraviolet lithography at 10 nanometers and below.
 &lt;/p&gt;
 &lt;p&gt;
  TSMC did not falter at 7 nanometers or 5 nanometers, but it also had a bit of trouble with the jump from 16 nanometers down to 7 nanometers. In fact, the only server CPU we can find with a 10 nanometer process outside of a desperate Intel with some of its Xeons is
  &lt;a href=&quot;https://www.nextplatform.com/2017/11/08/qualcomms-amberwing-arm-server-chip-finally-takes-flight/&quot;&gt;
   the ill-fated “Amberwing” Centriq 2400 chip from Qualcomm
  &lt;/a&gt;
  , a 48-core Arm chip launched in November 2017, which was etched by Samsung. AMD used the 14 nanometer processes from GlobalFoundries (which was significantly assisted by the IBM Microelectronics acquisition) for
  &lt;a href=&quot;https://www.nextplatform.com/2017/06/20/competition-returns-x86-servers-epyc-fashion/&quot;&gt;
   its “Naples” Epyc 7001
  &lt;/a&gt;
  server CPUs launched in June 2017, and saw the mess that was 10 nanometers and 7 nanometers at GlobalFoundries, Samsung, and TSMC and decided to jump right over it all to 7 nanometer processes from TSMC with
  &lt;a href=&quot;https://www.nextplatform.com/2019/08/07/amd-doubles-down-and-up-with-rome-epyc-server-chips/&quot;&gt;
   the “Rome” Epyc 7002 processors
  &lt;/a&gt;
  in August 2019; AMD processed to the 5 nanometer shrink at TSMC with
  &lt;a href=&quot;https://www.nextplatform.com/2021/03/15/the-third-time-charm-of-amds-milan-epyc-processors/&quot;&gt;
   the “Milan” Epyc 7003s
  &lt;/a&gt;
  in March 2021 and has been eating Intel’s market share lunch in server CPUs since then.
 &lt;/p&gt;
 &lt;p&gt;
  IBM has been stuck in a holding pattern with Power10 and Power11 on Samsung’s 7 nanometer processes, and used 7 nanometer processes for the z16 processor chip for mainframes and shrank down to Samsung’s 5 nanometer processes for the z17 chip follow-on that is ramping now.
 &lt;/p&gt;
 &lt;p&gt;
  Given all of the years of delays that Intel had getting 10 nanometer processes out, culminating with Intel 7 (which is a refined 10 nanometer process), and getting Intel 4 and Intel 3 out, which we think are 7 nanometer and 5 nanometer processes, respectively, but people argue over this, you might have thought that the plan of former Intel chief executive officer, Pat Gelsinger,
  &lt;a href=&quot;https://www.nextplatform.com/2024/02/22/intel-i-was-lostry-but-now-i-am-foundry/&quot;&gt;
   to deliver five new process nodes in four years to get Intel Foundry back on track
  &lt;/a&gt;
  was insane. And maybe it was. 5N4Y, as the effort was called, is a lot.
 &lt;/p&gt;
 &lt;p&gt;
  But what Gelsinger knows, and what his replacement Lip-Bu Tan knows, is that manufacturing knowledge in the chip business is cumulative. You can’t skip steps. You have to stand on them – or, if you trip, as Intel has done, try to land on the one after the one you skipped by accident.
 &lt;/p&gt;
 &lt;p&gt;
  Instead, Intel tripped on the steps with its Intel 4 process, which was never used as the compute cores in a Xeon processor or in a Max Series datacenter GPU. (Intel 4 was used for the I/O dies on the
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/24/intel-rounds-out-granite-rapids-xeon-6-with-a-slew-of-chips/&quot;&gt;
   “Granite Rapids” P-core Xeon 6
  &lt;/a&gt;
  or
  &lt;a href=&quot;https://www.nextplatform.com/2024/06/03/intel-brings-a-big-fork-to-a-server-cpu-knife-fight/&quot;&gt;
   “Sierra Forrest” E-core Xeon 6
  &lt;/a&gt;
  , which had their compute cores etched with Intel 3.
 &lt;/p&gt;
 &lt;p&gt;
  And a few years later, Intel tripped down and skipped a step again – that’s hard on the back – with Intel 20A, the company’s first crack at production EUV lithography married to RibbonFET transistors and PowerVia backside power. The 20A process was snuffed in September last year so the company could focus on trying to ramp and commercialize the 18A process.
 &lt;/p&gt;
 &lt;p&gt;
  Everyone tripped with 10 nanometers, so Intel can get a pass with that one. But if Intel wants for customers to adopt 18A, which the company says will be doing high volumes well into the next decade, and embrace the even better 14A, Intel has to go through all of the steps and not falter.
 &lt;/p&gt;
 &lt;p&gt;
  The trouble is, Intel can’t wait forever for Nvidia to decide that Intel 18A or Intel 14A is good enough to etch its Arm server processors or GPU dies indigenously here in the United States – something that we are certain the Trump administration would love to see – particularly now that the US government has a 10 percent stake in Intel. There has not, to our knowledge, been any political pressure to use the Intel foundry for nationalistic reasons, but there is no reason to believe that this will not happen starting next year as Intel gets better yields for 18A and 14A. The situation is particularly dire for 14A, which
  &lt;a href=&quot;https://www.nextplatform.com/2025/07/25/intel-puts-the-process-horse-back-in-front-of-the-foundry-cart/&quot;&gt;
   Intel said back in July
  &lt;/a&gt;
  may not make it if outside customers do not adopt it. Intel Products cannot shoulder the burden alone.
 &lt;/p&gt;
 &lt;p&gt;
  Intel and Amazon Web Services were talking about collaboration on 18A and 14A exploration in the Intel Ohio fabs last September, when 20A was spiked, but that foundry buildout is now stalled.
  &lt;a href=&quot;https://www.nextplatform.com/2024/09/17/the-irony-of-aws-being-intels-latest-savior/&quot;&gt;
   AWS is getting a custom Xeon 6 processor
  &lt;/a&gt;
  – no one knows if it is a
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/24/intel-rounds-out-granite-rapids-xeon-6-with-a-slew-of-chips/&quot;&gt;
   “Granite Rapids” P-core Xeon 6
  &lt;/a&gt;
  or a
  &lt;a href=&quot;https://www.nextplatform.com/2024/06/03/intel-brings-a-big-fork-to-a-server-cpu-knife-fight/&quot;&gt;
   “Sierra Forrest” E-core Xeon 6
  &lt;/a&gt;
  – and might be working with Intel to create a fabric interconnect chip for its Tranium and Inferentia AI XPU compute engines based on 18A. (The statement was vague, and intentionally so.)
 &lt;/p&gt;
 &lt;p&gt;
  It doesn’t help that Intel’s own “Clearwater Forest” Xeon 7 E-core processor, which uses the 18A technology, which has Intel “Darkmont” Atom-style cores, and which was due this year,
  &lt;a href=&quot;https://www.nextplatform.com/2025/01/31/intel-pushes-out-clearwater-forest-xeon-7-sidelines-falcon-shores-accelerator/&quot;&gt;
   was pushed out back in January 2025
  &lt;/a&gt;
  until sometime in 2026. Back then, Intel said the yields for 18A were progressing just fine but that Clearwater Forest had “some complicated packaging expectations” that would move it into 2026. Intel has said very little about the “Diamond Rapids” P-core Xeon 7 chips, which will use the 18A process, will be based on the “Panther Cove” Xeon cores, will reportedly have four blocks of core tiles with a total of 192 cores (that’s 48 on each tile). There is a rumor that Diamond Rapids does not have hyperthreading on its cores, which we find hard to believe. Diamond Rapids is slated for 2H 2026, and Clearwater Forest might come ahead of it or at the same time. Intel has not said.
 &lt;/p&gt;
 &lt;p&gt;
  The point is, a delay in the Xeon lineup that is not fully explained for an 18A process will taint the outside perception of the 18A process, and that doesn’t help Intel Foundry. But, then again, neither does saying that 18A chips in the pipeline have some packaging challenges.
 &lt;/p&gt;
 &lt;p&gt;
  What seems clear from all of this is that if Intel does not get some 14A customers, it is perfectly happy – well,
  &lt;em&gt;
   willing
  &lt;/em&gt;
  , if not happy – to do future Xeons further down the roadmap on 18A or go to TSMC for cores for its compute engines using its rival’s A16 or A14 processes. If that happens, Intel Foundry will stall at 18A just like it did at 14 nanometers, and this will not be good for either Intel Products or Intel Foundry. It may be the end of the line, as 14 nanometers and its 12 nanometer refinement, was for GlobalFoundries.
 &lt;/p&gt;
 &lt;p&gt;
  Intel is in a very pesky situation, and Tan, Intel’s current chief executive officer, and David Zinsner, its chief financial officer, were pretty blunt about it. When pressed about 18A yields, Zinsner did a lot of the talking, and said this:
 &lt;/p&gt;
 &lt;blockquote&gt;
  &lt;p&gt;
   &lt;em&gt;
    “The yields are adequate to address the supply, but they are not where we need them to be in order to drive the appropriate level of margins. And by the end of next year, we will probably be in that space. And certainly, the year after that, I think they will be in what would be kind of an industry acceptable level on the yields.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “I would tell you, on 14A, we are off to a great start. And if you look at 14A in terms of its maturity relative to 18A at that same point of maturity, we are better in terms of performance and yield. So we are off to an even better start on 14A. We have just got to continue that progress.”
   &lt;/em&gt;
  &lt;/p&gt;
 &lt;/blockquote&gt;
 &lt;p&gt;
  Well, if I was the Intel Products group, I would want 14A over 18A, just like I wanted 18A over 20A and helped kill it off. But that can’t happen this time because Intel needs to ramp a process to good yield so it can make Xeon CPUs for servers and Core CPUs for PCs profitably. No more skipping steps, either. And to that, Zinsner said that the 18A yields are where Intel expected them to be right now, and that they will improve over 2026 to get to the right cost structure.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s the funny bit. Intel said that its server CPU business is supply constrained right now, and in part due to the limited capacity it has for the Intel 3 and Intel 4 processes it uses for current Xeon CPUs and also due to the shortage of substrates, which is affecting every chip maker. When demand is greater than supply, you would expect for margins to rise – one need only look at Nvidia financials to see how wonderfully this can work. And you cannot just blame the older 10 nanometer and 7 nanometer processes, apparently, which were never low-cost nodes. It looks like 18A, right now, is worse on that front.
 &lt;/p&gt;
 &lt;p&gt;
  Here is how Zinsner put it, and we are quoting him at length because it warrants it:
 &lt;/p&gt;
 &lt;blockquote&gt;
  &lt;p&gt;
   &lt;em&gt;
    “I would say there are two dynamics, one of which is the high cost of older processes versus the better cost structure for the newer processes. And that’s obviously meaningful. I mean we are in negative gross margin territory for Foundry. That makes a meaningful improvement if you move it even into the positive territory.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “But the other aspect of our gross margin is a function of just the product quality. We are in reasonably decent shape on client in terms of product performance and competitiveness with a few exceptions. But we are not where we need to be on a cost basis. And so we have got to make improvements there. And we have that on the roadmap. The team recognizes it, but that is a multiyear process to get there.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “It is more pronounced on the datacenter side. Not only do we not have the right cost structure, but we also don’t have the right competitiveness to really get the right margins from our customers. And so we have got work to do there. And so that’s what Lip-Bu and the team has pulled in, are hyper-focused on, is getting great products at the right cost structure to drive better gross margins. That to me, I think, is the linchpin in all this.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “The improvements on the Foundry side are just going to come, I think. We’re going to mix higher and higher to Intel 4, 3, and then 18A, and ultimately 14A. The cost structures of all of those are actually pretty similar. And it will just be a function of the fact that the value that is provided by those leading edge nodes is going to be significantly more and that is going to just materially drive the gross margins up.”
   &lt;/em&gt;
  &lt;/p&gt;
  &lt;p&gt;
   &lt;em&gt;
    “The other thing that I would say is we are seeing a lot of start-up costs by virtue of the fact that we have jammed a whole bunch of new processes in rapid fashion. As we get into 14A, our cadence will be more normalized. And so you won’t see so much start-up costs stacked on top of each other, which is affecting gross margins. And that’s billions of dollars. So I think as you get beyond a few years, that rolls off and will also help.”
   &lt;/em&gt;
  &lt;/p&gt;
 &lt;/blockquote&gt;
 &lt;p&gt;
  So, Intel will stop tripping down the stairs accidentally and stop jumping stairs to try to catch up to TSMC.
 &lt;/p&gt;
 &lt;p&gt;
  Which brings us all the way down to the 14A step. Who is going to use 14A?
 &lt;/p&gt;
 &lt;p&gt;
  Tan didn’t tell any secrets, but as we have said before, it may take some strong-arming from the White House to get some enthusiastic contracts for Intel. And we will not be at all surprised if we do, and perhaps in the longest of runs, this is the best thing to drive down the costs of chip manufacturing. (Assuming this is of value anymore. Such competition used to directly help millions of companies, but now it will help maybe dozens a whole lot more.) Then again, Nvidia has much higher profits than TSMC, and together, these two companies account for the lion’s share of all of the profits from the GenAI boom. Without question.
 &lt;/p&gt;
 &lt;p&gt;
  There is apparently one customer, aside from Intel Products we presume, that is very interested in 14A.
 &lt;/p&gt;
 &lt;p&gt;
  “Since the last quarter, I think our engagement with the customer for 14A increased, and we are very heavily engaging with the customer in terms of defining the technology, the process, the yield, and the IP requirement to serve them,” Tan explained. “And they clearly see the tremendous demand that they need to have Intel to be strong on the 14A. And so we are delighted and more confident. And in the meantime, we are also attracting some of the key talent for the process technology that can really drive success, and that’s why it gives me a lot more confidence to drive that.”
 &lt;/p&gt;
 &lt;p&gt;
  We will be surprised if there are not, at the very least, some gaming GPUs from Nvidia using the 14A process. And Nvidia co-founder and chief executive officer Jensen Huang may have already had dinner with Trump to layout how this might happen.
 &lt;/p&gt;
 &lt;p&gt;
  But here is the thing. Getting Nvidia to buy $5 billion of diluted Intel shares helped the Intel balance sheet, and it is great that Intel is working with Nvidia to add NVLink ports to future Xeon server CPUs so they can be a peer to Nvidia’s own “Grace” and “Vera” Arm server chips in rackscale AI systems. But Intel has walked away from the $200 billion datacenter GPU market that might be worth $500 billion by 2030, and is going to try to make do with a rackscale inference engine that doesn’t upset Nvidia too much. It is hard for us to see this as a win, even if it is what Intel has to do to survive. Clearly, Intel is banking on winning some foundry deals as the trade war with China heats up, but TSMC ramping up foundry operations in Arizona and showing off Nvidia “Blackwell” GPUs coming out id the fabs there, hurt Intel’s cause even if it may eventually avert World War III.
 &lt;/p&gt;
 &lt;p&gt;
  With all of that, let’s talk about Intel’s financial results for the third quarter, which have stopped tripping down the stairs.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s the big table of financial results since Q1 2023, when things started to get ugly for Intel:
 &lt;/p&gt;
 &lt;p&gt;
  With Altera sold off, some of the profits come to Intel without any of the costs, which helps. But there is also less profit coming in because it has to share with Silver Lake Partners, which is the downside. Mobileye is now part of Other revenue, and we don’t care to try to extract it and model it. It has no effect on datacenter product revenues. Network and Edge Group (NEX) has been carved up, with pieces put back into the Datacenter and AI (DCAI) and the Client Computing (CCG) groups.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the same data visually:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-groups.jpg&quot; rel=&quot;attachment wp-att-146507&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;474&quot; sizes=&quot;(max-width: 687px) 100vw, 687px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-groups.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-groups.jpg 687w, https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-groups-600x414.jpg 600w&quot; width=&quot;687&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The situation has not gotten a lot better, but it has stopped getting worse.
 &lt;/p&gt;
 &lt;p&gt;
  Intel’s revenues rose by 2.7 percent to $13.65 billion, and were up 6.2 percent sequentially, and operating profit across its groups was $683 million, or 5 percent of revenues, compared to a whopping $9.1 billion operating loss after writeoffs and layoffs in the year ago period. Net income after some infusions from Nvidia was $4.27 billion, which was a far cray better than the nearly $17 billion loss Intel posted in the year ago period.
 &lt;/p&gt;
 &lt;p&gt;
  Intel ended the quarter, after share sales of $5 billion to Nvidia and $11.1 billion to the US government, with $30.94 billion in cash. It paid off $4.3 billion in debts in the quarter as well.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-datacenter.jpg&quot; rel=&quot;attachment wp-att-146506&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;453&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 695px) 100vw, 695px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-datacenter.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-datacenter.jpg 695w, https://www.nextplatform.com/wp-content/uploads/2025/10/intel-q3-2025-datacenter-600x391.jpg 600w&quot; width=&quot;695&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Intel’s DCAI group, which sells server CPUs, motherboards, networking stuff, and some other do-dads here and there, had $4.12 billion in sales, down six-tenths of a point year on year but up 4.5 percent sequentially. Significantly, operating profit for DCAI was $964 million, up by a factor of 2.5X from the year ago period and up 1.5X sequentially. This is absolutely a reflection of better product mix and better yields on the Intel 3 and Intel 4 processes used in the current Xeon 6 lineup.
 &lt;/p&gt;
 &lt;p&gt;
  Intel Foundry is limping along with pretty huge losses, as you can see from the big table. Not as bad as Nutanix or Pure Storage had for many of their growth years, but half as bad for sure, and that is still not good.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Engineering CPO Is Easy – Scaling It For AI Will Take An Ecosystem </title>
<link>https://www.nextplatform.com/2025/10/24/engineering-cpo-is-easy-scaling-it-for-ai-will-take-an-ecosystem/#respond</link>
<pubDate>Fri, 24 Oct 2025 13:43:34 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/Ayar_Labs_Solution-1030x438.jpg&quot; title=&quot;Ayar_Labs_Solution&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It has taken a lot of dreaming and even more engineering, but the many promises of silicon photonics are starting to make their way into real products in the datacenter. It doesn’t hurt that high-bandwidth and low-latency interconnects are a necessity for GenAI platforms and that the world is willing to spend enormous amounts of money to fulfill the promises of superintelligence and its agentic AI killer app.
 &lt;/p&gt;
 &lt;p&gt;
  Scale out networks have been using pluggable optical transceivers for decades because of the need for network reach that is larger than can be built using electrical wires. And now, with co-packaged optics, the optical interconnect is being brought down into the package for datacenter switches and, in the future, for compute engines and even memory subsystems that feed into them.
 &lt;/p&gt;
 &lt;p&gt;
  It is not a question of when, much less if, but more a question of when will co-packaged optics, or CPO, be manufacturable at the high volumes and relatively low costs that are demanded in AI clusters. CPO will get traction first, it seems, in scale-out networks, just like optical transceivers did, because the volumes are relatively low and can be used as means of learning all of the tricks of volume manufacturing that will be needed before CPO can be used to stitch compute engines and memories together in rackscale systems and across rows and rows of gear. (And deeper down into the compute engine socket, optical interposers may be used to stitch together components.)
 &lt;/p&gt;
 &lt;p&gt;
  Here is the thing: Optics alone does not increase the bandwidth of links, but it does allow for a much larger number of links to be brought to bear, bandwidth that will be absolutely necessary for interconnects between XPUs and eventually their memories. The challenge is an order of magnitude more complexity, and then very quickly another order of magnitude will have to come. This is what will be required to scale from tens of thousands of XPUs to hundreds of thousands to millions in a single AI system, which is a requirement for training models in the coming years.
 &lt;/p&gt;
 &lt;p&gt;
  How can the industry manufacture the necessary components, at scale and without breaking the bank, to scale up compute and extend memory for these systems before the end of the decade? This is the question. and we will get some answers from experts in a webinar on November 5 from 9 am to 10 am Pacific Standard Time. Our panelists include:
 &lt;/p&gt;
 &lt;ul&gt;
  &lt;li&gt;
   Erez Shaizaf, chief technology officer at Alchip
  &lt;/li&gt;
  &lt;li&gt;
   Adit Narasimha, vice president and general manager of emerging technology at Astera Labs
  &lt;/li&gt;
  &lt;li&gt;
   Vladimir Stojanovic, chief technology officer and co-founder at Ayar Labs
  &lt;/li&gt;
 &lt;/ul&gt;
 &lt;p&gt;
  Join moderator Timothy Prickett Morgan from
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  along with these industry experts to discuss how CPO will be used to weave future AI systems together. You can register for the webinar
  &lt;a href=&quot;https://bit.ly/475K7zM&quot;&gt;
   at this link
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Engineering CPO Is Easy – Scaling It For AI Will Take An Ecosystem </title>
<link>https://www.nextplatform.com/2025/10/24/engineering-cpo-is-easy-scaling-it-for-ai-will-take-an-ecosystem/</link>
<pubDate>Fri, 24 Oct 2025 13:43:31 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/Ayar_Labs_Solution-1030x438.jpg&quot; title=&quot;Ayar_Labs_Solution&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It has taken a lot of dreaming and even more engineering, but the many promises of silicon photonics are starting to make their way into real products in the datacenter. It doesn’t hurt that high-bandwidth and low-latency interconnects are a necessity for GenAI platforms and that the world is willing to spend enormous amounts of money to fulfill the promises of superintelligence and its agentic AI killer app.
 &lt;/p&gt;
 &lt;p&gt;
  Scale out networks have been using pluggable optical transceivers for decades because of the need for network reach that is larger than can be built using electrical wires. And now, with co-packaged optics, the optical interconnect is being brought down into the package for datacenter switches and, in the future, for compute engines and even memory subsystems that feed into them.
 &lt;/p&gt;
 &lt;p&gt;
  It is not a question of when, much less if, but more a question of when will co-packaged optics, or CPO, be manufacturable at the high volumes and relatively low costs that are demanded in AI clusters. CPO will get traction first, it seems, in scale-out networks, just like optical transceivers did, because the volumes are relatively low and can be used as means of learning all of the tricks of volume manufacturing that will be needed before CPO can be used to stitch compute engines and memories together in rackscale systems and across rows and rows of gear. (And deeper down into the compute engine socket, optical interposers may be used to stitch together components.)
 &lt;/p&gt;
 &lt;p&gt;
  Here is the thing: Optics alone does not increase the bandwidth of links, but it does allow for a much larger number of links to be brought to bear, bandwidth that will be absolutely necessary for interconnects between XPUs and eventually their memories. The challenge is an order of magnitude more complexity, and then very quickly another order of magnitude will have to come. This is what will be required to scale from tens of thousands of XPUs to hundreds of thousands to millions in a single AI system, which is a requirement for training models in the coming years.
 &lt;/p&gt;
 &lt;p&gt;
  How can the industry manufacture the necessary components, at scale and without breaking the bank, to scale up compute and extend memory for these systems before the end of the decade? This is the question. and we will get some answers from experts in a webinar on November 5 from 9 am to 10 am Pacific Standard Time. Our panelists include:
 &lt;/p&gt;
 &lt;ul&gt;
  &lt;li&gt;
   Erez Shaizaf, chief technology officer at Alchip
  &lt;/li&gt;
  &lt;li&gt;
   Adit Narasimha, vice president and general manager of emerging technology at Astera Labs
  &lt;/li&gt;
  &lt;li&gt;
   Vladimir Stojanovic, chief technology officer and co-founder at Ayar Labs
  &lt;/li&gt;
 &lt;/ul&gt;
 &lt;p&gt;
  Join moderator Timothy Prickett Morgan from
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  along with these industry experts to discuss how CPO will be used to weave future AI systems together. You can register for the webinar
  &lt;a href=&quot;https://bit.ly/475K7zM&quot;&gt;
   at this link
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> IBM Is Playing A Very Long AI Game With Its Customers </title>
<link>https://www.nextplatform.com/2025/10/23/ibm-is-playing-a-very-long-ai-game-with-its-customers/#respond</link>
<pubDate>Thu, 23 Oct 2025 19:15:51 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/07/ibm-power11-e1180-logo-1011x438.jpg&quot; title=&quot;ibm-power11-e1180-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Over the next few years, Big Blue may not build the biggest AI business in the world by any stretch of the imagination, but we do have confidence that it will build a collection of AI products and services that are among the most profitable. And the company has not done a very good job articulating this to customers, partners, and Wall Street.
 &lt;/p&gt;
 &lt;p&gt;
  International Business Machines is an infrastructure company that dabbles in higher level and lower level stuff in the datacenter from time to time, and it is finally growing faster than global gross domestic product instead of flat-lining. This is progress, and progress that IBM has paid for by acquiring Red Hat and HashiCorp and adding features to its Power and z systems to support AI, and despite its abandonment of an HPC business that drove a reasonable amount of revenue through the national labs and academic HPC centers but never made the company any profits.
 &lt;/p&gt;
 &lt;p&gt;
  IBM may have sold off its Microelectronics division a long time ago, but it still designs its own Power and z system processors and it still does fundamental research in circuits and packaging because it has a stake in the outcome and it can also create intellectual property that others will license at a profit. Similarly, thousands of researchers and consultants have deep expertise in hundreds of industries, with knowledge of optimal processes and algorithms that can be used to increase efficiencies for businesses and governments. Big Blue has the expertise to help customers learn how to deploy AI, and its own Client Zero creation of AI applications to run its own business units – notably in sales, supply chain, and human resources – were expected to drive $2 billion in annualized “productivity savings” and will hit an annualized rate of $4.5 billion as 2025 comes to a close.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/2025/07/24/for-now-ai-helps-ibms-bottom-line-more-than-its-top-line/&quot;&gt;
   As we pointed out a few months ago
  &lt;/a&gt;
  , IBM is saving more money on its bottom line than it is making selling AI hardware, software, and services. But eventually this will change, and that is because IBM has well over 100,000 enterprise customers who will look to it for systems that can do AI in a measured, secure, and familiar way. They cannot afford to buy scads of GPUs, much less learn a new platform. They want AI integrated into what they have, and the modifications that IBM has made with its Power and z processors, including
  &lt;a href=&quot;https://www.nextplatform.com/2025/04/24/ibm-will-catch-a-piece-of-the-genai-wave-with-next-gen-systems/&quot;&gt;
   adding matrix math units to them
  &lt;/a&gt;
  and augmenting them with
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/10/ibm-ships-homegrown-spyre-accelerators-embraces-anthropic-for-ai-push/&quot;&gt;
   its homegrown “Spyre” AI inference accelerator
  &lt;/a&gt;
  , as well as the RHEL.AI and OpenShift.AI augmentations for its Linux and Kubernetes platforms, are what customers need.
 &lt;/p&gt;
 &lt;p&gt;
  Moreover, a new generation of code assistant,
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/10/ibm-ships-homegrown-spyre-accelerators-embraces-anthropic-for-ai-push/&quot;&gt;
   called Project Bob
  &lt;/a&gt;
  , is based on Anthropic’s Sonnet 4.5 models and has scrapped two separate code assistants based on IBM’s own Granite models that the System z and Power Systems divisions were working on separately. This assistant will eventually run in Linux partitions on IBM’s systems and use Spyre accelerators to do its inference, and it is created because IBM knows that around 70 percent of its Power Systems customers and nearly all of its System z customers create their own back office applications rather than buy them from Oracle (which supports IBM’s platforms) or Microsoft (which does not).
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-income-cash.jpg&quot; rel=&quot;attachment wp-att-146501&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;475&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-income-cash.jpg&quot; width=&quot;588&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This was no different in the Dot Com boom, when Sun Microsystems, EMC, and Oracle because the poster children for Web infrastructure. IBM created Web middleware from open source tools, added integration with existing platforms and enterprise support, and wrapped RISC systems running Unix and then X86 systems running Windows or Linux around its systems to in turn run this Web infrastructure. And it made plenty of money. It also created integrated versions of the software stack that could stay completely on Power or z iron, and many customers opted for this, driving up sales of IBM’s indigenous systems.
 &lt;/p&gt;
 &lt;p&gt;
  Wall Street is not nearly as patient as IBM’s customers are conservative, and so there was an impendence mismatch between investor expectations and corporate expectations in the wake of IBM reporting its financial results for the third quarter this week. But here’s the thing. Just because Wall Street doesn’t understand what IBM is doing does not mean IBM, and its customers, do not understand what it is doing. IBM knows it cannot be the next big thing. But it knows it can be Big Blue for those 100,000 customers, and that is precisely what it is going to do.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-groups.jpg&quot; rel=&quot;attachment wp-att-146498&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;573&quot; sizes=&quot;(max-width: 647px) 100vw, 647px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-groups.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-groups.jpg 647w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-groups-600x531.jpg 600w&quot; width=&quot;647&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in September, IBM brought in $16.33 billion in sales, up 9.1 percent year on year. Gross profits rose by 11.2 percent to $9.36 billion, and importantly net income came in to $1.74 billion, which was 10.7 percent of revenues and which was a whole lot better than the $317 million loss IBM had in the year ago period.
 &lt;/p&gt;
 &lt;p&gt;
  For those of you who like the details, we have done a breakdown of revenues and gross profits by groups and divisions for IBM since the beginning of 2024. We have kept some of the older categories for trend analysis. IBM consolidated a few divisions two quarters ago, and eventually we will have the time to backcast the new divisions into the old data. But for now, the chart above and the table below shows the old divisions in the Software and Consulting groups.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-big-table-2024-2025.jpg&quot; rel=&quot;attachment wp-att-146496&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;624&quot; sizes=&quot;(max-width: 761px) 100vw, 761px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-big-table-2024-2025.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-big-table-2024-2025.jpg 761w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-big-table-2024-2025-600x492.jpg 600w&quot; width=&quot;761&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  IBM exited the third quarter with $14.89 billion in cash and equivalents, which gives it some cash to do minor AI acquisitions as it goes into 2026 and beyond. We do not expect any silly acquisitions, like trying to buy one of the big model builders like OpenAI or Anthropic. (It would not be financially feasible even if it was desirable.)
 &lt;/p&gt;
 &lt;p&gt;
  Thanks to the Power11 and z17 server upgrade cycles, which got going this summer, IBM’s Infrastructure group had a sales bump of 17 percent to $3.56 billion. System z mainframe sales were up 60 percent, and that is expected given that the “Telum II” z17 ramp started earlier in 2025 than did the Power11 ramp. (We need a code-name for Power11. . . . ) Distributed infrastructure, which is IBM’s combination of products sold by the Power Systems and Storage divisions, had a 10 percent increase, which was a whole lot better than the declines seen as the Power10 chips came to the end of their cycle.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-hybrid-infrastructure-versus-support.jpg&quot; rel=&quot;attachment wp-att-146499&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;354&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-hybrid-infrastructure-versus-support.jpg&quot; width=&quot;565&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Add it up, and IBM’s datacenter hardware business – which it calls Hybrid Infrastructure because it includes sales of System z and Power Systems capacity sold on the IBM Cloud – was up 28 percent to $2.2 billion. Infrastructure support rose by 1 point, and in our model that came out to $1.33 billion. Pre-tax income was up 52.6 percent to $644 million, and represented 18.1 percent of sales.
 &lt;/p&gt;
 &lt;p&gt;
  IBM said on the call with Wall Street analysts that Software group revenues were a little bit soft in Q3 2025 as customers prioritized the acquisition of hardware over software, but added that in the long term, traditional back office software revenues – for operating systems, middleware, databases, and transaction processing software – would rise.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-real-systems.jpg&quot; rel=&quot;attachment wp-att-146502&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;444&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 596px) 100vw, 596px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-real-systems.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-real-systems.jpg 596w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-real-systems-80x60.jpg 80w&quot; width=&quot;596&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In Q3, Software group posted $7.21 billion in revenues, up 10.5 percent. In our model, Red Hat accounted for $2.09 billion of that, up 12 percent and also lighter than the 14 percent to 15 percent that IBM is shooting for each quarter for Red Hat.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-red-hat-versus-real-systems.jpg&quot; rel=&quot;attachment wp-att-146500&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;486&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 610px) 100vw, 610px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-red-hat-versus-real-systems.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-red-hat-versus-real-systems.jpg 610w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-red-hat-versus-real-systems-600x478.jpg 600w&quot; width=&quot;610&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  IBM’s transaction processing software had a 1 point decline to $1.89 billion, and that means all of its other software – databases, middleware, and development and automation tools – accounted for $3.23 billion, up 17.5 percent. Software group had a pre-tax income of $2.37 billion, up 20.6 percent and comprising 32.9 percent of total software sales.
 &lt;/p&gt;
 &lt;p&gt;
  IBM’s Consulting group had $5.32 billion in sales, up 3.3 percent, with pretax income of $686 million, up 22.7 percent and comprising 12.9 percent of revenues. This is a tough business and it is IBM’s least profitable one at that. But it is often what gets Big Blue in the door, and this is particularly true with AI.
 &lt;/p&gt;
 &lt;p&gt;
  Jim Kavanaugh, IBM’s chief financial officer, said on the call with Wall Street analysts that IBM added around $1.5 billion in incremental GenAI consulting bookings in Q3 and that the number of projects booked more than doubling from the year ago period. GenAI consulting, Kavanaugh added, represented 22 percent of IBM’s $31 billion consulting backlog.
 &lt;/p&gt;
 &lt;p&gt;
  IBM has only been talking about GenAI revenues since Q3 2023, so we don’t have a lot of trend data. But here is what we know and modeled:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-genai-bookings-table.jpg&quot; rel=&quot;attachment wp-att-146497&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;199&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 733px) 100vw, 733px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-genai-bookings-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-genai-bookings-table.jpg 733w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-genai-bookings-table-600x163.jpg 600w&quot; width=&quot;733&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As usual, items shown in bold red italics are our estimates.
 &lt;/p&gt;
 &lt;p&gt;
  To date from Q3 2023, IBM has more than $9.5 billion in bookings for GenAI software and services. GenAI hardware has been embedded in Power10, Power11, z16, and z17 systems, so it is hard to extract that value out even if matrix math units were helping drive sales, but sales of Spyre accelerators will be clearly aimed at AI since they are no good for any other kinds of work. If IBM prices Spyre correctly and makes its use transparent for Power and z shops, Spyre can double the Power Systems revenue stream in the coming years and maybe do the same for the System z line.
 &lt;/p&gt;
 &lt;p&gt;
  A lot depends on the performance of Spyre accelerators on real inference models and how inexpensive and easy IBM makes it for its customers to choose Spyre over Nvidia or AMD GPUs. One thing we know for sure: None of these 100,000 enterprise customers really wants to run their AI in a public cloud, and they really don’t want to fight OpenAI, Anthropic, xAI, Microsoft, Amazon Web Services, Google, and others to try to get dozens to hundreds to thousands of them to do their AI training and inference.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> IBM Is Playing A Very Long AI Game With Its Customers </title>
<link>https://www.nextplatform.com/2025/10/23/ibm-is-playing-a-very-long-ai-game-with-its-customers/</link>
<pubDate>Thu, 23 Oct 2025 19:15:49 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/07/ibm-power11-e1180-logo-1011x438.jpg&quot; title=&quot;ibm-power11-e1180-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Over the next few years, Big Blue may not build the biggest AI business in the world by any stretch of the imagination, but we do have confidence that it will build a collection of AI products and services that are among the most profitable. And the company has not done a very good job articulating this to customers, partners, and Wall Street.
 &lt;/p&gt;
 &lt;p&gt;
  International Business Machines is an infrastructure company that dabbles in higher level and lower level stuff in the datacenter from time to time, and it is finally growing faster than global gross domestic product instead of flat-lining. This is progress, and progress that IBM has paid for by acquiring Red Hat and HashiCorp and adding features to its Power and z systems to support AI, and despite its abandonment of an HPC business that drove a reasonable amount of revenue through the national labs and academic HPC centers but never made the company any profits.
 &lt;/p&gt;
 &lt;p&gt;
  IBM may have sold off its Microelectronics division a long time ago, but it still designs its own Power and z system processors and it still does fundamental research in circuits and packaging because it has a stake in the outcome and it can also create intellectual property that others will license at a profit. Similarly, thousands of researchers and consultants have deep expertise in hundreds of industries, with knowledge of optimal processes and algorithms that can be used to increase efficiencies for businesses and governments. Big Blue has the expertise to help customers learn how to deploy AI, and its own Client Zero creation of AI applications to run its own business units – notably in sales, supply chain, and human resources – were expected to drive $2 billion in annualized “productivity savings” and will hit an annualized rate of $4.5 billion as 2025 comes to a close.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://www.nextplatform.com/2025/07/24/for-now-ai-helps-ibms-bottom-line-more-than-its-top-line/&quot;&gt;
   As we pointed out a few months ago
  &lt;/a&gt;
  , IBM is saving more money on its bottom line than it is making selling AI hardware, software, and services. But eventually this will change, and that is because IBM has well over 100,000 enterprise customers who will look to it for systems that can do AI in a measured, secure, and familiar way. They cannot afford to buy scads of GPUs, much less learn a new platform. They want AI integrated into what they have, and the modifications that IBM has made with its Power and z processors, including
  &lt;a href=&quot;https://www.nextplatform.com/2025/04/24/ibm-will-catch-a-piece-of-the-genai-wave-with-next-gen-systems/&quot;&gt;
   adding matrix math units to them
  &lt;/a&gt;
  and augmenting them with
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/10/ibm-ships-homegrown-spyre-accelerators-embraces-anthropic-for-ai-push/&quot;&gt;
   its homegrown “Spyre” AI inference accelerator
  &lt;/a&gt;
  , as well as the RHEL.AI and OpenShift.AI augmentations for its Linux and Kubernetes platforms, are what customers need.
 &lt;/p&gt;
 &lt;p&gt;
  Moreover, a new generation of code assistant,
  &lt;a href=&quot;https://www.nextplatform.com/2025/10/10/ibm-ships-homegrown-spyre-accelerators-embraces-anthropic-for-ai-push/&quot;&gt;
   called Project Bob
  &lt;/a&gt;
  , is based on Anthropic’s Sonnet 4.5 models and has scrapped two separate code assistants based on IBM’s own Granite models that the System z and Power Systems divisions were working on separately. This assistant will eventually run in Linux partitions on IBM’s systems and use Spyre accelerators to do its inference, and it is created because IBM knows that around 70 percent of its Power Systems customers and nearly all of its System z customers create their own back office applications rather than buy them from Oracle (which supports IBM’s platforms) or Microsoft (which does not).
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-income-cash.jpg&quot; rel=&quot;attachment wp-att-146501&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;475&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-income-cash.jpg&quot; width=&quot;588&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This was no different in the Dot Com boom, when Sun Microsystems, EMC, and Oracle because the poster children for Web infrastructure. IBM created Web middleware from open source tools, added integration with existing platforms and enterprise support, and wrapped RISC systems running Unix and then X86 systems running Windows or Linux around its systems to in turn run this Web infrastructure. And it made plenty of money. It also created integrated versions of the software stack that could stay completely on Power or z iron, and many customers opted for this, driving up sales of IBM’s indigenous systems.
 &lt;/p&gt;
 &lt;p&gt;
  Wall Street is not nearly as patient as IBM’s customers are conservative, and so there was an impendence mismatch between investor expectations and corporate expectations in the wake of IBM reporting its financial results for the third quarter this week. But here’s the thing. Just because Wall Street doesn’t understand what IBM is doing does not mean IBM, and its customers, do not understand what it is doing. IBM knows it cannot be the next big thing. But it knows it can be Big Blue for those 100,000 customers, and that is precisely what it is going to do.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-groups.jpg&quot; rel=&quot;attachment wp-att-146498&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;573&quot; sizes=&quot;(max-width: 647px) 100vw, 647px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-groups.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-groups.jpg 647w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-groups-600x531.jpg 600w&quot; width=&quot;647&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In the quarter ended in September, IBM brought in $16.33 billion in sales, up 9.1 percent year on year. Gross profits rose by 11.2 percent to $9.36 billion, and importantly net income came in to $1.74 billion, which was 10.7 percent of revenues and which was a whole lot better than the $317 million loss IBM had in the year ago period.
 &lt;/p&gt;
 &lt;p&gt;
  For those of you who like the details, we have done a breakdown of revenues and gross profits by groups and divisions for IBM since the beginning of 2024. We have kept some of the older categories for trend analysis. IBM consolidated a few divisions two quarters ago, and eventually we will have the time to backcast the new divisions into the old data. But for now, the chart above and the table below shows the old divisions in the Software and Consulting groups.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-big-table-2024-2025.jpg&quot; rel=&quot;attachment wp-att-146496&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;624&quot; sizes=&quot;(max-width: 761px) 100vw, 761px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-big-table-2024-2025.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-big-table-2024-2025.jpg 761w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-big-table-2024-2025-600x492.jpg 600w&quot; width=&quot;761&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  IBM exited the third quarter with $14.89 billion in cash and equivalents, which gives it some cash to do minor AI acquisitions as it goes into 2026 and beyond. We do not expect any silly acquisitions, like trying to buy one of the big model builders like OpenAI or Anthropic. (It would not be financially feasible even if it was desirable.)
 &lt;/p&gt;
 &lt;p&gt;
  Thanks to the Power11 and z17 server upgrade cycles, which got going this summer, IBM’s Infrastructure group had a sales bump of 17 percent to $3.56 billion. System z mainframe sales were up 60 percent, and that is expected given that the “Telum II” z17 ramp started earlier in 2025 than did the Power11 ramp. (We need a code-name for Power11. . . . ) Distributed infrastructure, which is IBM’s combination of products sold by the Power Systems and Storage divisions, had a 10 percent increase, which was a whole lot better than the declines seen as the Power10 chips came to the end of their cycle.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-hybrid-infrastructure-versus-support.jpg&quot; rel=&quot;attachment wp-att-146499&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;354&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-hybrid-infrastructure-versus-support.jpg&quot; width=&quot;565&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Add it up, and IBM’s datacenter hardware business – which it calls Hybrid Infrastructure because it includes sales of System z and Power Systems capacity sold on the IBM Cloud – was up 28 percent to $2.2 billion. Infrastructure support rose by 1 point, and in our model that came out to $1.33 billion. Pre-tax income was up 52.6 percent to $644 million, and represented 18.1 percent of sales.
 &lt;/p&gt;
 &lt;p&gt;
  IBM said on the call with Wall Street analysts that Software group revenues were a little bit soft in Q3 2025 as customers prioritized the acquisition of hardware over software, but added that in the long term, traditional back office software revenues – for operating systems, middleware, databases, and transaction processing software – would rise.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-real-systems.jpg&quot; rel=&quot;attachment wp-att-146502&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;444&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 596px) 100vw, 596px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-real-systems.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-real-systems.jpg 596w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-rev-real-systems-80x60.jpg 80w&quot; width=&quot;596&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In Q3, Software group posted $7.21 billion in revenues, up 10.5 percent. In our model, Red Hat accounted for $2.09 billion of that, up 12 percent and also lighter than the 14 percent to 15 percent that IBM is shooting for each quarter for Red Hat.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-red-hat-versus-real-systems.jpg&quot; rel=&quot;attachment wp-att-146500&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;486&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 610px) 100vw, 610px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-red-hat-versus-real-systems.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-red-hat-versus-real-systems.jpg 610w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-red-hat-versus-real-systems-600x478.jpg 600w&quot; width=&quot;610&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  IBM’s transaction processing software had a 1 point decline to $1.89 billion, and that means all of its other software – databases, middleware, and development and automation tools – accounted for $3.23 billion, up 17.5 percent. Software group had a pre-tax income of $2.37 billion, up 20.6 percent and comprising 32.9 percent of total software sales.
 &lt;/p&gt;
 &lt;p&gt;
  IBM’s Consulting group had $5.32 billion in sales, up 3.3 percent, with pretax income of $686 million, up 22.7 percent and comprising 12.9 percent of revenues. This is a tough business and it is IBM’s least profitable one at that. But it is often what gets Big Blue in the door, and this is particularly true with AI.
 &lt;/p&gt;
 &lt;p&gt;
  Jim Kavanaugh, IBM’s chief financial officer, said on the call with Wall Street analysts that IBM added around $1.5 billion in incremental GenAI consulting bookings in Q3 and that the number of projects booked more than doubling from the year ago period. GenAI consulting, Kavanaugh added, represented 22 percent of IBM’s $31 billion consulting backlog.
 &lt;/p&gt;
 &lt;p&gt;
  IBM has only been talking about GenAI revenues since Q3 2023, so we don’t have a lot of trend data. But here is what we know and modeled:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-genai-bookings-table.jpg&quot; rel=&quot;attachment wp-att-146497&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;199&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 733px) 100vw, 733px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-genai-bookings-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-genai-bookings-table.jpg 733w, https://www.nextplatform.com/wp-content/uploads/2025/10/ibm-q3-2025-genai-bookings-table-600x163.jpg 600w&quot; width=&quot;733&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As usual, items shown in bold red italics are our estimates.
 &lt;/p&gt;
 &lt;p&gt;
  To date from Q3 2023, IBM has more than $9.5 billion in bookings for GenAI software and services. GenAI hardware has been embedded in Power10, Power11, z16, and z17 systems, so it is hard to extract that value out even if matrix math units were helping drive sales, but sales of Spyre accelerators will be clearly aimed at AI since they are no good for any other kinds of work. If IBM prices Spyre correctly and makes its use transparent for Power and z shops, Spyre can double the Power Systems revenue stream in the coming years and maybe do the same for the System z line.
 &lt;/p&gt;
 &lt;p&gt;
  A lot depends on the performance of Spyre accelerators on real inference models and how inexpensive and easy IBM makes it for its customers to choose Spyre over Nvidia or AMD GPUs. One thing we know for sure: None of these 100,000 enterprise customers really wants to run their AI in a public cloud, and they really don’t want to fight OpenAI, Anthropic, xAI, Microsoft, Amazon Web Services, Google, and others to try to get dozens to hundreds to thousands of them to do their AI training and inference.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> HPC Gets A Reconfigurable Dataflow Engine To Take On CPUs And GPUs </title>
<link>https://www.nextplatform.com/2024/10/29/hpc-gets-a-reconfigurable-dataflow-engine-to-take-on-cpus-and-gpus/</link>
<pubDate>Wed, 22 Oct 2025 20:29:26 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-logo-2.jpg&quot; title=&quot;nextsilicon-maverick-2-chip-shot-logo-2&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  No matter how elegant and clever the design is for a compute engine, the difficulty and cost of moving existing – and sometimes very old – code from the device it currently runs on to that new compute engine is a very big barrier to adoption.
 &lt;/p&gt;
 &lt;p&gt;
  This is a particularly high barrier with the inevitable offload approach that was new in the supercomputing racket when
  &lt;a href=&quot;https://www.theregister.com/2013/04/01/los_alamos_decommissions_roadrunner/&quot;&gt;
   the “Roadrunner” supercomputer
  &lt;/a&gt;
  at Los Alamos National Laboratory was developed in the early 2000s by IBM under the auspices of the US Department of Energy’s National Nuclear Security Administration. Roadrunner was an architectural trailblazer, and gave Nvidia and others the idea that they, too, could build offload engines with powerful calculating capabilities.
 &lt;/p&gt;
 &lt;p&gt;
  Roadrunner’s architectural design, lashing four of IBM’s “Cell” PowerXCell floating point vector engines with a pair of AMD Opteron X86 server CPUs, broke through the petaflops barrier in 2008 and set the pace for future hybrid compute architectures that are familiar today for both HPC and AI applications.
 &lt;/p&gt;
 &lt;p&gt;
  But no one ever said that programming for these hybrid devices was easy, and Roadrunner was particularly hard to program because it was the first of its kind. There was much complaining, and it took some of the smartest people on Earth to get work out of the machine. (Luckily, Los Alamos has a fairly large number of the smartest people on Earth.) Nvidia GPUs are only relatively easy to program today because of the enormous amount of work that Nvidia has done to create the CUDA programming environment and a vast trove of libraries, frameworks, and algorithms. Remember: 75 percent of Nvidia’s employees work on software, even if the lion’s share of its revenues – it is 90 percent, is it 95 percent, is it 99.5 percent? – come from hardware.
 &lt;/p&gt;
 &lt;p&gt;
  What if none of this was necessary? What if you just threw your C++ or Fortran code at a massive dataflow engine that could reconfigure itself for your code, and do so automagically as it runs, constantly tuning and retuning itself as different chunks of code are activated?
 &lt;/p&gt;
 &lt;p&gt;
  That is the dream of Elad Raz and the team he has built at NextSilicon, which is dropping out of stealth mode this week with its second generation Maverick-2 dataflow engine and taking on the HPC market with a new approach to both hardware and software.
 &lt;/p&gt;
 &lt;p&gt;
  It is hard to believe, isn’t it? How many innovative and novel architectures with “magic compilers” have we heard about over the years? More than we can count. But like the HPC market itself, we remain hopeful that with the right level of abstraction and the right amount of automation the job of executing code across a complex of different kinds of compute engines can get easier. Perhaps this is the time. It is either this or to leave to job of porting code and creating new codes to GenAI bots because there just are not enough people in the world who want to do this very difficult task even if you pay them hundreds of thousands of dollars a year.
 &lt;/p&gt;
 &lt;p&gt;
  NextSilicon, which was founded in 2017, way before the GenAI craze but when it became apparent that HPC and AI compute engine architectures were going to diverge – and not in favor of the HPC simulation and modeling crowd that is focused on 64-bit and 32-bit floating point calculations. And even without an initial plan to go after the AI market directly, as Cerebras Systems, Graphcore, Groq, Habana Labs, Nervana Systems, SambaNova Systems, and others have done, NextSilicon has been able to raise $202.6 million in funding in three rounds, with its Series C round coming in June 2021 at $120 million.
 &lt;/p&gt;
 &lt;p&gt;
  At that time, that gave NextSilicon a valuation of around $1.5 billion, and the funds and the prototyping work completed meant that the US Department of Energy could listen to what NextSilicon was up to. Sandia National Laboratory and NextSilicon collaborated on the design and testing of the Maverick-1 dataflow engine, and Sandia is now building a novel architecture supercomputer nicknamed “Spectra” as part of its Vanguard-II program. Presumably this will be built using the Maverick-2 dataflow engine that is being revealed today – Sandia has not said, and NextSilicon is not at liberty to say. We expect for Spectra to be installed in Q1 2025 and for a deeper dive on the Maverick-2 chip and systems using it at that time.
 &lt;/p&gt;
 &lt;p&gt;
  What Raz can say is that the Department of Energy and the Department of Defense are working with it, as are a number of other organizations in the United States and Europe.
 &lt;/p&gt;
 &lt;h3&gt;
  Why We Need Another HPC Accelerator
 &lt;/h3&gt;
 &lt;p&gt;
  The good news for the HPC centers of the world is that Maverick-2 is aimed at them, and NextSilicon is not going to try to chase the AI training and inference market just yet.
 &lt;/p&gt;
 &lt;p&gt;
  “There isn’t an accelerator just for high performance computing,” Raz tells
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  . “We have hundreds of companies doing acceleration for AI and machine learning, and most of the big vendors are pivoting away and going into AI machine learning. You can see what the big supercomputers mean for them – they just build a new GPU cluster that is twice as expensive, has twice as much power consumption, and you get the same FP64 flops. NextSilicon is an HPC-first company.”
 &lt;/p&gt;
 &lt;p&gt;
  Raz adds that in the long run, NextSilicon will create compute engines suitable for AI work, which makes sense because you cannot ignore a market that will drive more than half of system sales in the coming years if all of the prognostications are correct. (Here are
  &lt;a href=&quot;https://www.nextplatform.com/2024/10/28/system-spending-forecast-goes-through-the-datacenter-roof/&quot;&gt;
   Gartner’s most recent take
  &lt;/a&gt;
  and
  &lt;a href=&quot;https://www.nextplatform.com/2024/09/13/the-difficulty-and-necessity-of-parsing-out-ai-spending/&quot;&gt;
   IDC’s most recent forecasts
  &lt;/a&gt;
  , both with our riffs on their data.)
 &lt;/p&gt;
 &lt;p&gt;
  For the moment, NextSilicon is not saying much specific about the internals of compute engine it has created, and that is intentional. The company wants to get people to focus on the software problem it is solving first and then early next year get into the precise feeds and speeds of the guts of the Maverick-2 dataflow engine.
 &lt;/p&gt;
 &lt;p&gt;
  “The point of NextSilicon is to use software to accelerate your application,” explains Raz. “At the heart of this is a sophisticated software algorithm that understands what matters in the code and accelerates that. By contrast, most CPUs and GPUs are banks of processor cores in one form or another. They are getting instructions and they are trying to build a sophisticated pipeline and vector instruction set, with out of order execution, to reduce latency. We think this is the wrong approach. The better approach is to apply the Pareto Principle and look where 20 percent of your code is talking up 80 percent of the runtime. Why aren’t we applying the 80/20 rule for compute and memory? Why can’t we automatically identify the computational kernels that matter and try to focus solely on them?”
 &lt;/p&gt;
 &lt;p&gt;
  The quick answer is that this is hard, but that is the secret sauce of the Maverick platform. The dataflow engine etched in transistors is just how it is manifested physically once a graph of the flow of the data and operations embodied in a program is created by the Maverick compiler and scheduler.
 &lt;/p&gt;
 &lt;p&gt;
  And Raz then describes the secret sauce: “The application starts running on the host, and then we automatically identify those compute intensive portions of the code. We stay in the intermediate representation of the computational graph. We don’t change the graph to instructions. You need to think about this as a just in time compiler for hardware. We are keeping the graph for the program and we are placing it on the dataflow hardware. We are getting telemetry from the hardware, and we are doing that in a recursive way so we always keep optimizing compute and memory as the program is running.”
 &lt;/p&gt;
 &lt;p&gt;
  Conceptually, this is what the process looks like converting a C, C++ or Fortran program that ran on a CPU host or a GPU accelerator to a Maverick dataflow engine. The first step is to identify what Raz calls the “likely flows” in the graph of the running program:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-flow-step-1.jpg&quot; rel=&quot;attachment wp-att-144904&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;409&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-flow-step-1.jpg&quot; width=&quot;459&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The likely flows and unlikely flows in the code are projected down onto the grid of processing and memory elements in the Maverick dataflow engine, like this:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-flow-step-2.jpg&quot; rel=&quot;attachment wp-att-144905&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;366&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-flow-step-2.jpg&quot; width=&quot;459&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As the code is running on the dataflow engine, performance bottlenecks in the first pass of on-the-fly compilation are identified and with telemetry passed back up to the Maverick compiler, the flows are iteratively rebalanced in an asymmetric way to create more “hardware” to favor the likely flows and to give less “hardware” to the unlikely flows. Certain kinds of serial work are offloaded to local and more traditional cores on the Maverick dies, and very heavy serial work is passed back to the host where it can run faster. In this case, the host CPU is actually an offload series engine for the parallel Maverick dataflow engine.
 &lt;/p&gt;
 &lt;p&gt;
  At the point where the Maverick compiler has fully optimized the configurations of the hardware to run the likely and unlikely flows, the system creates what is called a mill core:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-flow-step-3.jpg&quot; rel=&quot;attachment wp-att-144906&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;389&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-flow-step-3.jpg&quot; width=&quot;459&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The mill cores aim to use as much of the resources in the dataflow engine as possible given the application snippets being offloaded to it, and they are, in essence, a software-defined core, created on the fly to run the most probable portions of the HPC code to accelerate them. The mill cores are optimized for throughput more than latency, and emphasize power efficiency over brute force. Importantly, a mill core can run hundreds or thousands of data streams in parallel, and they can be replicated across the dataflow engine to do work in parallel, just like real CPU cores and real GPU streaming processors are in physical devices. Thusly:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-flow-step-4.jpg&quot; rel=&quot;attachment wp-att-144907&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;319&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-flow-step-4.jpg&quot; width=&quot;459&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  When you replicate hundreds to thousands of data streams over hundreds of mill cores, you get massively parallel processing that can improve runtimes by orders of magnitude.
 &lt;/p&gt;
 &lt;p&gt;
  The other central idea is to get the work and data onto the Maverick dataflow engine and keep as much of it there as possible to minimize data movement, which is the killer in any hybrid architecture. If you do that, you get a distribution of likely flows, reasonably likely flows, and unlikely flows that looks like a futuristic skyscraper across the three tiers of compute:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-code-distribution.jpg&quot; rel=&quot;attachment wp-att-144903&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;527&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 854px) 100vw, 854px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-code-distribution.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-code-distribution.jpg 854w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-code-distribution-768x474.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-code-distribution-600x370.jpg 600w&quot; width=&quot;854&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Pretty, isn’t it?
 &lt;/p&gt;
 &lt;p&gt;
  So is this spider graph that shows how the salient characteristics of the Maverick dataflow engine stack up against CPUs, GPUs, and FPGAs:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-spider-graph-compare.jpg&quot; rel=&quot;attachment wp-att-144911&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;562&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 797px) 100vw, 797px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-spider-graph-compare.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-spider-graph-compare.jpg 797w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-spider-graph-compare-768x542.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-spider-graph-compare-600x423.jpg 600w&quot; width=&quot;797&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The idea is to get the flexibility, portability, and programmability of a CPU and better power efficiency and throughput than a GPU and FPGA, and sacrificing on single threaded performance that can be done with the Maverick embedded cores (E-cores) or the host CPU (likely an Arm or X86 device these days).
 &lt;/p&gt;
 &lt;p&gt;
  At this point, Raz is not revealing what the E-cores on the Maverick-2 chip are, but we are pretty much certain that they are not an Atom-inspired E-core of the same name from Intel, and we are pretty sure that they are either a licensed Arm core or a licensed or homegrown RISC-V core. There are not really other practical options in 2024. (No one is going to use a heavy Power core from IBM here.)
 &lt;/p&gt;
 &lt;p&gt;
  Now that we have turned our whole way of looking at the world on its head by talking about the software first, let’s finally take a look at the Maverick-1 and Maverick-2 hardware. Here is the Maverick-1 die:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot.jpg&quot; rel=&quot;attachment wp-att-144901&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;789&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 641px) 100vw, 641px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot.jpg 641w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-600x739.jpg 600w&quot; width=&quot;641&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  We don’t know a lot about the Maverick-1 chip, but we count what looks like two four banks of 24 E-cores and what looks to our eye as 256 compute elements – four elements in a block, four blocks in a row, and sixteen rows in a chip. We do not know the process used to make the Maverick-1 chip, but we presume it is either 16 nanometer or 7 nanometer processes from Taiwan Semiconductor Manufacturing Co.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the Maverick-1 PCI-Express card:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-1-pci-card-1.jpg&quot; rel=&quot;attachment wp-att-144916&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;406&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 775px) 100vw, 775px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-1-pci-card-1.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-1-pci-card-1.jpg 775w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-1-pci-card-1-768x402.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-1-pci-card-1-600x314.jpg 600w&quot; width=&quot;775&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The Maverick-2 chip is etched in 5 nanometer processes from TSMC, and is, like many startups, hyperscalers, and cloud builders that are designing chips using a third party to shepherd the compute engine through the TSMC foundry and packaging partners to a finished product. The chip has an area of 615 mm
  &lt;sup&gt;
   2
  &lt;/sup&gt;
  , which is not reticle busting but which is not small either:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg&quot; rel=&quot;attachment wp-att-144917&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;735&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 845px) 100vw, 845px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg 845w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2-768x668.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2-600x522.jpg 600w&quot; width=&quot;845&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The Maverick-2 has a total of 32 E-cores and what looks to our eye as 224 compute elements – it looks like four blocks, with each block having a grid of seven by eight elements.
 &lt;/p&gt;
 &lt;p&gt;
  The frequency of the device, according to the spec sheet below, is 1.5 GHz, and we presume that both the E-cores and the dataflow processing elements are running at the same speed. (This could turn out to not be true. You might want to run the E-cores a lot faster, say 3 GHz, to tackle the serial work and knock it down. If we were designing the E-cores on the Maverick-2, they would have a base frequency of 1.5 GHz and turbo up to 3 GHz when they are running.)
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-package.jpg&quot; rel=&quot;attachment wp-att-144909&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;533&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 550px) 100vw, 550px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-package.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-package.jpg 1049w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-package-768x745.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-package-600x582.jpg 600w&quot; width=&quot;550&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  As you can see from the specs below and the package shot above, the Maverick-2 chip has four banks of HBM memory, in this case HBM3E memory.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-specs-3.jpg&quot; rel=&quot;attachment wp-att-144913&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;399&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 605px) 100vw, 605px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-specs-3.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-specs-3.jpg 605w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-specs-3-600x396.jpg 600w&quot; width=&quot;605&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The dual-chip version of Maverick-2 supports the Open Compute Accelerator Module (OAM) form factor created by Microsoft and Meta Platforms to get a universal accelerator socket for datacenter accelerators. Intel and AMD use the OAM socket for their accelerators; Nvidia does not. The OAM version only exposes a total of 16 lanes of PCI-Express 5.0 I/O to the outside world instead of 32 lanes, too.
 &lt;/p&gt;
 &lt;p&gt;
  Each chip has a thermal design point of 300 watts, so the OAM unit specs out at 600 watts and is only available in a liquid cooled version. This is the new norm for supercomputing, so no big D.
 &lt;/p&gt;
 &lt;p&gt;
  Here is a conceptual diagram showing these two form factors:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-form-factors.jpg&quot; rel=&quot;attachment wp-att-144908&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;573&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 815px) 100vw, 815px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-form-factors.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-form-factors.jpg 815w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-form-factors-768x540.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-form-factors-600x422.jpg 600w&quot; width=&quot;815&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Don’t get hung up on the physical layout in the diagram above. We do not think it is the same as the actual chip shot shown above. What you will note is that each Maverick-2 chiplet has a 100 Gb/sec Ethernet port to link the accelerators together.
 &lt;/p&gt;
 &lt;p&gt;
  It is not clear how Maverick-2 devices can be linked in a shared memory cluster. This is something we expect to learn more about early next year.
 &lt;/p&gt;
 &lt;p&gt;
  The Maverick-2 chip supports C, C++, and Fortran applications with OpenMP and Kokkos frameworks. Raz says that NextSilicon will eventually support for Nvidia CUDA and AMD HIP/ROCm environments and popular AI frameworks.
 &lt;/p&gt;
 &lt;p&gt;
  The peak theoretical performance of the scalar, vector, and tensor units on the dataflow engine are not being revealed until Q1 2025, and Raz doesn’t put a lot of stock in these numbers anyway given that CPUs and GPUs do not come close to their peak performance in the field.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/NextSilicon-Maverick-2-X-board-scaled.jpg&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;1707&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 2560px) 100vw, 2560px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/NextSilicon-Maverick-2-X-board-scaled.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/NextSilicon-Maverick-2-X-board-scaled.jpg 2560w, https://www.nextplatform.com/wp-content/uploads/2024/10/NextSilicon-Maverick-2-X-board-768x512.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/NextSilicon-Maverick-2-X-board-1536x1024.jpg 1536w, https://www.nextplatform.com/wp-content/uploads/2024/10/NextSilicon-Maverick-2-X-board-2048x1365.jpg 2048w, https://www.nextplatform.com/wp-content/uploads/2024/10/NextSilicon-Maverick-2-X-board-600x400.jpg 600w&quot; width=&quot;2560&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “Peak performance doesn’t matter when you are seeing a lot of hardware vendors adding lots of teraflops, but you cannot reach them because it’s only in the GEMM, only in the matrix multiplication, only when you have local data,” says Raz. “The point with real applications is to utilize the hardware more efficiently rather than to add a lot of floating point that no one can reach.”
 &lt;/p&gt;
 &lt;p&gt;
  Could not have said that better ourselves. And given this, we would not be surprised to see a Maverick-2 chip come in with peaks in the tens of teraflops for vector and tensor FP64 and more fully utilize them on actual codes than GPUs can. This is, in fact, the idea that NextSilicon has based its company upon.
 &lt;/p&gt;
 &lt;p&gt;
  In its backgrounder document, NextSilicon says that the Maverick-2 will deliver 4X the performance per watt over the Nvidia “Blackwell” B200 GPU. We know that the B200 comes in at somewhere between 1,000 and 1,200 watts, and that a single Maverick-2 comes in at 300 watts and a pair in an OAM package comes in at 600 watts.
 &lt;/p&gt;
 &lt;p&gt;
  The document goes further and says that on a mix of HPC simulations, the Maverick-2 delivers over 20X the performance per watt of a 32-core Intel “Sapphire Rapids” Xeon SP-8352Y Platinum processor, which is rated at 1.8 teraflops at FP64 precision on its AVX-512 vector units and that burns 205 watts.
 &lt;/p&gt;
 &lt;p&gt;
  But as Raz points out, in this case in particular, you are testing out the strength of the Maverick compiler and its ability to create and replicate mill cores that are laid down on the dataflow engine as much as you are the raw FP64 and FP32 oomph of the chip. What we need to know as benchmarks run is what share of the device’s total capability is executing as it runs at a given rate, and then adjust that for cost.
 &lt;/p&gt;
 &lt;p&gt;
  We look forward to seeing results out of Sandia on the Spectra supercomputer. This could be a lot of fun, and hopefully it will upturn a lot of apple carts. HPC compute could use some love and not the floppy seconds of the AI crowd.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> NextSilicon Takes Aim At CPUs And GPUs With “Maverick-2” Dataflow Engine </title>
<link>https://www.nextplatform.com/2025/10/22/nextsilicon-takes-aim-at-cpus-and-gpus-with-maverick-2-dataflow-engine/#respond</link>
<pubDate>Wed, 22 Oct 2025 20:29:21 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-elan-raz-logo.jpg&quot; title=&quot;nextsilicon-maverick-2-elan-raz-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It has taken eight years and $303 million in seed and three rounds of venture funding, but NextSilicon is today delivering several incarnations of its 64-bit dataflow engine, called Maverick-2,
  &lt;a href=&quot;https://www.nextplatform.com/2024/10/29/hpc-gets-a-reconfigurable-dataflow-engine-to-take-on-cpus-and-gpus/&quot;&gt;
   which was revealed this time last year
  &lt;/a&gt;
  when the company dropped out of stealth mode.
 &lt;/p&gt;
 &lt;p&gt;
  The company is also unveiling a very interesting homegrown RISC-V processor called Arbel that, we presume, will be paired with Maverick-2 to create “superchip” host-accelerator combinations (as Nvidia calls them), representing a true novel architecture that will be appealing to the HPC centers of the world, who still care very much about 64-bit floating point computing. First up to deploy a production Maverick-2 system will very likely be Sandia National Laboratory, which assisted with development of the Maverick-1 proof of concept processor that launched in 2022.
 &lt;/p&gt;
 &lt;p&gt;
  As we pointed out last year in our deep dive on the company last October, NextSilicon is interesting for a number of different reasons.
 &lt;/p&gt;
 &lt;p&gt;
 &lt;/p&gt;
 &lt;p&gt;
  First, it is unabashedly an HPC-first company, which is something we have not seen in a compute engine maker for a long, long time. Second, NextSilicon has created a multi-tier computing architecture that has a reconfigurable dataflow engine at the central and most important tier where the bulk of computing for any HPC simulation or model is expected to run. (And by the way, nothing is preventing anyone from running AI applications on this processor.) And third, the Maverick architecture has automated the way code is ported, run, and continuously optimized as it is moved off of a CPU and run across NextSilicon’s own host processor, the special RISC-V cores embedded on the Maverick socket, and the banks of arithmetic units that do the bulk of computing and that represent most of the transistor count in a Maverick chip.
 &lt;/p&gt;
 &lt;p&gt;
  This last bit of the hardware architecture, which enables the software architecture to work, is the new revelation as the Maverick-2 chip comes to market today. So let’s start with the hardware and work our way up to the software.
 &lt;/p&gt;
 &lt;p&gt;
  Ilan Tayari, NextSilicon co-founder and vice president of architecture and formerly director of software at Mellanox (now the networking arm of Nvidia), walked through the differences between the Von Neumann architecture of a classical CPU and the new dataflow engine, which quite frankly for the visual thinkers in the crowd makes a lot more sense and is visually pleasing. With a dataflow engine such as NextSilicon has invented, the hardware literally maps itself to the software rather trying to be a short order cook with too many orders but with 27 arms and eyes in the back of the head, flailing around to make up for the difficulties of getting the right data and the right instructions to collide billions of times per second.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture.jpg&quot; rel=&quot;attachment wp-att-146486&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;591&quot; sizes=&quot;(max-width: 883px) 100vw, 883px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture.jpg 883w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture-768x514.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture-600x402.jpg 600w&quot; width=&quot;883&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In terms of area, Tayari says that around 2 percent of the typical CPU is dedicated to arithmetic logic units, or ALUs – ya know, the things that actually
  &lt;em&gt;
   do math
  &lt;/em&gt;
  to transform data. (When Tayari points this out, it does sound a little stupid.) Everything else on the chip is just to stage the instructions and the data so they collide way down in the ALU.
 &lt;/p&gt;
 &lt;p&gt;
  With the Von Neumann architecture, created back in the 1940s with the first vacuum tube systems, there is a single, unified memory space that holds instructions and data. An instruction fetch unit reads instructions from memory in an order determined by a program counter, and an execution unit does the computation. Memory is accessed using registers and a memory management unit translates memory addresses. Over the years, various levels of SRAM cache have been added to processors, all to increase the odds that the right instructions and data can be found when needed, and circuits for branch prediction, speculative execution, and out of order processing have been added even as instructions moved from CISC (fat) to RISC (skinny) to push more bits through the goose.
 &lt;/p&gt;
 &lt;p&gt;
  “These solutions can increase performance, but they also have high costs,” Tayari explained at the Maverick-2 launch. “They take a lot of silicon real estate, and the mechanism to revert mispredictions has a large negative impact on performance. Here is the shocking reality. Today’s high end processors have become complicated and chunky, both physically and practically. They dedicate 98 percent of their silicon to overhead, traffic management, data shuffling – not actual computation – and add more complexity. Clock speeds suffer, and slower clocks means slower execution. Some GPUs devote up to 30 percent of their silicon to compute, but because the blocks are mutually exclusive, only a few can run at once. All of these efforts to improve processor design are merely reactive solutions – workarounds. End users pay three penalties. The chip costs more. Power consumption increases. Cooling requirements grow. Everything becomes more expensive.”
 &lt;/p&gt;
 &lt;p&gt;
  In the Intelligent Computing Architecture, or ICA, approach developed by NextSilicon, the idea was to create logic blocks that were comprised of hundreds of interlinked ALUs, with instructions called in an application literally mapped to each ALU. (In this sense, an ALU is akin to an instruction.) NextSilicon is not releasing the specific number of ALUs per compute block, but we do know a few things. First look at this chip shot of the monolithic Maverick-2 die:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg&quot; rel=&quot;attachment wp-att-144917&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;735&quot; sizes=&quot;(max-width: 845px) 100vw, 845px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg 845w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2-768x668.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2-600x522.jpg 600w&quot; width=&quot;845&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  There are four compute regions, with the 32 RISC-V E-cores on the outside edges on the left and right of the chip. By our count, there is a grid of seven columns of compute blocks that each have eight compute blocks, for a total of 224 compute blocks on the die. At hundreds of ALUs per compute block, you can easily get many tens of thousands to close to a hundred thousand ALUs. This doesn’t seem unreasonable for a Maverick-2 chip that weighs in at 54 billion transistors at 5 nanometer processes from Taiwan Semiconductor Manufacturing Co. If you do a 14 by 14 grid as NextSilicon shows in its charts, then there are 196 ALUs per compute block; we do not know how many floating point units are in a compute block. It would make sense that every ALU had an FPU.
 &lt;/p&gt;
 &lt;p&gt;
  The “Ampere” A100 GPU from Nvidia was etched in 7 nanometer processes from TSMC, had 54.2 billion transistors, and 6,912 FP32 CUDA cores, while the “Hopper” H100 and H200 GPUs were made in 4 nanometer processes, have 80 billion transistors, and have 18,432 FP32 cores. The Blackwell B200 socket has two chiplets, each with 104 billion transistors but with only 16,896 CUDA cores each, made in a 4 nanometer process. We surmise that the ALUs are smaller than CUDA cores and that there are more of them on a Maverick-2 die than there are CUDA cores on an Nvidia GPU.
 &lt;/p&gt;
 &lt;p&gt;
  Ultimately, the ALU count is not as important as the thread count that a collection of mill cores can support. Tayari says that a typical CPU has two threads, a GPU has between 32 and 64 threads, but a mill core can support hundreds of threads at once. The size and shape of the mill cores change, of course, but with maybe tens of mill cores per compute block and 224 compute blocks per Maverick-2, you are easily up to thousands of threads, all running at 1.5 GHz – about the speed of a slow CPU or a normal GPU – and all linked to HBM3E memory for fast bandwidth.
 &lt;/p&gt;
 &lt;p&gt;
  What we really wanted to know is what the average utilization rates for the ALUs and FPUs were as applications start running and then as the optimizations are done and the resources on the Maverick chip get more fully utilized. We think that utilization rate doesn’t have crazy high to match a CPU doing branch prediction, speculative execution, out of order processing, and other kinds of unnatural acts to make a serial processor run faster. Still, after tuning and across a wide variety of HPC applications, we would not be surprised if the ALU and FPU utilization got up to 75 percent or 80 percent of the potential blocks across thousands of threads flitting in and out of the chip as mill cores come and go.
 &lt;/p&gt;
 &lt;p&gt;
  Anyway, you pour data into the flow at the top of the Maverick-2 chip, it flows through all of the transformations and calculations, and out pops the answer at the bottom.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dataflow-architecture-details.jpg&quot; rel=&quot;attachment wp-att-146487&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;395&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dataflow-architecture-details.jpg&quot; width=&quot;571&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This main logic unit, as you see in the chart above on the right, is attached to a memory bus, which has a reservation station to temporarily store data before an ALU calls for it. (It is this combination of reservation station, dispatcher, and dataflow compute block that NextSilicon has patented.) Like regular CPUs, the Maverick ICA uses memory management units and a table lookaside buffer, but these are used sparingly and only when an ALU calls for specific data. There is no speculation or prediction, just fetching.
 &lt;/p&gt;
 &lt;p&gt;
  “NextSilicon’s dataflow architecture allows us to achieve significantly lower overhead compared to traditional CPUs and GPUs,” brags Tayari. “We pivot the silicon allocation ratio. We dedicate the majority of the resources to actual computation rather than control overhead. Our approach uniquely eliminates instruction handling overhead. We minimize unnecessary data movement, and the result is compute units stay fully utilized. We’re not trying to hide latency, but to tolerate and minimize it by design.”
 &lt;/p&gt;
 &lt;p&gt;
  When an application is compiled for the dataflow engine, it is literally mapped onto it, into something called a mill core (it looks like a graph). It looks like the intermediate representation graph of a program before it is compiled, and it is laid down on the ALUs. Many mill cores can be laid down on the same compute block, Tetris style, and the mill cores can be loaded up and deleted as needed in a matter of nanoseconds as needed by the workload, according to Elad Raz, co-founder and chief executive officer at NextSilicon.
 &lt;/p&gt;
 &lt;p&gt;
  While this is all interesting, that is not the magic of NextSilicon’s ICA. (Wait for it – this is the magic compiler moment.) You can take existing C, C++, or Fortran code, grab its intermediate representation and plunk that down onto the ICA, and Maverick-2 will not only map it to its ALU blocks and thus compile it, but has algorithms that constantly analyze how that resulting dataflow is functioning and changes it on the fly, without human intervention, to improve it. The longer the code runs, the better it gets, and if you change the code at a higher level, you just run it and let it self-tune for a while. There is no porting to Nvidia CUDA-X or AMD ROCm or anything else – just an automated way to take CPU code and make it run on a massively parallel dataflow engine without you having to figure it out.
 &lt;/p&gt;
 &lt;p&gt;
  As we explained a year ago, the entire application is not ported to the Maverick chips. Just the parts of the code that are commonly used and represent 80 percent or so of the instruction runtime. Any code that does not benefit from running on the ALU blocks in dataflow mode can run on 32 RISC-V E-cores that are embedded on the Maverick-2 package, and for those parts of the code that would benefit from a beefier set of cores and memory, execution will take place on the CPU host processor, which for now is an unnamed X86 chip.
 &lt;/p&gt;
 &lt;p&gt;
  Here is what the entire workflow looks like:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow.jpg&quot; rel=&quot;attachment wp-att-146492&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;447&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1057px) 100vw, 1057px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow.jpg 1057w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow-768x325.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow-600x254.jpg 600w&quot; width=&quot;1057&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  One of the tricks in the Maverick architecture is to know what code to leave on the host CPU, what code to leave on the RISC-V E-cores (short for embedded cores), and what code must be moved to the ALU blocks. The other trick is that if there is a particular routine that could be parallelized and replicated to speed up the overall application, then the Maverick compiler can just plunk down more copies of the mill cores created for any specific routine. The compiler is, in effect, creating software cores in the shape and number that you need for this application at this time rather than taking a specific number of static compute units with specific integer and floating point precisions – and there are many combinations of static compute elements on CPUs and GPUs these days. This the dataflow ICA is almost as malleable as an FPGA in this regard, but it programs itself. (As many FPGA tools claim to do, by the way, converting C, C++, or Fortran code to HDL automagically.)
 &lt;/p&gt;
 &lt;p&gt;
  This spider graph helps illustrate some things about Maverick-2:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph.jpg&quot; rel=&quot;attachment wp-att-146494&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;391&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 550px) 100vw, 550px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph.jpg 772w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph-768x546.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph-600x427.jpg 600w&quot; width=&quot;550&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The story this chart tells is that the dataflow processor created by NextSilicon has the compatibility, portability, programmability, and flexibility of the CPU, better power efficiency and throughput than a GPU, but abiut the single threaded performance of an FPGA. Nothinbg has the single threaded performance of a CPU, which is why you put CPU cores inside of FPGAs and now DFPs and also allow host CPUs to pick up the slack on serial tasks.
 &lt;/p&gt;
 &lt;p&gt;
  Here is an updated salient characteristics table for the single-die and dual-die implementations of the Maverick-2 chip:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-salient-table.jpg&quot; rel=&quot;attachment wp-att-146491&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;596&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 702px) 100vw, 702px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-salient-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-salient-table.jpg 702w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-salient-table-600x509.jpg 600w&quot; width=&quot;702&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The chip max power is higher than what was expected when we did our story last year, with the TDP on the single chip Maverick-2 now at 400 watts (up from 300 watts) and on the dual-chip version for OAM sockets now at 750 watts (instead of 600 watts). Everything else is the same.
 &lt;/p&gt;
 &lt;p&gt;
  What is also different this time around is that we have peak flops performance at different floating point precisions, as you can see in the table above. We are not sure how to configure the Maverick-2 as a matrix/tensor unit, but that sounds neat. Clearly it can be done and it does boost performance.
 &lt;/p&gt;
 &lt;p&gt;
  These peak numbers for Maverick-2 are not very high compared to recent GPUs. With an Nvidia H100, for instance, the peak FP64 performance on dense data is 33.5 teraflops on the vector units and 67 teraflops on the tensor units. A single Maverick-2 is about a third of that. But Raz
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/13/the-hidden-cost-of-compromise-why-hpc-still-demands-precision/&quot;&gt;
   wrote about the gaming that happens with peak theoretical performance
  &lt;/a&gt;
  and the fact that sustained performance is what matters in the real world back in February of this year in a column in
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  . And we agree.
 &lt;/p&gt;
 &lt;p&gt;
  Still, peak flops also gives us a ceiling from which to measure against, so it is important to know both sustained and peak performance. That way we can judge the computational efficiency of a compute or networking engine. The limited evidence we have suggests that Maverick-2 is very efficient, and GPUs are not. Take a look:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance.jpg&quot; rel=&quot;attachment wp-att-146490&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;575&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 931px) 100vw, 931px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance.jpg 931w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance-768x474.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance-600x371.jpg 600w&quot; width=&quot;931&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://en.wikipedia.org/wiki/Giga-updates_per_second&quot;&gt;
   The GUPS benchmark
  &lt;/a&gt;
  , short for Giga Updates Per Second, is designed to stress test the bandwidth and latency of the memory subsystem of a compute engine, and frankly it is not one that we are very familiar with. On the GUPS test, the Maverick-2 was rated at 32.6 GUPS running at 460 watts. This was, according to NextSilicon, 22X faster than a CPU and nearly 6X faster than a GPU, but we have no idea what CPU or GPU was tested. (This should be in the presentation notes, like AMD, Intel, and Nvidia do.)
 &lt;/p&gt;
 &lt;p&gt;
  The STREAM benchmark is the better known memory bandwidth test that is commonly in the suite of benchmarks used by HPC shops. The chart seems to suggest that Maverick-2 is getting peak bandwidth as measured bandwidth, but we do not believe this is possible. It could be close, but it can’t be perfect. In any event, in different notes, NextSilicon says it delivered 5.2 TB/sec, which implies it was on the dual-chip OAM module, which is 83.9 percent of peak and 1.86X better performance per watt than a GPU. (Again, which one?)
 &lt;/p&gt;
 &lt;p&gt;
  Here is what the Maverick-2 OAM package looks like:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card.jpg&quot; rel=&quot;attachment wp-att-146488&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;575&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1001px) 100vw, 1001px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card.jpg 1001w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card-768x441.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card-600x345.jpg 600w&quot; width=&quot;1001&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  On
  &lt;a href=&quot;https://en.wikipedia.org/wiki/HPCG_benchmark&quot;&gt;
   the HPCG test
  &lt;/a&gt;
  , which shames all HPC systems large and small and is the best indicator of how a system will work on very tough HPC problems, a single Maverick-2 was rated at 600 gigaflops running at 600 watts (by which we presume it was actually a pair of Maverick-2s in the OAM socket). NextSilicon says that this was “matching the leading GPU performance” while consuming half the power.
 &lt;/p&gt;
 &lt;p&gt;
  And finally, on the PageRank graph analytics benchmark based on the web page ranking algorithm created by Google, Maverick-2 did 10X better than “leading GPUs.”
 &lt;/p&gt;
 &lt;p&gt;
  We want to get more clarification on this, and look forward to fuller performance and price/performance benchmark comparisons. We also want to understand how NextSilicon is going to be able to scale performance beyond a single socket with scale up and scale out networks.
 &lt;/p&gt;
 &lt;h3&gt;
  A Dataflow Engine Still Needs A CPU Host
 &lt;/h3&gt;
 &lt;p&gt;
  That leaves us with the Arbel RISC-V processor, also announced by NextSilicon today.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu.jpg&quot; rel=&quot;attachment wp-att-146483&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;450&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 450px) 100vw, 450px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu.jpg 931w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-300x300.jpg 300w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-150x150.jpg 150w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-768x767.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-600x599.jpg 600w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-100x100.jpg 100w&quot; width=&quot;450&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  NextSilicon is calling Arbel a “test chip,” but we think that the company is looking for a companion to its Maverick dataflow processors (DFPs?) that allows customers to have a fully integrated CPU that is not just a off-the-shelf X86 or Arm CPU from someone else – and does not come with a licensing fee owed to Arm Ltd.
 &lt;/p&gt;
 &lt;p&gt;
  Here are the feeds and speeds of the Arbel CPU:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds.jpg&quot; rel=&quot;attachment wp-att-146484&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;367&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 827px) 100vw, 827px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds.jpg 827w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds-768x341.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds-600x266.jpg 600w&quot; width=&quot;827&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The Arbel chip has a completely homegrown RISC-V core, just as the Maverick-2 does. (It is not clear how similar these cores are, but don’t assume they are they same.)
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram.jpg&quot; rel=&quot;attachment wp-att-146482&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;422&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 848px) 100vw, 848px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram.jpg 848w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram-768x382.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram-600x299.jpg 600w&quot; width=&quot;848&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The Arbel core has a 10-wide issue decoder and six ALUs on the integer side and four 128-bit FPUs on the vector side. The core can support 16 scalar instructions in parallel. The core has 64 KB of L1 instruction cache and 64 KB of L1 data cache close to the ALUs and a 1 MB L2 cache close to the FPUs. (Both caches are obviously cross-linked to all the compute elements.) There is a 2 MB cache per core, but again, we don’t know how many cores are on the Arbel chip.
 &lt;/p&gt;
 &lt;p&gt;
  What we do know is that NextSilicon says that the Arbel core can “stand toe-to-toe” with Intel’s “LionCove” Xeon core and AMD’s “Zen5” Epyc core.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> NextSilicon Takes Aim At CPUs And GPUs With “Maverick-2” Dataflow Engine </title>
<link>https://www.nextplatform.com/2025/10/22/nextsilicon-takes-aim-at-cpus-and-gpus-with-maverick-2-dataflow-engine/</link>
<pubDate>Wed, 22 Oct 2025 20:29:18 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-elan-raz-logo.jpg&quot; title=&quot;nextsilicon-maverick-2-elan-raz-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  It has taken eight years and $303 million in seed and three rounds of venture funding, but NextSilicon is today delivering several incarnations of its 64-bit dataflow engine, called Maverick-2,
  &lt;a href=&quot;https://www.nextplatform.com/2024/10/29/hpc-gets-a-reconfigurable-dataflow-engine-to-take-on-cpus-and-gpus/&quot;&gt;
   which was revealed this time last year
  &lt;/a&gt;
  when the company dropped out of stealth mode.
 &lt;/p&gt;
 &lt;p&gt;
  The company is also unveiling a very interesting homegrown RISC-V processor called Arbel that, we presume, will be paired with Maverick-2 to create “superchip” host-accelerator combinations (as Nvidia calls them), representing a true novel architecture that will be appealing to the HPC centers of the world, who still care very much about 64-bit floating point computing. First up to deploy a production Maverick-2 system will very likely be Sandia National Laboratory, which assisted with development of the Maverick-1 proof of concept processor that launched in 2022.
 &lt;/p&gt;
 &lt;p&gt;
  As we pointed out last year in our deep dive on the company last October, NextSilicon is interesting for a number of different reasons.
 &lt;/p&gt;
 &lt;p&gt;
 &lt;/p&gt;
 &lt;p&gt;
  First, it is unabashedly an HPC-first company, which is something we have not seen in a compute engine maker for a long, long time. Second, NextSilicon has created a multi-tier computing architecture that has a reconfigurable dataflow engine at the central and most important tier where the bulk of computing for any HPC simulation or model is expected to run. (And by the way, nothing is preventing anyone from running AI applications on this processor.) And third, the Maverick architecture has automated the way code is ported, run, and continuously optimized as it is moved off of a CPU and run across NextSilicon’s own host processor, the special RISC-V cores embedded on the Maverick socket, and the banks of arithmetic units that do the bulk of computing and that represent most of the transistor count in a Maverick chip.
 &lt;/p&gt;
 &lt;p&gt;
  This last bit of the hardware architecture, which enables the software architecture to work, is the new revelation as the Maverick-2 chip comes to market today. So let’s start with the hardware and work our way up to the software.
 &lt;/p&gt;
 &lt;p&gt;
  Ilan Tayari, NextSilicon co-founder and vice president of architecture and formerly director of software at Mellanox (now the networking arm of Nvidia), walked through the differences between the Von Neumann architecture of a classical CPU and the new dataflow engine, which quite frankly for the visual thinkers in the crowd makes a lot more sense and is visually pleasing. With a dataflow engine such as NextSilicon has invented, the hardware literally maps itself to the software rather trying to be a short order cook with too many orders but with 27 arms and eyes in the back of the head, flailing around to make up for the difficulties of getting the right data and the right instructions to collide billions of times per second.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture.jpg&quot; rel=&quot;attachment wp-att-146486&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;591&quot; sizes=&quot;(max-width: 883px) 100vw, 883px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture.jpg 883w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture-768x514.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-cpu-isa-vs-dataflow-architecture-600x402.jpg 600w&quot; width=&quot;883&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  In terms of area, Tayari says that around 2 percent of the typical CPU is dedicated to arithmetic logic units, or ALUs – ya know, the things that actually
  &lt;em&gt;
   do math
  &lt;/em&gt;
  to transform data. (When Tayari points this out, it does sound a little stupid.) Everything else on the chip is just to stage the instructions and the data so they collide way down in the ALU.
 &lt;/p&gt;
 &lt;p&gt;
  With the Von Neumann architecture, created back in the 1940s with the first vacuum tube systems, there is a single, unified memory space that holds instructions and data. An instruction fetch unit reads instructions from memory in an order determined by a program counter, and an execution unit does the computation. Memory is accessed using registers and a memory management unit translates memory addresses. Over the years, various levels of SRAM cache have been added to processors, all to increase the odds that the right instructions and data can be found when needed, and circuits for branch prediction, speculative execution, and out of order processing have been added even as instructions moved from CISC (fat) to RISC (skinny) to push more bits through the goose.
 &lt;/p&gt;
 &lt;p&gt;
  “These solutions can increase performance, but they also have high costs,” Tayari explained at the Maverick-2 launch. “They take a lot of silicon real estate, and the mechanism to revert mispredictions has a large negative impact on performance. Here is the shocking reality. Today’s high end processors have become complicated and chunky, both physically and practically. They dedicate 98 percent of their silicon to overhead, traffic management, data shuffling – not actual computation – and add more complexity. Clock speeds suffer, and slower clocks means slower execution. Some GPUs devote up to 30 percent of their silicon to compute, but because the blocks are mutually exclusive, only a few can run at once. All of these efforts to improve processor design are merely reactive solutions – workarounds. End users pay three penalties. The chip costs more. Power consumption increases. Cooling requirements grow. Everything becomes more expensive.”
 &lt;/p&gt;
 &lt;p&gt;
  In the Intelligent Computing Architecture, or ICA, approach developed by NextSilicon, the idea was to create logic blocks that were comprised of hundreds of interlinked ALUs, with instructions called in an application literally mapped to each ALU. (In this sense, an ALU is akin to an instruction.) NextSilicon is not releasing the specific number of ALUs per compute block, but we do know a few things. First look at this chip shot of the monolithic Maverick-2 die:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg&quot; rel=&quot;attachment wp-att-144917&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;735&quot; sizes=&quot;(max-width: 845px) 100vw, 845px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2.jpg 845w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2-768x668.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/10/nextsilicon-maverick-2-chip-shot-2-600x522.jpg 600w&quot; width=&quot;845&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  There are four compute regions, with the 32 RISC-V E-cores on the outside edges on the left and right of the chip. By our count, there is a grid of seven columns of compute blocks that each have eight compute blocks, for a total of 224 compute blocks on the die. At hundreds of ALUs per compute block, you can easily get many tens of thousands to close to a hundred thousand ALUs. This doesn’t seem unreasonable for a Maverick-2 chip that weighs in at 54 billion transistors at 5 nanometer processes from Taiwan Semiconductor Manufacturing Co. If you do a 14 by 14 grid as NextSilicon shows in its charts, then there are 196 ALUs per compute block; we do not know how many floating point units are in a compute block. It would make sense that every ALU had an FPU.
 &lt;/p&gt;
 &lt;p&gt;
  The “Ampere” A100 GPU from Nvidia was etched in 7 nanometer processes from TSMC, had 54.2 billion transistors, and 6,912 FP32 CUDA cores, while the “Hopper” H100 and H200 GPUs were made in 4 nanometer processes, have 80 billion transistors, and have 18,432 FP32 cores. The Blackwell B200 socket has two chiplets, each with 104 billion transistors but with only 16,896 CUDA cores each, made in a 4 nanometer process. We surmise that the ALUs are smaller than CUDA cores and that there are more of them on a Maverick-2 die than there are CUDA cores on an Nvidia GPU.
 &lt;/p&gt;
 &lt;p&gt;
  Ultimately, the ALU count is not as important as the thread count that a collection of mill cores can support. Tayari says that a typical CPU has two threads, a GPU has between 32 and 64 threads, but a mill core can support hundreds of threads at once. The size and shape of the mill cores change, of course, but with maybe tens of mill cores per compute block and 224 compute blocks per Maverick-2, you are easily up to thousands of threads, all running at 1.5 GHz – about the speed of a slow CPU or a normal GPU – and all linked to HBM3E memory for fast bandwidth.
 &lt;/p&gt;
 &lt;p&gt;
  What we really wanted to know is what the average utilization rates for the ALUs and FPUs were as applications start running and then as the optimizations are done and the resources on the Maverick chip get more fully utilized. We think that utilization rate doesn’t have crazy high to match a CPU doing branch prediction, speculative execution, out of order processing, and other kinds of unnatural acts to make a serial processor run faster. Still, after tuning and across a wide variety of HPC applications, we would not be surprised if the ALU and FPU utilization got up to 75 percent or 80 percent of the potential blocks across thousands of threads flitting in and out of the chip as mill cores come and go.
 &lt;/p&gt;
 &lt;p&gt;
  Anyway, you pour data into the flow at the top of the Maverick-2 chip, it flows through all of the transformations and calculations, and out pops the answer at the bottom.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dataflow-architecture-details.jpg&quot; rel=&quot;attachment wp-att-146487&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;395&quot; loading=&quot;lazy&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dataflow-architecture-details.jpg&quot; width=&quot;571&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This main logic unit, as you see in the chart above on the right, is attached to a memory bus, which has a reservation station to temporarily store data before an ALU calls for it. (It is this combination of reservation station, dispatcher, and dataflow compute block that NextSilicon has patented.) Like regular CPUs, the Maverick ICA uses memory management units and a table lookaside buffer, but these are used sparingly and only when an ALU calls for specific data. There is no speculation or prediction, just fetching.
 &lt;/p&gt;
 &lt;p&gt;
  “NextSilicon’s dataflow architecture allows us to achieve significantly lower overhead compared to traditional CPUs and GPUs,” brags Tayari. “We pivot the silicon allocation ratio. We dedicate the majority of the resources to actual computation rather than control overhead. Our approach uniquely eliminates instruction handling overhead. We minimize unnecessary data movement, and the result is compute units stay fully utilized. We’re not trying to hide latency, but to tolerate and minimize it by design.”
 &lt;/p&gt;
 &lt;p&gt;
  When an application is compiled for the dataflow engine, it is literally mapped onto it, into something called a mill core (it looks like a graph). It looks like the intermediate representation graph of a program before it is compiled, and it is laid down on the ALUs. Many mill cores can be laid down on the same compute block, Tetris style, and the mill cores can be loaded up and deleted as needed in a matter of nanoseconds as needed by the workload, according to Elad Raz, co-founder and chief executive officer at NextSilicon.
 &lt;/p&gt;
 &lt;p&gt;
  While this is all interesting, that is not the magic of NextSilicon’s ICA. (Wait for it – this is the magic compiler moment.) You can take existing C, C++, or Fortran code, grab its intermediate representation and plunk that down onto the ICA, and Maverick-2 will not only map it to its ALU blocks and thus compile it, but has algorithms that constantly analyze how that resulting dataflow is functioning and changes it on the fly, without human intervention, to improve it. The longer the code runs, the better it gets, and if you change the code at a higher level, you just run it and let it self-tune for a while. There is no porting to Nvidia CUDA-X or AMD ROCm or anything else – just an automated way to take CPU code and make it run on a massively parallel dataflow engine without you having to figure it out.
 &lt;/p&gt;
 &lt;p&gt;
  As we explained a year ago, the entire application is not ported to the Maverick chips. Just the parts of the code that are commonly used and represent 80 percent or so of the instruction runtime. Any code that does not benefit from running on the ALU blocks in dataflow mode can run on 32 RISC-V E-cores that are embedded on the Maverick-2 package, and for those parts of the code that would benefit from a beefier set of cores and memory, execution will take place on the CPU host processor, which for now is an unnamed X86 chip.
 &lt;/p&gt;
 &lt;p&gt;
  Here is what the entire workflow looks like:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow.jpg&quot; rel=&quot;attachment wp-att-146492&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;447&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1057px) 100vw, 1057px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow.jpg 1057w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow-768x325.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-workflow-600x254.jpg 600w&quot; width=&quot;1057&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  One of the tricks in the Maverick architecture is to know what code to leave on the host CPU, what code to leave on the RISC-V E-cores (short for embedded cores), and what code must be moved to the ALU blocks. The other trick is that if there is a particular routine that could be parallelized and replicated to speed up the overall application, then the Maverick compiler can just plunk down more copies of the mill cores created for any specific routine. The compiler is, in effect, creating software cores in the shape and number that you need for this application at this time rather than taking a specific number of static compute units with specific integer and floating point precisions – and there are many combinations of static compute elements on CPUs and GPUs these days. This the dataflow ICA is almost as malleable as an FPGA in this regard, but it programs itself. (As many FPGA tools claim to do, by the way, converting C, C++, or Fortran code to HDL automagically.)
 &lt;/p&gt;
 &lt;p&gt;
  This spider graph helps illustrate some things about Maverick-2:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph.jpg&quot; rel=&quot;attachment wp-att-146494&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;391&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 550px) 100vw, 550px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph.jpg 772w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph-768x546.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-spider-graph-600x427.jpg 600w&quot; width=&quot;550&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The story this chart tells is that the dataflow processor created by NextSilicon has the compatibility, portability, programmability, and flexibility of the CPU, better power efficiency and throughput than a GPU, but abiut the single threaded performance of an FPGA. Nothinbg has the single threaded performance of a CPU, which is why you put CPU cores inside of FPGAs and now DFPs and also allow host CPUs to pick up the slack on serial tasks.
 &lt;/p&gt;
 &lt;p&gt;
  Here is an updated salient characteristics table for the single-die and dual-die implementations of the Maverick-2 chip:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-salient-table.jpg&quot; rel=&quot;attachment wp-att-146491&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;596&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 702px) 100vw, 702px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-salient-table.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-salient-table.jpg 702w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-salient-table-600x509.jpg 600w&quot; width=&quot;702&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The chip max power is higher than what was expected when we did our story last year, with the TDP on the single chip Maverick-2 now at 400 watts (up from 300 watts) and on the dual-chip version for OAM sockets now at 750 watts (instead of 600 watts). Everything else is the same.
 &lt;/p&gt;
 &lt;p&gt;
  What is also different this time around is that we have peak flops performance at different floating point precisions, as you can see in the table above. We are not sure how to configure the Maverick-2 as a matrix/tensor unit, but that sounds neat. Clearly it can be done and it does boost performance.
 &lt;/p&gt;
 &lt;p&gt;
  These peak numbers for Maverick-2 are not very high compared to recent GPUs. With an Nvidia H100, for instance, the peak FP64 performance on dense data is 33.5 teraflops on the vector units and 67 teraflops on the tensor units. A single Maverick-2 is about a third of that. But Raz
  &lt;a href=&quot;https://www.nextplatform.com/2025/02/13/the-hidden-cost-of-compromise-why-hpc-still-demands-precision/&quot;&gt;
   wrote about the gaming that happens with peak theoretical performance
  &lt;/a&gt;
  and the fact that sustained performance is what matters in the real world back in February of this year in a column in
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  . And we agree.
 &lt;/p&gt;
 &lt;p&gt;
  Still, peak flops also gives us a ceiling from which to measure against, so it is important to know both sustained and peak performance. That way we can judge the computational efficiency of a compute or networking engine. The limited evidence we have suggests that Maverick-2 is very efficient, and GPUs are not. Take a look:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance.jpg&quot; rel=&quot;attachment wp-att-146490&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;575&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 931px) 100vw, 931px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance.jpg 931w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance-768x474.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-performance-600x371.jpg 600w&quot; width=&quot;931&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://en.wikipedia.org/wiki/Giga-updates_per_second&quot;&gt;
   The GUPS benchmark
  &lt;/a&gt;
  , short for Giga Updates Per Second, is designed to stress test the bandwidth and latency of the memory subsystem of a compute engine, and frankly it is not one that we are very familiar with. On the GUPS test, the Maverick-2 was rated at 32.6 GUPS running at 460 watts. This was, according to NextSilicon, 22X faster than a CPU and nearly 6X faster than a GPU, but we have no idea what CPU or GPU was tested. (This should be in the presentation notes, like AMD, Intel, and Nvidia do.)
 &lt;/p&gt;
 &lt;p&gt;
  The STREAM benchmark is the better known memory bandwidth test that is commonly in the suite of benchmarks used by HPC shops. The chart seems to suggest that Maverick-2 is getting peak bandwidth as measured bandwidth, but we do not believe this is possible. It could be close, but it can’t be perfect. In any event, in different notes, NextSilicon says it delivered 5.2 TB/sec, which implies it was on the dual-chip OAM module, which is 83.9 percent of peak and 1.86X better performance per watt than a GPU. (Again, which one?)
 &lt;/p&gt;
 &lt;p&gt;
  Here is what the Maverick-2 OAM package looks like:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card.jpg&quot; rel=&quot;attachment wp-att-146488&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;575&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1001px) 100vw, 1001px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card.jpg 1001w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card-768x441.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-maverick-2-dual-die-oam-card-600x345.jpg 600w&quot; width=&quot;1001&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  On
  &lt;a href=&quot;https://en.wikipedia.org/wiki/HPCG_benchmark&quot;&gt;
   the HPCG test
  &lt;/a&gt;
  , which shames all HPC systems large and small and is the best indicator of how a system will work on very tough HPC problems, a single Maverick-2 was rated at 600 gigaflops running at 600 watts (by which we presume it was actually a pair of Maverick-2s in the OAM socket). NextSilicon says that this was “matching the leading GPU performance” while consuming half the power.
 &lt;/p&gt;
 &lt;p&gt;
  And finally, on the PageRank graph analytics benchmark based on the web page ranking algorithm created by Google, Maverick-2 did 10X better than “leading GPUs.”
 &lt;/p&gt;
 &lt;p&gt;
  We want to get more clarification on this, and look forward to fuller performance and price/performance benchmark comparisons. We also want to understand how NextSilicon is going to be able to scale performance beyond a single socket with scale up and scale out networks.
 &lt;/p&gt;
 &lt;h3&gt;
  A Dataflow Engine Still Needs A CPU Host
 &lt;/h3&gt;
 &lt;p&gt;
  That leaves us with the Arbel RISC-V processor, also announced by NextSilicon today.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu.jpg&quot; rel=&quot;attachment wp-att-146483&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;450&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 450px) 100vw, 450px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu.jpg 931w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-300x300.jpg 300w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-150x150.jpg 150w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-768x767.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-600x599.jpg 600w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-cpu-100x100.jpg 100w&quot; width=&quot;450&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  NextSilicon is calling Arbel a “test chip,” but we think that the company is looking for a companion to its Maverick dataflow processors (DFPs?) that allows customers to have a fully integrated CPU that is not just a off-the-shelf X86 or Arm CPU from someone else – and does not come with a licensing fee owed to Arm Ltd.
 &lt;/p&gt;
 &lt;p&gt;
  Here are the feeds and speeds of the Arbel CPU:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds.jpg&quot; rel=&quot;attachment wp-att-146484&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;367&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 827px) 100vw, 827px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds.jpg 827w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds-768x341.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-feeds-speeds-600x266.jpg 600w&quot; width=&quot;827&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The Arbel chip has a completely homegrown RISC-V core, just as the Maverick-2 does. (It is not clear how similar these cores are, but don’t assume they are they same.)
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram.jpg&quot; rel=&quot;attachment wp-att-146482&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;422&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 848px) 100vw, 848px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram.jpg 848w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram-768x382.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nextsilicon-arbel-risc-v-block-diagram-600x299.jpg 600w&quot; width=&quot;848&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The Arbel core has a 10-wide issue decoder and six ALUs on the integer side and four 128-bit FPUs on the vector side. The core can support 16 scalar instructions in parallel. The core has 64 KB of L1 instruction cache and 64 KB of L1 data cache close to the ALUs and a 1 MB L2 cache close to the FPUs. (Both caches are obviously cross-linked to all the compute elements.) There is a 2 MB cache per core, but again, we don’t know how many cores are on the Arbel chip.
 &lt;/p&gt;
 &lt;p&gt;
  What we do know is that NextSilicon says that the Arbel core can “stand toe-to-toe” with Intel’s “LionCove” Xeon core and AMD’s “Zen5” Epyc core.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Software Pushes The AI Pareto Frontier More Than Hardware </title>
<link>https://www.nextplatform.com/2025/10/21/software-pushes-the-ai-pareto-frontier-more-than-hardware/#respond</link>
<pubDate>Tue, 21 Oct 2025 18:02:39 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/03/nvidia-blackwell-hopper-jensen-logo-734x438.jpg&quot; title=&quot;nvidia-blackwell-hopper-jensen-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  One of the neat things about modern AI is that a whole new generation of people in a field outside of economics (but certainly driving the modern economy) has been introduced to curves showing the Pareto frontier.
 &lt;/p&gt;
 &lt;p&gt;
  Famously, Nvidia co-founder and chief executive officer used these Pareto frontier curves in describing the tradeoff between AI inference throughput and response time performance during his GTC 2025. Vilfredo Pareto, of course, is the Italian mathematician, civil engineer, and economist who created the 80-20 rule during Gilded Age I, observing that 80 percent of the wealth in Italy was concentrated in the hands of 20 percent of the population. (We are pretty sure that he might have also observed that 80 percent of the work in his home country was done by 20 percent of the people, and that ultimately this was true outside of Italy as well. . . . )
 &lt;/p&gt;
 &lt;p&gt;
  Pareto curves were initially used to plot out quality control improvements in the Industrial Revolution, and Pereto frontiers are special kinds of curves that balance multiple objectives and help to show various tradeoffs between them as variables are changed. You make one variable worse, the other gets better, and somewhere in the middle of the Pareto frontier curve is probably the right balance between the two for most cases. (There is probably a Pareto distributions of exceptions to this rule. . . . But that hurts our head.)
 &lt;/p&gt;
 &lt;p&gt;
  Here is a Pareto frontier curve that Huang showed off at the GTC 2025 opening keynote in March that showed the throughput and response time optimization points available by tweaking the number of GPUs and parallelism type in an unnamed large language model. (We are guessing that this is GPT, and probably GPT4 and maybe GPT4.5 given the timing.)
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model.jpg&quot; rel=&quot;attachment wp-att-146475&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;945&quot; sizes=&quot;(max-width: 1441px) 100vw, 1441px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model.jpg 1441w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model-768x504.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model-600x393.jpg 600w&quot; width=&quot;1441&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  NVIDIA BLACKWELL 25X HOPPER PARETO FRONTIER CURVES BLURTY
 &lt;/p&gt;
 &lt;p&gt;
  The Hopper curve on the bottom left shows the distribution of performance with and without the Dynamo and TensorRT inference stack, which obviously shifts out the curve, on an eight-way GPU node using H200 GPUs running at FP8 precision. The Blackwell curve shows a much larger shared memory GPU cluster, with 72 of the B200s as well as a drop to FP4 precision. Half the precision doubles the throughput, and having a rackscale system instead of a node gives you 9X on top of that. Add in Dynamo and TensorRT optimizations, and at the sweet spots in the H200 and B200 curves, the Blackwell system at a given level of parallelism delivers about 5X more tokens per megawatts and about second 5X more tokens per user for a multiplicative 25X boost in performance. (We measured the lines on the chart to check that, and it looks more like it was closer to 31X by our measure and math, but perhaps Huang was rounding for simplicity.
 &lt;/p&gt;
 &lt;p&gt;
  That was using a dense, blurty, monolithic model that is entirely activated with each token generated for each query, and as you can see the throughput normalized for megawatts peaks out at about 9.5 million tokens per second per megawatt for pretty low throughput per user for a GB200 NVL72 compared to about 2.3 million tokens per second per megawatt for a relatively puny H200 NVL8 system.
 &lt;/p&gt;
 &lt;p&gt;
  But look at how the throughout per megawatt drops when you shift to a reasoning model:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model.jpg&quot; rel=&quot;attachment wp-att-146476&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;943&quot; sizes=&quot;(max-width: 1493px) 100vw, 1493px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model.jpg 1493w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model-768x485.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model-600x379.jpg 600w&quot; width=&quot;1493&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This model behind this curve might be any number of so-called chain of thought models, which do a lot of token generating and consumption across different models strung together before they come to a consensus about the answer and give it to you. It might be a GPT, it might be a DeepSeek. Nvidia did not say.
 &lt;/p&gt;
 &lt;p&gt;
  As you can see, the throughput per megawatt is down by a factor of 11X by shifting to a reasoning model – something we didn’t catch while looking at these curves back in March – but the throughput per user is about the same depending on the layers of expert, tensor, data, and model parallelism used. And the performance advantage of the sweet spot for the B200 system versus the H200 system is 40X (about 6.5X more tokens per user and about 6.5X more tokens per second per megawatt.)
 &lt;/p&gt;
 &lt;p&gt;
  We got another set of Pareto frontier curves from Nvidia when the company described the hardware and software performance of systems using its “Hopper” H200 and “Blackwell” B200 GPUs on the three different AI models that comprise
  &lt;a href=&quot;https://inferencemax.semianalysis.com/&quot;&gt;
   the new InferenceMax v1 benchmark suite from SemiAnalysis
  &lt;/a&gt;
  . And that got us to not just thinking about the performance of Nvidia systems, but how in the AI space, the models are changing so fast and software is being constantly tweaked and tuned to better take advantage of hardware that in a very short period of time the Pareto frontier of throughput and performance is shifted out like a shock wave.
 &lt;/p&gt;
 &lt;p&gt;
  These InferenceMax benchmark uses the GPT-OSS 120B, DeepSeek R1-0528, and Llama 3.3 70B Instruct models; the first two are reasoning models, the latter one is a dense, blurty one. InferenceMax lets you look at throughput per XPU – it only has a few configurations of Nvidia and AMD GPUs right now, but different types of XPUs will hopefully follow – as well as throughout for the cluster sizes tested, and cost per million tokens to buy, to host in a neocloud while owned, and to rent from a cloud. You can vary the input and output sequence lengths of each model to see how that affects the shapes of the Pareto frontier curves.
 &lt;/p&gt;
 &lt;h3&gt;
  The Rules Of Thumb: 2X and Then 5X More
 &lt;/h3&gt;
 &lt;p&gt;
  Since the machine learning resolution took off in earnest around 2012 or so, Nvidia has been working feverishly on both hardware and software. Speaking very generally, performance per GPU for specific kinds of math units – 32-bit vector cores or 16-bit tensor cores – for instance – has gone up by somewhere between 1.5X and 3X, averaging around 2X or so. And then, once the hardware is out and in the field for two years, the performance has been boosted by software by around another 5X or so to yield a 10X jump by the time the next hardware generation comes along and there is another 2X or so hardware bump and the cycle starts again.
 &lt;/p&gt;
 &lt;p&gt;
  This story is not so much about the Pareto frontier curves as it is in gauging the speed of change in the Pareto frontier curves, and Nvidia walked us through the past few months of testing starting with the InferenceMax benchmark submissions it did and then optimizations it did in the following weeks – not months – that moved that Pareto shockwave out.
 &lt;/p&gt;
 &lt;p&gt;
  Two other Pareto rules occur to us. One, 80 percent of Nvidia’s money comes from hardware, but only 20 percent of the money comes from software. But two, 20 percent of Nvidia employees work on hardware, while 80 percent work on software. And that is how software ends up driving 60 percent of the performance gains on any given generation of GPU systems from Nvidia. (Oh man we wish that was 80 percent, but it doesn’t average out that way. Software is just plain harder.)
 &lt;/p&gt;
 &lt;p&gt;
  Here is a pretty chart showing the GPT-OSS reasoning model Pareto frontier curve with throughput per GPU on the Y axis and interactivity (tokens per second per user) on the X axis for a GB200 NVL72 rackscale system:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1.jpg&quot; rel=&quot;attachment wp-att-146472&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;432&quot; sizes=&quot;(max-width: 848px) 100vw, 848px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1.jpg 848w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-768x391.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-600x306.jpg 600w&quot; width=&quot;848&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The performance almost doubled all along the Pareto frontier between the beginning of August and the end of September when the InferenceMax v1 test results were submitted.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch.jpg&quot; rel=&quot;attachment wp-att-146473&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;350&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 690px) 100vw, 690px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch.jpg 690w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-600x304.jpg 600w&quot; width=&quot;690&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  And then on October 3, when Nvidia made some enhancements in the TensorRT inference stack as well as new ways to parallelize data accesses across the NVSwitch memory interconnect in the rackscale system, the Pareto frontier curve was not only pushed out, but the ends of the curve were stretched out closer to the Y and X axes, boosting the maximum throughput for a small number of users to more than 60,000 tokens per second (TPS) per GPU and boosting the maximum user interactivity to nearly 500 TPS per GPU for a high level of interactivity for a small number of users.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp.jpg&quot; rel=&quot;attachment wp-att-146474&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;428&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 883px) 100vw, 883px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp.jpg 883w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp-768x372.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp-600x291.jpg 600w&quot; width=&quot;883&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Then less than week later, on October 9, Nvidia added multi-token prediction, a kind of speculative execution for AI models, to the software stack and this changed the shape of the Pareto frontier curve such that Nvidia could push 1,000 TPS per user for the maximum interactivity and could deliver 5X the throughput at around the 100 TPS per user rate set as the peak throughput of the original GPT-OSS benchmark run in August.
 &lt;/p&gt;
 &lt;p&gt;
  What used to take Nvidia two years or so to do in software – deliver a 5X performance improvement on the same hardware – it did in a matter of weeks.
 &lt;/p&gt;
 &lt;p&gt;
  To which we quipped to Nvidia when we talked about this:
  &lt;em&gt;
   “Well, why the hell didn’t you do this all in the first place?”
  &lt;/em&gt;
  Which got an initial shock as well as laughter.
 &lt;/p&gt;
 &lt;p&gt;
  The answer is, of course, that the hardware is changing fast, and the software is changing even faster. GenAI is one of those parts of the IT sector where it really does matter to keep current on the software. It is worth billions of dollars in performance to do so.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> Software Pushes The AI Pareto Frontier More Than Hardware </title>
<link>https://www.nextplatform.com/2025/10/21/software-pushes-the-ai-pareto-frontier-more-than-hardware/</link>
<pubDate>Tue, 21 Oct 2025 18:02:35 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/03/nvidia-blackwell-hopper-jensen-logo-734x438.jpg&quot; title=&quot;nvidia-blackwell-hopper-jensen-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  One of the neat things about modern AI is that a whole new generation of people in a field outside of economics (but certainly driving the modern economy) has been introduced to curves showing the Pareto frontier.
 &lt;/p&gt;
 &lt;p&gt;
  Famously, Nvidia co-founder and chief executive officer used these Pareto frontier curves in describing the tradeoff between AI inference throughput and response time performance during his GTC 2025. Vilfredo Pareto, of course, is the Italian mathematician, civil engineer, and economist who created the 80-20 rule during Gilded Age I, observing that 80 percent of the wealth in Italy was concentrated in the hands of 20 percent of the population. (We are pretty sure that he might have also observed that 80 percent of the work in his home country was done by 20 percent of the people, and that ultimately this was true outside of Italy as well. . . . )
 &lt;/p&gt;
 &lt;p&gt;
  Pareto curves were initially used to plot out quality control improvements in the Industrial Revolution, and Pereto frontiers are special kinds of curves that balance multiple objectives and help to show various tradeoffs between them as variables are changed. You make one variable worse, the other gets better, and somewhere in the middle of the Pareto frontier curve is probably the right balance between the two for most cases. (There is probably a Pareto distributions of exceptions to this rule. . . . But that hurts our head.)
 &lt;/p&gt;
 &lt;p&gt;
  Here is a Pareto frontier curve that Huang showed off at the GTC 2025 opening keynote in March that showed the throughput and response time optimization points available by tweaking the number of GPUs and parallelism type in an unnamed large language model. (We are guessing that this is GPT, and probably GPT4 and maybe GPT4.5 given the timing.)
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model.jpg&quot; rel=&quot;attachment wp-att-146475&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;945&quot; sizes=&quot;(max-width: 1441px) 100vw, 1441px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model.jpg 1441w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model-768x504.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-25x-hopper-pareto-frontier-curves-blurty-model-600x393.jpg 600w&quot; width=&quot;1441&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  NVIDIA BLACKWELL 25X HOPPER PARETO FRONTIER CURVES BLURTY
 &lt;/p&gt;
 &lt;p&gt;
  The Hopper curve on the bottom left shows the distribution of performance with and without the Dynamo and TensorRT inference stack, which obviously shifts out the curve, on an eight-way GPU node using H200 GPUs running at FP8 precision. The Blackwell curve shows a much larger shared memory GPU cluster, with 72 of the B200s as well as a drop to FP4 precision. Half the precision doubles the throughput, and having a rackscale system instead of a node gives you 9X on top of that. Add in Dynamo and TensorRT optimizations, and at the sweet spots in the H200 and B200 curves, the Blackwell system at a given level of parallelism delivers about 5X more tokens per megawatts and about second 5X more tokens per user for a multiplicative 25X boost in performance. (We measured the lines on the chart to check that, and it looks more like it was closer to 31X by our measure and math, but perhaps Huang was rounding for simplicity.
 &lt;/p&gt;
 &lt;p&gt;
  That was using a dense, blurty, monolithic model that is entirely activated with each token generated for each query, and as you can see the throughput normalized for megawatts peaks out at about 9.5 million tokens per second per megawatt for pretty low throughput per user for a GB200 NVL72 compared to about 2.3 million tokens per second per megawatt for a relatively puny H200 NVL8 system.
 &lt;/p&gt;
 &lt;p&gt;
  But look at how the throughout per megawatt drops when you shift to a reasoning model:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model.jpg&quot; rel=&quot;attachment wp-att-146476&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;943&quot; sizes=&quot;(max-width: 1493px) 100vw, 1493px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model.jpg 1493w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model-768x485.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-blackwell-40x-hopper-pareto-frontier-reasoning-model-600x379.jpg 600w&quot; width=&quot;1493&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  This model behind this curve might be any number of so-called chain of thought models, which do a lot of token generating and consumption across different models strung together before they come to a consensus about the answer and give it to you. It might be a GPT, it might be a DeepSeek. Nvidia did not say.
 &lt;/p&gt;
 &lt;p&gt;
  As you can see, the throughput per megawatt is down by a factor of 11X by shifting to a reasoning model – something we didn’t catch while looking at these curves back in March – but the throughput per user is about the same depending on the layers of expert, tensor, data, and model parallelism used. And the performance advantage of the sweet spot for the B200 system versus the H200 system is 40X (about 6.5X more tokens per user and about 6.5X more tokens per second per megawatt.)
 &lt;/p&gt;
 &lt;p&gt;
  We got another set of Pareto frontier curves from Nvidia when the company described the hardware and software performance of systems using its “Hopper” H200 and “Blackwell” B200 GPUs on the three different AI models that comprise
  &lt;a href=&quot;https://inferencemax.semianalysis.com/&quot;&gt;
   the new InferenceMax v1 benchmark suite from SemiAnalysis
  &lt;/a&gt;
  . And that got us to not just thinking about the performance of Nvidia systems, but how in the AI space, the models are changing so fast and software is being constantly tweaked and tuned to better take advantage of hardware that in a very short period of time the Pareto frontier of throughput and performance is shifted out like a shock wave.
 &lt;/p&gt;
 &lt;p&gt;
  These InferenceMax benchmark uses the GPT-OSS 120B, DeepSeek R1-0528, and Llama 3.3 70B Instruct models; the first two are reasoning models, the latter one is a dense, blurty one. InferenceMax lets you look at throughput per XPU – it only has a few configurations of Nvidia and AMD GPUs right now, but different types of XPUs will hopefully follow – as well as throughout for the cluster sizes tested, and cost per million tokens to buy, to host in a neocloud while owned, and to rent from a cloud. You can vary the input and output sequence lengths of each model to see how that affects the shapes of the Pareto frontier curves.
 &lt;/p&gt;
 &lt;h3&gt;
  The Rules Of Thumb: 2X and Then 5X More
 &lt;/h3&gt;
 &lt;p&gt;
  Since the machine learning resolution took off in earnest around 2012 or so, Nvidia has been working feverishly on both hardware and software. Speaking very generally, performance per GPU for specific kinds of math units – 32-bit vector cores or 16-bit tensor cores – for instance – has gone up by somewhere between 1.5X and 3X, averaging around 2X or so. And then, once the hardware is out and in the field for two years, the performance has been boosted by software by around another 5X or so to yield a 10X jump by the time the next hardware generation comes along and there is another 2X or so hardware bump and the cycle starts again.
 &lt;/p&gt;
 &lt;p&gt;
  This story is not so much about the Pareto frontier curves as it is in gauging the speed of change in the Pareto frontier curves, and Nvidia walked us through the past few months of testing starting with the InferenceMax benchmark submissions it did and then optimizations it did in the following weeks – not months – that moved that Pareto shockwave out.
 &lt;/p&gt;
 &lt;p&gt;
  Two other Pareto rules occur to us. One, 80 percent of Nvidia’s money comes from hardware, but only 20 percent of the money comes from software. But two, 20 percent of Nvidia employees work on hardware, while 80 percent work on software. And that is how software ends up driving 60 percent of the performance gains on any given generation of GPU systems from Nvidia. (Oh man we wish that was 80 percent, but it doesn’t average out that way. Software is just plain harder.)
 &lt;/p&gt;
 &lt;p&gt;
  Here is a pretty chart showing the GPT-OSS reasoning model Pareto frontier curve with throughput per GPU on the Y axis and interactivity (tokens per second per user) on the X axis for a GB200 NVL72 rackscale system:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1.jpg&quot; rel=&quot;attachment wp-att-146472&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;432&quot; sizes=&quot;(max-width: 848px) 100vw, 848px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1.jpg 848w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-768x391.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-600x306.jpg 600w&quot; width=&quot;848&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The performance almost doubled all along the Pareto frontier between the beginning of August and the end of September when the InferenceMax v1 test results were submitted.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch.jpg&quot; rel=&quot;attachment wp-att-146473&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;350&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 690px) 100vw, 690px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch.jpg 690w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-600x304.jpg 600w&quot; width=&quot;690&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  And then on October 3, when Nvidia made some enhancements in the TensorRT inference stack as well as new ways to parallelize data accesses across the NVSwitch memory interconnect in the rackscale system, the Pareto frontier curve was not only pushed out, but the ends of the curve were stretched out closer to the Y and X axes, boosting the maximum throughput for a small number of users to more than 60,000 tokens per second (TPS) per GPU and boosting the maximum user interactivity to nearly 500 TPS per GPU for a high level of interactivity for a small number of users.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp.jpg&quot; rel=&quot;attachment wp-att-146474&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;428&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 883px) 100vw, 883px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp.jpg 883w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp-768x372.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/nvidia-gb200-gpt-oss-launch-inferencemax-v1-trt-nvswitch-mtp-600x291.jpg 600w&quot; width=&quot;883&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Then less than week later, on October 9, Nvidia added multi-token prediction, a kind of speculative execution for AI models, to the software stack and this changed the shape of the Pareto frontier curve such that Nvidia could push 1,000 TPS per user for the maximum interactivity and could deliver 5X the throughput at around the 100 TPS per user rate set as the peak throughput of the original GPT-OSS benchmark run in August.
 &lt;/p&gt;
 &lt;p&gt;
  What used to take Nvidia two years or so to do in software – deliver a 5X performance improvement on the same hardware – it did in a matter of weeks.
 &lt;/p&gt;
 &lt;p&gt;
  To which we quipped to Nvidia when we talked about this:
  &lt;em&gt;
   “Well, why the hell didn’t you do this all in the first place?”
  &lt;/em&gt;
  Which got an initial shock as well as laughter.
 &lt;/p&gt;
 &lt;p&gt;
  The answer is, of course, that the hardware is changing fast, and the software is changing even faster. GenAI is one of those parts of the IT sector where it really does matter to keep current on the software. It is worth billions of dollars in performance to do so.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> “Polaris” AmpereOne M Arm CPUs Sighted In Oracle A4 Instances </title>
<link>https://www.nextplatform.com/2025/10/20/polaris-ampereone-m-arm-cpus-sighted-in-oracle-a4-instances/#respond</link>
<pubDate>Tue, 21 Oct 2025 02:35:45 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-computing-datacenter-rack-logo-1030x438.jpg&quot; title=&quot;ampere-computing-datacenter-rack-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Japanese tech conglomerate SoftBank announced
  &lt;a href=&quot;https://www.nextplatform.com/2025/03/21/why-did-softbank-just-buy-ampere-computing/&quot;&gt;
   a $6.5 billion acquisition of Arm server CPU upstart Ampere Computing
  &lt;/a&gt;
  back in March, and that deal has not yet closed. And that means business has to continue as usual – well, as close to usual as possible – until the dominant shareholders of Ampere Computing have traded their stock for cash.
 &lt;/p&gt;
 &lt;p&gt;
  And so, it was not surprise when Ampere Computing said last week at Oracle AI World that Big Red was launching instances on Oracle Cloud Infrastructure based on the “Polaris” AmpereOne M processor, which has been shipping in volume since Q4 2024. It is a bit of a wonder why it took so long to get the Polaris machines inside of OCI, given that Oracle is the only one of the major clouds and hyperscalers that does not have its own homegrown Arm-based CPU and that Oracle is also a 32.3 percent shareholder in of Ampere Computing. (Private equity firm The Carlyle Group, which put together Ampere Computing from some bits and pieces of Arm server upstart Applied Micro and Intel, has just under a 60 percent stake of the company.)
 &lt;/p&gt;
 &lt;p&gt;
  The Polaris AmpereOne M CPU, which we gave its codename because Ampere Computing stopped giving us synonyms after the “Siryn” AmpereOne from 2023, has 192 cores and twelve channels of DDR5 memory and was quietly announced in December 2024. The Siryn AmpereOne (no letter after the name) also had 192 cores, but only had eight DDR5 memory channels. So Polaris had a more than 50 percent boost in memory bandwidth, which is important for database workloads as well as AI inference workloads running on CPUs.
 &lt;/p&gt;
 &lt;p&gt;
  The ”Magnetrix” AmpereOne MX chip with 256 cores and twelve DDR5 memory channels was in fabrication last July, and
  &lt;a href=&quot;https://www.nextplatform.com/2024/07/31/ampere-arm-server-cpus-to-get-512-cores-ai-accelerator/&quot;&gt;
   Ampere Computing was working on the 512-core “Aurora” Ampere CPU
  &lt;/a&gt;
  for further down the road with integrated AI accelerators on the chip package at some point in the future. The key thing is that Aurora has a homegrown scalable mesh interconnect to link Arm cores with the accelerators, as well as a third generation of homegrown Ampere cores that we presumed would have lots of vector units as well as an external matrix math unit on the package or inside of each core.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s a roadmap refresher:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024.jpg&quot; rel=&quot;attachment wp-att-144488&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;526&quot; sizes=&quot;(max-width: 1057px) 100vw, 1057px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024.jpg 1057w, https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024-768x382.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024-600x299.jpg 600w&quot; width=&quot;1057&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://amperecomputing.com/blogs/ampereone-m-powered-a4-instances-coming-to-oracle-cloud&quot;&gt;
   According to a statement put out by Ampere Computing
  &lt;/a&gt;
  , which has been quiet as a mouse in church with a hungry cat watching from the lectern, OCI will have new A4 instances based on the AmpereOne M processor in November, in both bare metal and virtualized variants. For whatever reason, there is not an A3 instance type, but the A1 instance on OCI is based on the 3.0 GHz 80-core “Mystique” Ampere Altra processor and the A2 instance is based on a 160-core variant of the Siryn AmpereOne chip also running at 3.0 GHz.
 &lt;/p&gt;
 &lt;p&gt;
  The Ampere Computing CPUs do not offer hyperthreading, which the company believes is more secure than having hyperthreads behaving like cores. But with the A2 instances as well as the new A4 instances, Oracle is pairing up to physical, single-threaded cores to look like a single, two-threaded core so it looks like an X86 processor with hyperthreads. So the A4 instance looks like it has 96 two-threaded cores running at 3.6 GHz. Those Polaris cores are running 20 percent faster, which is important for both integer and vector workloads. The extra 50 percent plus in memory bandwidth is also important. The Polaris chip also has 100 Gb/sec Ethernet ports in the package.
 &lt;/p&gt;
 &lt;p&gt;
  Add it all up and you can run AI inference on these CPUs, says Ampere Computing. The data is a bit vague, but Ampere Computing and Oracle say that running the Llama 3.1 8B transformer model, the A4 instance on OCI will offer 83 percent better price/performance than an instance running the Nvidia “Ampere” A10 GPU accelerator that is four years old. The benefit is that you can run it all on the CPU, and you don’t have to move to Nvidia’s AI Enterprise stack and CUDA-X framework.
 &lt;/p&gt;
 &lt;p&gt;
  Oracle says that the A4 instances will deliver up to 45 percent more oomph on “cloud native workloads” compared to its A2 instances and are expected to deliver 30 percent better price/performance than its E6 instances that run on the semi-custom 128-core Epyc 9J45 processor from AMD.
 &lt;/p&gt;
 &lt;p&gt;
  Oracle adds that it has over 1,000 customers renting time on the A1 and A2 instances.
 &lt;/p&gt;
 &lt;p&gt;
  The database, middleware, and application software supplier also says that its Fusion Applications ERP software suite, which is written in Java, are currently running on the AI instances and are being ported to the A4 instances, which have a lot more oomph and better bang for the buck. Oracle’s eponymous database management system has been ported to the AmpereOne chips and is implementing
  &lt;a href=&quot;https://amperecomputing.com/blogs/delivering-memory-safety-in-production-data-centers&quot;&gt;
   the memory tagging feature that Ampere Computing has added into its architecture
  &lt;/a&gt;
  , which helps secure memory accesses as well as recover from memory faults better.
 &lt;/p&gt;
 &lt;p&gt;
  While this is all well and good, and will help Carlyle and Oracle get their money from SoftBank, what we really want to know is how SoftBank, through its Arm, Ampere Computing, and Graphcore subsidiaries is going to help OpenAI create its own CPUs and XPUs to build its own iron so it doesn’t have to pay so much to Nvidia and AMD. Oracle will be happy to build such systems, being agnostic about compute engines.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> “Polaris” AmpereOne M Arm CPUs Sighted In Oracle A4 Instances </title>
<link>https://www.nextplatform.com/2025/10/20/polaris-ampereone-m-arm-cpus-sighted-in-oracle-a4-instances/</link>
<pubDate>Tue, 21 Oct 2025 02:35:43 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-computing-datacenter-rack-logo-1030x438.jpg&quot; title=&quot;ampere-computing-datacenter-rack-logo&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  Japanese tech conglomerate SoftBank announced
  &lt;a href=&quot;https://www.nextplatform.com/2025/03/21/why-did-softbank-just-buy-ampere-computing/&quot;&gt;
   a $6.5 billion acquisition of Arm server CPU upstart Ampere Computing
  &lt;/a&gt;
  back in March, and that deal has not yet closed. And that means business has to continue as usual – well, as close to usual as possible – until the dominant shareholders of Ampere Computing have traded their stock for cash.
 &lt;/p&gt;
 &lt;p&gt;
  And so, it was not surprise when Ampere Computing said last week at Oracle AI World that Big Red was launching instances on Oracle Cloud Infrastructure based on the “Polaris” AmpereOne M processor, which has been shipping in volume since Q4 2024. It is a bit of a wonder why it took so long to get the Polaris machines inside of OCI, given that Oracle is the only one of the major clouds and hyperscalers that does not have its own homegrown Arm-based CPU and that Oracle is also a 32.3 percent shareholder in of Ampere Computing. (Private equity firm The Carlyle Group, which put together Ampere Computing from some bits and pieces of Arm server upstart Applied Micro and Intel, has just under a 60 percent stake of the company.)
 &lt;/p&gt;
 &lt;p&gt;
  The Polaris AmpereOne M CPU, which we gave its codename because Ampere Computing stopped giving us synonyms after the “Siryn” AmpereOne from 2023, has 192 cores and twelve channels of DDR5 memory and was quietly announced in December 2024. The Siryn AmpereOne (no letter after the name) also had 192 cores, but only had eight DDR5 memory channels. So Polaris had a more than 50 percent boost in memory bandwidth, which is important for database workloads as well as AI inference workloads running on CPUs.
 &lt;/p&gt;
 &lt;p&gt;
  The ”Magnetrix” AmpereOne MX chip with 256 cores and twelve DDR5 memory channels was in fabrication last July, and
  &lt;a href=&quot;https://www.nextplatform.com/2024/07/31/ampere-arm-server-cpus-to-get-512-cores-ai-accelerator/&quot;&gt;
   Ampere Computing was working on the 512-core “Aurora” Ampere CPU
  &lt;/a&gt;
  for further down the road with integrated AI accelerators on the chip package at some point in the future. The key thing is that Aurora has a homegrown scalable mesh interconnect to link Arm cores with the accelerators, as well as a third generation of homegrown Ampere cores that we presumed would have lots of vector units as well as an external matrix math unit on the package or inside of each core.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s a roadmap refresher:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024.jpg&quot; rel=&quot;attachment wp-att-144488&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;526&quot; sizes=&quot;(max-width: 1057px) 100vw, 1057px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024.jpg 1057w, https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024-768x382.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2024/07/ampere-server-chip-roadmap-2024-600x299.jpg 600w&quot; width=&quot;1057&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://amperecomputing.com/blogs/ampereone-m-powered-a4-instances-coming-to-oracle-cloud&quot;&gt;
   According to a statement put out by Ampere Computing
  &lt;/a&gt;
  , which has been quiet as a mouse in church with a hungry cat watching from the lectern, OCI will have new A4 instances based on the AmpereOne M processor in November, in both bare metal and virtualized variants. For whatever reason, there is not an A3 instance type, but the A1 instance on OCI is based on the 3.0 GHz 80-core “Mystique” Ampere Altra processor and the A2 instance is based on a 160-core variant of the Siryn AmpereOne chip also running at 3.0 GHz.
 &lt;/p&gt;
 &lt;p&gt;
  The Ampere Computing CPUs do not offer hyperthreading, which the company believes is more secure than having hyperthreads behaving like cores. But with the A2 instances as well as the new A4 instances, Oracle is pairing up to physical, single-threaded cores to look like a single, two-threaded core so it looks like an X86 processor with hyperthreads. So the A4 instance looks like it has 96 two-threaded cores running at 3.6 GHz. Those Polaris cores are running 20 percent faster, which is important for both integer and vector workloads. The extra 50 percent plus in memory bandwidth is also important. The Polaris chip also has 100 Gb/sec Ethernet ports in the package.
 &lt;/p&gt;
 &lt;p&gt;
  Add it all up and you can run AI inference on these CPUs, says Ampere Computing. The data is a bit vague, but Ampere Computing and Oracle say that running the Llama 3.1 8B transformer model, the A4 instance on OCI will offer 83 percent better price/performance than an instance running the Nvidia “Ampere” A10 GPU accelerator that is four years old. The benefit is that you can run it all on the CPU, and you don’t have to move to Nvidia’s AI Enterprise stack and CUDA-X framework.
 &lt;/p&gt;
 &lt;p&gt;
  Oracle says that the A4 instances will deliver up to 45 percent more oomph on “cloud native workloads” compared to its A2 instances and are expected to deliver 30 percent better price/performance than its E6 instances that run on the semi-custom 128-core Epyc 9J45 processor from AMD.
 &lt;/p&gt;
 &lt;p&gt;
  Oracle adds that it has over 1,000 customers renting time on the A1 and A2 instances.
 &lt;/p&gt;
 &lt;p&gt;
  The database, middleware, and application software supplier also says that its Fusion Applications ERP software suite, which is written in Java, are currently running on the AI instances and are being ported to the A4 instances, which have a lot more oomph and better bang for the buck. Oracle’s eponymous database management system has been ported to the AmpereOne chips and is implementing
  &lt;a href=&quot;https://amperecomputing.com/blogs/delivering-memory-safety-in-production-data-centers&quot;&gt;
   the memory tagging feature that Ampere Computing has added into its architecture
  &lt;/a&gt;
  , which helps secure memory accesses as well as recover from memory faults better.
 &lt;/p&gt;
 &lt;p&gt;
  While this is all well and good, and will help Carlyle and Oracle get their money from SoftBank, what we really want to know is how SoftBank, through its Arm, Ampere Computing, and Graphcore subsidiaries is going to help OpenAI create its own CPUs and XPUs to build its own iron so it doesn’t have to pay so much to Nvidia and AMD. Oracle will be happy to build such systems, being agnostic about compute engines.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Third Time Will Be The Charm For Broadcom Switch Co-Packaged Optics </title>
<link>https://www.nextplatform.com/2025/10/17/the-third-time-will-be-the-charm-for-broadcom-switch-co-packaged-optics/#respond</link>
<pubDate>Fri, 17 Oct 2025 16:10:57 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-968x438.jpg&quot; title=&quot;broadcom-cpo-switch&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  If Broadcom says that co-packaged optics is ready for prime time and can compete with other ways of linking switch ASICs to fiber optic cables, then it is very unlikely that Broadcom is wrong.
 &lt;/p&gt;
 &lt;p&gt;
  The company that is the modern Broadcom has a long and deep – and acquired – expertise in optical communications. The lineage runs on one branch from the original Hewlett Packard to its Agilent Technologies spinoff, which a bunch of private equity companies acquired a chunk of and called Avago Technologies. Another line of optical expertise comes out of AT&amp;T Bell Labs through the Lucent Technologies spinout in 1996, which runs through Agere Systems, which was bought by LSI Logic and which was in turn acquired by Avago. Avago, of course, bought Broadcom for its switch ASIC and other chip businesses in 2016 for $37 billion, which seemed like an incredibly high number at the time but which turns out to be prophetic in the age of AI.
 &lt;/p&gt;
 &lt;p&gt;
  There have been plenty of naysayers about CPO in the datacenter, but for many years now Broadcom’s techies have told us that using CPO inside of switches will not only improve the reliability of switches and reduce power consumption in the datacenter network, but will also lower costs. (Eventually, CPO or an optical interposer or something will be added to compute engines of all kinds for the same reasons, and when that happens, we will get much more malleable racks and stop pushing for rack density for the sake of latency.)
 &lt;/p&gt;
 &lt;p&gt;
  These CPO claims for the past several years were tough for many to swallow. But, as Broadcom readies its third generation of CPO add-ons for its Tomahawk line of Ethernet switch ASICs, the data is in from a real-world, large-scale deployment of Broadcom’s second generation CPO technology, and the results are clear. We will get to the testing that Meta Platforms has done using switches based on Broadcom’s “Bailly” Tomahawk 5 CPO switch ASIC in a moment; the paper is not yet public,
  &lt;a href=&quot;https://ecoc2025.abstractserver.com/program/#/details/presentations/167&quot;&gt;
   but here is the link for it
  &lt;/a&gt;
  . We have a copy of the paper as well as summary data that we can present compliments of Broadcom. Right now, let’s look at the CPO switch ASIC roadmap from Broadcom and what it means, and how we can interpret the data on the Bailly ASIC and how the future “Davisson” Tomahawk 6 CPO device will be even better and that much more ready to be mainstreamed.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap.jpg&quot; rel=&quot;attachment wp-att-146462&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;485&quot; sizes=&quot;(max-width: 1089px) 100vw, 1089px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap.jpg 1089w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap-768x342.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap-600x267.jpg 600w&quot; width=&quot;1089&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  For a lot of complicated reasons, we never did get around to writing about the first generation “Humboldt” Tomahawk 4 CPO switch chips, which were announced in January 2021 and which were deployed as a development platform by Chinese hyperscaler Tencent. (Broadcom names its CPO switch ASICs after craters on the Moon, which are in turn named after famous people on Earth throughout history as well as various Latin and English names having to do with
  &lt;em&gt;
   lunological
  &lt;/em&gt;
  features.)
 &lt;/p&gt;
 &lt;p&gt;
  The Tomahawk 4 CPO ASIC had 25.6 Tb/sec of aggregate switching capacity,
  &lt;a href=&quot;https://www.nextplatform.com/2019/12/12/broadcom-launches-another-tomahawk-into-the-datacenter/&quot;&gt;
   like the plain vanilla TH4 on which it is based
  &lt;/a&gt;
  , and had four 3.2 Tb/sec optical engines for 12.8 Tb/sec of bandwidth (which could be allocated as 400 Gb/sec or 800 Gb/sec ports) as well as 12.8 Tb/sec of electrical lanes that could be allocated the same way. Importantly, an 800 Gb/sec port on the CPO consumed about 6.4 watts, compared to somewhere around 16 watts to 18 watts for regular pluggable optics running on the same Tomahawk 4 switch without CPO. The Humboldt ASIC had remote laser modules, but if something went wrong with the lasers or the optics, you had to replace the entire switch.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp.jpg&quot; rel=&quot;attachment wp-att-146463&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;499&quot; sizes=&quot;(max-width: 1050px) 100vw, 1050px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp.jpg 1050w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp-768x365.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp-600x285.jpg 600w&quot; width=&quot;1050&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It would be great to see the port counts in that chart to the right above, not just the change.
 &lt;/p&gt;
 &lt;p&gt;
  With the second generation Bailly CPO switch ASICs, which are based on
  &lt;a href=&quot;https://www.nextplatform.com/2022/08/16/like-a-drumbeat-broadcom-doubles-ethernet-bandwidth-with-tomahawk-5/&quot;&gt;
   the 51.2 Tb/sec Tomahawk 5 chip
  &lt;/a&gt;
  and which started shipping to a few hyperscalers in 2023, Broadcom put eight 6.4 Tb/sec optical engines on the devices and no electrical connections at all. The SerDes ran at 100 Gb/sec per lane, and 800 Gb/sec of bandwidth (meaning eight lanes) took 5.5 watts, which was a 14.1 percent reduction in power compared to a Humboldt port and that much less than the power used with pluggable optics. The Bailly CPO switch chip only did optical links and saw some adoption by a couple of hyperscalers (we know of Meta Platforms and presume Tencent did as well). The Bailly design had detachable lasers, which meant they could be field replaceable, easing the minds of many potential switch buyers who are understandably nervous about giant shared laser sources that might fail in the field.
 &lt;/p&gt;
 &lt;p&gt;
  The good news – sort of – is that in a massively parallel AI cluster, if one pluggable optics module fails, then the whole job stops anyway. So a failed pluggable optics module is as bad as a failed laser that kills an entire switch in this regard. (This is what happens when a workload spans all accelerators and all switches.)
 &lt;/p&gt;
 &lt;p&gt;
  That brings us to next year’s Davisson TH6 CPO device, which Broadcom is now shipping to early access customers. As the name suggests, the Davisson switch chip is based
  &lt;a href=&quot;https://www.nextplatform.com/2025/06/03/the-ai-datacenter-is-ravenous-for-102-4-tb-sec-ethernet/&quot;&gt;
   on the Tomahawk 6 ASIC that we told you about back in June
  &lt;/a&gt;
  . The CPO version of TH6 is using the one that has four Serdes chiplets wrapped around the packet processing engines, which have native 100 Gb/sec speeds plus PAM4 modulation to give two bits per signal and an effective 200 Gb/sec of bandwidth per lane. (There is another version of TH6 that has 50 Gb/sec native signaling with PAM4 modulation that delivers 100 Gb/sec effective bandwidth per lane, like TH4 and TH5 do.) On a 102.4 Tb/sec ASIC, you can get 64 ports running at 1.6 Tb/sec or 128 ports running at 800 Gb/sec – or significantly, 512 ports running at 200 Gb/sec, which is enough for certain kinds of inference workloads and modest training workloads to have one ASIC link to 512 XPUs without anything other than a direct connection. Broadcom adds that the Davisson chip, like the stock Tomahawk 6, can link 131,072 XPUs together in a two tier network. The spec sheets say more than 100,000, but that is the precise number.
 &lt;/p&gt;
 &lt;p&gt;
  Among the many things that are cool about the Davisson TH6 CPO setup is the fact that the laser modules for the switch are now field replaceable, not just detachable, through the front panel of the switch and are no longer close to the switch ASIC and optical interfaces welded onto them and therefore the heat coming off of them, which messes with lasers and can kill them. This makes the lasers more reliable as well as more serviceable. An 800 Gb/sec port will burn about 3.5 watts, says Broadcom, which is 36.4 percent lower than with the Tomahawk 5 CPO port at the same bandwidth and more than 70 percent lower than pluggable optics at the same bandwidth.
 &lt;/p&gt;
 &lt;p&gt;
  This is accomplished in large part through Broadcom working with Taiwan Semiconductor Manufacturing Co on its Compact Universal Photonic Engine (COUPE) packaging technique, which is used to add 16 6.4 Gb/sec optical engines around the TH6 package, which comes in at around 120 mm x 120 mm compared to 75 mm by 75 mm for Bailly.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the stress testing gear for the Bailly CPO switches at Broadcom:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test.jpg&quot; rel=&quot;attachment wp-att-146464&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;374&quot; sizes=&quot;(max-width: 1099px) 100vw, 1099px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test.jpg 1099w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test-768x261.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test-600x204.jpg 600w&quot; width=&quot;1099&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  And here is one of the production rows of Bailly CPO switches tested at hyperscalers Meta Platforms:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment.jpg&quot; rel=&quot;attachment wp-att-146457&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;454&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment-768x340.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment-600x266.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “We’ve continued to develop, learn, and evolve over the last five years,” Manesh Mehta, vice president of marketing and operations for the Optical Systems Division at Broadcom, tells
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  . “I think there are a few areas that we are really focused on. First, it’s clear to us that our customers are starting to really get excited about and value the reliability and link performance of CPO platform the way that we’re building it, which is basically a high density optical engine built using very scalable foundry and OSAT-based manufacturing techniques and then solder-attaching that optical engine to a common substrate with the core ASIC package. And second, what Meta presented two weeks ago was that the first million device hours that they ran on Bailly, there were zero link flaps observed. These link flaps drive a pretty high inefficiency or underutilization of XPU’s compute time with all the checkpoint retries.”
 &lt;/p&gt;
 &lt;p&gt;
  So what is a link flap, you ask? Is it some weird kind of sausage made of an even weird kind of meat? No. it is when a communication link – a port, a lane, what have you – cycles from being up to being down when there is a faulty cable, some software configuration issue, a bad connection, dust on an optical transceiver, and dozens of other possible causes. The link is flapping around like one of those hand-wavy tubular balloons on used car lots.
 &lt;/p&gt;
 &lt;p&gt;
  Over the first 1 million device hours with the Bailly switches, which we think were manufactured by Micas Networks,
  &lt;a href=&quot;https://www.nextplatform.com/2023/10/17/micas-takes-on-arista-and-the-whiteboxes-in-datacenter-switching/&quot;&gt;
   who we told you all about here
  &lt;/a&gt;
  way back in October 2023, there were no link flaps:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps.jpg&quot; rel=&quot;attachment wp-att-146458&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;467&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps-768x350.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps-600x273.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “To the best of our knowledge, this is the highest device hours reported for CPO technology operation at the system level,” the techies at Meta Platforms write in their paper. “Over the period of the experiment, each unit ran continuously without interruption or clearing the FEC [forward error correction] counters and we did not see any failures or uncorrectable codewords (UCWs) in the links.”
 &lt;/p&gt;
 &lt;p&gt;
  One port had some funniness over the test run, and it was traced back to a faulty fiber cable.
 &lt;/p&gt;
 &lt;p&gt;
  But here’s important thing in all of the paper: “The demonstrated lower bound mean time between failures of (MTBFs) of optical links can readily support a 24K GPU AI cluster with &amp;gt;90 percent training efficiency without interconnect failures being the bottleneck.”
 &lt;/p&gt;
 &lt;p&gt;
  Meta added that the Bailly CPO switch optics delivered 65 percent lower power per 100 Gb/sec lane compared to retimed pluggable optical models. Here’s the data on that from the paper:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power.jpg&quot; rel=&quot;attachment wp-att-146460&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;467&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power-768x350.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power-600x273.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Linear drive pluggable optics, or LPO, burns somewhere on the order of 10 watts, which is possible because there no DSP in the network path and the switch ASIC drives the signal processing itself, and therefore can reduce the power draw compared to pluggable optics – and
  &lt;a href=&quot;https://www.nextplatform.com/2024/08/26/bechtolsheim-outlines-scaling-xpu-performance-by-100x-by-2028/&quot;&gt;
   is an approach famously advocated by Andy Bechtolsheim of Arista Networks
  &lt;/a&gt;
  – the CPO approach burns 35 percent less power per 100 Gb/sec lane than the LPO approach.
 &lt;/p&gt;
 &lt;p&gt;
  This may not seem like a lot until you do the math on a 100,000 XPU cluster with 4 KW XPUs, as Bechtolsheim did in August 2024 in the story linked above. The 6.4 million pluggable optical transceivers needed to interlink the GPUs burned 192 MW of power, compared to 400 MW for the GPUs. LPO dropped this to 64 MW. But CPO will drop this to 42 MW, which is only 10.5 percent of the XPU power.
 &lt;/p&gt;
 &lt;p&gt;
  This is real money. A few years ago, the rates that supercomputing centers were budgeting for power was $1 million per MW-year, but in high demand areas like Northern Virginia or Silicon Valley, it is more like $1.2 million to $1.5 million per MW-year. So at the high-end of that pricing, pluggable optics for 100,000 XPUs would cost $1.44 billion to run over five years, while LPO would cost $480 million and CPO would cost $315 million. The savings in power from switching from pluggable optics to CPO would cover the cost of around 32,000 “Blackwell” GPU accelerators at $35,000 a pop. For lower-cost XPUs, it could easily be twice that number of XPU units for that $1.13 billion in incremental spending on electricity.
 &lt;/p&gt;
 &lt;p&gt;
  This seems like a freaking no-brainer to us. Particularly when the CPO units are actually more reliable and the lasers are going to be field replaceable with the Davisson generation of ASICs from Broadcom.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s the reliability data from the Meta Platforms paper, starting with annual link failure rates:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr.jpg&quot; rel=&quot;attachment wp-att-146456&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;467&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr-768x350.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr-600x273.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Remember, an AI cluster doing training is a shared everything architecture and the stoppage of one link or one GPU stalls all computation in the cluster. A failure is a big deal. 5X fewer failures is therefore a very, very big deal. There is no data presented for LPO in the Meta Platform paper, unfortunately.
 &lt;/p&gt;
 &lt;p&gt;
  And here is the mean time between failure for the CPO versus pluggable optics from the Meta Platform paper:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf.jpg&quot; rel=&quot;attachment wp-att-146459&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;467&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf-768x350.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf-600x273.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The pluggable optics use actual datacenter failure rates, while the CPO is using lab stress failure rates because Meta Platforms did not see any failure rates in its datacenter test. And by the way, Mehta says that the lab conditions where the Bailly CPO was tested were much harsher than the environment in the Meta Platforms datacenters because the lab is explicitly trying to cause failures, not to avoid them, by being too hot or vibrating a lot.
 &lt;/p&gt;
 &lt;p&gt;
  All of this brings us back to the Davisson CPO switch, which we expect will see much broader adoption for scale-out networks in AI clusters as well as for the Clos networks in use by hyperscalers and cloud builders for more generic infrastructure and data analytics workloads.
 &lt;/p&gt;
 &lt;p&gt;
  Here is a zoom shot on the Davisson package, with the 102.4 Tb/sec Tomahawk 6 ASIC in the center and the sixteen optical interconnects wrapped around the perimeter of the chip:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-davisson-asic.jpg&quot; rel=&quot;attachment wp-att-146454&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;503&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 750px) 100vw, 750px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-davisson-asic.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-davisson-asic.jpg 750w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-davisson-asic-600x402.jpg 600w&quot; width=&quot;750&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  And here is an early version of a Davisson CPO switch:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch.jpg&quot; rel=&quot;attachment wp-att-146455&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;572&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 968px) 100vw, 968px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch.jpg 968w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-768x454.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-600x355.jpg 600w&quot; width=&quot;968&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Corning is partnering with Broadcom to deliver fiber harnesses and cable assemblies to hook the optical ports to the front of the switch chassis, TSMC and SPIL do the packaging, and we presume that Micas Networks, Celestica, and Nexthop.ai – who have been working with Broadcom on the Bailly CPO switches – will be working with the company on Davisson CPO switches, too.
 &lt;/p&gt;
 &lt;p&gt;
  What we hope, however, is that Broadcom makes a CPO version of its Tomahawk Ultra “InfiniBand Killer” switch ASIC,
  &lt;a href=&quot;https://www.nextplatform.com/2025/07/17/broadcom-tries-to-kill-infiniband-and-nvswitch-with-one-ethernet-stone/&quot;&gt;
   which was announced in July
  &lt;/a&gt;
  and which is being positioned as the Ethernet for scale up networks for sharing XPU memories in rackscale nodes used in AI clusters. We think it would be very interesting to see CPO ports on accelerators matched to CPO ports on switch chips, as we have pointed out time and again. It is OK to start with the scale out network, but ultimately, we need such links everywhere, even for memory banks and flash banks, so we can have more options to connect components and to save power.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> The Third Time Will Be The Charm For Broadcom Switch Co-Packaged Optics </title>
<link>https://www.nextplatform.com/2025/10/17/the-third-time-will-be-the-charm-for-broadcom-switch-co-packaged-optics/</link>
<pubDate>Fri, 17 Oct 2025 16:10:54 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-968x438.jpg&quot; title=&quot;broadcom-cpo-switch&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  If Broadcom says that co-packaged optics is ready for prime time and can compete with other ways of linking switch ASICs to fiber optic cables, then it is very unlikely that Broadcom is wrong.
 &lt;/p&gt;
 &lt;p&gt;
  The company that is the modern Broadcom has a long and deep – and acquired – expertise in optical communications. The lineage runs on one branch from the original Hewlett Packard to its Agilent Technologies spinoff, which a bunch of private equity companies acquired a chunk of and called Avago Technologies. Another line of optical expertise comes out of AT&amp;T Bell Labs through the Lucent Technologies spinout in 1996, which runs through Agere Systems, which was bought by LSI Logic and which was in turn acquired by Avago. Avago, of course, bought Broadcom for its switch ASIC and other chip businesses in 2016 for $37 billion, which seemed like an incredibly high number at the time but which turns out to be prophetic in the age of AI.
 &lt;/p&gt;
 &lt;p&gt;
  There have been plenty of naysayers about CPO in the datacenter, but for many years now Broadcom’s techies have told us that using CPO inside of switches will not only improve the reliability of switches and reduce power consumption in the datacenter network, but will also lower costs. (Eventually, CPO or an optical interposer or something will be added to compute engines of all kinds for the same reasons, and when that happens, we will get much more malleable racks and stop pushing for rack density for the sake of latency.)
 &lt;/p&gt;
 &lt;p&gt;
  These CPO claims for the past several years were tough for many to swallow. But, as Broadcom readies its third generation of CPO add-ons for its Tomahawk line of Ethernet switch ASICs, the data is in from a real-world, large-scale deployment of Broadcom’s second generation CPO technology, and the results are clear. We will get to the testing that Meta Platforms has done using switches based on Broadcom’s “Bailly” Tomahawk 5 CPO switch ASIC in a moment; the paper is not yet public,
  &lt;a href=&quot;https://ecoc2025.abstractserver.com/program/#/details/presentations/167&quot;&gt;
   but here is the link for it
  &lt;/a&gt;
  . We have a copy of the paper as well as summary data that we can present compliments of Broadcom. Right now, let’s look at the CPO switch ASIC roadmap from Broadcom and what it means, and how we can interpret the data on the Bailly ASIC and how the future “Davisson” Tomahawk 6 CPO device will be even better and that much more ready to be mainstreamed.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap.jpg&quot; rel=&quot;attachment wp-att-146462&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; fetchpriority=&quot;high&quot; height=&quot;485&quot; sizes=&quot;(max-width: 1089px) 100vw, 1089px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap.jpg 1089w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap-768x342.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-roadmap-600x267.jpg 600w&quot; width=&quot;1089&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  For a lot of complicated reasons, we never did get around to writing about the first generation “Humboldt” Tomahawk 4 CPO switch chips, which were announced in January 2021 and which were deployed as a development platform by Chinese hyperscaler Tencent. (Broadcom names its CPO switch ASICs after craters on the Moon, which are in turn named after famous people on Earth throughout history as well as various Latin and English names having to do with
  &lt;em&gt;
   lunological
  &lt;/em&gt;
  features.)
 &lt;/p&gt;
 &lt;p&gt;
  The Tomahawk 4 CPO ASIC had 25.6 Tb/sec of aggregate switching capacity,
  &lt;a href=&quot;https://www.nextplatform.com/2019/12/12/broadcom-launches-another-tomahawk-into-the-datacenter/&quot;&gt;
   like the plain vanilla TH4 on which it is based
  &lt;/a&gt;
  , and had four 3.2 Tb/sec optical engines for 12.8 Tb/sec of bandwidth (which could be allocated as 400 Gb/sec or 800 Gb/sec ports) as well as 12.8 Tb/sec of electrical lanes that could be allocated the same way. Importantly, an 800 Gb/sec port on the CPO consumed about 6.4 watts, compared to somewhere around 16 watts to 18 watts for regular pluggable optics running on the same Tomahawk 4 switch without CPO. The Humboldt ASIC had remote laser modules, but if something went wrong with the lasers or the optics, you had to replace the entire switch.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp.jpg&quot; rel=&quot;attachment wp-att-146463&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;499&quot; sizes=&quot;(max-width: 1050px) 100vw, 1050px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp.jpg 1050w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp-768x365.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-savings-ramp-600x285.jpg 600w&quot; width=&quot;1050&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  It would be great to see the port counts in that chart to the right above, not just the change.
 &lt;/p&gt;
 &lt;p&gt;
  With the second generation Bailly CPO switch ASICs, which are based on
  &lt;a href=&quot;https://www.nextplatform.com/2022/08/16/like-a-drumbeat-broadcom-doubles-ethernet-bandwidth-with-tomahawk-5/&quot;&gt;
   the 51.2 Tb/sec Tomahawk 5 chip
  &lt;/a&gt;
  and which started shipping to a few hyperscalers in 2023, Broadcom put eight 6.4 Tb/sec optical engines on the devices and no electrical connections at all. The SerDes ran at 100 Gb/sec per lane, and 800 Gb/sec of bandwidth (meaning eight lanes) took 5.5 watts, which was a 14.1 percent reduction in power compared to a Humboldt port and that much less than the power used with pluggable optics. The Bailly CPO switch chip only did optical links and saw some adoption by a couple of hyperscalers (we know of Meta Platforms and presume Tencent did as well). The Bailly design had detachable lasers, which meant they could be field replaceable, easing the minds of many potential switch buyers who are understandably nervous about giant shared laser sources that might fail in the field.
 &lt;/p&gt;
 &lt;p&gt;
  The good news – sort of – is that in a massively parallel AI cluster, if one pluggable optics module fails, then the whole job stops anyway. So a failed pluggable optics module is as bad as a failed laser that kills an entire switch in this regard. (This is what happens when a workload spans all accelerators and all switches.)
 &lt;/p&gt;
 &lt;p&gt;
  That brings us to next year’s Davisson TH6 CPO device, which Broadcom is now shipping to early access customers. As the name suggests, the Davisson switch chip is based
  &lt;a href=&quot;https://www.nextplatform.com/2025/06/03/the-ai-datacenter-is-ravenous-for-102-4-tb-sec-ethernet/&quot;&gt;
   on the Tomahawk 6 ASIC that we told you about back in June
  &lt;/a&gt;
  . The CPO version of TH6 is using the one that has four Serdes chiplets wrapped around the packet processing engines, which have native 100 Gb/sec speeds plus PAM4 modulation to give two bits per signal and an effective 200 Gb/sec of bandwidth per lane. (There is another version of TH6 that has 50 Gb/sec native signaling with PAM4 modulation that delivers 100 Gb/sec effective bandwidth per lane, like TH4 and TH5 do.) On a 102.4 Tb/sec ASIC, you can get 64 ports running at 1.6 Tb/sec or 128 ports running at 800 Gb/sec – or significantly, 512 ports running at 200 Gb/sec, which is enough for certain kinds of inference workloads and modest training workloads to have one ASIC link to 512 XPUs without anything other than a direct connection. Broadcom adds that the Davisson chip, like the stock Tomahawk 6, can link 131,072 XPUs together in a two tier network. The spec sheets say more than 100,000, but that is the precise number.
 &lt;/p&gt;
 &lt;p&gt;
  Among the many things that are cool about the Davisson TH6 CPO setup is the fact that the laser modules for the switch are now field replaceable, not just detachable, through the front panel of the switch and are no longer close to the switch ASIC and optical interfaces welded onto them and therefore the heat coming off of them, which messes with lasers and can kill them. This makes the lasers more reliable as well as more serviceable. An 800 Gb/sec port will burn about 3.5 watts, says Broadcom, which is 36.4 percent lower than with the Tomahawk 5 CPO port at the same bandwidth and more than 70 percent lower than pluggable optics at the same bandwidth.
 &lt;/p&gt;
 &lt;p&gt;
  This is accomplished in large part through Broadcom working with Taiwan Semiconductor Manufacturing Co on its Compact Universal Photonic Engine (COUPE) packaging technique, which is used to add 16 6.4 Gb/sec optical engines around the TH6 package, which comes in at around 120 mm x 120 mm compared to 75 mm by 75 mm for Bailly.
 &lt;/p&gt;
 &lt;p&gt;
  Here is the stress testing gear for the Bailly CPO switches at Broadcom:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test.jpg&quot; rel=&quot;attachment wp-att-146464&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;374&quot; sizes=&quot;(max-width: 1099px) 100vw, 1099px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test.jpg 1099w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test-768x261.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-stress-test-600x204.jpg 600w&quot; width=&quot;1099&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  And here is one of the production rows of Bailly CPO switches tested at hyperscalers Meta Platforms:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment.jpg&quot; rel=&quot;attachment wp-att-146457&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;454&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment-768x340.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-deployment-600x266.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “We’ve continued to develop, learn, and evolve over the last five years,” Manesh Mehta, vice president of marketing and operations for the Optical Systems Division at Broadcom, tells
  &lt;em&gt;
   The Next Platform
  &lt;/em&gt;
  . “I think there are a few areas that we are really focused on. First, it’s clear to us that our customers are starting to really get excited about and value the reliability and link performance of CPO platform the way that we’re building it, which is basically a high density optical engine built using very scalable foundry and OSAT-based manufacturing techniques and then solder-attaching that optical engine to a common substrate with the core ASIC package. And second, what Meta presented two weeks ago was that the first million device hours that they ran on Bailly, there were zero link flaps observed. These link flaps drive a pretty high inefficiency or underutilization of XPU’s compute time with all the checkpoint retries.”
 &lt;/p&gt;
 &lt;p&gt;
  So what is a link flap, you ask? Is it some weird kind of sausage made of an even weird kind of meat? No. it is when a communication link – a port, a lane, what have you – cycles from being up to being down when there is a faulty cable, some software configuration issue, a bad connection, dust on an optical transceiver, and dozens of other possible causes. The link is flapping around like one of those hand-wavy tubular balloons on used car lots.
 &lt;/p&gt;
 &lt;p&gt;
  Over the first 1 million device hours with the Bailly switches, which we think were manufactured by Micas Networks,
  &lt;a href=&quot;https://www.nextplatform.com/2023/10/17/micas-takes-on-arista-and-the-whiteboxes-in-datacenter-switching/&quot;&gt;
   who we told you all about here
  &lt;/a&gt;
  way back in October 2023, there were no link flaps:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps.jpg&quot; rel=&quot;attachment wp-att-146458&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;467&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps-768x350.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-link-flaps-600x273.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  “To the best of our knowledge, this is the highest device hours reported for CPO technology operation at the system level,” the techies at Meta Platforms write in their paper. “Over the period of the experiment, each unit ran continuously without interruption or clearing the FEC [forward error correction] counters and we did not see any failures or uncorrectable codewords (UCWs) in the links.”
 &lt;/p&gt;
 &lt;p&gt;
  One port had some funniness over the test run, and it was traced back to a faulty fiber cable.
 &lt;/p&gt;
 &lt;p&gt;
  But here’s important thing in all of the paper: “The demonstrated lower bound mean time between failures of (MTBFs) of optical links can readily support a 24K GPU AI cluster with &amp;gt;90 percent training efficiency without interconnect failures being the bottleneck.”
 &lt;/p&gt;
 &lt;p&gt;
  Meta added that the Bailly CPO switch optics delivered 65 percent lower power per 100 Gb/sec lane compared to retimed pluggable optical models. Here’s the data on that from the paper:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power.jpg&quot; rel=&quot;attachment wp-att-146460&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;467&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power-768x350.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-power-600x273.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Linear drive pluggable optics, or LPO, burns somewhere on the order of 10 watts, which is possible because there no DSP in the network path and the switch ASIC drives the signal processing itself, and therefore can reduce the power draw compared to pluggable optics – and
  &lt;a href=&quot;https://www.nextplatform.com/2024/08/26/bechtolsheim-outlines-scaling-xpu-performance-by-100x-by-2028/&quot;&gt;
   is an approach famously advocated by Andy Bechtolsheim of Arista Networks
  &lt;/a&gt;
  – the CPO approach burns 35 percent less power per 100 Gb/sec lane than the LPO approach.
 &lt;/p&gt;
 &lt;p&gt;
  This may not seem like a lot until you do the math on a 100,000 XPU cluster with 4 KW XPUs, as Bechtolsheim did in August 2024 in the story linked above. The 6.4 million pluggable optical transceivers needed to interlink the GPUs burned 192 MW of power, compared to 400 MW for the GPUs. LPO dropped this to 64 MW. But CPO will drop this to 42 MW, which is only 10.5 percent of the XPU power.
 &lt;/p&gt;
 &lt;p&gt;
  This is real money. A few years ago, the rates that supercomputing centers were budgeting for power was $1 million per MW-year, but in high demand areas like Northern Virginia or Silicon Valley, it is more like $1.2 million to $1.5 million per MW-year. So at the high-end of that pricing, pluggable optics for 100,000 XPUs would cost $1.44 billion to run over five years, while LPO would cost $480 million and CPO would cost $315 million. The savings in power from switching from pluggable optics to CPO would cover the cost of around 32,000 “Blackwell” GPU accelerators at $35,000 a pop. For lower-cost XPUs, it could easily be twice that number of XPU units for that $1.13 billion in incremental spending on electricity.
 &lt;/p&gt;
 &lt;p&gt;
  This seems like a freaking no-brainer to us. Particularly when the CPO units are actually more reliable and the lasers are going to be field replaceable with the Davisson generation of ASICs from Broadcom.
 &lt;/p&gt;
 &lt;p&gt;
  Here’s the reliability data from the Meta Platforms paper, starting with annual link failure rates:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr.jpg&quot; rel=&quot;attachment wp-att-146456&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;467&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr-768x350.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-alfr-600x273.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Remember, an AI cluster doing training is a shared everything architecture and the stoppage of one link or one GPU stalls all computation in the cluster. A failure is a big deal. 5X fewer failures is therefore a very, very big deal. There is no data presented for LPO in the Meta Platform paper, unfortunately.
 &lt;/p&gt;
 &lt;p&gt;
  And here is the mean time between failure for the CPO versus pluggable optics from the Meta Platform paper:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf.jpg&quot; rel=&quot;attachment wp-att-146459&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;467&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 1025px) 100vw, 1025px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf.jpg 1025w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf-768x350.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-meta-mtbf-600x273.jpg 600w&quot; width=&quot;1025&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  The pluggable optics use actual datacenter failure rates, while the CPO is using lab stress failure rates because Meta Platforms did not see any failure rates in its datacenter test. And by the way, Mehta says that the lab conditions where the Bailly CPO was tested were much harsher than the environment in the Meta Platforms datacenters because the lab is explicitly trying to cause failures, not to avoid them, by being too hot or vibrating a lot.
 &lt;/p&gt;
 &lt;p&gt;
  All of this brings us back to the Davisson CPO switch, which we expect will see much broader adoption for scale-out networks in AI clusters as well as for the Clos networks in use by hyperscalers and cloud builders for more generic infrastructure and data analytics workloads.
 &lt;/p&gt;
 &lt;p&gt;
  Here is a zoom shot on the Davisson package, with the 102.4 Tb/sec Tomahawk 6 ASIC in the center and the sixteen optical interconnects wrapped around the perimeter of the chip:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-davisson-asic.jpg&quot; rel=&quot;attachment wp-att-146454&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;503&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 750px) 100vw, 750px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-davisson-asic.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-davisson-asic.jpg 750w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-davisson-asic-600x402.jpg 600w&quot; width=&quot;750&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  And here is an early version of a Davisson CPO switch:
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;http://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch.jpg&quot; rel=&quot;attachment wp-att-146455&quot;&gt;
   &lt;img alt=&quot;&quot; decoding=&quot;async&quot; height=&quot;572&quot; loading=&quot;lazy&quot; sizes=&quot;auto, (max-width: 968px) 100vw, 968px&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch.jpg&quot; srcset=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch.jpg 968w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-768x454.jpg 768w, https://www.nextplatform.com/wp-content/uploads/2025/10/broadcom-cpo-switch-600x355.jpg 600w&quot; width=&quot;968&quot;/&gt;
  &lt;/a&gt;
 &lt;/p&gt;
 &lt;p&gt;
  Corning is partnering with Broadcom to deliver fiber harnesses and cable assemblies to hook the optical ports to the front of the switch chassis, TSMC and SPIL do the packaging, and we presume that Micas Networks, Celestica, and Nexthop.ai – who have been working with Broadcom on the Bailly CPO switches – will be working with the company on Davisson CPO switches, too.
 &lt;/p&gt;
 &lt;p&gt;
  What we hope, however, is that Broadcom makes a CPO version of its Tomahawk Ultra “InfiniBand Killer” switch ASIC,
  &lt;a href=&quot;https://www.nextplatform.com/2025/07/17/broadcom-tries-to-kill-infiniband-and-nvswitch-with-one-ethernet-stone/&quot;&gt;
   which was announced in July
  &lt;/a&gt;
  and which is being positioned as the Ethernet for scale up networks for sharing XPU memories in rackscale nodes used in AI clusters. We think it would be very interesting to see CPO ports on accelerators matched to CPO ports on switch chips, as we have pointed out time and again. It is OK to start with the scale out network, but ultimately, we need such links everywhere, even for memory banks and flash banks, so we can have more options to connect components and to save power.
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
<item>
<title> How HPC Is Igniting Discoveries In Dinosaur Locomotion – And Beyond </title>
<link>https://www.nextplatform.com/2025/10/17/how-hpc-is-igniting-discoveries-in-dinosaur-locomotion-and-beyond/#respond</link>
<pubDate>Fri, 17 Oct 2025 14:52:14 -0000</pubDate>
<description>
&lt;div&gt;
 &lt;figure&gt;
  &lt;img alt=&quot;&quot; src=&quot;https://www.nextplatform.com/wp-content/uploads/2025/10/dinosaur-footprint-shutterstock_2427362037-1030x438.jpg&quot; title=&quot;Large,,Carnivorous,Dinosaur,Footprints,It,Was,Intricately,Carved,Into,The&quot;/&gt;
 &lt;/figure&gt;
 &lt;p&gt;
  In the basement of Connecticut’s Beneski Museum of Natural History sit the fossilized footprints of a small, chicken-sized dinosaur. Each track is more than a preserved impression; it’s a data record, holding clues to how the animal once moved. Unlocking that information, however, requires a tool that never existed in the Mesozoic: high performance computing.
 &lt;/p&gt;
 &lt;p&gt;
  Today, advanced simulations can reconstruct those steps in the sand grain by grain, revealing not just how dinosaurs walked, but the core mechanics of locomotion itself.
 &lt;/p&gt;
 &lt;p&gt;
  “We’re looking at using dinosaur tracks, footprints, to reconstruct how dinosaurs moved around 200 million years ago,” explained professor Peter Falkingham, a paleobiologist at Liverpool John Moores University.
 &lt;/p&gt;
 &lt;p&gt;
  Together with postdoctoral researcher Ben Griffin, the pair are combining paleontology, biomechanics, and HPC to peer into the tiniest details of fossilized tracks, down to the millions of sand grains that shaped them. By layering fossil scans with computer simulations, their work replays the exact moment this ancient theropod pressed its foot into prehistoric mud. The implications go far beyond paleontology, offering insights that could influence robotics, medicine, and the science of movement itself.
 &lt;/p&gt;
 &lt;h3&gt;
  The Unlikely HPC Pioneers
 &lt;/h3&gt;
 &lt;p&gt;
  The scientists behind these simulations are at different stages of their careers, yet share the same goal of using computation to unlock their curiosity. Falkingham is a unique specimen in his own right. With degrees in both paleontology and computer science, he embodies a new generation of researchers who can bridge fossil study with computational power.
 &lt;/p&gt;
 &lt;p&gt;
  “I did my PhD using HPC to simulate dinosaur tracks using finite element analysis,” he recalls. “I’ve always been interested in using computing to explore the biological aspects where paleontology is concerned.”
 &lt;/p&gt;
 &lt;p&gt;
  Falkingham’s current European Research Council grant funds the work of Griffin, a postdoctoral researcher who found himself immersed in HPC not by design, but by chance.
 &lt;/p&gt;
 &lt;p&gt;
  “This was my first experience with HPC,” Griffin explains. “Most of my background was in PowerShell or Windows CMD environments. Switching to Linux command lines was a bit of an adjustment.”
 &lt;/p&gt;
 &lt;p&gt;
  With help from his mentor, he mapped out strategies for running simulations on the LJMU “Prospero” Cluster, which was launched in August 2020. Different nodes handled short test runs or longer, more computationally intensive simulations.
 &lt;/p&gt;
 &lt;p&gt;
  “It’s been pretty painless, for the most part,” he tells us. “Peter very kindly explained how everything worked, and we walked through the setup together.”
 &lt;/p&gt;
 &lt;p&gt;
  Griffin is quick to encourage newcomers to lean on mentorship when learning HPC. “The first thing you want to do is talk to someone who is already using the specific HPC system you want to use,” he advises. “Get a copy of someone’s submission script and have them walk you through it. That will make your life way easier.” He laughs when recalling an early misstep: “We pushed something into the substrate too fast, and particles were exploding all over the place.”
 &lt;/p&gt;
 &lt;p&gt;
  These learning curves highlight an important point: HPC isn’t a gated world reserved for experts. With the right mentorship, accessible systems, and a willingness to experiment, even a “Windows-native” researcher can open the door to large-scale simulation science.
 &lt;/p&gt;
 &lt;h3&gt;
  The Need For Speed
 &lt;/h3&gt;
 &lt;p&gt;
  To understand why this research demands supercomputing, one must consider the sheer scale of the data being explored. “These simulations are recreating very, very tiny independent particles that represent the grains of the substrate,” Falkingham explains. “If you imagine a chicken-sized dinosaur walking through sand, those grains are about a millimeter across. Even in a relatively small shoebox-sized volume of sand, you’re talking about tens or even hundreds of millions of particles.”
 &lt;/p&gt;
 &lt;p&gt;
  On a standard desktop, that level of computation could take months or even years. “Simulating the interactions between all of those particles requires a massive amount of computational power,” Falkingham emphasizes. “Every particle is interacting, bouncing off the others, and for each of those interactions, you’re calculating forces, accelerations, and other factors. It just doesn’t work on anything less than an HPC system.”
 &lt;/p&gt;
 &lt;p&gt;
  In theory, the team could reduce the particle count by making each grain larger, enough to run on a laptop. But that shortcut destroys the science. “The problem is, once the particles are that large, they start to exceed the size of the toes that are making the footprint,” Falkingham notes. “At that point, you don’t get a realistic footprint anymore.”
 &lt;/p&gt;
 &lt;p&gt;
  And particle count is only part of the challenge. Time and sequence also push the limits. “The need for HPC really comes from the sheer number of calculations that must happen for every tiny fraction of a second in real time,” Falkingham explains.
 &lt;/p&gt;
 &lt;p&gt;
  A single simulation can take days to complete on the Prospero system. Refining the parameters means running multiple full-scale versions. “You can’t just run a quick, small test,” he explains, adding that the physics behave differently at different scales. “You have to go all-in, run a full-scale simulation that might take a day, or even three days, on an HPC system.”
 &lt;/p&gt;
 &lt;h3&gt;
  Unexpected Pathways To Innovation
 &lt;/h3&gt;
 &lt;p&gt;
  What began as a quest to understand ancient creatures is now sparking collaborations far beyond paleontology. “I promise you, any practical application of this work is purely accidental,” Falkingham jokes. But those “accidents” are opening surprising avenues of exploration.
 &lt;/p&gt;
 &lt;p&gt;
  “We’ve started collaborating with colleagues who have a big robotic arm,” Falkingham shares. This new team is trying to understand how their insights might help robots navigate natural terrain. While legged robots perform well on flat pavement, they often fail on soft, uneven surfaces like sand or mud.
 &lt;/p&gt;
 &lt;p&gt;
  “They’re more interested in how robots interact with deformable substrates and the feedback forces applied when moving through such materials,” he says. By studying how toes spread or curve to adapt, Falkingham notes, engineers can design robotic feet that stabilize machines and extend their range of motion.
 &lt;/p&gt;
 &lt;p&gt;
  The implications don’t stop with robotics. “It’s already well known that walking on sand is helpful for people recovering from hip injuries because sand adapts to imbalanced forces,” Falkingham observes. Their research may someday inform new approaches to physiotherapy or prosthetics. “Can we develop footwear that provides the same kind of passive stability we see in dinosaur and bird feet?” he wonders.
 &lt;/p&gt;
 &lt;p&gt;
  Even sports science is finding applications. The team is collaborating with colleagues who are studying human locomotion, tracking people traversing over sand and mud. “We’re working on simulating those footprints, and then studying the movement back up the limbs,” says Falkingham. These collaborations weren’t part of the original plan, but they underscore how HPC-fueled science often spills over into unexpected fields, creating breakthroughs simply by replaying steps from the past.
 &lt;/p&gt;
 &lt;h3&gt;
  Igniting Discovery Across Disciplines
 &lt;/h3&gt;
 &lt;p&gt;
  Listening to Falkingham describe the collaborations sparked by this research, it’s clear the impact goes far beyond paleobiology. The work has drawn in robotics, computer visualization, art, and even virtual reality. “A few years ago, we collaborated with a computer visualization team at Brown University on ways to visualize these volumetric tracks. That led to collaborations with our local art school, where students tried to figure out new ways to present the data.”
 &lt;/p&gt;
 &lt;p&gt;
  The scale of the output makes this challenge clear. Each simulation produces hundreds, even thousands, of text files, many with hundreds of millions of lines representing each particle. “What do you do with that?” he asks.
 &lt;/p&gt;
 &lt;p&gt;
  The art students responded in unexpected ways, including using virtual reality to explore motion through time. For Falkingham and Griffin, it underlined the need for communication and interpretation as a key part of the discipline of science.
 &lt;/p&gt;
 &lt;p&gt;
  For Griffin, the process has been eye-opening. “It’s been fascinating to see how this work could apply to other fields, and lead to collaboration across all these different areas,” he reflects. “Yeah, I’d say that curiosity is the main thing that HPC has ignited.”
 &lt;/p&gt;
 &lt;p&gt;
  In their hands, HPC doesn’t just solve problems. It sparks new questions, expanding the boundaries of science and creativity alike.
 &lt;/p&gt;
 &lt;h3&gt;
  Connect, Learn, And Innovate At SC25
 &lt;/h3&gt;
 &lt;p&gt;
  From capturing the mechanics of a 200 million year old footprint to inspiring robotic design, the story of HPC is one of curiosity sparking collaboration, one discovery at a time. This fall, St Louis becomes the gathering point for that journey as the HPC community convenes for a week of sessions, speakers, and networking.
 &lt;/p&gt;
 &lt;p&gt;
  &lt;a href=&quot;https://bit.ly/47o32a9&quot;&gt;
   Join us this November at SC25
  &lt;/a&gt;
  to discover how HPC is igniting breakthroughs across science and industry. SC is where disciplines converge: paleobiologists meet computational engineers, visualization experts connect with AI practitioners, and students find mentors who can help shape their careers.
 &lt;/p&gt;
 &lt;p&gt;
  If you want to experience firsthand how the HPC industry is transforming fields, from art to medicine to fundamental science, there is no better place to be than
  &lt;a href=&quot;https://bit.ly/47o32a9&quot;&gt;
   SC25
  &lt;/a&gt;
  .
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   Contributed by SC25.
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;p&gt;
  &lt;em&gt;
   Cristin Merritt is SC25 communications chair and chief marketing officer for Alces Flight Ltd, an HPC consultancy and cloud provider based in the United Kingdom.
  &lt;/em&gt;
 &lt;/p&gt;
 &lt;!-- QUIZ HERE --&gt;
 &lt;div&gt;
 &lt;/div&gt;
&lt;/div&gt;

</description>
</item>
</channel>
</rss>
